<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="网络结构解密, 大模型,llm,搜推算法,多模态,cv">
    <meta name="description" content="计算机视觉与图像处理，大模型算法，搜索算法，多模态算法以及其他相关的技术博客">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>网络结构解密 | 月源</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
	<link rel="stylesheet" type="text/css" href="/libs/artitalk/artitalk.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="月源" type="application/atom+xml">
</head>





    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: #FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: #49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: #2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>


<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
	
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
					<div>
						
						<img src="./medias/loading.gif" data-original="/medias/logo.png" class="logo-img" alt="LOGO">
						
						<span class="logo-span">月源</span>
					</div>
				</a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/artitalk" class="waves-effect waves-light">
      
      <i class="fas fa-heartbeat" style="zoom: 0.6;"></i>
      
      <span>说说</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="./medias/loading.gif" data-original="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">月源</div>
        <div class="logo-desc">
            
            计算机视觉与图像处理，大模型算法，搜索算法，多模态算法以及其他相关的技术博客
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/artitalk" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-heartbeat"></i>
			
			ArtiTalk
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/chenghaoDong666" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>See Me in github
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/chenghaoDong666" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="See Me in github" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

	
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('密码提示：帅不帅？')).toString(CryptoJS.enc.Hex)) {
                alert('你不够帅，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">网络结构解密</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/">
                                <span class="chip bg-color">网络结构</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-category">
                                深度学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2020-11-23
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.9k
                </div>
                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31561570">深度学习基础</a></p>
<p><strong>representation learning 表示学习</strong></p>
<p><strong>transfer learning 迁移学习</strong> </p>
<p><strong>multi-task learning 多任务学习</strong> 低层特征共享，产生分支完成各自的任务</p>
<p> <strong>end-to-end learning 端到端学习</strong> 一步到位，就是一种表示学习</p>
<h3 id="计算机视觉四大基本任务"><a href="#计算机视觉四大基本任务" class="headerlink" title="计算机视觉四大基本任务"></a>计算机视觉四大基本任务</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31727402">计算机视觉四大基本任务</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34142321">目标检测入门</a></p>
<h3 id="计算机视觉其他应用"><a href="#计算机视觉其他应用" class="headerlink" title="计算机视觉其他应用"></a>计算机视觉其他应用</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31727405">计算机视觉其他应用</a></p>
<h2 id="统计学习知识和网络细节"><a href="#统计学习知识和网络细节" class="headerlink" title="统计学习知识和网络细节"></a>统计学习知识和网络细节</h2><h3 id="常识"><a href="#常识" class="headerlink" title="常识"></a>常识</h3><p>通俗理解的话，<strong>离散即分类，连续即回归</strong>，回归是有误差度量的，比如5和6差为1,5和7差为2，分类除了分类为5为对其他均为错，误差度量离散二值化了</p>
<p>李宏毅GAN教程:</p>
<p>**Regression-**output a scalar **Classification-**output a “class” **Structured Learning/Prediction-**output a sequence, a matrix , a graph, a tree…</p>
<p><strong>output is composed of components with dependency 带有依赖的元素的组合</strong></p>
<h4 id="Head-Neck-Backbone"><a href="#Head-Neck-Backbone" class="headerlink" title="Head/Neck/Backbone"></a>Head/Neck/Backbone</h4><p>输入-&gt;主干-&gt;脖子-&gt;头-&gt;输出。主干网络提取特征，脖子提取一些更复杂的特征，然后头部计算预测输出</p>
<p><strong>backbone：</strong>主干网络，就代表其是网络的一部分，那么是哪部分呢？这个主干网络大多时候指的是提取特征的网络，其作用就是提取图片中的信息，共后面的网络使用。这些网络经常使用的是resnet、VGG等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对他进行微调，使得其更适合于我们自己的任务。</p>
<p><strong>head：</strong>head是获取网络输出内容的网络，利用之前提取的特征，head利用这些特征，做出预测。</p>
<p>**neck:**是放在backbone和head之间的，是为了更好的利用backbone提取的特征。</p>
<p><strong>bottleneck:**在ResNet中,bottleneck主要是指</strong>每个block进来一个特征图现有一个压缩的步骤**,然后3x3卷积,然后再扩张。<strong>如果bottleneck是指行为的话,就是指压缩这一行为,如果bottleneck是指特征图的话,就是指压缩后或扩张前的通道数较少的特增图。</strong></p>
<p><strong>GAP：</strong>Global Average Pool全局平均池化，就是将某个通道的特征取平均值，经常使用AdaptativeAvgpoold(1),在pytorch中，这个代表自适应性全局平均池化，说人话就是将某个通道的特征取平均值。</p>
<p><strong>Embedding</strong>:深度学习方法都是利用使用线性和非线性转换对复杂的数据进行自动特征抽取，并将特征表示为“向量”（vector），这一过程一般也称为“嵌入”（embedding）</p>
<p><strong>False Positive</strong>:假阳性,其实这个说法就已经揭示了如何分辨,就比如一个试纸,测出来是阳性，但其实是假的,换句话说就是网络预测出来是真的,其实是假的</p>
<p>3x3的卷积有padding一般是1,5x5的卷积有padding一般是2,7x7的卷积有padding一般是3</p>
<h3 id="归一-标准-正则"><a href="#归一-标准-正则" class="headerlink" title="归一/标准/正则"></a>归一/标准/正则</h3><p><a target="_blank" rel="noopener" href="https://maristie.com/2018/02/Normalization-Standardization-and-Regularization">Differences between Normalization, Standardization and Regularization</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20455227/answer/370658612">归一化和标准化的作用于区别</a></p>
<p>翻译成中文是归一化，标准化，正则化，前两个是特征缩放，最后一个是减少过拟合</p>
<h4 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h4><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210317213036238.png" alt="归一化的作用" style="zoom: 67%;">

<h4 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h4><p>特征缩放，在许多学习算法中，标准化是一种<strong>广泛使用的预处理步骤，其目的是将特征缩放到零均值和单位方差:</strong><br>$$<br>x^{’}=\frac{x-u}{\sigma}<br>$$<br>也是特征缩放,但是更<strong>加动态和具有弹性,与整体样本的分布有关,加速收敛</strong></p>
<p><strong>当整体较为集中时,方差更小,标准化后就会更宽松;如果较为宽松,方差更大,标准化后就会变紧。</strong></p>
<p>用这些大数据集的均值和方差,代替真实的均值和方差</p>
<p><strong>coco数据集的均值和方差（三分量顺序是RGB）</strong></p>
<p><code>mean = [0.471, 0.448, 0.408]</code><br><code>std = [0.234, 0.239, 0.242]</code></p>
<p><strong>ImageNet数据集的均值和方差（三分量顺序是RGB）</strong></p>
<p><code>mean = [0.485, 0.456, 0.406]</code><br><code>std = [0.229, 0.224, 0.225]</code></p>
<p>注意,经过数据标准化之后图像的范围可能就不在[0-1]范围内了</p>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>正则化,减少过拟合，损失函数后加正则项</p>
<p><strong>贝叶斯学派观点来看，正则项是在模型训练过程中引入了某种模型参数</strong>的先验分布[后验由数据得出，先验就是理性推断，不拘束于数据。</p>
<p><strong>范数</strong>$L_P-norm$</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201125202729072.png" alt="Lp范数"></p>
<p>p=1，<strong>曼哈顿距离</strong>，p=2，<strong>欧式距离</strong>，p为无穷，<strong>无穷范数或最大范数</strong>。</p>
<p>做正则项时，称为Lp-正则项。L1-正则项也叫<strong>LASSO</strong>正则项，L2-正则项也叫<strong>Tikhonov或Ringe正则项</strong></p>
<p><a target="_blank" rel="noopener" href="https://liam.page/2017/03/30/L1-and-L2-regularizer/">详解</a></p>
<p>L1正则化主要是在损失函数中添加权重参数的绝对值的和，得到的<strong>参数通常比较稀疏，常用于特征选择</strong>。L2正则化则是在损失函数中添加权重参数的平方项，<strong>得到的模型参数通常比较小</strong>。</p>
<p>在这里，我们需要关注的最主要是范数的「非负性」。我们刚才讲，损失函数通常是一个有下确界的函数。而这个性质保证了我们可以对损失函数做最优化求解。如果我们要保证目标函数依然可以做最优化求解，那么我们就必须让正则项也有一个下界。非负性无疑提供了这样的下界，而且它是一个下确界——由齐次性保证[当 c=0 时]</p>
<p>因此，我们说，范数的性质使得它天然地适合作为机器学习的正则项。而范数需要的向量，则是<strong>机器学习的学习目标——参数向量</strong>。</p>
<p>L0范数表示向量中非零元素的个数 </p>
<p><strong>我们考虑这样一种普遍的情况，即：预测目标背后的真是规律，可能只和某几个维度的特征有关；而其它维度的特征，要不然作用非常小，要不然纯粹是噪声。在这种情况下，除了这几个维度的特征对应的参数之外，其它维度的参数应该为零。若不然，则当其它维度的特征存在噪音时，模型的行为会发生预期之外的变化，导致过拟合。</strong>引入L0范数</p>
<p>L0范数不好，又引入L1范数，L1范数也可以在梯度更新时使得参数趋于0.</p>
<p>L1使得参数稀疏化，L2使得参数稠密的趋近于0</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201125205147790.png" alt="稀疏和稠密的原因"></p>
<p><strong>提前停止</strong></p>
<p>提前停止可看做是<strong>时间维度上的正则化</strong>。直觉上，随着迭代次数的增加，如梯度下降这样的训练算法倾向于学习愈加复杂的模型。在实践维度上进行正则化有助于控制模型复杂度，提升泛化能力。在实践中，<strong>提前停止一般是在训练集上进行训练，而后在统计上独立的验证集上进行评估；当模型在验证集上的性能不在提升时，就提前停止训练</strong>。最后，可在测试集上对模型性能做最后测试。</p>
<p><a target="_blank" rel="noopener" href="https://alisure.github.io/2018/04/14/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%93%E6%B7%B7%E6%A6%82%E5%BF%B5%E4%B9%8B%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E4%B8%8E%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9/#:~:text=%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9(Structural%20Risk)%EF%BC%9A,%EF%BC%88%E4%BE%8B%E5%A6%82%E6%AD%A3%E5%88%99%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%89%E3%80%82">期望风险、经验风险、结构风险 简洁</a></p>
<h3 id="偏差-方差分解"><a href="#偏差-方差分解" class="headerlink" title="偏差-方差分解"></a>偏差-方差分解</h3><p><a target="_blank" rel="noopener" href="https://liam.page/2017/03/25/bias-variance-tradeoff/">详解</a></p>
<p>误差来源有三种：随机误差、偏差和方差</p>
<p>随机误差是高斯白噪声</p>
<p>偏差bias描述的是通过学习拟合出来的结果之期望，与真实规律之间的差距，记作$Bias(X)=E[\hat{f}(X)]-f(X)$</p>
<p>方差variance即是统计学中的定义，描述的是通过学习拟合出来的结果自身的不稳定性，记作$Var(X)=E[(\hat{f}(X)-E[\hat{}f(X)])^2]$</p>
<p>均方误差可以分解为:$Bias^2+Variance+Random Error$</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201125210219206.png" alt="直观图示" style="zoom:67%;">

<h3 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h3><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/903e35e1c95a">期望风险、经验风险、结构风险2深入</a></p>
<p>损失函数：度量模型一次预测的好坏</p>
<p>风险函数：度量平均意义下的模型预测好坏</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70821070">谁才是合格的激活函数 </a></p>
<p>激活函数应满足的性质：</p>
<ul>
<li>非线性,这是必须的,也是添加激活函数的原因</li>
<li>几乎处处可微：反向传播中，损失函数要对参数求偏导，如果激活函数不可微，那就无法使用梯度下降方法更新参数了。(ReLU只在零点不可微，但是梯度下降几乎不可能收敛到梯度为0)</li>
<li>计算简单：神经元(units)越多，激活函数计算的次数就越多，复杂的激活函数会降低训练速度。</li>
<li>非饱和性：<strong>饱和指在某些区间梯度接近于零，使参数无法更新</strong>。Sigmoid和tanh都有这个问题，而ReLU就没有，所以普遍效果更好。因为是链式求导,所以你一个是0乘以其他的还是0,不好。</li>
<li>有限的值域：这样可以使网络更稳定，即使有很大的输入，激活函数的输出也不会太大。</li>
</ul>
<h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>$$<br>\sigma(x)=\frac{1}{1+e^{-x}}\<br>\sigma’(x)=\sigma(x)[1-\sigma(x)]<br>$$</p>
<p><strong>易饱和问题:</strong></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20210708170146109.png" alt="image-20210708170146109" style="zoom:50%;">

<p>$x$在大部分范围的梯度都趋近于0,同时该函数单调递增,所以梯度始终&gt;0</p>
<p><strong>非零均值问题:</strong></p>
<p>如果每一层的激活函数都是用$sigmoid$,那么假设有:</p>
<p>$f(x,w,b)=\sigma(\sum w_ix_i+b)$,$x_i$是上一层的输出,这一层的出入。</p>
<p>$w_i$的更新为:$w_i-\alpha \frac{\partial L}{\partial w_i} \to w_i$</p>
<p>$L(x)$对$w_i$求偏导:$\frac{\partial L}{\partial w_i}=\frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial a}\cdot \frac{\partial a}{\partial w_i}$</p>
<p>$\frac{\partial L}{\partial w_i}=\frac{\partial L}{\partial y}\cdot y(1-y)\cdot x_i$</p>
<p>$y(1-y)&gt;0$,对于$\frac{\partial L}{\partial y}$,设$L=(y_{true}-y)^2/2$,求完导之后:$y-y_{true}$,是个常数,跟具体的某个$w_i$无关,又因为$x_i$是上一层的输出,那么经过$sigmoid$激活之后,$x_i&gt;0$,所以所有的$w_i$更新的方向一致。所有的权重只能一起变大或变小，这样模型收敛就需要消耗很多的时间,如下图所示。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201217191234682.png" alt="这时的收敛是这样的" style="zoom:50%;">

<p><strong>幂运算问题：</strong>幂运算相对较为耗时,这个不算太大的问题。</p>
<h4 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h4><p>Tanh函数是 0 均值的，因此实际应用中 Tanh 会比 sigmoid 更好。但是仍然存在<strong>梯度饱和</strong>与<strong>exp计算</strong>的问题。<br>$$<br>f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\<br>f(x)’=1-[f(x)]^2<br>$$</p>
<h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p><strong>ReLU的Dead ReLU问题</strong></p>
<p>如果某一次权重更新用力过猛，啪的一下，大部分权重都是负的了，下一次前向输出很容易是负的，一激活，就是0，之后就不更新了，就完蛋了</p>
<p>两个原因：<strong>1初始化太糟糕 2learning rate设太高导致用力过猛</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/71882757">不同激活函数的优缺点总结</a></p>
<p>了解<strong>ELU</strong>和<strong>Maxout</strong>，之前的问题解决了，但计算量和参数量会有损失。</p>
<h4 id="输出层选择"><a href="#输出层选择" class="headerlink" title="输出层选择"></a>输出层选择</h4><p><strong>输出层的激活函数</strong>和<strong>损失函数</strong>由任务类型决定，见下表。它们与<strong>隐藏层的激活函数</strong>的选择是独立的。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20210708204741317.png" alt="image-20210708204741317">

<p>隐藏层一般就是$ReLU$,至于为什么输出层的激活函数不用$ReLU$呢?输出是$0$到正无穷,没办法分类,你**还得自己设置阈值来确定哪个是哪一类!**输入过来的值是负的的话,所有的输出值都是0,那就没办法区分了。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数，就是计算真实分布与预测分布的差异性，差异性越小，损失函数越小，预测的越好</p>
<h4 id="概率与似然"><a href="#概率与似然" class="headerlink" title="概率与似然"></a>概率与似然</h4><p>概率大家都比较了解,主要是似然的了解。</p>
<p>在数理统计学中,<strong>似然函数(likelihood function)**是一种关于统计模型中的参数的函数，表示模型参数中的</strong>似然性**（英语：likelihood）。似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。文字意义上，“似然性”与“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“概率”（或然性）有明确的区分：==概率，用于在已知一些参数的情况下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，也就是说已观察到某事件后，对相关参数进行猜测。==</p>
<p>在这种意义上，似然函数可以理解为条件概率的逆反。在已知某个参数<strong>B</strong>时，事件<strong>A</strong>会发生的概率写作：<br>$$<br>P(A|B)=\frac{P(A,B)}{P(B)}<br>$$<br>利用贝叶斯定理,<br>$$<br>P(B|A)=\frac{P(A|B)P(B)}{P(A)}<br>$$<br>因此，我们可以反过来构造表示似然性的方法：已知有事件<strong>A</strong>发生，运用似然函数$L(B|A)$，我们估计或猜测参数<strong>B</strong>的不同值的可能性。<strong>形式上，似然函数也是一种条件概率函数，但我们关注的变量改变了</strong>：<br>$$<br>b\to P(A|B=b)<br>$$<br>在英语语境里，likelihood 和 probability 的日常使用是可以互换的，都表示对机会 (chance) 的同义替代。但在数学中，probability 这一指代是有严格的定义的，即符合柯尔莫果洛夫公理 (Kolmogorov axioms) 的一种<strong>数学对象</strong>（换句话说，<strong>不是</strong>所有的可以用0到1之间的数所表示的对象都能称为概率），而 likelihood (function) 这一概念是由Fisher提出，他采用这个词，也是为了凸显他所要表述的<strong>数学对象</strong>既和 probability 有千丝万缕的联系，但又不完全一样的这一感觉。中文把它们一个翻译为<strong>概率</strong>一个翻译为<strong>似然</strong>也是独具匠心。</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/54082000/answer/470252492">HiTao知乎回答</a></p>
<p>概率和似然类似$2^b$和$a^2$,而<code>p(x|θ)</code>也是一个有着两个变量的函数。<strong>如果，你将θ设为常量，则你会得到一个概率函数（关于x的函数）；如果，你将x设为常量你将得到似然函数（关于θ的函数）。</strong></p>
<p>有一个硬币，它有θ的概率会正面向上，有1-θ的概率反面向上。θ是存在的，但是你不知道它是多少。为了获得θ的值，你做了一个实验：将硬币抛10次，得到了一个正反序列：x=HHTTHTHHHH。</p>
<p>无论θ的值是多少，这个序列的概率值为 θ⋅θ⋅(1-θ)⋅(1-θ)⋅θ⋅(1-θ)⋅θ⋅θ⋅θ⋅θ = θ⁷ (1-θ)³</p>
<p>比如，如果θ值为0，则得到这个序列的概率值为0。如果θ值为1/2，概率值为1/1024。</p>
<p>但是，我们应该得到一个更大的概率值，所以我们尝试了所有θ可取的值，画出了下图：</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210601213407084.png" alt="image-20210601213407084" style="zoom:50%;">

<p>这个曲线就是θ的似然函数，<strong>通过了解在某一假设下，已知数据发生的可能性，来评价哪一个假设更接近θ的真实值。</strong></p>
<p>==因为这么多种情况偏偏这种情况发生了,肯定是这种情况发生的概率尽可能大,此时的$\theta$就更可能是真实的$\theta$==</p>
<p>如图所示，最有可能的假设是在θ=0.7的时候取到。但是，你无须得出最终的结论θ=0.7。事实上，根据贝叶斯法则，0.7是一个不太可能的取值（如果你知道几乎所有的硬币都是均质的，那么这个实验并没有提供足够的证据来说服你，它是均质的）。但是，0.7却是最大似然估计的取值。</p>
<p>因为这里仅仅试验了一次，得到的样本太少，所以最终求出的最大似然值偏差较大，如果经过多次试验，扩充样本空间，则最终求得的最大似然估计将接近真实值0.5。在<a href="https://link.zhihu.com/?target=http://blog.sina.com.cn/s/blog_e8ef033d0101oa4k.html">这篇博客</a>中有详细的过程，就不再赘述。</p>
<h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70804197">为什么用交叉熵做损失函数？</a></p>
<p><strong>信息熵是消除不确定性所需信息量的度量。</strong>信息熵就是信息的不确定性程度，信息熵越小，信息越确定。</p>
<p>信息熵=$\sum_{x=1}^N$(信息$x$发生的概率$\times$验证信息$x$需要的信息量)</p>
<p>举个列子，比如说：今年中国取消高考了，这句话我们很不确定(甚至心里还觉得这TM是扯淡)，那我们就要去查证了，这样就需要很多信息量(去查证)；反之如果说今年正常高考，大家回想：这很正常啊，不怎么需要查证，这样需要的信息量就很小。从这里我们可以学到：根据信息的<strong>真实分布</strong>，我们能够找到一个最优策略，以<strong>最小的代价消除系统的不确定性</strong>，即<strong>最小信息熵</strong>。</p>
<p>简而言之，<strong>概率越低，需要越多的信息去验证</strong>，所以<strong>验证真假需要的信息量和概率成反比</strong>。我们需要用数学表达式把它描述出来，推导：</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210707193548113.png" alt="image-20210707193548113" style="zoom: 80%;">

<p>信息熵即所有信息量的期望:<br>$$<br>H(x)=-\sum_xp(x)log(p(x))=-\sum_{i=1}^Np(x_i)log(p(x_i))<br>$$<br>编码层面的解释:</p>
<p><strong>一个事件结果的出现概率越低，对其编码的bit长度就越长。以期在整个随机事件的无数次重复试验中，用最少的 bit 去记录整个实验历史。即无法压缩的表达，代表了真正的信息量。</strong></p>
<p>熵的本质的另一种解释:<strong>最短平均编码长度</strong>。编码方案完美时,最短平均编码长度。编码方案完美就是指一个事件结果的出现概率越低，对齐编码的bit长度就越长。</p>
<p>假设一个随机变量X的正确分布p如下:</p>
<p>$A:\frac{1}{2}\quad B:\frac{1}{4}\quad C:\frac{1}{4}$</p>
<p>可以简单认为，平均每发送4个符号其中就有2个A，1个B，1个C。因为我们知道信道传输是只能传二进制编码的，所以必须对A、B、C进行编码，根据哈夫曼树来实现，如下所示：</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%A7%A3%E5%AF%86%5Cimage-20210707203744894.png" alt="image-20210707203744894"></p>
<p>A被编码为0，B被编码为10，C被编码为11。所以每4个字符的平均编码长度为：<br>$$<br>\frac{2+2+2}{4}=1.5<br>$$<br>那么这个随机变量的信息熵是什么呢？</p>
<p>$H(x)=-1/2log(1/2)-1/4log(1/4)-1/4log(1/4)=1.5$</p>
<p>编码的话以2为底。</p>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h4><p>有了信息熵的概念后，然后就去看看交叉熵的物理含义。假设我们用一个错误的分布q，对随机变量编码，q的分布如下：</p>
<p>$A:\frac{1}{4}\quad B:\frac{1}{4}\quad C:\frac{1}{2}$</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%A7%A3%E5%AF%86%5Cimage-20210707204259161-1625661782418.png" alt="image-20210707204259161"></p>
<p>也就是说A被编码为11，B被编码为10，C被编码为0。但是实际上，平均每4个字符还是2个A，1个B，1个C。所以在这种情况下，平均编码长度变成了:<br>$$<br>\frac{4+1+2}{4}=1.75<br>$$<br>交叉熵此时等于:</p>
<p>$H(p,q)=-1/2log(1/4)-1/4log(1/4)-1/4log(1/2)=1.75$</p>
<h4 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h4><p>有了交叉熵和信息熵，那么相对熵更好理解了。其<strong>实际含义就是用错误分布对随机变量编码时，其产生多余的编码长度</strong>。这里就是 $D_{KL}(p||q)=1.75-1.5=0.25$.</p>
<p>相对熵衡量两个概率分布之间的差值，如下:<br>$$<br>D_{KL}(p||q)=\sum_{i=1}^Np(x_{i})\cdot (logp(x_i)-logq(x_i))\<br>=E[logp(x)-logq(x)]\<br>=\sum_{i=1}^Np(x_{i})\cdot log\frac{p(x_i)}{q(x_i)}\<br>\quad \quad \quad \quad \quad=\sum_{i=1}^Np(x_{i})logp(x_i)-\sum_{i=1}^Np(x_i)logq(x_i))\<br>=-H(x)+cross_entropy<br>$$<br>在机器学习中，往往$p$用来描述<strong>真实分布</strong>,$q$用来描述<strong>预测分布</strong>。</p>
<p>第一行为啥是$p(x_i)$？<strong>后面的是他们的信息量，用它们的概率分布，但实际算期望的时候还是要看真实的概率。</strong>实际$A,B,C$出现还是要按照真实的概率出现。</p>
<p>实际KL散度算的是<strong>预测分布减去真实分布的熵</strong>,其实就是错误分布多使用的信息量。</p>
<p><strong>交叉熵,第四行和第五行显示了交叉熵，因为真实分布是确定的，所以H(x)是固定的，交叉熵可以替代KL散度</strong>。</p>
<p>总结:<span class="github-emoji"><span>😄</span><img src="./medias/loading.gif" data-original="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p>
<blockquote>
<p>信息熵完美编码，<br>交叉熵不完美编码，<br>相对熵是两者的差值，交叉熵减去信息熵。差值即差异，也即KL散度。</p>
</blockquote>
<p>以上内容参考:</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/41252833/answer/140950659">如何通俗的解释交叉熵与相对熵？ - 张一山的回答 - 知乎 </a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/41252833/answer/515095695">如何通俗的解释交叉熵与相对熵？ - erwin的回答 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/41252833/answer/182891314">如何通俗的解释交叉熵与相对熵？ - 小锋子Shawn的回答 - 知乎</a></p>
<h4 id="为什么能用交叉熵作为损失函数？"><a href="#为什么能用交叉熵作为损失函数？" class="headerlink" title="为什么能用交叉熵作为损失函数？"></a>为什么能用交叉熵作为损失函数？</h4><p>在机器学习中，比如分类问题，如果把结果当作是概率分布来看:</p>
<ul>
<li><strong>标签表示的就是数据真实的概率分布</strong>，实际所属类别概率为1,其他类别概率为0。</li>
<li>由softmax函数产生的结果其实是对于数据的预测分布,每个类别有一个预测的概率。</li>
</ul>
<p>按照之前的理解,相当于对于某一张图片分类或语义分割的某一个像素:</p>
<p>真实分布为:$a:1,b:0,c:0,…,z:0$</p>
<p>预测分布为:$a:p_a,b:p_b,c:p_c,…,z:p_z$</p>
<h4 id="二元交叉熵解决sigmoid梯度饱和"><a href="#二元交叉熵解决sigmoid梯度饱和" class="headerlink" title="二元交叉熵解决sigmoid梯度饱和"></a>二元交叉熵解决sigmoid梯度饱和</h4><p>与$MSE$对比,对$w$链式求导时:</p>
<p>$MSE=(y_{true}-y)^2/2,\part MSE/\part w_i=(y-y_{true})\cdot \sigma’\cdot x_i$,结果是包含$\sigma’$,这就会导致梯度饱和；</p>
<p>$\part BCE/\part w_i=(y-y_{true})\cdot x_i$,二元交叉熵只包含$\sigma$(即$y$),不包含$\sigma’$,所以缓解了$sigmoid$的梯度饱和问题。</p>
<h4 id="Bootstrapped-Cross-Entropy-Loss"><a href="#Bootstrapped-Cross-Entropy-Loss" class="headerlink" title="Bootstrapped Cross Entropy Loss"></a>Bootstrapped Cross Entropy Loss</h4><h4 id="对数损失函数-负对数似然"><a href="#对数损失函数-负对数似然" class="headerlink" title="对数损失函数/负对数似然"></a>对数损失函数/负对数似然</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/silver1225/article/details/88914652">负对数似然函数</a></p>
<p><strong>概率</strong>用于在已知参数的情况下，预测接下来的观测结果；<strong>似然性</strong>用于根据一些观测结果，估计给定模型的参数可能值。数值上相等，意义上不同</p>
<p>$L(\theta|x)=f_\theta(x)=P_\theta(X=x)=P(X=x|\theta)=P(X=x;\theta)$</p>
<p>负对数似然:$L(y)=-log(y)$ 负对数似然常和softmax结合使用,$0&lt;=P&lt;=1$</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201223183034910.png" alt="log"></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201223183214140.png" alt="a>1,p"></p>
<p>我们希望得到的概率越大越好，因此概率越接近于1，则函数整体值越接近于0，即使得损失函数取到最小值。</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201125152106078.png" alt="对数损失函数" style="zoom:67%;"><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20210920113522304.png" alt="image-20210920113522304" style="zoom: 67%;"></p>
<p>==这里的$P(y_i|x_i)$就是图片为$x_i$,预测出来正好就是正确的对应类别$y_i$的概率。只把这个预测正确的概率加起来。交叉熵损失除了$c_{ii}$为1，别的都为0，就只剩下了$-loga_{ii}$。综上两个都是只把类别预测正确的那个概率加起来了。所以是一致的。==</p>
<p>右边的关键的一步应该是应用大数定律</p>
<p><strong>结构风险本质</strong></p>
<p>结构风险是对经验风险模型复杂度的惩罚</p>
<p><strong>结构化风险（正则项）其实是加入了模型参数分布的先验知识</strong>，也就是贝叶斯学派为了将模型往人们期望的地方去发展，继而加入了先验分布，由于是人为的先验，因此也就是一个规则项（这也就是正则项名称的由来）。这样一来，风险函数将进一步考虑了被估计量的先验概率分布</p>
<p><strong>统计学习方法=模型+策略+算法</strong></p>
<p>由决策函数表示的模型为非概率模型，由条件概率表示的模型为概率模型。</p>
<p><strong>当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。</strong></p>
<p><strong>当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计</strong></p>
<h4 id="logsoftmax"><a href="#logsoftmax" class="headerlink" title="logsoftmax"></a>logsoftmax</h4><p>首先是softmax公式:<br>$$<br>softmax(x)=\frac{e^{x_i}}{\sum_je^{x_i}}<br>$$<br>求导如下:</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210601202104509.png" alt="image-20210601202104509" style="zoom: 25%;">

<p>logsoftmax()就是在softmax()之后对结果再多一次log操作:<br>$$<br>softmax(x)=log\frac{e^{x_i}}{\sum_je^{x_i}}<br>$$<br>求导:</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210601202516351.png" alt="image-20210601202516351" style="zoom: 25%;">

<p>为什么要加一次log呢?</p>
<p>log_softmax能够解决函数上溢出和下溢出的问题,加快运算速度,提高数据稳定性。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210601204136195.png" alt="image-20210601204136195" style="zoom: 50%;">

<p>由于是指数操作,所以当输入比较大的时候,可能导致上溢出,为啥下溢出？</p>
<p>尽管在数学表示式上是对softmax取对数，但在实操中是通过:</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210601204816037.png" alt="image-20210601204816037" style="zoom: 50%;">

<p>来实现，其中$M=max(x_i),i=1,2,3,…,n$,即$M$为所有$x_i$中最大的值。在加快运算速度的同时，可以保持数值的稳定性。</p>
<h4 id="L1和L2损失函数"><a href="#L1和L2损失函数" class="headerlink" title="L1和L2损失函数"></a>L1和L2损失函数</h4><p>L1,又称为LAD:<br>$$<br>\sum^n_{i=1}|y_i-f(x_i)|<br>$$<br>L2,又称为LSE：<br>$$<br>\sum^n_{i=1}(y_i-f(x_i))^2<br>$$<br>在大多数情况下,通常首选使用L2损失函数。但是L2收到离群点的影响更大(因为有了平方),所以当出现异常值的时候,L2损失函数的效果就不佳了</p>
<h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><h4 id="Precision-amp-Recall"><a href="#Precision-amp-Recall" class="headerlink" title="Precision&amp;Recall"></a>Precision&amp;Recall</h4><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210707211929357.png" alt="image-20210707211929357" style="zoom: 50%;">

<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210707211958788.png" alt="image-20210707211958788" style="zoom:67%;">

<p>有时候查准率更重要，有时候查全率更重要，一般两者都是越高越好。</p>
<h3 id="机器学习问答"><a href="#机器学习问答" class="headerlink" title="机器学习问答"></a>机器学习问答</h3><h4 id="梯度下降or-求导-0"><a href="#梯度下降or-求导-0" class="headerlink" title="梯度下降or 求导=0"></a>梯度下降or 求导=0</h4><p>$$<br>\theta=(X^TX)^{-1}X^TY<br>$$</p>
<p><strong>使用梯度下降进行线性回归的主要原因是计算复杂性：在某些情况下，使用梯度下降找到解决方案的计算成本较低（较快）。</strong></p>
<p>$(X^TX)^{-1}X^T$是X 的伪逆</p>
<p>数据量很大的情况下求$X^TX$的逆是非常<strong>消耗内存以及时间</strong>的。</p>
<h4 id="梯度下降为什么不用二阶求导？"><a href="#梯度下降为什么不用二阶求导？" class="headerlink" title="梯度下降为什么不用二阶求导？"></a>梯度下降为什么不用二阶求导？</h4><p>a. 牛顿法使用的是目标函数的二阶导数，在高维情况下<strong>这个矩阵非常大，计算和存储都是问题。</strong></p>
<p>b. 在小批量的情况下，牛顿法对于<strong>二阶导数的估计噪声太大。</strong></p>
<p>c**.目标函数非凸的时候，牛顿法容易受到鞍点或者最大值点的吸引。**</p>
<p>最大的问题就是*<em>计算复杂度。二阶一次迭代更新的复杂度是n</em>n，这在高维的时候是不可行的**<br><strong>稳定性。越简单的东西往往越robust，对于优化算法也是这样。</strong><br><strong>二阶求导不易</strong><br>二阶方法能够更快地求得更高精度的解，<strong>但是在神经网络这类深层模型中，不高的精度对模型还有益处，能够提高模型的泛化能力。</strong></p>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><h4 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29895933">指数加权平均</a></p>
<p>$v_t=\beta v_{t-1} + (1-\beta)\theta_t $</p>
<p><strong>指数式递减加权   优势</strong>:我们可以看到指数加权平均的求解过程实际上是一个递推的过程，那么这样就会有一个非常大的好处，每当我要求从0到某一时刻（n）的平均值的时候，我并不需要像普通求解平均值的作为，保留所有的时刻值，类和然后除以n。</p>
<p>而是只需要保留0-(n-1)时刻的平均值和n时刻的温度值即可。也就是每次只需要保留常数值，然后进行运算即可，这对于深度学习中的海量数据来说，是一个很好的<strong>减少内存和空间的做法</strong>。</p>
<p>$v_t=(1-\beta)\theta_t+(1-\beta)\beta\theta_{t-1}+(1-\beta)\beta^2\theta_{t-1}+(1-\beta)\beta^3\theta_{t-2}+…$</p>
<p>形成了指数衰减，所以越靠前的影响就越小，所以$\beta$越大，则相当于平均了过去越多天的信息。</p>
<p>偏差修正:比如$\beta=0.98$,那么$(1-\beta)\times\theta_t$远小于初始值，这会导致刚开始的时候有偏差，刚开始的平均值预测小于实际值，所以需要偏差修正，如下:</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210704131948198.png" alt="image-20210704131948198" style="zoom:67%;">

<p>应得到绿色曲线，实际得到紫色曲线，偏差修正即$v_t$除以$1-\beta^t$，即$\frac{v_t}{1-\beta^t}$,初始时修正，随着t变大，分母趋向1.</p>
<p>当然很多人不用修正是因为只要熬过初始偏差就好，但是如果关心初始时期的正确率的话，就要用到偏差修正。</p>
<h4 id="动量梯度下降法"><a href="#动量梯度下降法" class="headerlink" title="动量梯度下降法"></a>动量梯度下降法</h4><p>momentum:动量</p>
<p>计算梯度的指数加权平均数，并利用该梯度更新你的权重</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210705105913587.png" alt="image-20210705105913587"></p>
<p>On iteration t:</p>
<p>Compute $dw$,$db$ on current mini-batch.</p>
<p>$V_{dw}=\beta V_{dw}+(1-\beta)d_w$</p>
<p>$V_{db}=\beta V_{db}+(1-\beta)d_b$</p>
<p>$w=w-\alpha V_{dw}$</p>
<p>$b=b-\alpha V_{db}$</p>
<p>纵轴的摆动被平均了，而横轴一直向右。</p>
<p>我倾向于这么理解，$\beta$和$1-\beta$是质量，而$V_{dw}$和$d_w$是速度，$\beta V_{dw}$是原有动量，而$(1-\beta)d_w$是新加的动量，$\beta$越大，惯性越大，不同左右摇摆，而是垂直落入碗中。</p>
<h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>root mean square prop</p>
<p>指数加权平均平方的根</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210705112839995.png" alt="image-20210705112839995" style="zoom:80%;">

<p>On iteration t:</p>
<p>Compute $d_w,d_b$ on current mini-batch:</p>
<p>$S_{dw}=\beta_2 S_{dw}+(1-\beta_2)d_w^2$</p>
<p>$S_{db}=\beta_2 S_{db}+(1-\beta_2)d_b^2$</p>
<p>$w=w-\alpha \frac{d_w}{\sqrt{S_{dw}+\epsilon}}$</p>
<p>$b=b-\alpha\frac{d_b}{\sqrt{S_{db}+\epsilon}}$</p>
<p>上下抖动大，除以大的变小；左右幅度小，除以小的变大。</p>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p><em>Adaptive Momentum</em> Estimation</p>
<p>相当于将Momentum和RMSprop结合起来，具体公式如下:</p>
<p>$V_{dw}=0,S_{dw}=0,V_{db}=0,S_{db}=0$</p>
<p>On iteration t:</p>
<p>Compute $d_w,d_b$using current mini-batch</p>
<p>$V_{dw}=\beta_1V_{dw}+(1-\beta_1)d_w,V_{db}=\beta_1V_{db}+(1-\beta_1)d_b$</p>
<p>$S_{dw}=\beta_2S_{dw}+(1-\beta_2)d_w^2,S_{db}=\beta_2S_{db}+(1-\beta_2)d_b^2$</p>
<p>$V_{dw}^{corrected}=\frac{V_{dw}}{1-\beta_1^t},V_{db}^{corrected}=\frac{V_{db}}{1-\beta_1^t}$</p>
<p>$S_{dw}^{corrected}=\frac{S_{dw}}{1-\beta_2^t},S_{db}^{corrected}=\frac{S_{db}}{1-\beta_2^t}$<br>$$<br>w=w-\alpha\frac{V_{dw}^{corrected}}{\sqrt{S_{dw}}+\epsilon},b=b-\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}}+\epsilon}<br>$$<br>通常$\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$</p>
<h4 id="学习率设计"><a href="#学习率设计" class="headerlink" title="学习率设计"></a>学习率设计</h4><p><em>learing rate decay</em></p>
<p><strong>polynominal learning rate</strong></p>
<p>$base\cdot (1-\frac{iter}{max_iter})^{power}$</p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54756923">函数图像变换的知识点</a></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210704124810149.png" alt="image-20210704124810149" style="zoom:50%;">

<p>呈多项式曲线下降</p>
<h4 id="weight-decay"><a href="#weight-decay" class="headerlink" title="weight decay"></a>weight decay</h4><p><em>Weight decay</em> (commonly called $L2$ regularization), might be the most widely-used technique for regularizing parametric machine learning models.</p>
<p>To penalize the size of the weight vector, we must somehow add $||W||$ to the loss function, but how should the model trade off the standard loss for this new additive penalty? In practice, we characterize this tradeoff via the <em>regularization constant</em>  λ, a non-negative hyperparameter that we fit using validation data:<br>$$<br>L(w,b)+\frac{\lambda}{2}||w||^2<br>$$</p>
<p>$$<br>\begin{split}\begin{aligned} w_1 &amp;\leftarrow \left(1- \eta\lambda \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\ w_2 &amp;\leftarrow \left(1- \eta\lambda \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}\end{split}<br>$$</p>
<p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。</p>
<h4 id="warm-up"><a href="#warm-up" class="headerlink" title="warm up"></a>warm up</h4><p>为什么warm up策略有效?</p>
<p><strong>warm up策略:在训练最初使用较小的学习率来启动，并很快切换到大学习率而后进行常见的 decay。</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/338066667/answer/771252708">香侬科技的回答</a></p>
<p>这个问题目前还没有被充分证明，我们只能从直觉上和已有的一些论文[1,2,3]得到推测：</p>
<ul>
<li>有助于减缓模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳</li>
<li>有助于保持模型深层的稳定性</li>
</ul>
<p>刚开始模型对数据的“分布”理解为零，或者是说“均匀分布”（当然这取决于你的初始化）；在第一轮训练的时候，每个数据点对模型来说都是新的，<strong>模型会很快地进行数据分布修正，如果这时候学习率就很大，极有可能导致开始的时候就对该数据“过拟合”，后面要通过多轮训练才能拉回来</strong>，浪费时间。当训练了一段时间（比如两轮、三轮）后，模型已经对每个数据点看过几遍了，或者说对当前的batch而言有了一些正确的先验，较大的学习率就不那么容易会使模型学偏，所以可以适当调大学习率。这个过程就可以看做是warmup。那么为什么之后还要decay呢？<strong>当模型训到一定阶段后（比如十个epoch），模型的分布就已经比较固定了，或者说能学到的新东西就比较少了。如果还沿用较大的学习率，就会破坏这种稳定性</strong>，用我们通常的话说，就是<strong>已经接近loss的local optimal了</strong>，为了靠近这个point，我们就要慢慢来。</p>
<p><strong>mini-batch size较小，样本方差较大。</strong>在训练的过程中，如果有mini-batch内的数据分布方差特别大，这就会<strong>导致模型学习剧烈波动，使其学得的权重很不稳定，这在训练初期最为明显，最后期较为缓解</strong>(所以我们要对数据进行scale也是这个道理)。</p>
<h4 id="局部最优问题"><a href="#局部最优问题" class="headerlink" title="局部最优问题"></a>局部最优问题</h4><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210705143024838.png" alt="image-20210705143024838" style="zoom:67%;">

<p>不太可能困在局部最优，而更有可能是鞍点。</p>
<p>鞍点会导致平滑段训练速度减慢，这也是优化算法的作用。</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/84614490">Transformer中改变LayerNorm的位置甚至可以不同warm up</a></p>
<h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/shine-lee/p/12069176.html">这篇太优质了 ！！！！</a></p>
<p><strong>感受野是个相对概念，某层feature map上的元素看到前面不同层上的区域范围是不同的，通常在不特殊指定的情况下，感受野指的是看到输入图像上的区域。</strong></p>
<p>感受野大小<br>$$<br>r_l=r_{l-1}+(k_l-1)\cdot j_{l-1}\<br> =r_{l-1}+(k_l-1)\cdot \prod_{i=1}^{l-1} s_i<br>$$</p>
<p><strong>Layer l 一个元素的感受野rl等价于Layer l−1 上k×k 个感受野的叠加；</strong></p>
<p><strong>Layer l−1 上连续k 个元素的感受野可以看成是，第1个元素看到的感受野加上剩余k−1步扫过的范围</strong></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210312012129435.png" alt="感受野计算示例" style="zoom:67%;">

<p>如上图,Conv1的感受野是3,Conv2的感受野是5</p>
<h3 id="所有的卷积"><a href="#所有的卷积" class="headerlink" title="所有的卷积"></a>所有的卷积</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/tsyccnh/article/details/87357447">一篇非常好的教程关于卷积和转置卷积</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/50573160">腾讯云教程卷积和转置卷积关系</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ywheunji/p/11887906.html">卷积核的参数量和计算量</a></p>
<h4 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h4><p><strong>卷积核</strong></p>
<ol>
<li><strong>卷积核默认第三维与输入图片的第三维（通道数）一样，并进行多层卷积，产生一个二维结果</strong></li>
<li><strong>1个卷积核产生一个二维结果，n个卷积核产生第三维为n的三维结果</strong></li>
<li><strong>卷积核n的个数就是输出的通道数</strong></li>
<li>卷积核个数通常为奇数</li>
</ol>
<p>为什么out_channel要大于in_channel?</p>
<p>Out_channels要能反映图片的多种特征,<strong>每个kernel对不同的特征有不同的敏感度</strong></p>
<p><strong>大的卷积核尺寸意味着更大的感受野</strong></p>
<p><a target="_blank" rel="noopener" href="https://github.com/vdumoulin/conv_arithmetic">结合github的图演示看</a></p>
<p><strong>1 通用公式</strong><br>$$<br>o=\frac{i+2p-k}{s}+1 \<br>D2=K(filters_num)<br>$$</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/512123584">https://zhuanlan.zhihu.com/p/512123584</a> 解释，注意o向下取整</p>
<p><strong>2 没有填充,单位步长</strong><br>$$<br>s=1,p=0,o=i-k+1<br>$$</p>
<p><strong>3有零填充,单位步长</strong></p>
<p><strong>3.1same padding</strong></p>
<p>$$<br>p=0,s=1\<br>o=i-k+1<br>$$<br><strong>3 Half/same padding</strong>         $p=\lfloor \frac{k}{2}\rfloor$<br>$$<br>k=2n+1,n\in N \<br>s=1,p=\lfloor \frac{k}{2}\rfloor=n\<br>o=i<br>$$</p>
<p><strong>3.2 Full padding</strong>            $p=k-1$<br>$$<br>p=k-1,s=1\<br>o=i+k-1<br>$$<br><strong>4 没有零填充,非单位步长</strong><br>$$<br>p=0,o=\lfloor\frac{i-k}{s}\rfloor+1<br>$$</p>
<p><strong>5 零填充,非单位步长</strong>     $odd$<br>$$<br>o=\lfloor \frac{i+2p-k}{s}\rfloor+1<br>$$<br><strong>填充方式</strong>有三种：full same  valid,一般为<strong>零填充</strong></p>
<p>我们平时<strong>所接触的卷积其实是滤波</strong>，不是<strong>真正的数学定义上的卷积</strong>。</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201209205446747.png" alt="步长为2的卷积"></p>
<p>步长为2的卷积可以<strong>看做步长为1的卷积结果的S=2的采样</strong>，因此可以认为是<strong>下采样</strong>的一种。</p>
<h4 id="卷积的性质"><a href="#卷积的性质" class="headerlink" title="卷积的性质"></a>卷积的性质</h4><p>普通卷积的计算看起来是挪动，其实不是挪动，效率太低。<strong>计算机会将卷积核转换成等效的矩阵，将输入转换为向量。通过输入向量和卷积核矩阵的相乘获得输出向量。输出的向量经过整形便可得到我们的二维输出特征。</strong></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201220214743847.png" alt="卷积示例"></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201220214800536.png" alt="四个位置补0"></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201220214820691.png" alt="拉长，然后就可以矩阵相乘"></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201220214911375.png" alt="卷积核所展开的全连接层的权重矩阵"></p>
<p>在CNN提出之前，我们所提到的<strong>人工神经网络应该多数情况下都是前馈神经网络</strong>，两者区别主要在于<strong>CNN使用了卷积层，而前馈神经网络用的都是全连接层</strong>，而这<strong>两个layer的区别又在于全连接层认为上一层的所有节点下一层都是需要的，通过与权重矩阵相乘层层传递，而卷积层则认为上一层的有些节点下一层其实是不需要的，所以提出了卷积核矩阵的概念</strong>。</p>
<p>如果卷积核的大小是nxm，那么意味着该卷积核认为上一层节点每次映射到下一层节点都只有nxm个节点是有意义的。到这里，有些初学者会认为<strong>全连接层也可以做到，只要让权重矩阵某些权重赋值为0就可以实现了</strong>，例如假设在计算当前层第2个节点时认为上一层的第1个节点我不需要，那么设置w01=0就可以了。其实没错，<strong>卷积层是可以看做全连接层的一种特例,卷积核矩阵是可以展开为一个稀疏的包含很多0的全连接层的权重矩阵</strong>。</p>
<p><strong>卷积的key ideas:</strong></p>
<p><strong>局部连接</strong></p>
<p>卷积神经网络有两种神器可以降低参数数目，局部感知野和权值共享。先来说说局部感知也，一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。即，局部感受野指卷积层的神经元只和上一层map的局部相联系。</p>
<p><strong>权值共享</strong></p>
<p>==隐含的原理是：图像中的一部分的统计特性与其他部分是一样的。==</p>
<p><strong>平移不变形Translation Invariance</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhangting2020.github.io/2018/05/30/Transform-Invariance/">参考1</a> </p>
<p>不变形意味着即使目标的外观发生了某种变化,但是你依然可以把它识别出来。这对==图像分类来说是一种很好的特性，==因为我们希望图像中目标无论是被平移，被旋转，还是被缩放，甚至是不同的光照条件、视角，都可以被成功地识别出来。</p>
<p>还有各种不变形,如旋转/视角不变形 Ration/Viewpoint Invariance 尺度不变性Size Invariance 光照不变性illumination Invariance</p>
<p>==而对于其它问题，比如物体检测(detection)、物体分割(segmentation)来说，这个性质则不应该有，原因是当输入发生平移时，输出也应该相应地进行平移。这种性质又称为平移等价性(translation equivalence)。这两个概念是比较混淆的，但确实是两个不同的东西(敲黑板)。==</p>
<p><strong>图像在平移后再特征图上的表示也是同样平移的，这就使图像拥有了一定的平移不变性。</strong></p>
<p>但有人也提出了反驳:</p>
<blockquote>
<p>Why do deep convolutional networks generalize so poorly to small image transformations?</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1661000">看这篇讲解的最全面了</a></p>
<p>【<strong>平移不变性</strong>对应的有一个概念是<strong>平移同变性（translation equivariance）</strong>，这个是用在图像的目标检测中的，如果输入图像中的目标进行了平移，那么最终检测出来的候选框应该也相应的移动，这就是同时改变。】</p>
<p>==卷积到底有没有平移不变性？有的说有，有的说没有，我的观点是没有，但两个观点我都会讲明白==</p>
<p><strong>对于卷积，参数共享的特殊形式使得神经网络层具有==平移等变性(equivariance)==。例如，当处理时间序列数据时，这意味着通过卷积可以得到一个由输入中出现不同特征的时刻所组成的时间轴。如果我们把输入中的一个事件向后延时，在输出中仍然会有完全相同的表示，只是时间延后了。也就是说，卷积对输入数据的时刻是敏感的，在输出中有对应的表示。</strong></p>
<p>关于池化，无论采用<strong>何种池化函数</strong>，当<strong>输入作出少量平移</strong>时，池化能<strong>帮助输入的表示近似不变(invariant)**。对于平移的不变性是指当我们对输入进行少量平移时，经过</strong>池化函数后的大多数输出并不会发生改变**。这意味着==池化对特征位置不敏感，只有当我们不关心特征具体出现的位置时，池化才是合理的，这正是胶囊网络的动机之一。==</p>
<p>==总结来说，CNN 中的卷积操作具有平移等变性，但池化操作具有局部平移不变性。两者矛盾地统一于 CNN 中。胶囊网络完全去掉了池化操作，达到了对平移等变性的追求。==</p>
<p><strong>为什么说近似不变？</strong></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/gs3zt2oucw.gif" alt="微小平移"></p>
<p>平移小鸟位置的时候,预测结果是有很大的波动</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210419212506628.png" alt="shift:0" style="zoom: 67%;">

<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210419212547579.png" alt="shift:1" style="zoom:67%;">

<p>差距很大</p>
<p><strong>平移不变性的公式</strong><br>$$<br>U(D(X))=U(D(X_{shift}))<br>$$<br><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210419212739432.png" alt="shift:0" style="zoom:50%;"></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210419213109362.png" alt="shifit:1" style="zoom:50%;">

<p>回不去了</p>
<p><strong>如何实现平移不变性:</strong></p>
<p>==全局平均池化,消除了位置的影响==</p>
<h4 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h4><p><a target="_blank" rel="noopener" href="https://github.com/vdumoulin/conv_arithmetic">结合github的图演示看</a></p>
<p><strong>1 p=0,s=1</strong><br>$$<br>p=0,s=1\<br>k’=k,s’=s,p’=k-1\<br>o’=i’+(k-1)<br>$$<br><strong>2 s=1</strong><br>$$<br>s=1\<br>p’=k-p-1\<br>o’=i’+(k-1)-2p<br>$$<br><strong>3 Half/same padding</strong><br>$$<br>k=2n+1,n\in N,s=1,p=\lfloor\frac{k}{2}\rfloor=n\<br>k’=k,s’=s,p’=p\<br>o’=i’<br>$$<br><strong>4 Full padding</strong><br>$$<br>s=1,p=k-1\<br>k’=k,s’=s,p’=0\<br>o’=i’-(k-1)<br>$$<br><strong>5 p=0 ,non-unit strides</strong><br>$$<br>p=0,(i-k)%s=0\<br>k’=k,s’=1,p’=k-1\<br>o’=s(\widetilde{i}’-1)+k\<br>\widetilde{i}’ :adding,s-1,zeros,between,each,input,unit<br>$$<br><strong>6 zero-padding,non-unit strides</strong><br>$$<br>(i+2p-k)%s=0\<br>k’=k,s’=1,p’=k-p-1\<br>o’=s(i’-1)+k-2p<br>$$<br><strong>7 zero-padding,non-unit strides odd</strong><br>$$<br>k’=k,s’=1,p’=k-p-1\<br>a=(i+2p-k)%s\<br>o’=s(i’-1)+a+k-2p<br>$$<br>也叫反卷积/分数步长卷积</p>
<p>先说一下为什么人们很喜欢叫转置卷积为反卷积或逆卷积。首先举一个例子，将一个4x4的输入通过3x3的卷积核在进行普通卷积（无padding, stride=1），将得到一个2x2的输出。而转置卷积将一个2x2的输入通过同样3x3大小的卷积核将得到一个4x4的输出，看起来似乎是普通卷积的逆过程。就好像是加法的逆过程是减法，乘法的逆过程是除法一样，人们自然而然的认为这两个操作<strong>似乎是一个可逆的过程。但事实上两者并没有什么关系，操作的过程也不是可逆的。</strong></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201221160433547.png" alt="转置乘过来"></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201221160511466.png" alt="列向量再转化成卷积核"></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201221160546890.png" alt="所有的" style="zoom:67%;">

<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201221160601788.png" alt="看起来就像大的在小的上滑动" style="zoom:67%;">

<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201221160626802.png" alt="如图"></p>
<p><strong>转置卷积公式</strong></p>
<h4 id="空洞-扩张卷积"><a href="#空洞-扩张卷积" class="headerlink" title="空洞/扩张卷积"></a>空洞/扩张卷积</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37644085/article/details/89480326">空洞卷积总结</a></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/dilation.gif" alt="dilation"></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210311181332266.png" alt="空洞卷积增加感受野" style="zoom:67%;">

<p>扩张卷积增加了感受野，而不增加核的大小,<strong>因为中间插入的是空格，需要训练的卷积核参数量是不变的</strong>。</p>
<p>卷积核大小为k,dilation_rate=d,扩张后的卷积大小如下:<br>$$<br>\hat{k} = k + (k − 1)(d − 1)<br>$$<br>则<br>$$<br>o = \lfloor\frac{i+2p-\hat{k}}{s}\rfloor+1<br>$$<br><strong>空洞卷积的作用</strong></p>
<ul>
<li><strong>不丢失分辨率的情况下扩大感受野</strong>：在deep net中为了增加感受野且降低计算量，总要进行降采样(pooling或s2/conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。</li>
<li><strong>调整扩张率获得多尺度上下文信息：</strong>空洞卷积有一个参数可以设置dilation rate，具体含义就是在卷积核中填充dilation rate-1个0，因此，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。<strong>多尺度信息在视觉任务中相当重要啊。</strong></li>
<li>ps: 空洞卷积虽然有这么多优点，但在实际中不好优化，速度会大大折扣。</li>
</ul>
<p><strong>空洞卷积gridding问题</strong></p>
<ul>
<li><strong>局部信息丢失</strong>：由于空洞卷积的计算方式类似于棋盘格式，某一层得到的卷积结果，来自上一层的独立的集合，没有相互依赖，因此该层的卷积结果之间没有相关性，即局部信息丢失。</li>
<li><strong>远距离获取的信息没有相关性</strong>：由于空洞卷积稀疏的采样输入信号，使得远距离卷积得到的信息之间没有相关性，影响分类结果。</li>
</ul>
<h4 id="1x1卷积-Network-in-Network"><a href="#1x1卷积-Network-in-Network" class="headerlink" title="1x1卷积/Network in Network"></a>1x1卷积/Network in Network</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35814486">这篇写的好啊，是我太笨了，不带例子就老觉得读不透彻</a></p>
<p><strong>feature map是卷积核卷出来的，一个卷积核卷出来一个feature，所有feature构成featuremap，feature map数量就是channels</strong></p>
<p><strong>作用：</strong></p>
<ol>
<li><strong>进行卷积核通道数的降维和升维</strong></li>
<li><strong>实现跨通道的交互和信息整合</strong></li>
<li><strong>增加非线性特性</strong></li>
</ol>
<p><strong>Inception中：</strong></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201222151157720.png" alt="加入1x1"></p>
<p>原图feature map:28x28x192  1x1 channel:64 3x3 channel:128 5x5 channel:32</p>
<p>左:$192 × (1×1×64) +192 × (3×3×128) + 192 × (5×5×32) = 387072$ </p>
<p>右:$192 × (1×1×64) +（192×1×1×96+ 96 × 3×3×128）+（192×1×1×16+16×5×5×32）= 157184$</p>
<p>对于右边的池化层，原始的不能降channel，然后就会越来越多，这样也可以用1x1降channel</p>
<p><strong>ResNet：</strong></p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201222154809733.png" alt="ResNet"></p>
<p>假设输入输出都是256 channel,参数真的差太多了！</p>
<p>左:$3 × 3 ×256×256×2=1179648$</p>
<p>右:$ 1×1×256×64 + 3×3×64×64 + 1×1×64×256 = 69632$</p>
<p><strong>全连接层角度：</strong></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201222155618108.png" alt="全连接层角度" style="zoom:67%;">

<p>原channel 6,之后channel 5,只需要一个1x1x5的卷积核就行，参数很少，FC至少$w_0\cdot h_0 \cdot 6\cdot w_1\cdot h_1\cdot 5$</p>
<blockquote>
<p><em>In Convolutional Nets, there is no such thing as “fully-connected layers”. There are only convolution layers with 1x1 convolution kernels and a full connection table-Yann LeCun</em></p>
</blockquote>
<h4 id="Separable-Convlution"><a href="#Separable-Convlution" class="headerlink" title="Separable Convlution"></a>Separable Convlution</h4><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728#5892">可分离卷积</a></p>
<p>two main types:<em>spatial separable conv</em> and <em>depthwise separable conv</em></p>
<p><strong>spatial separable conv</strong>:</p>
<p> illustrates the idea of separating one convolution into two well，but have significant limitations，so not heavily used</p>
<p>这么命名是因为主要处理spatial dimensions:the <strong>width</strong> and the <strong>height</strong>.The other dimension, the “<strong>depth</strong>” dimension, is the number of channels of each image</p>
<p>一个kernel分解成两个更小的kernel,eg:</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/%5C%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%A7%A3%E5%AF%86%5Cimage-20210706145221143.png" alt="image-20210706145221143"></p>
<img src="./medias/loading.gif" data-original="\网络结构解密\image-20210706145549709.png" alt="image-20210706145549709" style="zoom:67%;">

<p>9次乘法下降到6次,网络运行速度更快。</p>
<p>主要的问题就是并不是所有的卷积核都可以分离，导致的结果就是所有可能分离的卷积中最终只有小部分可以分离。</p>
<p><strong>depthwise separable conv:</strong></p>
<p>更适用。This is the type of separable convolution seen in <code>keras.layers.SeparableConv2D</code> or <code>tf.layers.separable_conv2d</code>.</p>
<blockquote>
<p>The depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension — the number of channels — as well. An input image may have 3 channels: RGB. After a few convolutions, an image may have multiple channels. You can image each channel as a particular interpretation of that image; in for example, the “red” channel interprets the “redness” of each pixel, the “blue” channel interprets the “blueness” of each pixel, and the “green” channel interprets the “greenness” of each pixel. An image with 64 channels has 64 different interpretations of that image.</p>
</blockquote>
<p>和spatial separable conv类似,depthwise separable conv将一个kernel分成两个kernel做两次卷积:<em>the depthwise conv</em> and <em>the pointwise conv</em>.</p>
<p><em>depthwise conv</em>:</p>
<img src="./medias/loading.gif" data-original="\网络结构解密\image-20210706151630767.png" alt="image-20210706151630767" style="zoom:67%;">

<p>For an image:12x12x3,to get an output with size 8x8x3.正常的卷积需要5x5x3x3,对于depthwise conv,则需要5x5x1x3,channel之间不交互。</p>
<p><em>Pointwise conv:</em></p>
<p>但是如果我们需要8x8x256,那么就需要提升channel了。</p>
<blockquote>
<p>The pointwise convolution is so named because it uses a 1x1 kernel, or a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has; in our case, 3. Therefore, we iterate a 1x1x3 kernel through our 8x8x3 image, to get a 8x8x1 image.We can create 256 1x1x3 kernels that output a 8x8x1 image each to get a final image of shape 8x8x256.</p>
</blockquote>
<p><strong>What’s the point of creating a depthwise separable convolution?</strong></p>
<p>参数量:$19200\to843$,乘法次数:$1228800\to53952$</p>
<h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p><strong>池化一般不涉及零填充</strong><br>$$<br>o=\lfloor\frac{i-k}{s}\rfloor+1<br>$$</p>
<p><strong>图像中的相邻像素倾向于具有相似的值，因此通常卷积层相邻的输出像素也具有相似的值</strong>。这意味着，卷积层输出中包含的大部分信息都是<strong>冗余</strong>的。</p>
<p>如果我们使用边缘检测滤波器并在某个位置找到强边缘，那么我们也可能会在距离这个像素1个偏移的位置找到相对较强的边缘。但是它们都一样是边缘，我们并没有找到任何新东西。</p>
<p>池化层解决了这个问题。这个网络层所做的就是通过<strong>减小输入的大小降低输出值的数量</strong>。</p>
<p>池化一般通过<strong>简单的最大值、最小值或平均值操作完成</strong>。</p>
<p><strong>最大池化可以提取特征纹理</strong></p>
<p><strong>平均池化可以保留背景信息。</strong></p>
<p><a target="_blank" rel="noopener" href="https://firstai.blog.csdn.net/article/details/105299448">综述:最大池化，平均池化，全局最大池化和全局平均池化？区别原来是这样</a></p>
<h3 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34919792/article/details/102697817">上采样方法总结</a></p>
<p>三种方法：</p>
<ol>
<li>基于<strong>线性插值</strong>的上采样</li>
<li>基于<strong>深度学习</strong>的上采样(转置卷积)</li>
<li><strong>Unpooling</strong>的方法 简单的补0或扩充</li>
</ol>
<h4 id="线性插值"><a href="#线性插值" class="headerlink" title="线性插值"></a>线性插值</h4><p><strong>双线性插值？</strong></p>
<p>应用：</p>
<ol>
<li>对数据中的缺失进行合理补偿 </li>
<li>对数据进行放大或缩小</li>
<li>其他</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/linkr/p/3630902.html">通俗易懂啊</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yssongest/p/5303151.html">图像插值的举例说明</a></p>
<p>根据下式计算目标像素在源图像中的位置：<br>$$<br>srcX=dstX\cdot \frac{srcWidth}{dstWidth}\<br>srcY=dstY\cdot \frac{srcHeight}{dstHeight}<br>$$<br>单线性插值：单变量只有一个x，对y进行插值，线性插值原理是两个点可以确定一条直线，又可以根据x确定y<br>$$<br>\frac{y-y_0}{x-x_0}=\frac{y_1-y_0}{x_1-x_0}\<br>y=\frac{x_1-x}{x_1-x_0}y_0+\frac{x-x_0}{x_1-x_0}y_1\<br>y=k\cdot y0+(1-k)\cdot y_1 \quad \quad<br>$$</p>
<p>双线性插值：有两个变量x、y，分别在x、y两个方向进行插值</p>
<p>对于一个目的像素，设置坐标通过反向变换得到的浮点坐标为$(i+u,j+v)$(其中$i、j$均为浮点坐标的整数部分，$u、v$为浮点坐标的小数部分，是取值[0,1)区间的浮点数)，则这个像素得值$f(i+u,j+v)$可由原图像中坐标为$(i,j)(i+1,j)(i,j+1)(i+1,j+1)$所对应的周围四个像素的值决定，即:$f(i+u,j+v) = (1-u)(1-v)f(i,j) + (1-u)vf(i,j+1) + u(1-v)f(i+1,j) + uvf(i+1,j+1)$,<strong>在哪个方向离得近，比如距离是0.3，权重大，是0.7</strong></p>
<p>现在假如目标图的象素坐标为$(1,1)$，那么反推得到的对应于源图的坐标是$(0.75,0.75)$, 这其实只是一个概念上的虚拟象素,实际在源图中并不存在这样一个象素,那么目标图的象素$（1,1）$的取值不能够由这个虚拟象素来决定，而只能由源图的这四个象素共同决定：$(0,0)(0,1)(1，0)(1,1)$，而由于$(0.75,0.75)$离$(1,1)$要更近一些，那么$(1,1)$所起的决定作用更大一些，这从公式1中的系数$uv=0.75×0.75$就可以体现出来，而$(1,1)$离$(0.75,0.75)$最远，所以$(0,0)$所起的决定作用就要小一些，公式中系数为$(1-u)(1-v)=0.25×0.25$也体现出了这一特点。</p>
<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201120155633758.png" alt="很棒！"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment">#矩阵运算最简单，所以最重要的是坐标如何对应！</span>
a<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">0.25</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment">#注意2和3位置反过来了</span>
            <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
c<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.25</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token punctuation">[</span><span class="token number">0.75</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
d<span class="token operator">=</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span>
e<span class="token operator">=</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>d<span class="token punctuation">,</span>c<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>  <span class="token comment"># align_corners=False</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.2500</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">1.5000</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">2.2500</span><span class="token punctuation">,</span>  <span class="token number">2.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">2.5000</span><span class="token punctuation">,</span>  <span class="token number">2.7500</span><span class="token punctuation">,</span>  <span class="token number">3.2500</span><span class="token punctuation">,</span>  <span class="token number">3.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.2500</span><span class="token punctuation">,</span>  <span class="token number">3.7500</span><span class="token punctuation">,</span>  <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.3333</span><span class="token punctuation">,</span>  <span class="token number">1.6667</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">1.6667</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">,</span>  <span class="token number">2.3333</span><span class="token punctuation">,</span>  <span class="token number">2.6667</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">2.3333</span><span class="token punctuation">,</span>  <span class="token number">2.6667</span><span class="token punctuation">,</span>  <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.3333</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.3333</span><span class="token punctuation">,</span>  <span class="token number">3.6667</span><span class="token punctuation">,</span>  <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20201222100731710.png" alt="两种方式"></p>
<p>第一种，四个角对齐，均匀插值,公式：<br>$$<br>scale = (input_size-1)/(output_size-1)\<br>srcIndex=scale\cdot dstIndex<br>$$<br>第二种，不均匀，出界的只和有原坐标的算，公式：<br>$$<br>scale=input_size/output_size\<br>srcIndex=scale\cdot (dstIndex+0.5)-0.5<br>$$</p>
<h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><p><strong>PixelShuffle</strong></p>
<h3 id="Normalization-1"><a href="#Normalization-1" class="headerlink" title="Normalization"></a>Normalization</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_25737169/article/details/79048516">BN教程</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/shine-lee/p/11989612.html">BN教程2</a></p>
<p>直译过来就是批归一化。</p>
<p>激活之前进行BN，增加了两个学习参数scale和shift，再从线性变到非线性</p>
<p>有轻微的正则化作用，因为使用mint-batch，不同的mini-bath不同，所以会有一些噪音，所以有轻微的正则化效果</p>
<p>测试时逐样本处理，batch norm从之前训练时的$\mu$和$\sigma$做指数加权平均，方式很多，反正是从训练集得到的</p>
<h5 id="第一节：Batchnorm主要解决的问题"><a href="#第一节：Batchnorm主要解决的问题" class="headerlink" title="第一节：Batchnorm主要解决的问题"></a>第一节：Batchnorm主要解决的问题</h5><p>深度神经网络主要就是为了学习训练数据的分布，并在测试集上达到很好的泛化效果。但是：</p>
<ol>
<li>如果我们<strong>每一个batch输入的数据都具有不同的分布</strong>，显然会给网络的训练带来困难</li>
<li><strong>数据经过一层层网络计算后，其数据分布也在发生着变化</strong>，此现象称为Internal Covariate Shift.</li>
</ol>
<p><strong>1.1 Internal Covariate Shift</strong></p>
<p>首先<strong>Internal Covariate Shift</strong>是指训练深度网络的时候经常发生训练困难的问题，因为，<strong>每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难（神经网络本来就是要学习数据的分布，要是分布一直在变，学习就很难了）</strong></p>
<p><strong>1.2 Covariate Shift</strong></p>
<p><strong>Internal Covariate Shift和Covariate Shift</strong>是不同的，差就差在这个Internal,<strong>一个发生在神经网络内部，一个发生在输入数据上</strong><br>==BatchNorm是归一化的一种手段，减小图像之间的绝对差异，突出相对差异==</p>
<p>举个例子，如下图所示：</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210510212843757.png" alt="image-20210510212843757" style="zoom:50%;">

<p>假定当前输入$x1$和$x2$的分布如图中圆点所示，本次更新的方向是将直线$H_1$更新成$H_2$，本以为切分得不错，但是当前面层的权重更新完毕，当前层输入的分布换成了另外一番样子，直线相对输入分布的位置可能变成了$H_3$，下一次更新又要根据新的分布重新调整。<strong>直线调整了位置，输入分布又在发生变化，直线再调整位置，</strong>==就像是直线和分布之间的“追逐游戏”。==<strong>对于浅层模型，比如SVM，输入特征的分布是固定的，即使拆分成不同的batch，每个batch的统计特性也是相近的，因此只需调整直线位置来适应输入分布，显然要容易得多。而深层模型，每层输入的分布和权重在同时变化，训练相对困难。</strong></p>
<h5 id="第二节：Batchnorm-原理解读"><a href="#第二节：Batchnorm-原理解读" class="headerlink" title="第二节：Batchnorm 原理解读"></a>第二节：Batchnorm 原理解读</h5><p>为了减小Internal Covariate Shift，对神经网络的每一层做归一化不就可以了，假设将每一层输出后的数据都归一化到0均值，1方差，满足正态分布，但是，此时有一个问题，<strong>每一层的数据分布都是标准正态分布，导致其完全学习不到输入数据的特征，</strong>因为，<strong>费劲心思学习到的特征分布被归一化</strong>了，因此，直接对每一层做归一化显然是不合理的。但如果加上可学习的参数就好多了。</p>
<p>如果batch size为$m$，则在前向传播过程中，网络中每个节点都有$m$个输出，所谓的Batch Normalization，就是对该层每个节点的这$m$个输出进行归一化再输出，具体计算方式如下：</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210510213910551.png" alt="image-20210510213910551" style="zoom:50%;">

<p><strong>$\gamma$和$\beta$为待学习的scale和shift参数，用于控制$y_i$的方差和均值。</strong>==这样的话就是具有不同均值和方差的正态分布了，有利于权重和分布的相互协调==</p>
<h5 id="第三节：BatchNorm实现"><a href="#第三节：BatchNorm实现" class="headerlink" title="第三节：BatchNorm实现"></a>第三节：BatchNorm实现</h5><h5 id="第四节：Batchnorm的优点"><a href="#第四节：Batchnorm的优点" class="headerlink" title="第四节：Batchnorm的优点"></a>第四节：Batchnorm的优点</h5><ul>
<li><strong>可以使用更大的学习率</strong>，训练过程更加稳定，极大提高了训练速度。</li>
<li><strong>可以将bias置为0</strong>，因为Batch Normalization的Standardization过程会移除直流分量，所以不再需要bias。</li>
<li><strong>权重初始化不再敏感</strong>，通常权重采样自0均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了Batch Normalization后，对与同一个输出节点相连的权重进行放缩，其标准差σ也会放缩同样的倍数，相除抵消。</li>
<li><strong>对权重的尺度不再敏感</strong>，理由同上，尺度统一由γ参数控制，在训练中决定。</li>
<li><strong>深层网络可以使用sigmoid和tanh了</strong>，理由同上，BN抑制了梯度消失。</li>
<li><strong>Batch Normalization具有某种正则作用，不需要太依赖dropout，减少过拟合</strong>。</li>
</ul>
<h5 id="什么时候不能用BN？"><a href="#什么时候不能用BN？" class="headerlink" title="什么时候不能用BN？"></a>什么时候不能用BN？</h5><p><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210510215346460.png" alt="image-20210510215346460"></p>
<p>![image-20230903160740141](/Users/dongchenghao/Library/Application Support/typora-user-images/image-20230903160740141.png)</p>
<p>==BN是保留Channel，在N,H,W上做操作==</p>
<p>肯定是batch size越大，使用BN的效果越好。为什么BN不做channel只做NHW，可能是因为每个channel都对应之前一个卷积核，逐channel代表逐卷积核的输出做归一化。</p>
<h4 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35005794">吴育昕 何恺明的GN教程</a></p>
<ol>
<li>BN依赖于batch size，batch size较小时，BN效果不好，有些任务往往batch size只有1-2</li>
<li>训练，验证，测试这三个阶段存在inconsistency。</li>
</ol>
<p>Layer Norm和Instance Norm就是Group Norm的特例，归一化避开了batch。</p>
<p>为什么工作？个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。</p>
<h4 id="Instance-Norm"><a href="#Instance-Norm" class="headerlink" title="Instance Norm"></a>Instance Norm</h4><p>Instance Normalization(IN),一种更适合对单个像素有更高要求的场景的归一化算法(IST,GAN等)。IN的算法非常简单,计算归一化统计量时考虑单个样本,单个通道的所有元素。</p>
<p>对于图像风格迁移任务来说,每个样本的每个信息点都是非常重要的。对于BN对整个batch做归一化,可能造成每个样本独特细节的丢失;对于LN,忽略了不同通道之间的差异。</p>
<h4 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h4><img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20220901105559979.png" alt="image-20220901105559979" style="zoom:67%;">

<p><strong>均值为0,方差为1</strong>,只对最后的C进行归一化</p>
<h3 id="网络权重初始化"><a href="#网络权重初始化" class="headerlink" title="网络权重初始化"></a>网络权重初始化</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/shine-lee/p/11908610.html">权重初始化方法总结</a></p>
<h3 id="如何判断模型收敛"><a href="#如何判断模型收敛" class="headerlink" title="如何判断模型收敛"></a>如何判断模型收敛</h3><h3 id="特征融合"><a href="#特征融合" class="headerlink" title="特征融合"></a>特征融合</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/141685352">知乎教程</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xys430381_1/article/details/88370733">CSDN类似的教程</a></p>
<p>很多工作通过融合多层来提升检测和分割的性能，按照融合与预测的先后顺序，分类为<strong>早融合(Early fusion)和晚融合(Late fusion)。</strong></p>
<h4 id="Early-fusion"><a href="#Early-fusion" class="headerlink" title="Early fusion"></a>Early fusion</h4><p>先融合多层的特征，然后在融合后的特征上训练预测器(<strong>只在完全融合之后，才统一进行检测</strong>)。这类方法也被称为<strong>skip connection，即采用concat、add操作</strong>。这一思路的代表是Inside-Outside Net(ION)和HyperNet。</p>
<ol>
<li>concat</li>
<li>add</li>
</ol>
<h4 id="Late-fusion"><a href="#Late-fusion" class="headerlink" title="Late fusion"></a>Late fusion</h4><p>同过<strong>结合不同层的检测结果</strong>改进检测性能(<strong>尚未完成最终的融合之前，在部分融合的层上就开始进行检测，会有多层的检测，最终将多个检测结果进行融合</strong>)。</p>
<p>代表思路有两种:</p>
<ol>
<li><strong>feature不融合，多尺度的feture分别进行预测，然后对预测结果进行综合，</strong>如Single Shot MultiBox Detector (SSD) , Multi-scale CNN(MS-CNN)</li>
<li><strong>feature进行金字塔融合，融合后进行预测</strong>，如Feature Pyramid Network(<strong>FPN</strong>)等。</li>
</ol>
<p>接下来主要归纳晚融合方法</p>
<h3 id="多尺度设计"><a href="#多尺度设计" class="headerlink" title="多尺度设计"></a>多尺度设计</h3><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/57cfa4fdd423?from=timeline">AI不惑境教程</a></p>
<h3 id="判断函数是否收敛？"><a href="#判断函数是否收敛？" class="headerlink" title="判断函数是否收敛？"></a>判断函数是否收敛？</h3><p>两个算法,batch_size为4,epoch为30和batch_size为8,epoch为60,不同的算法是否有对应的大小关系呢?</p>
<p>主要是看两个，第一个epoch多少是否有影响，<strong>要看函数是否收敛？</strong></p>
<p>第二个batch_size不同,要看<strong>是否能收敛到最小点</strong>,batch_size小<strong>容易收敛到鞍点</strong>,大的话容易收敛到最小点。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>back propagation<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/new1\jianguoyun_posts\网络结构解密\image-20210707214021643.png" alt="image-20210707214021643">传播,扩展</p>
<h4 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h4><img src="./medias/loading.gif" data-original="\网络结构解密\image-20210707221902911.png" alt="image-20210707221902911" style="zoom:67%;">

<p>$x_1,x_2$是一个独立样本的两个维度,如果样本的维度是15维，那么MLP输入层的神经元就有15个。</p>
<p><strong>链式法则-BP的基础</strong></p>
<p>假设我们现在更新$w_1$,那么就要求$\frac{\partial E}{\partial w_1}$<br>$$<br>\frac{\partial E}{\partial w_1}=\frac{\partial E}{\partial y}\cdot \frac{\partial y}{\partial h_1}\cdot \frac{\partial h_1}{\partial w_1}<br>$$<br>最开始是损失函数,做分子的只能是神经元的值,参数只能做分母.</p>
<p>这里面还有激活函数,如果$y=\sigma(w_1x+b_1)$直接展开,那么就是$\frac{\partial y}{\partial w_1}$,如果看成是两步$y=\sigma(a),a=w_1x+b_1$,则为$\frac{\partial y}{\partial a}\cdot\frac{\partial a}{\partial w_1}$根据链式法则,算出来是一样的。</p>
<p>例子:$y=3a^2+1,a=3x+1$,算出来都是$54x+18$(如果是对$x$求偏导)</p>
<p><strong>为什么现在是对$w$和$b$求导？</strong></p>
<p>因为现在$x$和$y$数据已知,就像极大似然估计一样,现在是把$w$和$b$看做参数来反推$w$和$b$,所以需要对$w$和$b$求导。</p>
<h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><h4 id="池化-1"><a href="#池化-1" class="headerlink" title="池化"></a>池化</h4><p>池化层一般对应在卷积层的后面，属于一对一的关系（它只影响当前深度的一个节点），不与其它卷积核做连接，所以没有权重参数需要学习。所以反向传播的时候，只需对输入参数求导，不需要进行权值更新。</p>
<p>对于最大池化和平均池化，也有不同:<a target="_blank" rel="noopener" href="https://blog.csdn.net/kieven2008/article/details/81603994">教程</a></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210303003004886.png" alt="最大池化" style="zoom:50%;">

<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210303003024968.png" alt="平均池化" style="zoom:50%;">

<h3 id="Self-attention与Transformer"><a href="#Self-attention与Transformer" class="headerlink" title="Self attention与Transformer"></a>Self attention与Transformer</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">知乎教程</a></p>
<p><a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>
<p>直接看上面这篇文章</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/yujianmin1990/article/details/85221271">The Illustrated Transformer的翻译参考</a></p>
<h4 id="A-High-level-Lokok"><a href="#A-High-level-Lokok" class="headerlink" title="A High-level Lokok"></a>A High-level Lokok</h4><p>Transformer中的编码器是完全结构相同的,但是并不共享参数，每一个编码器都可以拆解成以下两个子部分:</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426100431432.png" alt="Encoder" style="zoom:80%;">

<p>解码器同样也有这些子层，但是在两个子层间增加了attention层，该层有助于解码器能够关注到输入句子的相关部分，与 <a target="_blank" rel="noopener" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">seq2seq model</a> 的Attention作用相似。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426100534638.png" alt="image-20210426100534638" style="zoom:80%;">

<h4 id="Bringing-The-Tensors-Into-the-Picture"><a href="#Bringing-The-Tensors-Into-the-Picture" class="headerlink" title="Bringing The Tensors Into the Picture"></a>Bringing The Tensors Into the Picture</h4><p>正如NLP应用的常见例子，先将输入单词使用<a target="_blank" rel="noopener" href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">embedding algorithm</a>转成向量。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426100735512.png" alt="image-20210426100735512" style="zoom:80%;">

<p>词的向量化仅仅发生在最底层的编码器的输入时，<strong>这样每个编码器的都会接收到一个list（每个元素都是512维的词向量）</strong>，只不过其他编码器的输入是前个编码器的输出。list的尺寸是可以设置的超参，==通常是训练集的最长句子的长度==。</p>
<p>在对输入序列做词的向量化之后，它们流经编码器的如下两个子层:</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426100904390.png" alt="image-20210426100904390" style="zoom:80%;">

<p>这里能看到Transformer的一个关键特性，==每个位置的词仅仅流过它自己的编码器路径。在self-attention层中，这些路径两两之间是相互依赖的。前向网络层则没有这些依赖性，但这些路径在流经前向网络时可以并行执行。==</p>
<h4 id="Now-We’re-Encoding"><a href="#Now-We’re-Encoding" class="headerlink" title="Now We’re Encoding !"></a>Now We’re Encoding !</h4><p>正如之前所提，编码器接收向量的list作输入。然后将其送入self-attention处理，再之后送入前向网络，最后将输入传入下一个编码器。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426101726843.png" alt="image-20210426101726843" style="zoom:80%;">

<p>每个位置的词向量被送入self-attention模块，然后是前向网络(对每个向量都是完全相同的网络结构)。</p>
<h4 id="Self-Attention-at-a-High-Level"><a href="#Self-Attention-at-a-High-Level" class="headerlink" title="Self-Attention at a High Level"></a>Self-Attention at a High Level</h4><p>不要被self-attention这个词迷惑了，看起来好像每个人对它都很熟悉，但是在我读到Attention is All You Need这篇文章之前，我个人都没弄懂这个概念。下面我们逐步分解下它是如何工作的。</p>
<p>以下面这句话为例，作为我们想要翻译的输入语句“The animal didn’t cross the street because it was too tired”。句子中”it”指的是什么呢？“it”指的是”street” 还是“animal”？对人来说很简单的问题，但是对算法而言并不简单。</p>
<p>当模型处理单词“it”时，self-attention允许将“it”和“animal”联系起来。==当模型处理每个位置的词时，self-attention允许模型看到句子的其他位置信息作辅助线索来更好地编码当前词。==如果你对RNN熟悉，就能想到RNN的隐状态是如何允许之前的词向量来解释合成当前词的解释向量。Transformer使用self-attention来将相关词的理解编码到当前词中。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210420102549063.png" alt="Scaled Dot-Product Attention" style="zoom:67%;">

<h4 id="Self-Attention-in-Detail"><a href="#Self-Attention-in-Detail" class="headerlink" title="Self-Attention in Detail"></a>Self-Attention in Detail</h4><p>我们先看下如何计算self-attention的向量，再看下如何以矩阵方式计算。<br><strong>第一步</strong>，==根据编码器的输入向量，生成三个向量==，比如，对每个词向量，生成$query$, $key$, $value$三个vector</p>
<p>生成方法为分别乘以三个矩阵，这些<strong>矩阵在训练过程中需要学习</strong>。【注意：==不是每个词向量独享3个matrix，而是所有输入共享3个转换矩阵；==<strong>权重矩阵是基于输入位置的转换矩阵</strong>；有个可以尝试的点，如果每个词独享一个转换矩阵，会不会效果更厉害呢？】<br>注意到这些新向量的维度比输入词向量的维度要小（512–&gt;64），并不是必须要小的，是为了让多头attention的计算更稳定。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426104949349.png" alt="image-20210426104949349" style="zoom: 67%;">

<p><strong>第二步</strong>，计算attention就是计算一个分值。对“Thinking Matchines”这句话，对“Thinking”（pos#1）计算attention 分值。我们需要计算每个词与“Thinking”的评估分，这个分决定着编码“Thinking”时（某个固定位置时），每个输入词需要集中多少关注度。<br>==这个分，通过“Thing”对应query-vector与所有词的key-vec依次做点积得到。==所以当我们处理位置#1时，第一个分值是q1和k1的点积，第二个分值是q1和k2的点积。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426105236082.png" alt="image-20210426105236082" style="zoom:67%;">

<p><strong>每个是一个Vector,可以用矩阵加速计算</strong></p>
<p><strong>第三步和第四步</strong>，除以8($d_k$,为query和key向量的维度)，这样梯度会更稳定。然后加上softmax操作，<strong>归一化分值使得全为正数且加和为1。</strong></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426145803111.png" alt="image-20210426145803111" style="zoom: 67%;">

<p>softmax分值决定着在这个位置,每个词的表达程度(关注度)。<strong>很明显,这个位置的词应该有最高的归一化分数,但大部分时候总是有助于关注该词的相关的词。</strong></p>
<p><strong>第五步</strong>，将softmax分值与value-vec按位相乘。==保留关注词的value值，削弱非相关词的value值。==</p>
<p><strong>第六步</strong>，将所有加权向量加和，产生该位置的self-attention的输出结果。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426145954742.png" alt="image-20210426145954742" style="zoom:67%;">

<p>上述就是self-attention的计算过程，生成的向量流入前向网络。在实际应用中，上述计算是以速度更快的矩阵形式进行的。下面我们看下在单词级别的矩阵计算。</p>
<h4 id="Matrix-Calculation-of-Self-Attention"><a href="#Matrix-Calculation-of-Self-Attention" class="headerlink" title="Matrix Calculation of Self-Attention"></a>Matrix Calculation of Self-Attention</h4><p>第一步是计算Query,Key和Value矩阵。 为此，我们将embeddings堆叠成矩阵X，然后将其乘以我们训练过的权重矩阵（WQ，WK，WV）。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426151301289.png" alt="image-20210426151301289" style="zoom:67%;">

<p><strong>X矩阵中的每一行对应于输入句子中的一个单词。</strong></p>
<p>最后，由于我们要处理矩阵，因此我们可以将步骤2到6压缩成一个公式，以计算self attention layer的输出。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426151533778.png" alt="image-20210426151533778" style="zoom:67%;">

<h4 id="The-Beast-With-Many-Heads"><a href="#The-Beast-With-Many-Heads" class="headerlink" title="The Beast With Many Heads"></a>The Beast With Many Heads</h4><p>本文通过添加一种称为“多头”注意力的机制，进一步完善了self attention layer。 这样可以通过两种方式提高attention layer的性能：</p>
<ol>
<li>它==扩展了模型专注于不同位置的能力==。在上面的例子中，==z1只包含了其他所有encoding的一点点，但是它很可能由实际该单词本身主导==。如果我们要翻译这样的句子: “The animal didn’t cross the street because it was too tired”,那么我们会想知道”it”指什么,多头注意力很有用。</li>
<li>它为关注层提供了多个“表示子空间”。正如我们接下来要看到的，在multi-headed attention下有多组Query/Key/Value的权重matrix,而非仅仅一组（论文中使用8-heads）。每一组都是随机初始化，==然后，在训练之后，将每组用于将input embeddings(或 vectors from lower encoders/decoders)投影到不同的表示子空间representation subspace中。==</li>
</ol>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426153133207.png" alt="image-20210426153133207" style="zoom:67%;">

<p>为每个头维护单独的$W_Q$,$W_K$和$W_V$,与$X$乘完之后获得$Q$,$K$,$V$</p>
<p>如果我们执行与上面的概述相同的self attention计算，我们最终将得到八个不同的$Z$矩阵</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426153614905.png" alt="image-20210426153614905" style="zoom:67%;">

<p>这给我们带来了一些挑战。==前馈层不希望有8个矩阵,它只要一个矩阵(a vector for each word)==。因此,我们需要一种将这8个矩阵压缩为1个矩阵的方法。</p>
<p>==我们该怎么做?concat 然后乘以额外的权重矩阵==$W_O$</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426153909094.png" alt="image-20210426153909094" style="zoom:67%;">

<p>这就是multi-headed self attention的全部。 我知道，矩阵很多。 让我尝试将它们全都放在一个视觉中，以便我们可以在一处查看它们。</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20210426161648133.png" alt="image-20210426161648133" style="zoom:80%;">

<h4 id="Representing-The-Order-of-The-Sequence-Using-Positional-Encoding"><a href="#Representing-The-Order-of-The-Sequence-Using-Positional-Encoding" class="headerlink" title="Representing The Order of The Sequence Using Positional Encoding"></a>Representing The Order of The Sequence Using Positional Encoding</h4><h3 id="扩散模型"><a href="#扩散模型" class="headerlink" title="扩散模型"></a>扩散模型</h3><p>参考:</p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000043744225">https://segmentfault.com/a/1190000043744225</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/563661713">https://zhuanlan.zhihu.com/p/563661713</a></p>
<h4 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h4><p>马尔科夫链定义本身比较简单，它假设某一时刻状态转移的概率只依赖于它的前一个状态。</p>
<p><strong>$p(z^{m+1}|z^{1},…,z^{m})=p(z^{m+1}|z^{m})$</strong></p>
<p>具有马尔可夫性的随机序列$X={X_0,X_1,…,X_T,… }$称为<strong>马尔可夫链</strong>，或马尔可夫过程。</p>
<h5 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h5><p><strong>扩散过程</strong></p>
<p>总共包含$T$ 步的扩散过程的每一步都是对上一步得到的数据$x_{t-1}$按如下方式增加高斯噪音<br>$$<br>q(x_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t I)<br>$$<br>这边这个分号前的$x_t$应该是意味着是关于$x_t$，后面两个分别是均值和方差。${\beta_t}^{T}_{t=1}$是每一步所采用的方差,介于0-1之间。</p>
<p>对于扩散模型，我们往往称不同step的方差设定为<strong>variance schedule</strong>或者<strong>noise schedule</strong>，通常情况下，越后面的step会采用更大的方差，即满足$\beta_1&lt;\beta_2&lt;…&lt;\beta_T$。</p>
<p>在一个设计好的<strong>variance schedule</strong>下，的如果扩散步数$T$足够大，那么最终得到的$X_T$就完全丢失了原始数据而变成了一个随机噪音。 扩散过程的每一步都生成一个带噪音的数据$x_t$，整个扩散过程也就是一个<strong>马尔可夫链</strong>：<br>$$<br>q(x_{1:T}|x_0)=\prod_{t=1}^Tq(x_t|x_{t-1})<br>$$<br>$q(x_2|x_1)\times q(x_1|x_0) = q(x_2|x_0) $,利用条件概率和马尔克夫的定义严格证明我还不会，但是对当前这个条件，$q(x_t|x_{t-1})= N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)$,当前这个确实满足啊~</p>
<p>上面好像是错误的,$x_{1:T}$好像是代表一个联合分布</p>
<p>条件概率的一般形式: </p>
<p>$1:P(A,B,C)=P(C|BA)P(B,A)=P(C|BA)P(B|A)P(A)$</p>
<p>$2: P(B,C|A)=P(B|A)P(C|A,B)$ </p>
<p>推导:$P(BC|A)=P(ABC)/P(A)=P(C|AB)P(AB)/P(A)=P(C|AB)\times P(B|A) $</p>
<p>马尔可夫概率的一般形式:</p>
<p>$1:P(A,B,C)=P(C|BA)P(B,A)=P(C|B)P(B|A)P(A)$</p>
<p>$2: P(B,C|A)=P(B|A)P(C|B)$</p>
<p>扩散过程的一个重要特性是我们可以<strong>直接基于原始数据$x_0$来对任意$t$步的$x_t$进行采样。</strong></p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20230608163339800.png" alt="image-20230608163339800" style="zoom:80%;">

<p>从前一步采样，如何从$x_{t-1}$直接算出来$x_t$呢,这一步公式是关键,$x_t=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon$,相当于对原图像乘以一个值,然后加上一个正态分布,其实相当于对$x_{t-1}$逐点进行高斯采样，采样就是以当前点的值为均值，方差为$\beta_t$,即$x_t\sim N(\sqrt{1-\beta_t}x_{t-1},\beta_tI)$.</p>
<p>$\sqrt{\bar{a}_t}$和$\sqrt{1-\bar{a}_t}$分别称为<strong>signal_rate</strong>和<strong>noise_rate</strong>,随着$\beta$增大，分别趋向0和1</p>
<p><strong>反向过程</strong></p>
<p>去噪声操作的数学形式是怎么样的？怎么让神经网络来学习它呢？数学原理表明，当$\beta_t$足够小时，每一步加噪声的逆操作也满足正态分布。<br>$$<br>x_{t-1} \sim N(\tilde{\mu_t},\tilde{\beta_t}I)<br>$$<br>为了描述所有去噪声操作，<strong>神经网络</strong>应该根据当前的时刻$t$、当前的图像$x_t$，拟合当前时刻的加噪声逆操作的正态分布，也就是拟合当前的均值$\tilde{\mu}_t$和方差$\tilde{\beta_t}$。</p>
<blockquote>
<p>加噪声的逆操作不太可能从理论上求得，我们只能用一个神经网络去拟合它。去噪声操作和加噪声逆操作的关系，就是神经网络的预测值和真值的关系。</p>
</blockquote>
<p>$$<br>q(x_{t-1}|x_t,x_0)=q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}<br>$$</p>
<p>推导:<br>$$<br>q(x_{t-1}|x_t,x_0)=\frac{q(x_{t-1},x_t,x_0)}{q(x_t,x_0)}\<br>=\frac{q(x_t|x_{t-1},x_0)\times q(x_{t-1}|x_{0})\times q(x_0)}{q(x_t,x_0)}\<br>=q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}<br>$$<br>原来如此~</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20230608203301129.png" alt="image-20230608203301129" style="zoom:80%;">

<p>神经网络拟合均值时，$x_t$是已知的（别忘了，图像是一步一步倒着去噪的）。式子里唯一不确定的只有$\epsilon$,所以干脆拟合噪声，所以最终噪声的误差函数可写成:<br>$$<br>L=||\epsilon_t-\epsilon_\theta(x_t,t)||^2<br>$$</p>
<h4 id="DDIM"><a href="#DDIM" class="headerlink" title="DDIM"></a>DDIM</h4><p>看了半天看不懂,简单总结</p>
<p>找到了一种能满足DDPM逆向条件，且能减少采样步骤的逆向公式：<br>$$<br>q_{\sigma}(x_{1:T}|x_0)=q_{\sigma}(x_{T}|x_0)\prod^t_{t=2}q_{\sigma}(x_{t-1}|x_t,x_0)\<br>=N(x_{t-1};\sqrt{\bar{\alpha}_{t-1}}x_0+\sqrt{1-\bar{\alpha}_{t-1} -\sigma^2_t}\cdot \frac{x_t-\sqrt{\bar{\alpha}_t}x_0}{\sqrt{1-\bar{\alpha}_t}},\sigma^2_tI)<br>$$<br>或者是这样:</p>
<img src="./medias/loading.gif" data-original="/wang-luo-jie-gou-jie-mi/image-20230609152618007.png" alt="image-20230609152618007" style="zoom:67%;">

<h4 id="Conditional-Diffusion-Models"><a href="#Conditional-Diffusion-Models" class="headerlink" title="Conditional Diffusion Models"></a>Conditional Diffusion Models</h4><p>训练的时候,sample $(x_0,\tilde{x})\sim q(x_0,\tilde{x})$从成对的数据分布,如一个clean和一个noisy,则在反向过程就可以提供$\tilde{x}$作为输入在反向过程中:<br>$$<br>p_\theta(x_{0:T}|\tilde{x})=p(x_T)\prod^T_{t=1}p_\theta(x_{t-1}|x_t,\tilde{x})<br>$$<br>所以现在对应的优化目标变为$\epsilon_\theta(x_t,\tilde{x},t)$,$x$和$\tilde{x}$在通道维度被concat，所以最终输入图像的channel是C=6</p>
<p>所以对应的DDIM中的公式为：<br>$$<br>x_{t-1}=\sqrt{\bar{\alpha}_{t-1}}(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\cdot \epsilon_\theta(x_t,\tilde<br>{x},t)}{\sqrt{\bar{\alpha}<em>t}})+\sqrt{1-\bar{\alpha}</em>{t-1}}\cdot \epsilon_\theta(x_t,\tilde{x},t)<br>$$<br>从$x_T\sim N(0,I)$开始</p>
<h3 id="搜广推相关八股"><a href="#搜广推相关八股" class="headerlink" title="搜广推相关八股"></a>搜广推相关八股</h3><p><strong>auc和roc是什么？</strong></p>

            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Dch</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://chenghaoDong666.github.io/wang-luo-jie-gou-jie-mi/">http://chenghaoDong666.github.io/wang-luo-jie-gou-jie-mi/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Dch</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/">
                                    <span class="chip bg-color">网络结构</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="./medias/loading.gif" data-original="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="./medias/loading.gif" data-original="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'TWPqcJz808PYWJLo2bO5Ni0A-gzGzoHsz',
        appKey: 'wBrlMIP11gP74GA1P1Rrkpbj',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: 'just go go'
    });
</script>

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/tensorflow-xue-xi/">
                    <div class="card-image">
                        
                        
                        <img src="./medias/loading.gif" data-original="/medias/featureimages/24.jpg" class="responsive-img" alt="tensorflow学习">
                        
                        <span class="card-title">tensorflow学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            简单记录tensorflow学习的一些总结
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-11-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/python/" class="post-category">
                                    python
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/tensorflow/">
                        <span class="chip bg-color">tensorflow</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/ren-xiang-pi-fu-jian-ce-2/">
                    <div class="card-image">
                        
                        
                        <img src="./medias/loading.gif" data-original="/medias/featureimages/6.jpg" class="responsive-img" alt="人像肤色检测-2">
                        
                        <span class="card-title">人像肤色检测-2</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人像肤色检测两篇综述
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-11-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%82%A4%E8%89%B2%E5%88%86%E7%BA%A7/" class="post-category">
                                    肤色分级
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E8%82%A4%E8%89%B2%E5%88%86%E7%BA%A7/">
                        <span class="chip bg-color">肤色分级</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        //if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
        newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        //}

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 月源<br />'
            + '文章作者: Dch<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="779322177"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="/about" target="_blank">Dch</a>
			<br>
            <span id="sitetime"></span>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">174.5k</span>&nbsp;字
            
            
            
            
            
            
				<span id="busuanzi_container_site_pv" style='display:none'> 
					<i class="fa fa-heart-o"></i> 
					本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span> 
				</span> 
			
			 
				<span id="busuanzi_container_site_uv" style='display:none'> 
					人次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人. 
				</span> 
			
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/chenghaoDong666" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1785246872@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1785246872" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1785246872" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>
<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */
        var t1 = Date.UTC(2020, 10, 13, 00, 00, 00); //北京时间2020-10-13 00:00:00
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "本站已勉强运行 " +diffYears+" 年 "+diffDays + " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
    }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
    siteTime();
</script>
<script> 
	$(document).ready(function () { 
		var int = setInterval(fixCount, 50); // 50ms周期检测函数 
		var pvcountOffset = 80000; // 初始化首次数据 
		var uvcountOffset = 20000; 
		function fixCount() { 
			if (document.getElementById("busuanzi_container_site_pv").style.display != "none") { 
				$("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + pvcountOffset); 
				clearInterval(int); 
			} 
			if ($("#busuanzi_container_site_pv").css("display") != "none") {
				$("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + uvcountOffset); // 加上初始数据 
				clearInterval(int); // 停止检测 
			} 
		} 
	}); 
</script>



    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    
    <script>
        (function (i, s, o, g, r, a, m) {
            i["DaoVoiceObject"] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o), m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            a.charset = "utf-8";
            m.parentNode.insertBefore(a, m)
        })(window, document, "script", ('https:' == document.location.protocol ? 'https:' : 'http:') +
            "//widget.daovoice.io/widget/6984b559.js", "daovoice")
        daovoice('init', {
            app_id: "6ef42b68"
        });
        daovoice('update');
    </script>
    

    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=i;var e=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight+240||document.documentElement.clientHeight+240)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}i(),n.addEventListener("scroll",function(){var t,e;t=i,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
