<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>codeforces-1</title>
      <link href="codeforces-1/"/>
      <url>codeforces-1/</url>
      
        <content type="html"><![CDATA[<h2 id="1A-Theatre-Square-math"><a href="#1A-Theatre-Square-math" class="headerlink" title="1A Theatre Square-math"></a>1A Theatre Square-math</h2><blockquote><p>#include&lt;bits/stdc++.h&gt;</p><p>using namespace std</p><p>注意查看$1\leq n,m,a\leq 10^9$,所以可能数超大，使用<strong>long long</strong>定义</p><p>向上取整<strong>ceil()</strong>,向下取整<strong>floor()</strong>,四舍五入到最近整数<strong>round()</strong>,注意返回值是<strong>浮点数</strong>！</p><p>c++API：<a href="https://devdocs.io/cpp/">https://devdocs.io/cpp/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> codeforces </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>组会总结</title>
      <link href="zu-hui-zong-jie/"/>
      <url>zu-hui-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="10-20"><a href="#10-20" class="headerlink" title="10.20"></a>10.20</h2><ol><li>ACCV 大概在六七月份</li><li>为什么定这几级，理论依据是什么？现有的分级标准，为什么这么分？理论依据？出于什么考量？</li><li>写论文博客时，要分两个过程，自己读主要写每个细节，按照论文的流程走下来；和别人讲、真正理解是要按自己的思路把整个流程理一遍</li><li><img src="/zu-hui-zong-jie/image-20201021103706939.png" alt="IJCAI"></li><li><img src="/zu-hui-zong-jie/image-20201021103921695.png" alt="ICCV"></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>conda使用</title>
      <link href="conda-shi-yong/"/>
      <url>conda-shi-yong/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>个人随想</title>
      <link href="ge-ren-sui-xiang/"/>
      <url>ge-ren-sui-xiang/</url>
      
        <content type="html"><![CDATA[<ol><li>早晨和下午的大块时间要利用好，攻坚论文或代码，晚上可以适当娱乐或者做一些其他的小事情，做事情不是给别人看的，而是你真的做出了成绩</li><li>适当的休息，不然眼睛太累了。</li><li>有时候商场打折的衣服比网上的还便宜还好！</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>奇奇怪怪不成体系问题合集</title>
      <link href="qi-qi-guai-guai-bu-cheng-ti-xi-wen-ti-he-ji/"/>
      <url>qi-qi-guai-guai-bu-cheng-ti-xi-wen-ti-he-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="HP打印机状态需要注意、打印显示用户干预如何处理？"><a href="#HP打印机状态需要注意、打印显示用户干预如何处理？" class="headerlink" title="HP打印机状态需要注意、打印显示用户干预如何处理？"></a>HP打印机状态需要注意、打印显示用户干预如何处理？</h2><p>和打印机连接在<strong>同一Wifi并已经添加了打印机</strong>的情况下，<strong>右键打印机</strong>-&gt;<strong>属性</strong>-&gt;<strong>Web服务</strong>，获得<strong>打印机IP</strong>，然后<strong>右键打印机</strong>-&gt;<strong>打印机属性</strong>-&gt;<strong>端口</strong>-&gt;<strong>添加端口</strong>-&gt;<strong>Standard TCP/IP Port</strong>-&gt;<strong>输入你的打印机的IP</strong>-&gt;<strong>填写随便一个端口名</strong>-&gt;<strong>应用</strong>-&gt;<strong>问题解决</strong>！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>肤色分级与系统</title>
      <link href="fu-se-fen-ji-yu-xi-tong/"/>
      <url>fu-se-fen-ji-yu-xi-tong/</url>
      
        <content type="html"><![CDATA[<h2 id="论文阅读-lt-lt-Brief-overview-of-PANTONE-SkinTone-Guide-chart-in-CIEL-a-b-color-space-gt-gt"><a href="#论文阅读-lt-lt-Brief-overview-of-PANTONE-SkinTone-Guide-chart-in-CIEL-a-b-color-space-gt-gt" class="headerlink" title="论文阅读-<<Brief overview of PANTONE SkinTone Guide chart in CIEL*a*b* color space>>"></a>论文阅读-&lt;&lt;Brief overview of PANTONE SkinTone Guide chart in CIEL*a*b* color space&gt;&gt;</h2><h3 id="Title-amp-Keywords-amp-Abstract-amp-Conclusion"><a href="#Title-amp-Keywords-amp-Abstract-amp-Conclusion" class="headerlink" title="Title&amp;Keywords&amp;Abstract&amp;Conclusion"></a>Title&amp;Keywords&amp;Abstract&amp;Conclusion</h3><h4 id="CIEL-a-b-color-space"><a href="#CIEL-a-b-color-space" class="headerlink" title="CIEL*a*b* color space?"></a>CIEL*a*b* color space?</h4><p><strong>颜色开发培训讲义</strong>：<a href="https://www.zhihu.com/column/cxqingzong-color">https://www.zhihu.com/column/cxqingzong-color</a>     <strong>这篇不能更赞</strong></p><p><strong>可见光谱:</strong></p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014213348777.png" alt="可见光谱"></p><p>我们所说的<strong>颜色主要分两种</strong>：</p><blockquote><p><strong>光源色（light source color）</strong>：来自发光体的颜色。如太阳，灯泡，led灯，等等。</p><p><strong>表面色（surface color</strong>）：不是来自发光体的物体色。物体本身不发光，但能看到物体的颜色，是因为这些物体能对来自于其他发光体的光的选择性的吸收和反射。</p></blockquote><p>颜色也可以简单分为两大类：</p><blockquote><p><strong>非彩色</strong>：黑白灰</p><p><strong>彩色</strong>：红黄蓝绿等</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014210822647.png" alt="彩色与非彩色"></p><p><strong>颜色感知的三要素，光源，物体和观察者，缺一不可，缺少一个要素，我们看不到颜色，或者其中一个要素发生改变，我们看到的颜色都会不一样。</strong></p><p><strong>1光源</strong></p><p><strong>不同光源性质是不一样的</strong>，有些光源会亮一点白一点，如太阳，或者就是太阳光，一天内不同时间段的太阳光也会有很大差异，导致在这些不同的光源下看相同一个颜色都会有很大差异.</p><p>所以我们在<strong>颜色开发或者颜色沟通交流的时候，会指定一个标准光源</strong>，这样能确保我们双方看颜色条件的一致性。我们比对颜色使用的光源都是有标准规定的光源。通常在下图所示的对色灯箱里看颜色。灯箱里面装有不同的常用光源。</p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014215437189.png" alt="对色灯塔及常见光源" style="zoom: 67%;"><blockquote><p><strong>Light sources：发光体（照明体）</strong>。泛指能发出光（可见光）的物体，如太阳，蜡烛，灯泡等。但是有些发光体发出的光是变化的不稳定的。例如太阳光，就算在同一天光照辐射都是不一样的，是变化的，更何况在不同的天气，不同的季节。所以很难用这些不稳定的光源来进行对颜色的描述和交流。</p><p><strong>lluminants：光源</strong>。是一个可以定量描述的发光体。是国际照明委员会 CIE（Commission Internationale de L’Eclairage）为了对颜色的评估和计算而定义了不同类型的，能用数学表（相对能量和波长)表示的标准光源。</p></blockquote><p><strong>色温Color temperature是照明光学中用于定义光源颜色的一个物理量。光源的色温是以光源发光时所显现的颜色与一个绝对黑体被高温燃烧时所显现的颜色相一致时的燃烧温度来定义的，它的单位是绝对温度Kelvin开尔文【K】。是为了量化光源色彩的一个物理量</strong></p><p>开尔文与摄氏度的转换关系如下：</p><p><strong>K(开尔文）=273.15+T(摄氏度）</strong></p><p>K值越高，显现的颜色就愈趋向于白蓝色；K值越低，显现的颜色就愈趋向于黄红色。</p><p>开尔文认为，假定纯黑体，能够将落在其上的所有热量吸收，而没有损失，同时又能够将热量生成的能量全部以“光”的形式释放出来的话，它产生辐射最大强度的波长随温度变化而变化。</p><p><strong>显色指数color rendering index (CRI)</strong> :<strong>与标准的参考光源相比较，一个光源对物体颜色外貌所产生的效果</strong>。换句话说，是<strong>一个光源与标准光源（例如日光）相比较下在颜色辨认方面的一种测量方式</strong>。CRI是一种得到普遍认可的度量标准，也是目前评价与报告<strong>光源显色性</strong>的惟一途径。</p><p><strong>Ra</strong> <strong>=</strong> <strong>物体在某一光源照射下所显现的颜色 ÷ 物体本身所具有的颜色</strong>。</p><p>Ra表示某光源的显色指数。Ra愈接近100%，<strong>表明在该光源照射下，物体所显现的颜色与物体本身所具有的颜色的差异就愈小</strong>。</p><p><strong>标准光源</strong>的光谱要求如下：</p><p>（1）光源的<strong>色温必须是5000K-6500K</strong>，在这种光源色温下观察颜色的效果基本类似于中国大部分地区上午8点至10点，下午3点至5点的自然光下的观察效果。</p><p>（2）光源的<strong>显色指数Ra&gt;90</strong></p><p>光源的性质，可以通过<strong>光谱功率分布曲线（SPD）</strong>来描述。不同的光源有着不同的光谱功率分布曲线。光谱功率分布（SPD）的意思就是光源发出可见光的不同光谱波长（400nm~700nm）的功率是不同的。功率可以理解成强度的大小。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015094547985.png" alt="常用光源的光谱功率分布曲线"></p><p><strong>2物体</strong></p><blockquote><p><strong>物体关于颜色的性质是对不同波长的电磁波的选择性吸收</strong>，所以我们用<strong>光谱反射率曲线</strong>来表达物体的这种性质。<strong>红色绿色蓝色</strong>的光谱反射率曲线的<strong>最大特征</strong>是，它<strong>有明显的波峰</strong>。<strong>波峰所在的位置的电磁波波长代表着这个物体的颜色</strong>。</p><p>但是<strong>黑白灰</strong>就不一样。物体之所以能够呈现出<strong>白色</strong>，是因为这个物体对<strong>不同波长的电磁波几乎都不吸收</strong>，所以都被反射出来。<strong>黑色</strong>跟白色刚好相反，黑色物体<strong>几乎完全吸收所有波长的电磁波</strong>，所以从黑色的光谱反射率曲线来看，所有波长的光谱的反射率都很低很低。</p></blockquote><p><strong>3观察者人眼：</strong></p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015095247048.png" alt="人眼观察黄光" style="zoom:67%;"><p>类似人眼三种视锥细胞对不同波长的光的响应，研究人员也得到一个标准观察者的三刺激值（x，y，z），作为测色仪辨别颜色的视锥细胞。通过这三个参数xyz，来描述一个颜色，也就是后面将要介绍的<strong>CIE-XYZ颜色空间</strong>。</p><p><strong>CIE-XYZ颜色空间：</strong></p><p>我们将<strong>光源</strong>、<strong>物体</strong>和<strong>观察者</strong>这三要素的性质相乘，也就是光源的光谱功率分布曲线乘以物体的光谱反射率曲线乘以标准观察者，得到三个参数<strong>X，Y，Z</strong>（都是大写字母），不同的颜色，有着不同XYZ值。</p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100147722.png" alt="CIEXYZ" style="zoom:80%;"><p>按照下图里的公式算出<strong>x</strong>（小写X），<strong>y</strong>（小写Y）。<strong>xyz值（小写）代表着XYZ（大写）的占比</strong>，这样三个参数缩减到两个参数，<strong>两个参数形成一个平面的二维颜色空间，也就是CIE-XYZ颜色空间</strong>。CIE XYZ颜色空间具有不均匀性。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100255681.png" alt="CIE-XYZ颜色空间"></p><p><strong>缺点：不容易对颜色差异的大小进行判定</strong>，<strong>无法非常直观的判定这个颜色</strong>就是我需要的颜色，不知道这个颜色跟我需要的颜色的差异的大小。</p><p>我们把<strong>人眼感觉不出的色彩差别量（变化范围）叫做颜色的宽容量</strong>。颜色的宽容量反映在<strong>CIExy色度图上即为两个色度点之间的距离</strong>。因为，每种颜色在色度图上是一个点，但<strong>对人的视感觉来说，当这种颜色的色度坐标位置变化很小时，人眼仍认为它是原来的颜色，感觉不出它的变化。</strong>所以，对视感觉效果来说，<strong>在这个变化的距离（或范围）以内的色彩差别量，在视觉效果上是等效的。</strong>对色彩复制和其它颜色工业部门来说这种位于人眼宽容量范围之内的色彩差别量是允许存在的。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015220402922.png" alt="不同标准色度点的颜色宽容量"></p><p><strong>CIE-L*a*b*颜色空间：</strong></p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201016174528078.png" alt="介绍"></p><p>跟之前介绍的孟塞尔颜色体系的颜色空间是一样，是三维空间中立体的球形。空间中有三个维度，形成三个互相垂直的轴，分别是：</p><ul><li>L*轴：从上到下；<strong>表示明度</strong>，范围由0到100，表示颜色从深（黑）到浅（白）。</li><li>a*轴：从左到右；<strong>表示红绿</strong>，数值变化由正到负，表示颜色从红（正）到绿（负）。a值越大颜色越红，a值越小颜色越绿。</li><li>b*轴：从里到外。<strong>表示黄蓝</strong>，数值变化由正到负，表示颜色从黄（正）到蓝（负）。b值越大颜色越黄，b值越小颜色越蓝。</li><li><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100941807.png" alt="CIE-L*a*b*"></li></ul><p><strong>两个颜色之间的差异大小</strong>。引入一个概念——<strong>色差△E</strong>。<strong>两个颜色的差异大小，就是这两个颜色的在颜色空间上两个点的距离</strong>。色差的计算公式如下：</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100802146.png" alt="色差的计算公式"></p><p><strong>同色异谱：</strong></p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019221820491.png" alt="同色异谱"></p><blockquote><p>有时候我们看两个物体的颜色，在某种场景下，比如上图左边的两个物体在室外太阳光底线看起来颜色的一样的，但是一旦我们拿到室内，如上图右边，在荧光灯管底线发现，其实这两个物体的颜色是相差非常大的。这就是同色异谱现象，<strong>同色异谱也叫做条件对色</strong>，顾名思义，这两个颜色只有符合一定观察条件下颜色才能相等，实际上这两个颜色并非完全一样。</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019222411178.png" alt="同色异谱"></p><blockquote><p><strong>同色异谱（条件等色）的根源在于两物体的光谱反射率曲线不同</strong>，也就是说有不同的颜色色粉配方。同色异谱中的<strong>“谱”指的就是光谱反射率曲线</strong>。如下图，就是上面两个颜色色卡的光谱反射率曲线，可以看到两者的差异是非常大的。也可以看到两个光谱反射率曲线的交叉点很多。</p><p>《颜色技术原理》中提到过史泰鲁斯 （stiles）和 维 泽 斯 基（ wyszecki）发现两个同色异谱的颜色的光谱反射曲线在可见光谱波段 （400~700nm） 内， 至少在三个不同波长上必须具有相同的反射率。也就是两者的光谱反射率曲线至少要有三个交叉点 。</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019222914891.png" alt="不同的光谱反射率曲线"></p><p>常见颜色空间介绍：<a href="https://blog.csdn.net/JiangHui1211/article/details/84592774?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">https://blog.csdn.net/JiangHui1211/article/details/84592774?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><p><strong>RGB颜色空间</strong>:</p><p><strong>任意色光F</strong>都可以用RGB<strong>三种颜色不同分量的相加混合而成</strong>：**F=r[R]+g[G]+b[B]**。</p><p><strong>一般我们读取图片获得的三维矩阵是RGB空间</strong></p><p><strong>色度学规则</strong>：<br>　　(1)通过<strong>R,G,B这三种颜色能产生任何颜色</strong>，并且<strong>这三种颜色混合后产生的颜色是唯一</strong>的。<br>　　(2)如果<strong>两个颜色相等，这三个颜色分量再乘以或者除以相同的数，得到的颜色仍然相等</strong>。<br>　　(3)<strong>混合色的亮度等于每种颜色亮度的和</strong>。</p><p><strong>RGB颜色空间</strong>的<strong>均匀性非常差，且两种颜色之间的知觉差异色差不能表示为该颜色空间中两点间的距离</strong>，但是<strong>利用线性或非线性变换</strong>，则<strong>可以从RGB颜色空间推导出其他的颜色特征空间</strong>。</p><p><strong>CMYK模式：</strong></p><p>俗称<strong>四色打印模式</strong>，是最佳的打印模式。因为在实际应用中，青色、洋红色和黄色很难叠加形成真正的黑色，最多不过是褐色而已。因此才引入了K——黑色。黑色的作用是强化暗调，加深暗部色彩。</p><p><strong>HSV颜色空间：</strong></p><p>感觉和孟塞尔颜色体系很像。**HSV即色相(Hue)、饱和度(Saturation)、明度(Value)，又称HSB(B即Brightness)**。</p><p>RGB和CMYK<strong>面向硬件</strong>，可用于<strong>图片编码</strong>；HSV<strong>面向用户</strong>，可用于<strong>图片编辑软件</strong>。</p><p><strong>sRGB色彩空间：</strong></p><p>standard Red Green Blue，<strong>标准红绿蓝色彩空间</strong>是惠普与微软于1996年一起开发的用于<strong>显示器、打印机以及因特网的一种标准RGB</strong>色彩空间。这种标准得到了W3C、Exif、英特尔、Pantone、Corel以及其它许多业界厂商的支持。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015222536629.png" alt="sRGB色域"></p><p>维基百科：<a href="https://zh.wikipedia.org/wiki/Lab%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4">https://zh.wikipedia.org/wiki/Lab%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4</a></p><p>在<strong>RGB</strong>或<strong>CMYK</strong>值与<strong>L*a*b*</strong> 之间没有转换的简单公式，因为<strong>RGB和CMYK色彩空间是设备依赖的</strong>。RGB或CMYK值<strong>首先必须被变换到特定绝对色彩空间中，比如sRGB或Adobe RGB</strong>。这种调整将是设备依赖的，但是<strong>变换的结果数据是设备无关的</strong>，允许把数据变换成<strong>CIE 1931色彩空间</strong>并接着变换成<strong>L*a*b*</strong>。</p><h4 id="Hue-Angle色相角"><a href="#Hue-Angle色相角" class="headerlink" title="Hue Angle色相角?"></a>Hue Angle色相角?</h4><blockquote><p>孟塞尔颜色体系：</p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014214736980.png" alt="image-20201014214736980" style="zoom:50%;"><p><strong>1）色相/色调/Hue</strong>，对于色相我们比较熟悉的是这个色环，色环上不同位置代表不同色相。色相的排列顺序是按照可见光波长从低到高，逆时针分布。这是色相环的概念。把一周均分成五5种主色互相调和成五种中间色，相邻的两个位置之间再均分10份，共100份</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015221249004.png" alt="色相带"></p><p><strong>2）明度/Value/Lightness</strong>，很容易理解，就是一个颜色中含有白和黑的比例：白越多黑越少，这个颜色的明度就越高。</p><p><strong>3）色度chroma</strong>。是一个颜色里面含有这个色相的浓度。<strong>很多人容易把饱和度和明度的概念混淆。是因为他们不理解饱和度和明度在色彩空间中的位置。明度在色彩空间中的位置是从顶部到底部，明度从高到低。而饱和度在色彩空间中的位置是从里到外，饱和度从低到高。</strong></p></blockquote><h4 id="色度-色域？"><a href="#色度-色域？" class="headerlink" title="色度?色域？"></a>色度?色域？</h4><p>研究颜色测量的学科叫做<strong>色度学</strong>，色度学的任务就是用数量化来表征色觉特性。色度”中的“度”是度量的意思。 类似于长度，高度等等概念。度量长度或高度使用的工具是尺子，而度量颜色的工具就是<strong>颜色感知三要素</strong>。</p><p><strong>色域</strong>是对一种颜色进行编码的方法，也指一个技术系统能够产生的颜色的总合。在计算机图形处理中，色域是<strong>颜色的某个完全的子集</strong>。颜色子集最常见的应用是用来精确地代表一种给定的情况。例如一个给定的色彩空间或是某个输出装置的呈色范围。</p><h3 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h3><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170452039.png" alt="Table1" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170512027.png" alt="Table2" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170533519.png" alt="Figure1" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170555412.png" alt="Figure2" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170613588.png" alt="Figure3" style="zoom:50%;"><blockquote><p>lightness and chroma:亮度和色度</p><p>yellow and red categories:Hue色相</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019215839363.png" alt="观察者"></p><blockquote><p>用于色彩排列和分类的这种三维系统已经融入目前广泛使用的<strong>色彩空间模型、色差公式和色容差系统</strong>。</p></blockquote><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019223925779.png" alt="转换" style="zoom:67%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019224245507.png" alt="从色彩到色彩测量" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019224402876.png" alt="从色彩到色彩测量" style="zoom:50%;"><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><blockquote><p><strong>Applications</strong>:</p><ol><li>diagnosis and treatments of cutaneous disorders </li><li>matching our body color to get maxillofacial soft tissue prostheses</li><li>face detection and recognition</li><li>cosmetics</li></ol><p>CIEL*a*b* is a <strong>device-independent color space</strong> which we used in our study.</p></blockquote><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><blockquote><p>110 colors numbered from 1Y01 SP to 4R15 SP</p><p> the first number <strong>indicates the chroma, varies from 1 to 5</strong> which it is the highest</p><p> the rigid represents <strong>yellowness (Y)</strong> or <strong>redness (R) as hue</strong> and </p><p><strong>two last numbers show the lightness, varies from 1 to 15</strong> which it is the darkest. These samples are sorted in <strong>decreasing order</strong> of lightness (5).</p></blockquote><h2 id="小实验"><a href="#小实验" class="headerlink" title="小实验"></a>小实验</h2><blockquote><p>MATLAB Api：<a href="https://www.mathworks.com/help/">https://www.mathworks.com/help/</a></p></blockquote><pre class=" language-matlab"><code class="language-matlab"><span class="token comment" spellcheck="true">% shows example of illuminant estimation based on Grey-World, Shades of</span><span class="token comment" spellcheck="true">% Gray, max-RGB, and Grey-Edge algorithm</span><span class="token comment" spellcheck="true">%example images</span>input_im<span class="token operator">=</span><span class="token function">double</span><span class="token punctuation">(</span><span class="token function">imread</span><span class="token punctuation">(</span><span class="token string">'test_3.jpg'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>input_im<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">title</span><span class="token punctuation">(</span><span class="token string">'input image'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% %Grey-World</span><span class="token comment" spellcheck="true">% [wR,wG,wB,out]=general_cc(input_im,0,1,0);</span><span class="token comment" spellcheck="true">% figure(2);</span><span class="token comment" spellcheck="true">% imshow(uint8(out));</span><span class="token comment" spellcheck="true">% title(</span><span class="token string">'Grey-World'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%max-RGB</span><span class="token comment" spellcheck="true">% [wR,wG,wB,out]=general_cc(input_im,0,-1,0);</span><span class="token comment" spellcheck="true">% figure(3);imshow(uint8(out));</span><span class="token comment" spellcheck="true">% title(</span><span class="token string">'max-RGB'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% Shades of Grey</span>mink_norm<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">% any number between 1 and infinity</span><span class="token punctuation">[</span>wR<span class="token punctuation">,</span>wG<span class="token punctuation">,</span>wB<span class="token punctuation">,</span>out<span class="token punctuation">]</span><span class="token operator">=</span><span class="token function">general_cc</span><span class="token punctuation">(</span>input_im<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>mink_norm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">title</span><span class="token punctuation">(</span><span class="token string">'Shades of Grey'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% Grey-Edge</span><span class="token comment" spellcheck="true">% mink_norm=5;    % any number between 1 and infinity</span><span class="token comment" spellcheck="true">% sigma=2;        % sigma </span><span class="token comment" spellcheck="true">% diff_order=1;   % differentiation order (1 or 2)</span><span class="token comment" spellcheck="true">% [wR,wG,wB,out]=general_cc(input_im,diff_order,mink_norm,sigma);</span><span class="token comment" spellcheck="true">% figure(5);imshow(uint8(out));</span><span class="token comment" spellcheck="true">% title(</span><span class="token string">'Grey-Edge'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% 截取皮肤区域</span><span class="token comment" spellcheck="true">% out=out(375:480,185:290,:);</span><span class="token comment" spellcheck="true">% out=out(360:420,360:420,:);</span>out<span class="token operator">=</span><span class="token function">out</span><span class="token punctuation">(</span><span class="token number">165</span><span class="token operator">:</span><span class="token number">240</span><span class="token punctuation">,</span><span class="token number">230</span><span class="token operator">:</span><span class="token number">320</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% 展示截取皮肤区域</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% convert to lab</span>labI <span class="token operator">=</span> <span class="token function">rgb2lab</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%seperate l,a,b</span><span class="token comment" spellcheck="true">%matlab的下标从1开始</span>l <span class="token operator">=</span> <span class="token function">labI</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>a <span class="token operator">=</span> <span class="token function">labI</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>b <span class="token operator">=</span> <span class="token function">labI</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% % 显示各维度直方图</span><span class="token comment" spellcheck="true">% figure(4);</span><span class="token comment" spellcheck="true">% hist(l);</span><span class="token comment" spellcheck="true">% figure(5);</span><span class="token comment" spellcheck="true">% hist(a);</span><span class="token comment" spellcheck="true">% figure(6);</span><span class="token comment" spellcheck="true">% hist(b);</span><span class="token comment" spellcheck="true">%网格曲面图</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">subplot</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">meshc</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>l<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">subplot</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">meshz</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>l<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><img src="/fu-se-fen-ji-yu-xi-tong/result_0.png" alt="result_0" style="zoom: 50%;"><img src="/fu-se-fen-ji-yu-xi-tong/result_1.png" alt="result_1" style="zoom: 50%;"><p><img src="/fu-se-fen-ji-yu-xi-tong/result_3.png" alt="result_3" style="zoom: 67%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019214729213.png" alt="image-20201019214729213"></p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019214825960.png" alt="三维图" style="zoom:80%;"><h2 id="接下来打算要做"><a href="#接下来打算要做" class="headerlink" title="接下来打算要做"></a>接下来打算要做</h2><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019155029016.png" alt="计划流程图"></p>]]></content>
      
      
      <categories>
          
          <category> 肤色分级 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 肤色分级 </tag>
            
            <tag> 颜色空间 </tag>
            
            <tag> MATLAB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献图书资源搜索与使用管理</title>
      <link href="wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/"/>
      <url>wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/</url>
      
        <content type="html"><![CDATA[<h2 id="文献引用信息有误"><a href="#文献引用信息有误" class="headerlink" title="文献引用信息有误"></a>文献引用信息有误</h2><blockquote><p>进入Mendeley网站，搜索出错的文献，然后“+Add to library”</p></blockquote><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201012160759194.png" alt="image-20201012160759194" style="zoom:67%;"><blockquote><p>回到Mendeley sync一下，出现该项，但是此时无法打开，如下图将你的论文添加：</p></blockquote><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201012160634578.png" alt="image-20201012160634578"></p><h2 id="SCI一区、二区、影响因子？"><a href="#SCI一区、二区、影响因子？" class="headerlink" title="SCI一区、二区、影响因子？"></a>SCI一区、二区、影响因子？</h2><p>一般SCI论文分四个区，一区都是国际顶级期刊，二区次之，三区和四区是一般的SCI期刊，有两种，一种是web of science的JCR（journal citation report）分区，在web of science 搜索论文下面会有按钮显示期刊影响力。另一种是中科院分区，请在高校或中科院内登录<a href="http://www.fenqubiao.com/">http://www.fenqubiao.com</a>。</p><p>影响因子（英文：Impact Factor），简称IF，是汤森路透（Thomson Reuters）出品的期刊引证报告（Journal Citation Reports，JCR）中的一项数据。 即某期刊前两年发表的论文在该报告年份（JCR year）中被引用总次数除以该期刊在这两年内发表的论文总数。这是一个国际上通行的期刊评价指标。影响因子现已成为国际上通用的期刊评价指标，它不仅是一种测度期刊有用性和显示度的指标，而且也是测度期刊的学术水平，乃至论文质量的重要指标。影响因子是一个相对统计量。</p><h3 id="图书搜索"><a href="#图书搜索" class="headerlink" title="图书搜索"></a>图书搜索</h3><blockquote><p><strong>虫部落快搜</strong>：<a href="https://search.chongbuluo.com/%EF%BC%8C%E9%9B%86%E5%90%88%E4%BA%86%E5%BE%88%E5%A4%9A%E6%90%9C%E7%B4%A2%EF%BC%8C%E5%8C%85%E6%8B%AC**%E9%B8%A0%E6%91%A9%E6%90%9C%E4%B9%A6">https://search.chongbuluo.com/，集合了很多搜索，包括**鸠摩搜书</a>**</p><p><strong>Z-Library：</strong><a href="https://1lib.net/%EF%BC%8C**%E7%9C%9F%E7%9A%84%E6%98%AF%E5%AE%9D%E8%97%8F">https://1lib.net/，**真的是宝藏</a>**</p><p>Library Genesis：<a href="https://libgen.lc/">https://libgen.lc/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Typora使用问题</title>
      <link href="typora-shi-yong-wen-ti/"/>
      <url>typora-shi-yong-wen-ti/</url>
      
        <content type="html"><![CDATA[<h2 id="无序列表如何退出？"><a href="#无序列表如何退出？" class="headerlink" title="无序列表如何退出？"></a>无序列表如何退出？</h2><p>左上角的选项很迷，可以使用源代码模式退出，还可以按一下退格两下enter退出</p><h2 id="公式块编辑常见命令？"><a href="#公式块编辑常见命令？" class="headerlink" title="公式块编辑常见命令？"></a>公式块编辑常见命令？</h2><blockquote><p>版权声明：本文为博主原创文章，遵循<a href="http://creativecommons.org/licenses/by-sa/4.0/"> CC 4.0 BY-SA </a>版权协议，转载请附上原文出处链接和本声明。</p><p>本文链接：<a href="https://blog.csdn.net/mingzhuo_126/article/details/82722455">https://blog.csdn.net/mingzhuo_126/article/details/82722455</a> </p></blockquote><p>\in 属于 \notin 不属于</p><h2 id="emoji表情md编写？"><a href="#emoji表情md编写？" class="headerlink" title="emoji表情md编写？"></a>emoji表情md编写？</h2><p>参考大神博客：<a href="https://sunhwee.com/posts/a927e90e.html">https://sunhwee.com/posts/a927e90e.html</a></p><h3 id="Markdown设置？"><a href="#Markdown设置？" class="headerlink" title="Markdown设置？"></a>Markdown设置？</h3><p>可以在偏好设置的Markdown处设置内联公示、代码行号、公式序号</p>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>颜色恒常性之&lt;&lt;A Multi-Hypothesis Approach to Color Constancy&gt;&gt;</title>
      <link href="yan-se-heng-chang-xing-zhi-a-multi-hypothesis-approach-to-color-constancy/"/>
      <url>yan-se-heng-chang-xing-zhi-a-multi-hypothesis-approach-to-color-constancy/</url>
      
        <content type="html"><![CDATA[<h2 id="Bayes"><a href="#Bayes" class="headerlink" title="Bayes"></a>Bayes</h2><h2 id="论文思路"><a href="#论文思路" class="headerlink" title="论文思路"></a>论文思路</h2><p>论文：&lt;&lt;A Multi-Hypothesis Approach to Color Constancy&gt;&gt;</p><h3 id="Title-amp-Abstract-amp-Conclusion"><a href="#Title-amp-Abstract-amp-Conclusion" class="headerlink" title="Title&amp;Abstract&amp;Conclusion"></a>Title&amp;Abstract&amp;Conclusion</h3><blockquote><p>Multi-Hypothesis？</p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>多假设都有什么假设？</p><ol><li>Under the prevalent assumption that the scene is illuminated by a single or dominant light source, the observed<br>pixels of an image are typically modelled using the physical model of Lambertian image formation captured under a<br>trichromatic photosensor:</li><li>we assume that the color of the light and the surface reflectance are independent.</li><li></li></ol><blockquote><p><strong>Our likelihood estimator</strong> learns to answer <strong>a camera-agnostic question</strong> and thus enables <strong>effective multi-camera training</strong> by disentangling illuminant estimation from the supervised learning task.</p><p>learning from image samples that were <strong>captured by multiple cameras</strong></p></blockquote><p><strong><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>相机无关和多相机图片训练到底是如何实现的？</strong></p><p>只是这个似然估计器与相机无关，多个相机获得多个数据集，对每个数据集利用KMeans找出候选光源，然后都喂入网络，实现了多相机图片训练</p><h3 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h3><p><strong>Figure1</strong></p><p>![Figure1](颜色恒常性之A Multi-Hypothesis Approach to Color Constancy/image-20201019104708991.png)</p><blockquote><p><strong><u>（d）use an illuminant candidate set per camera</u></strong>. <strong><u>[ r/g ,b/g ]</u></strong></p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong>每个摄像机获得一个候选集吗？最后是如何训练的？对于每个摄像机的候选集，是如何选取划分的？</strong></p><p>每个摄像机有自己的一个照片集，对这里的图片进行分类，每个摄像机获得一个候选集。训练应该就是将这些候选集都喂入。</p><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong>[r/g,b/g]这个图如何读?</strong></p><p>图上一个点应该是代表一个光源，<strong>为了将三维降成二维？</strong>，显示了光源的分布</p><p><strong>Figure2</strong></p><p>![pipeline](颜色恒常性之A Multi-Hypothesis Approach to Color Constancy/image-20201018164327078.png)</p><p><strong><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>如果现在有一张待还原的照片，如何还原，都要生成n个候选光源吗？怎么生成？</strong></p><p>n个候选光源已经生成好了，现在训练网络是需要不同光源的权重配比不同！所以到时候对于待还原照片，还是相同的过程，用每个候选光源修正图片，然后放入网络，得到权重。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>$$<br>\rho_{k}(X)=\int_{\Omega}E(\lambda)S(\lambda,X)C_{k}(\lambda)d\lambda\quad\quad\quad k\in{R,G,B}<br>$$</p><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong>为什么要积分？</strong></p><p>积分是因为比如绿色，打个比方是755~760这个频段的波长共同作用生成的，所以需要进行积分。<br>$$<br>\rho_{k}^E=\int_{\Omega}E(\lambda)C_{k}(\lambda)d\lambda\quad\quad\quad k\in{R,G,B}<br>$$</p><p>The goal of computational CC then becomes estimation of the <u><strong>global illumination color</strong></u>$\rho_k^E$？</p><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>为什么变成了这个形式？</p><blockquote><p>due to the <strong>ill-posed</strong> nature of the problem, <strong>multiple illuminant solutions are often possible with varying probability</strong>.</p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>什么是<strong>ill-posed</strong> ？</p><blockquote><p>avoid <u><strong>distribution shift</strong></u> and <u><strong>resulting domain gap problems</strong></u> [1, 41, 22], associated with camera specific training, and propose a well-founded strategy to leverage multiple data.</p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>什么是<strong>distributin shift&amp;domain gap</strong>？</p><p><strong>distribution shift</strong>: <a href="https://zh.d2l.ai/">https://zh.d2l.ai/</a> </p><p>**domain gap problem:**<a href="https://zhuanlan.zhihu.com/p/195704051">https://zhuanlan.zhihu.com/p/195704051</a></p><blockquote><p>Principled combination of datasets is of high value for learning based color constancy given the typically small nature of individual color constancy datasets (on the order of only hundreds of images).</p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>这句话在说啥？</p><blockquote><p>We provide <strong><u>a training-free model adaptation strategy for new cameras</u></strong>.</p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>加入一个新的摄像机，如何改进模型？</p><p>新加入一个摄像机，只要这个摄像机的候选光源已知了，就可以直接拿这个网络训练了，所以不需要再重新训练或微调。</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="Bayesian-framework"><a href="#Bayesian-framework" class="headerlink" title="Bayesian framework"></a>Bayesian framework</h4><blockquote><p> They <strong>model the prior of the illuminant and the surface reflectance as a <u>truncated multivariate normal distribution</u> on the weights of a linear model</strong></p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>什么是truncated multivariate normal distribution on the weights of a linear model?</p><p>截断正态分布：指限制变量x取值范围(scope)的一种分布。例如，限制x取值在0到50之间，即{0&lt;x&lt;50}。</p><blockquote><p>Bayesian works [44, 23], <strong>discretise the illuminant space</strong> and <strong>model the surface reflectance priors</strong> by <u><strong>learning real world histogram frequencies</strong></u>;</p></blockquote><p>通过学习真实世界的直方图频率，来离散化光源空间和对表面反射率进行先验建模。<strong>可以查看它如何学习真实世界的直方图频率应用到肤色定级。</strong></p><blockquote><p>in [44] the prior is modelled as a <strong><u>uniform distribution over a subset of illuminants</u></strong> while [23] uses the <strong><u>empirical distribution of the training illuminants</u></strong>.</p></blockquote><p><span class="github-emoji"><span>🉑</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f251.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>对于光源概率44和23有两种想法：直接建模成均匀分布和利用训练光源的经验分布。</p><p>经验分布函数：<a href="https://zh.wikipedia.org/zh-hans/%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0#:~:text=%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%EF%BC%88%E8%8B%B1%E8%AA%9E%EF%BC%9Aempirical,%E6%A0%B7%E6%9C%AC%E6%89%80%E5%8D%A0%E7%9A%84%E6%AF%94%E4%BE%8B%E3%80%82">https://zh.wikipedia.org/zh-hans/%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0#:~:text=%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%EF%BC%88%E8%8B%B1%E8%AA%9E%EF%BC%9Aempirical,%E6%A0%B7%E6%9C%AC%E6%89%80%E5%8D%A0%E7%9A%84%E6%AF%94%E4%BE%8B%E3%80%82</a></p><h4 id="Fully-supervised-methods"><a href="#Fully-supervised-methods" class="headerlink" title="Fully supervised methods"></a>Fully supervised methods</h4><blockquote><p>frame color constancy as a classification problem：CCC and FCCC using a color space that identifies image re-illumination with a histogram shift. </p></blockquote><p>CCC和FCCC待看</p><h4 id="Multi-device-training"><a href="#Multi-device-training" class="headerlink" title="Multi-device training"></a>Multi-device training</h4><blockquote><p> [37] affords <strong>fast adaptation to previously unseen cameras</strong>, and robustness to changes in capture device by leveraging annotated samples across different cameras and datasets in a <strong><u>meta-learning</u></strong> framework</p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>meta-learning?</p><blockquote><p>A recent approach [8], makes an assumption that sRGB images collected from the web are well white balanced, therefore, they apply <strong><u>a simple de-gamma correction</u></strong> to approximate an <strong><u>inverse tone mapping</u></strong> and then find achromatic pixels with a CNN to predict the illuminant. </p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>de-gamma correction？inverse tone mapping？</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><blockquote><p>Let y = (yr, yg, yb) be a pixel from an input image Y in <strong><u>linear RGB space</u></strong>. </p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>线性RGB空间？</p><p><a href="https://www.cnblogs.com/guanzz/p/7416821.html">https://www.cnblogs.com/guanzz/p/7416821.html</a></p><blockquote><p>We model the global illumination, Eq. (2), with the <strong><u>standard linear model</u></strong> [51] such that each pixel y is the product of the surface reflectance r = (rr, rg, rb) and a global illuminant ? = (?r, ?g, ?b) shared by all pixels such that</p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>标准线性模型？</p><p>可能就是三个函数相乘得到一个线性模型？</p><blockquote><p>we propose to frame the CC problem with a <strong><u>probabilistic generative model</u></strong> with unknown surface re- flectances and illuminant</p></blockquote><p><span class="github-emoji"><span>❔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2754.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>概率生成模型？</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3>]]></content>
      
      
      <categories>
          
          <category> Color Constancy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CC/AWB </tag>
            
            <tag> CCC </tag>
            
            <tag> FCCC </tag>
            
            <tag> Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人脸识别与肤色分类及dlib库</title>
      <link href="ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/"/>
      <url>ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/</url>
      
        <content type="html"><![CDATA[<h2 id="人脸识别pipeline"><a href="#人脸识别pipeline" class="headerlink" title="人脸识别pipeline"></a>人脸识别pipeline</h2><p>论文参考《Modern Face Recognition with Deep Learning》，其实整篇论文与<a href="https://link.zhihu.com/?target=https://medium.com/@ageitgey?source=post_header_lockup">Adam Geitgey</a>写的文章一致，中文链接如下：<a href="https://zhuanlan.zhihu.com/p/24567586">https://zhuanlan.zhihu.com/p/24567586</a></p><h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-0.png" alt="人脸识别流程图" style="zoom:67%;"><h3 id="Step-1-arrow-heading-down-：人脸检测-Face-Detection"><a href="#Step-1-arrow-heading-down-：人脸检测-Face-Detection" class="headerlink" title="Step 1:arrow_heading_down:：人脸检测-Face Detection"></a>Step 1<span class="github-emoji"><span>⤵</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2935.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>：人脸检测-Face Detection</h3><blockquote><p>​    我们使用**方向梯度直方图(HOG)**来标定人脸位置。</p><p>​    要在一张图片中找到脸，我们首先将图像转换为黑白，因为我们并不需要颜色数据来找到脸。</p><p>​    然后，我们将查看图片中的每一个像素。 对于单个像素，我们也要查看它周围的其他像素。我们的目标是找出并比较当前像素与直接围绕它的像素的深度。 然后我们要画一个箭头来代表图像变暗的方向，对图片中的每一个像素重复这个过程，最终每个像素会被一个箭头取代。这些箭头被称为梯度（gradients），它们能显示出图像上从明亮到黑暗的流动过程。</p><p>​    为了获取更泛化的特征，将图像分隔成16x16的小方块，统计每个主方向上有多少个梯度，最后用梯度最多的主方向代替原来小方块。<br>HOG主要优点就是减轻了图像明暗度的影响，获得的是像素之间的相对特征。</p></blockquote><p><strong>代码注意：</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#dlib  get_frontal_face_detector():生成HOG面部检测器 detector(image,1)1表示采样次数，越大识别出来的脸越多，小脸也能识别到，上采样也就是图像放大倍数</span><span class="token comment" spellcheck="true">#dlib.image_window：a gui window that can show image  win.set_image(image) </span><span class="token comment" spellcheck="true">#win.add_overlay(dlib.dlib.rectangle)：Add a list of rectangles to the image_window. They will be displayed as red boxes by default, but the color can be passed.</span><span class="token comment" spellcheck="true">#from skimage import io   io.imread()存储的图片是&lt;class 'ndarray'></span><span class="token comment" spellcheck="true">#'{}'.format 注意使用</span><span class="token comment" spellcheck="true">#enumrate    for i,data in enumerate():</span></code></pre><p><strong>图片示例：</strong></p><p><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-20201014155202049.png" alt="Figure 1：一张人脸示例"></p><p><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-20201014155225990.png" alt="Figure 2：HOG标定人脸位置示例"></p><h3 id="Step-II-arrow-heading-down-：人脸对齐-Face-Alignment"><a href="#Step-II-arrow-heading-down-：人脸对齐-Face-Alignment" class="headerlink" title="Step II:arrow_heading_down:：人脸对齐-Face Alignment"></a>Step II<span class="github-emoji"><span>⤵</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2935.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>：人脸对齐-Face Alignment</h3><blockquote><p>把图片中的脸部分离出来之后，我们将试图扭曲每个图片，使得眼睛和嘴唇总是在图像中的样本位置（sample place）。 这将使我们在接下来的步骤中，更容易比较脸部之间的不同。这被称为人脸对齐任务。</p><p>我们使用一种称为面部特征点估计(face landmark estimation)的算法(使用由 瓦希德·卡奇米Vahid Kazemi和约瑟菲娜·沙利文Josephine Sullivan在2014 年发明的方法)。</p><p>这一算法的基本思路是找到 68 个人脸上普遍存在的特定点（称为特征点， landmarks）——包括下巴的顶部、每只眼睛的外部轮廓、每条眉毛的内部轮廓等。</p><p>获得眼睛和嘴巴的位置后，我们使用仿射变换使得眼睛和嘴巴向中间挪动到大致相同的位置，这将使我们的下一步更加准确。</p></blockquote><p><strong>tips：</strong></p><blockquote><p>将试图扭曲每个图片，使得眼睛和嘴唇总是在图像中的样本位置（sample place）</p><p>face landmark estimation:68 landmarks</p><p>旋转、缩放和错切(shear mapping)</p></blockquote><p><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-1.png" alt="shear mapping"></p><blockquote><p>水平错切：</p><p>Affine transformation：仿射变换，简单来说就是：一个能保持线和平行但是不保持距离和角度的几何变换(维基百科：<a href="https://en.wikipedia.org/wiki/Affine_transformation">https://en.wikipedia.org/wiki/Affine_transformation</a>)</p></blockquote><p><strong>代码注意：</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#_init_(*args, **kwargs)  *args:*args 用来将参数打包成tuple给函数体调用    **kwargs: **kwargs 打包关键字参数成dict给函数体调用</span><span class="token keyword">def</span> <span class="token function">function</span><span class="token punctuation">(</span>arg<span class="token punctuation">,</span><span class="token operator">*</span>args<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>arg<span class="token punctuation">,</span><span class="token operator">*</span>args<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>function<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span>a<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>b<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>c<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#dlib.shape_predictor(predictor_model)   Loads a shape_predictor from a file that contains the output of the train_shape_predictor() routine.</span><span class="token comment" spellcheck="true">#pose_predictor(image, face_rect)   #__call__(self: dlib.shape_predictor, image: array, box: dlib.rectangle) → dlib.full_object_detection</span><span class="token comment" spellcheck="true">#dlib.full_object_detection:This object represents the location of an object in an image along with the positions of each of its constituent parts  </span>  <span class="token comment" spellcheck="true">#__init__(self: dlib.full_object_detection, rect: dlib.rectangle, parts: object) → None </span>        <span class="token comment" spellcheck="true">#rect: dlib rectangle</span>        <span class="token comment" spellcheck="true">#parts: list of dlib.point, or a dlib.points object</span></code></pre><p><strong>图片示例:</strong></p><p><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-20201014160336556.png" alt="Figure 3：68个特征点位置"></p><p><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-20201014160355457.png" alt="Figure 4：对齐示例"></p><h3 id="Step-3-arrow-heading-down-面部编码-Face-Encoding"><a href="#Step-3-arrow-heading-down-面部编码-Face-Encoding" class="headerlink" title="Step 3:arrow_heading_down::面部编码-Face Encoding"></a>Step 3<span class="github-emoji"><span>⤵</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2935.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>:面部编码-Face Encoding</h3><blockquote><p>我们利用resnet网络对大量人脸图片进行训练，每次训练要观察三个不同的脸部图像：1. 加载一张已知的人的面部训练图像;2. 加载同一个人的另一张照片;3. 加载另外一个人的照片。该网络最终可获得对人脸的128维特征值。</p></blockquote><p><strong>tips:</strong></p><blockquote><p>未知图片和已知图片循环比对，最简单但是数据量过大时太慢，不现实</p></blockquote><p><strong>图片示例</strong>:</p><p><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-20201014161438279.png" alt="训练示例"></p><h3 id="Step-4-arrow-heading-down-肤色分类-Race-Classification"><a href="#Step-4-arrow-heading-down-肤色分类-Race-Classification" class="headerlink" title="Step 4:arrow_heading_down::肤色分类-Race Classification"></a>Step 4<span class="github-emoji"><span>⤵</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2935.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>:肤色分类-Race Classification</h3><blockquote><p>​    利用前面获得resnet网络模型，对LFWA+数据集的所有图片进行编码，利用简单的MLP(多层感知器),对所有编码进行训练，然后可以根据MLP对不同图片进行肤色分类，肤色分为三个等级，分别为Asian、White、Black。</p></blockquote><h2 id="相关库学习："><a href="#相关库学习：" class="headerlink" title="相关库学习："></a>相关库学习：</h2><p><strong>库地址：</strong></p><blockquote><p>gender&amp;face classification：<a href="https://github.com/wondonghyeon/face-classification">https://github.com/wondonghyeon/face-classification</a></p><p>face_recognition：<a href="https://github.com/ageitgey/face_recognition/">https://github.com/ageitgey/face_recognition/</a></p><p>dlib ：<a href="http://dlib.net/python/index.html">http://dlib.net/python/index.html</a> Python Api</p></blockquote><h3 id="face-recognition库"><a href="#face-recognition库" class="headerlink" title="face_recognition库"></a>face_recognition库</h3><p><strong>代码注意：</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#face_recognition api:https://github.com/ageitgey/face_recognition/blob/55b5c136292dcd4a1f5953f3eb3181235086efab/face_recognition/api.py#L92</span><span class="token comment" spellcheck="true">#基本就是对dlib库进行了再封装，用的方法也就是以上人脸识别pipeline的过程</span><span class="token comment" spellcheck="true">#compute_face_descriptor(self: dlib.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),uint8], face: dlib.full_object_detection, num_jitters: int=0L, padding: float=0.25) -> dlib.vector</span><span class="token comment" spellcheck="true">#Takes an image and a full_object_detection that references a face in that image and converts it into a 128D face descriptor. If num_jitters>1 then each face will be randomly jittered slightly num_jitters times, each run through the 128D projection, and the average used as the face descriptor. Optionally allows to override default padding of 0.25 around the face.</span></code></pre><h3 id="gender-amp-face-classification"><a href="#gender-amp-face-classification" class="headerlink" title="gender&amp;face classification"></a>gender&amp;face classification</h3><p><strong>tips:</strong></p><blockquote><p>代码基本就是调的face_recognition库，face_recognition库又是封装的dlib库</p><p>最关键的是使用的数据集LFWA+，73分类</p><p>128维特征值，使用的模型是dlib_face_recognition_resnet_model_v1.dat <a href="https://github.com/ageitgey/face_recognition_models/tree/master/face_recognition_models/models">https://github.com/ageitgey/face_recognition_models/tree/master/face_recognition_models/models</a> ，该模型是dlib使用resnet训练生成的</p></blockquote><p><strong>图片示例:</strong></p><p><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-20201012212241700.png" alt="image-20201012212241700"></p><h2 id="GUI编写"><a href="#GUI编写" class="headerlink" title="GUI编写"></a>GUI编写</h2><h3 id="tkinter"><a href="#tkinter" class="headerlink" title="tkinter"></a>tkinter</h3><blockquote><p>大神博客：<a href="https://sunhwee.com/posts/80fa3a85.html#toc-heading-15">https://sunhwee.com/posts/80fa3a85.html#toc-heading-15</a></p><p>API(推荐)：<a href="https://web.archive.org/web/20190524140835/https://infohost.nmt.edu/tcc/help/pubs/tkinter/web/index.html">https://web.archive.org/web/20190524140835/https://infohost.nmt.edu/tcc/help/pubs/tkinter/web/index.html</a></p><p>还可以使用ttk，各种基本组件会变得更好看一点，上面api也有，</p></blockquote><h3 id="关于tkinter界面下点击按钮未响应的解决办法？"><a href="#关于tkinter界面下点击按钮未响应的解决办法？" class="headerlink" title="关于tkinter界面下点击按钮未响应的解决办法？"></a>关于tkinter界面下点击按钮未响应的解决办法？</h3><blockquote><p>参考：<a href="https://blog.csdn.net/aheress/article/details/105059955?utm_medium=distribute.pc_relevant.none-task-blog-OPENSEARCH-3.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-OPENSEARCH-3.channel_param">https://blog.csdn.net/aheress/article/details/105059955?utm_medium=distribute.pc_relevant.none-task-blog-OPENSEARCH-3.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-OPENSEARCH-3.channel_param</a></p></blockquote><h3 id="展示："><a href="#展示：" class="headerlink" title="展示："></a>展示：</h3><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-20201019151603976.png" alt="检测前" style="zoom:50%;"><img src="/ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/image-20201019151714317.png" alt="检测后" style="zoom:50%;">]]></content>
      
      
      <categories>
          
          <category> 人脸识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> face_recognition </tag>
            
            <tag> 肤色分类 </tag>
            
            <tag> tkinter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo-git-github博客搭建</title>
      <link href="hexo-git-github-bo-ke-da-jian/"/>
      <url>hexo-git-github-bo-ke-da-jian/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考：洪卫的博客教程<a href="https://sunhwee.com/posts/6e8839eb.html">https://sunhwee.com/posts/6e8839eb.html</a></p><p>参考：hexo-theme-matery主题<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md#%E9%85%8D%E7%BD%AE%E5%9F%BA%E6%9C%AC%E8%8F%9C%E5%8D%95%E5%AF%BC%E8%88%AA%E7%9A%84%E5%90%8D%E7%A7%B0%E8%B7%AF%E5%BE%84url%E5%92%8C%E5%9B%BE%E6%A0%87icon">https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md#%E9%85%8D%E7%BD%AE%E5%9F%BA%E6%9C%AC%E8%8F%9C%E5%8D%95%E5%AF%BC%E8%88%AA%E7%9A%84%E5%90%8D%E7%A7%B0%E8%B7%AF%E5%BE%84url%E5%92%8C%E5%9B%BE%E6%A0%87icon</a></p><p>参考：过客～励む的博客<a href="https://yafine-blog.cn/posts/4ab2.html">https://yafine-blog.cn/posts/4ab2.html</a></p></blockquote><h2 id="按流程搭建遇到的问题："><a href="#按流程搭建遇到的问题：" class="headerlink" title="按流程搭建遇到的问题："></a>按流程搭建遇到的问题：</h2><h3 id="搭建hexo博客时，到了最后一步，hexo-s后只出现代码，而不是首页？"><a href="#搭建hexo博客时，到了最后一步，hexo-s后只出现代码，而不是首页？" class="headerlink" title="搭建hexo博客时，到了最后一步，hexo s后只出现代码，而不是首页？"></a>搭建hexo博客时，到了最后一步，hexo s后只出现代码，而不是首页？</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012224004113.png" alt="错误代码"></p><p><strong>在npm install安装依赖时出现了错误</strong></p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012224051241.png" alt="错误为第10行"></p><p>仔细查看错误信息，我们不难发现是ejs出现了问题。我们可以先执行以下代码后再继续后续操作。</p><pre><code>npm install ejs@2.7.4 --ignore-scripts</code></pre><p><strong><em>注意：之后所有的Bash命令都在最后一个MyBlog文件夹下操作，也就是你之前安装hexo那个文件夹！</em></strong></p><h3 id="什么是github-io？"><a href="#什么是github-io？" class="headerlink" title="什么是github.io？"></a>什么是github.io？</h3><blockquote><p>官网的一句话来形容 Websites for you and your projects</p></blockquote><h3 id="购买个人域名之后打开失败？"><a href="#购买个人域名之后打开失败？" class="headerlink" title="购买个人域名之后打开失败？"></a>购买个人域名之后打开失败？</h3><blockquote><p>极有可能是你未设置域名解析！</p></blockquote><h3 id="写文章发布文章不生成文件夹及图片无法显示？"><a href="#写文章发布文章不生成文件夹及图片无法显示？" class="headerlink" title="写文章发布文章不生成文件夹及图片无法显示？"></a>写文章发布文章不生成文件夹及图片无法显示？</h3><h4 id="不生成文件夹？"><a href="#不生成文件夹？" class="headerlink" title="不生成文件夹？"></a>不生成文件夹？</h4><p>首先，新建博客一定要用hexo new post命令，不然很多信息识别不出来</p><p>然后将_config.yml文件中的post asset folder设置为true，之后会出现文件夹</p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012224940500.png" alt="设置为true"></p><h4 id="图片不显示？"><a href="#图片不显示？" class="headerlink" title="图片不显示？"></a>图片不显示？</h4><p>首先下载依赖</p><pre><code>npm install hexo-asset-image --save</code></pre><p>然后对于typora编辑，偏好设置为：</p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012225346785.png" alt="image-20201012225346785" style="zoom: 80%;"><p>然后图片编写时，使用相对路径，例如：</p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012225527582.png" alt="例子"></p><p><strong><em>另外注意：千万不要错误使用转义符’\‘!!!!</em></strong></p><h3 id="数学公式块无法正常显示？"><a href="#数学公式块无法正常显示？" class="headerlink" title="数学公式块无法正常显示？"></a>数学公式块无法正常显示？</h3><blockquote><p>后面配置了主题就可以了！</p><p>但是注意：<strong>数学公式中如果出现了连续两个{，中间一定要加空格！</strong></p></blockquote><h3 id="菜单导航配置在哪？"><a href="#菜单导航配置在哪？" class="headerlink" title="菜单导航配置在哪？"></a>菜单导航配置在哪？</h3><blockquote><p>菜单导航配置在themes/hexo-theme-matert/__config.yml</p></blockquote><h3 id="什么是RSS订阅？"><a href="#什么是RSS订阅？" class="headerlink" title="什么是RSS订阅？"></a>什么是RSS订阅？</h3><blockquote><p>​        RSS也称为RSS订阅或RSS提要，博客和新闻网站的一个常见做法是联合其内容。Web联合是指来自网站的内容可供其他站点或远程应用程序使用。Web联合的最常用方法是使用称为<strong>ReallySimpleSyndication</strong>的协议。RSS是一种协议，允许网站将其内容或其部分内容提供给其他网站或应用程序。</p></blockquote><h3 id="DaoVoice"><a href="#DaoVoice" class="headerlink" title="DaoVoice?"></a>DaoVoice?</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013144940429.png" alt="设置"></p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013144951426.png" alt="设置"></p><h3 id="新建文章模板修改失败？"><a href="#新建文章模板修改失败？" class="headerlink" title="新建文章模板修改失败？"></a>新建文章模板修改失败？</h3><blockquote><p>莫名其妙post.md上下都变成了两个—，奇怪</p></blockquote><h3 id="修改页脚"><a href="#修改页脚" class="headerlink" title="修改页脚?"></a>修改页脚?</h3><h3 id="修改社交链接？"><a href="#修改社交链接？" class="headerlink" title="修改社交链接？"></a>修改社交链接？</h3><h3 id="不蒜子？不蒜子访问量和人数无法区分问题？"><a href="#不蒜子？不蒜子访问量和人数无法区分问题？" class="headerlink" title="不蒜子？不蒜子访问量和人数无法区分问题？"></a>不蒜子？不蒜子访问量和人数无法区分问题？</h3><blockquote><p>是一个极简网页计数器</p></blockquote><h3 id="添加动漫人物"><a href="#添加动漫人物" class="headerlink" title="添加动漫人物?"></a>添加动漫人物?</h3><blockquote><p>由于一直要安包，不敢继续弄了</p></blockquote><h3 id="gitalk-error-not-found？"><a href="#gitalk-error-not-found？" class="headerlink" title="gitalk error not found？"></a>gitalk error not found？</h3><blockquote><p><del>既有可能是yml的设置错误！</del></p><p><del><strong>owner和admin都填写github用户名</strong>，<strong>repo填我们的博客github仓库名</strong></del></p><p>问题终于解决！</p><img src="/hexo-git-github-bo-ke-da-jian/image-20201018215249728.png" alt="config" style="zoom:80%;"><img src="/hexo-git-github-bo-ke-da-jian/image-20201018215408171.png" alt="outh app" style="zoom: 50%;"></blockquote><h3 id="yml、yaml格式不正确？"><a href="#yml、yaml格式不正确？" class="headerlink" title="yml、yaml格式不正确？"></a>yml、yaml格式不正确？</h3><blockquote><p>使用这个在线校验器校验：<a href="http://www.bejson.com/validators/yaml_editor/">http://www.bejson.com/validators/yaml_editor/</a></p></blockquote><h3 id="gitalk未找到相关issues？"><a href="#gitalk未找到相关issues？" class="headerlink" title="gitalk未找到相关issues？"></a>gitalk未找到相关issues？</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013183732541.png" alt="URL"></p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013183843079.png" alt="回调函数"></p><blockquote><p>一定要<strong>使用https而不是http</strong></p><p>还是不行？？？</p><p>失败！</p></blockquote><h3 id="樱花飘落？"><a href="#樱花飘落？" class="headerlink" title="樱花飘落？"></a>樱花飘落？</h3><blockquote><p>暂时配置失败！</p></blockquote><h3 id="网站根目录在哪里？"><a href="#网站根目录在哪里？" class="headerlink" title="网站根目录在哪里？"></a>网站根目录在哪里？</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013200031772.png" alt="网站根目录"></p><h3 id="如何删除文章："><a href="#如何删除文章：" class="headerlink" title="如何删除文章："></a>如何删除文章：</h3><p>先hexo clean，然后在直接删除，如果不hexo clean的话，还是会再生成。</p><h2 id="git学习"><a href="#git学习" class="headerlink" title="git学习:"></a>git学习:</h2><blockquote><p>廖雪峰git教程 <a href="https://www.liaoxuefeng.com/wiki/896043488029600">https://www.liaoxuefeng.com/wiki/896043488029600</a></p></blockquote><p>git add先将文件放到车里，git commit把一车的东西运往其他城市。</p><p>Git的<code>commit id</code>不是1，2，3……递增的数字，而是一个SHA1计算出来的一个非常大的数字，用十六进制表示。为什么<code>commit id</code>需要用这么一大串数字表示呢？因为Git是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。</p><h2 id="js、ejs学习："><a href="#js、ejs学习：" class="headerlink" title="js、ejs学习："></a>js、ejs学习：</h2><blockquote><p>js:<a href="https://www.runoob.com/js/js-tutorial.html">https://www.runoob.com/js/js-tutorial.html</a></p></blockquote><p>JavaScript 是<strong>脚本语言，浏览器会在读取代码时，逐行地执行脚本代码</strong>。而对于传统编程来说，会在执行前对所有代码进行编译。</p><p>对象最好使用**.**来调用，防止方法调用失败！</p><h2 id="博客编写规范"><a href="#博客编写规范" class="headerlink" title="博客编写规范"></a>博客编写规范</h2><p><strong>不做内容的搬运工！</strong>而是</p><ol><li><strong>记录遇到的问题，只针对问题进行解答</strong></li><li><strong>对于组会汇报内容进行详细编写</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> 博客搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> git </tag>
            
            <tag> ejs </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
