<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CV模型训练流程总结-Pytorch Lightning(一)</title>
      <link href="lightning/"/>
      <url>lightning/</url>
      
        <content type="html"><![CDATA[<h2 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h2><img src="/lightning/image-20240405232843132.png" alt="概览图" style="zoom:30%;"><pre class=" language-shell"><code class="language-shell"># 示例运行命令如下python tools/run.py fit --config configs/cityscapes_darkzurich/refign_daformer.yaml --trainer.gpus [0] --trainer.precision 16</code></pre><p>模型的入口，即<code>run.py</code>其实是实例化了一个参数解析器，Lightning自己改进python原始的argparse，即<code>LightningCLI</code>，这个参数解析器既可以从<strong>命令行</strong>，也可以使用<strong>yaml</strong>获取模型、数据集、trainer的参数。</p><p><code>fit</code>是训练+验证的子命令，还有<code>validate</code>、<code>test</code>、<code>predict</code>，用来分离不同的训练阶段。整体的逻辑大概是<code>LightningCLI</code>解析参数后，框架根据参数实例化<code>trainer</code>，<code>trainer</code>再根据<code>fit</code>还是<code>validate</code>等执行对应的训练逻辑，包括数据的处理和加载，模型的前向传播、反向传播、梯度更新等，最后利用<code>Logger</code>来记录试验结果，利用<code>Callback</code>来执行回调函数如<code>EarlyStopping</code>等。</p><h2 id="入口-LightningCLI"><a href="#入口-LightningCLI" class="headerlink" title="入口-LightningCLI"></a>入口-LightningCLI</h2><p>python内置的ArgumentParser使得代码的运行可以带有命令行参数，但是在大型项目中很不方便，因为<strong>参数会非常</strong>多，无论是在python代码里配置参数，还是在命令行指定参数都很复杂</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> argparse <span class="token keyword">import</span> ArgumentParserparser <span class="token operator">=</span> ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Trainer arguments</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--devices"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Hyperparameters for the model</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--layer_1_dim"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Parse the user inputs and defaults (returns a argparse.Namespace)</span>args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Use the parsed arguments in your program</span>trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>devices<span class="token operator">=</span>args<span class="token punctuation">.</span>devices<span class="token punctuation">)</span>model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span>layer_1_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>layer_1_dim<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 运行</span>python trainer<span class="token punctuation">.</span>py <span class="token operator">-</span><span class="token operator">-</span>layer_1_dim <span class="token number">64</span> <span class="token operator">-</span><span class="token operator">-</span>devices <span class="token number">1</span></code></pre><p>为此，pytorch Lightning提供了LightningCLI，使得一切参数可以用，简单实现CLI的方式</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># main.py</span><span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>cli <span class="token keyword">import</span> LightningCLI<span class="token comment" spellcheck="true"># simple demo classes for your convenience</span><span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>demos<span class="token punctuation">.</span>boring_classes <span class="token keyword">import</span> DemoModel<span class="token punctuation">,</span> BoringDataModule<span class="token keyword">def</span> <span class="token function">cli_main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    cli <span class="token operator">=</span> LightningCLI<span class="token punctuation">(</span>DemoModel<span class="token punctuation">,</span> BoringDataModule<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># note: don't call fit!!</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    cli_main<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># note: it is good practice to implement the CLI in a function and call it in the main if block</span></code></pre><p>可以查看帮助<code>python main.py --help</code>   </p><p><code>python main.py [subcommand] --help</code> subcommand指fit、validate、test、predict，根据你的训练需求来执行，这个命令能查看Lightning module和lightningDataModule的参数</p><p>With the Lightning CLI enabled, you can now change the parameters without touching your code:<code>python main.py fit --model.learning_rate 0.1</code>，当然也可以使用config来覆盖</p><blockquote><p>[!TIP]</p><p>The options that become available in the CLI are the <code>__init__</code> parameters of the <code>LightningModule</code> and <code>LightningDataModule</code> classes. Thus, to make hyperparameters configurable, just add them to your class’s <code>__init__</code>. It is highly recommended that these parameters are described in the docstring so that the CLI shows them in the help. Also, the parameters should have accurate type hints so that the CLI can fail early and give understandable error messages when incorrect values are given.</p></blockquote><p><strong>使用cli混合不同的模型和数据集</strong></p><p>这个要替换的是如下的切换模型或者数据集的逻辑：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># choose model</span><span class="token keyword">if</span> args<span class="token punctuation">.</span>model <span class="token operator">==</span> <span class="token string">"gan"</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> GAN<span class="token punctuation">(</span>args<span class="token punctuation">.</span>feat_dim<span class="token punctuation">)</span><span class="token keyword">elif</span> args<span class="token punctuation">.</span>model <span class="token operator">==</span> <span class="token string">"transformer"</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> Transformer<span class="token punctuation">(</span>args<span class="token punctuation">.</span>feat_dim<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># choose datamodule</span><span class="token keyword">if</span> args<span class="token punctuation">.</span>data <span class="token operator">==</span> <span class="token string">"MNIST"</span><span class="token punctuation">:</span>    datamodule <span class="token operator">=</span> MNIST<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">elif</span> args<span class="token punctuation">.</span>data <span class="token operator">==</span> <span class="token string">"imagenet"</span><span class="token punctuation">:</span>    datamodule <span class="token operator">=</span> Imagenet<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># mix them!</span>trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">,</span> datamodule<span class="token punctuation">)</span></code></pre><p>通过cli只需这样，省略掉<code>model_class</code>这个参数，DataModule类似:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># main.py</span><span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>cli <span class="token keyword">import</span> LightningCLI<span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>demos<span class="token punctuation">.</span>boring_classes <span class="token keyword">import</span> DemoModel<span class="token punctuation">,</span> BoringDataModule<span class="token keyword">class</span> <span class="token class-name">Model1</span><span class="token punctuation">(</span>DemoModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"⚡"</span><span class="token punctuation">,</span> <span class="token string">"using Model1"</span><span class="token punctuation">,</span> <span class="token string">"⚡"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>configure_optimizers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Model2</span><span class="token punctuation">(</span>DemoModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"⚡"</span><span class="token punctuation">,</span> <span class="token string">"using Model2"</span><span class="token punctuation">,</span> <span class="token string">"⚡"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>configure_optimizers<span class="token punctuation">(</span><span class="token punctuation">)</span>cli <span class="token operator">=</span> LightningCLI<span class="token punctuation">(</span>datamodule_class<span class="token operator">=</span>BoringDataModule<span class="token punctuation">)</span></code></pre><blockquote><p>[!TIP]</p><p>Instead of omitting the <code>datamodule_class</code> parameter, you can give a base class and <code>subclass_mode_data=True</code>. This will make the CLI <strong>only accept data modules that are a subclass of the given base class</strong>.</p></blockquote><pre class=" language-shell"><code class="language-shell"># use Model1python main.py fit --data FakeDataset1# use Model2python main.py fit --data FakeDataset2</code></pre><p><strong>使用cli混合不同的优化器和lr_scheduler</strong></p><p>Any custom subclass of <a href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer"><code>torch.optim.Optimizer</code></a> can be used as an optimizer:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># main.py</span><span class="token keyword">import</span> torch<span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>cli <span class="token keyword">import</span> LightningCLI<span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>demos<span class="token punctuation">.</span>boring_classes <span class="token keyword">import</span> DemoModel<span class="token punctuation">,</span> BoringDataModule<span class="token keyword">class</span> <span class="token class-name">LitAdam</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> closure<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"⚡"</span><span class="token punctuation">,</span> <span class="token string">"using LitAdam"</span><span class="token punctuation">,</span> <span class="token string">"⚡"</span><span class="token punctuation">)</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>step<span class="token punctuation">(</span>closure<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">FancyAdam</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> closure<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"⚡"</span><span class="token punctuation">,</span> <span class="token string">"using FancyAdam"</span><span class="token punctuation">,</span> <span class="token string">"⚡"</span><span class="token punctuation">)</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>step<span class="token punctuation">(</span>closure<span class="token punctuation">)</span>cli <span class="token operator">=</span> LightningCLI<span class="token punctuation">(</span>DemoModel<span class="token punctuation">,</span> BoringDataModule<span class="token punctuation">)</span></code></pre><p><strong>学习率策略</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># main.py</span><span class="token keyword">import</span> torch<span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>cli <span class="token keyword">import</span> LightningCLI<span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>demos<span class="token punctuation">.</span>boring_classes <span class="token keyword">import</span> DemoModel<span class="token punctuation">,</span> BoringDataModule<span class="token keyword">class</span> <span class="token class-name">LitLRScheduler</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>CosineAnnealingLR<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"⚡"</span><span class="token punctuation">,</span> <span class="token string">"using LitLRScheduler"</span><span class="token punctuation">,</span> <span class="token string">"⚡"</span><span class="token punctuation">)</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>cli <span class="token operator">=</span> LightningCLI<span class="token punctuation">(</span>DemoModel<span class="token punctuation">,</span> BoringDataModule<span class="token punctuation">)</span></code></pre><p>接受Standard learning rate schedulers from <code>torch.optim.lr_scheduler</code>，必须先指定optimizer才能指定学习率策略</p><p><strong>Classes from any package</strong></p><p>在前面的部分中，要选择的自定义类是在运行 LightningCLI 类的同一 python 文件中定义的。 要仅使用类名从任何包中选择类，请导入相应的包：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>cli <span class="token keyword">import</span> LightningCLI<span class="token keyword">import</span> my_code<span class="token punctuation">.</span>models  <span class="token comment" spellcheck="true"># noqa: F401</span><span class="token keyword">import</span> my_code<span class="token punctuation">.</span>data_modules  <span class="token comment" spellcheck="true"># noqa: F401</span><span class="token keyword">import</span> my_code<span class="token punctuation">.</span>optimizers  <span class="token comment" spellcheck="true"># noqa: F401</span>cli <span class="token operator">=</span> LightningCLI<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><strong>Help for specific classes</strong></p><p>当接受多个模型或数据集时，CLI 的主要帮助不包括其特定参数，需要额外指定，例如<code>python main.py fit --model.help Model1</code></p><p><strong>Control it all via YAML</strong></p><p>随着项目变得越来越复杂，<strong>可配置选项的数量变得非常大</strong>，使得通过单独的命令行参数进行控制变得不方便。 为了解决这个问题，使用 LightningCLI 实现的 CLI 始终支持从配置文件接收输入。 配置文件使用的默认格式是 <strong>YAML</strong>。</p><p><code>python main.py fit --config config.yaml --trainer.max_epochs 100</code>单独的参数会覆盖config里面的设置</p><p>Lightning会自动在日志里保存配置参数，从而帮助模型的<strong>可复现性</strong>。这是通过<code>SaveConfigCallback</code>实现的，该回调函数被自动加入<code>Trainer</code>，可通过参数设置调节其行为。</p><p><code>python main.py fit --config lightning_logs/version_7/config.yaml</code></p><p><strong>从头编写yaml配置可能很复杂</strong>，可以使用<code>python main.py fit --print_config</code>打印配置再进行修改。对于<strong>混合模型</strong>，需要额外指定，如下<code>python main.py fit --model DemoModel --print_config</code></p><p>配置项可以是<strong>简单的 Python 对象（例如 int 和 str）</strong>，也可以是由 <code>class_path</code>和<code>init_args</code>参数组成的复杂对象。<code>class_path</code>是指<strong>项目类的完整导入路径</strong>，而<code>init_args</code>是要<strong>传递给类构造函数的参</strong>数。 例如：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># model.py</span><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>L<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> criterion<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>criterion <span class="token operator">=</span> criterion<span class="token comment" spellcheck="true"># config.yaml</span>model<span class="token punctuation">:</span>  class_path<span class="token punctuation">:</span> model<span class="token punctuation">.</span>MyModel  init_args<span class="token punctuation">:</span>    criterion<span class="token punctuation">:</span>      class_path<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CrossEntropyLoss      init_args<span class="token punctuation">:</span>        reduction<span class="token punctuation">:</span> mean    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code></pre><p>LightningCLI 在底层使用<code>jsonargparse</code>来<strong>解析配置文件</strong>并<strong>自动创建对象</strong>，无需额外创建对象，也无需在<code>cli.py</code>去<strong>导入对应的</strong>类了。</p><p><strong>使用多个配置文件，按顺序解析</strong>。</p><pre class=" language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># config_1.yaml</span><span class="token key atrule">trainer</span><span class="token punctuation">:</span>  <span class="token key atrule">num_epochs</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token punctuation">...</span><span class="token comment" spellcheck="true"># config_2.yaml</span><span class="token key atrule">trainer</span><span class="token punctuation">:</span>  <span class="token key atrule">num_epochs</span><span class="token punctuation">:</span> <span class="token number">20 </span>  <span class="token punctuation">...</span><span class="token comment" spellcheck="true"># python main.py fit --config config_1.yaml --config config_2.yaml </span><span class="token comment" spellcheck="true"># The value from the last config will be used</span></code></pre><p><strong>使用参数组</strong></p><pre class=" language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># trainer.yaml</span><span class="token key atrule">num_epochs</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token comment" spellcheck="true"># model.yaml</span><span class="token key atrule">out_dim</span><span class="token punctuation">:</span> <span class="token number">7</span><span class="token comment" spellcheck="true"># data.yaml</span><span class="token key atrule">data_dir</span><span class="token punctuation">:</span> ./data<span class="token comment" spellcheck="true"># $ python main.py fit --trainer trainer.yaml --model model.yaml --data data.yaml [...]</span></code></pre><p><strong>Multiple models and/or datasets</strong></p><p>CLI 可以编写为<strong>由导入路径和初始化参数</strong>指定<strong>模型和/或数据模块</strong>。 例如，使用实现为以下形式的工具：</p><pre class=" language-python"><code class="language-python">cli <span class="token operator">=</span> LightningCLI<span class="token punctuation">(</span>MyModelBaseClass<span class="token punctuation">,</span> MyDataModuleBaseClass<span class="token punctuation">,</span> subclass_mode_model<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> subclass_mode_data<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><p>可能配置如下：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">model</span><span class="token punctuation">:</span>  <span class="token key atrule">class_path</span><span class="token punctuation">:</span> mycode.mymodels.MyModel  <span class="token key atrule">init_args</span><span class="token punctuation">:</span>    <span class="token key atrule">decoder_layers</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token number">2</span>    <span class="token punctuation">-</span> <span class="token number">4</span>    <span class="token key atrule">encoder_layers</span><span class="token punctuation">:</span> <span class="token number">12</span><span class="token key atrule">data</span><span class="token punctuation">:</span>  <span class="token key atrule">class_path</span><span class="token punctuation">:</span> mycode.mydatamodules.MyDataModule  <span class="token key atrule">init_args</span><span class="token punctuation">:</span>    <span class="token punctuation">...</span><span class="token key atrule">trainer</span><span class="token punctuation">:</span>  <span class="token key atrule">callbacks</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">class_path</span><span class="token punctuation">:</span> lightning.pytorch.callbacks.EarlyStopping      <span class="token key atrule">init_args</span><span class="token punctuation">:</span>        <span class="token key atrule">patience</span><span class="token punctuation">:</span> <span class="token number">5</span>    <span class="token punctuation">...</span></code></pre><p>仅允许使用属于 <code>MyModelBaseClass </code>子类的模型类，同样，仅允许使用<code> MyDataModuleBaseClass 的</code>子类。 如果给出 <code>LightningModule </code>和 <code>LightningDataModule</code> 基类，则 CLI 将允许任何Lightning模块和数据模块。</p><p><strong>Models with multiple submodules</strong></p><p>许多用例需要有<strong>多个模块</strong>，每个模块都有自己的可配置选项。 使用 LightningCLI 处理此问题的一种可能方法是实现一个<strong>将每个子模块作为初始化参数的模块</strong>。 这称为<code>依赖注入</code>，这是<strong>改进代码库解耦</strong>的好方法。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyMainModel</span><span class="token punctuation">(</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> decoder<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Example encoder-decoder submodules model        Args:            encoder: Instance of a module for encoding            decoder: Instance of a module for decoding        """</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder</code></pre><p>配置如下：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true">#If the CLI is implemented as LightningCLI(MyMainModel)</span><span class="token key atrule">model</span><span class="token punctuation">:</span>  <span class="token key atrule">encoder</span><span class="token punctuation">:</span>    <span class="token key atrule">class_path</span><span class="token punctuation">:</span> mycode.myencoders.MyEncoder    <span class="token key atrule">init_args</span><span class="token punctuation">:</span>      <span class="token punctuation">...</span>  <span class="token key atrule">decoder</span><span class="token punctuation">:</span>    <span class="token key atrule">class_path</span><span class="token punctuation">:</span> mycode.mydecoders.MyDecoder    <span class="token key atrule">init_args</span><span class="token punctuation">:</span>      <span class="token punctuation">...</span><span class="token comment" spellcheck="true"># If subclass_mode_model=True</span><span class="token key atrule">model</span><span class="token punctuation">:</span>    class_path<span class="token punctuation">:</span>mymodel.MyMainModel    <span class="token key atrule">init_args</span><span class="token punctuation">:</span>    <span class="token key atrule">encoder</span><span class="token punctuation">:</span>      <span class="token key atrule">class_path</span><span class="token punctuation">:</span> mycode.myencoders.MyEncoder      <span class="token key atrule">init_args</span><span class="token punctuation">:</span>        <span class="token punctuation">...</span>    <span class="token key atrule">decoder</span><span class="token punctuation">:</span>      <span class="token key atrule">class_path</span><span class="token punctuation">:</span> mycode.mydecoders.MyDecoder      <span class="token key atrule">init_args</span><span class="token punctuation">:</span>        <span class="token punctuation">...</span></code></pre><p><strong>Fixed optimizer and scheduler</strong></p><p>在某些情况下，可能希望<strong>固定优化器和/或学习率调度器</strong>，而<strong>不是允许多个</strong>选择。为此，你可以通过<strong>子类化CLI</strong>来手动添加特定类的参数。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyLightningCLI</span><span class="token punctuation">(</span>LightningCLI<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">add_arguments_to_parser</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> parser<span class="token punctuation">)</span><span class="token punctuation">:</span>        parser<span class="token punctuation">.</span>add_optimizer_args<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">)</span>        parser<span class="token punctuation">.</span>add_lr_scheduler_args<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ExponentialLR<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># config.yaml 直接指定，移除了model</span>optimizer<span class="token punctuation">:</span>  lr<span class="token punctuation">:</span> <span class="token number">0.01</span>lr_scheduler<span class="token punctuation">:</span>  gamma<span class="token punctuation">:</span> <span class="token number">0.2</span>model<span class="token punctuation">:</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>trainer<span class="token punctuation">:</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code></pre><p>上面的写法是指定了哪个优化器，也可以用类似基类的方式加<code>class_path</code>和<code>init_args</code></p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ConditioningLightningCLI</span><span class="token punctuation">(</span>LightningCLI<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># OPTIMIZER_REGISTRY.classes就是获取被注册过的类</span>    <span class="token comment" spellcheck="true"># nested_key是配置文件中最上层的命名空间的名字</span>    <span class="token comment" spellcheck="true"># 向parse传递额外的参数</span>    <span class="token comment" spellcheck="true"># link到model的optimizer_init和lr_scheduler_init</span>    <span class="token keyword">def</span> <span class="token function">add_arguments_to_parser</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> parser<span class="token punctuation">)</span><span class="token punctuation">:</span>        parser<span class="token punctuation">.</span>add_optimizer_args<span class="token punctuation">(</span>            OPTIMIZER_REGISTRY<span class="token punctuation">.</span>classes<span class="token punctuation">,</span> nested_key<span class="token operator">=</span><span class="token string">"optimizer"</span><span class="token punctuation">,</span> link_to<span class="token operator">=</span><span class="token string">"model.init_args.optimizer_init"</span><span class="token punctuation">)</span>        parser<span class="token punctuation">.</span>add_lr_scheduler_args<span class="token punctuation">(</span>            LR_SCHEDULER_REGISTRY<span class="token punctuation">.</span>classes<span class="token punctuation">,</span> nested_key<span class="token operator">=</span><span class="token string">"lr_scheduler"</span><span class="token punctuation">,</span> link_to<span class="token operator">=</span><span class="token string">"model.init_args.lr_scheduler_init"</span><span class="token punctuation">)</span>@MODEL_REGISTRY<span class="token keyword">class</span> <span class="token class-name">DomainAdaptationSegmentationModel</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 optimizer_init<span class="token punctuation">:</span> dict<span class="token punctuation">,</span>                 lr_scheduler_init<span class="token punctuation">:</span> dict<span class="token punctuation">,</span>                 backbone<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span>                 ···<span class="token comment" spellcheck="true"># config.yaml</span>model<span class="token punctuation">:</span>  class_path<span class="token punctuation">:</span> models<span class="token punctuation">.</span>DomainAdaptationSegmentationModel  init_args<span class="token punctuation">:</span>       backbone<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 不用指定optimizer_init和lr_scheduler_init</span>    ···optimizer<span class="token punctuation">:</span>  class_path<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW  init_args<span class="token punctuation">:</span>        ···lr_scheduler<span class="token punctuation">:</span>  class_path<span class="token punctuation">:</span> helpers<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>LinearWarmupPolynomialLR  init_args<span class="token punctuation">:</span>    ···</code></pre><h2 id="Data-Processing-DataModule"><a href="#Data-Processing-DataModule" class="headerlink" title="Data Processing-DataModule"></a>Data Processing-DataModule</h2><p>pytorch数据处理包含五个步骤：</p><p><strong>1.Download / tokenize / process.</strong></p><ul><li>这个步骤涉及从数据源<strong>获取原始数据</strong>，并进行必要的预处理，比如<strong>分词、解码</strong>等。</li></ul><p><strong>2.Clean and （maybe） save to disk.</strong></p><ul><li>清洗数据可能包括<strong>去除噪声、处理缺失值、标准化</strong>等。</li><li>清洗后的数据可以<strong>保存到磁盘</strong>，以便后续快速加载，而不是每次都重新处理。</li></ul><p><strong>3.Load inside Dataet</strong></p><ul><li><code>Dataset</code>是PyTorch中用于封装数据的类，它定义了如何从数据集中获取单个样本。</li><li>在<code>LightningDataModule</code>中，你需要定义自己的<code>Dataset</code>类，并在<code>setup</code>方法中实例化它</li></ul><p><strong>4.Apply transforms（rotate, tokenize, etc…）.</strong></p><ul><li>PyTorch提供了<code>torchvision.transforms</code>（针对图像）和<code>torchtext.transforms</code>（针对文本）等模块来简化这些操作。</li><li>在<code>LightningDataModule</code>中，你可以在<code>train_dataloader</code>、<code>val_dataloader</code>和<code>test_dataloader</code>方法中应用这些转换。</li></ul><p><strong>5.Wrap inside a DataLoader.</strong></p><ul><li><code>DataLoader</code>是PyTorch中用于加载数据并提供批处理、多线程/多进程数据加载等功能的类。</li><li>在<code>LightningDataModule</code>中，你需要定义<code>train_dataloader</code>、<code>val_dataloader</code>和<code>test_dataloader</code>方法，分别返回用于训练、验证和测试的数据加载器。</li></ul><p>经典的DataModule实现</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> lightning <span class="token keyword">as</span> L<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> random_split<span class="token punctuation">,</span> DataLoader<span class="token comment" spellcheck="true"># Note - you must have torchvision installed for this example</span><span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> MNIST<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token keyword">class</span> <span class="token class-name">MNISTDataModule</span><span class="token punctuation">(</span>L<span class="token punctuation">.</span>LightningDataModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_dir<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">"./"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>data_dir <span class="token operator">=</span> data_dir        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.1307</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.3081</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">prepare_data</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>              <span class="token comment" spellcheck="true"># download data</span>        MNIST<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_dir<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        MNIST<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_dir<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">setup</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> stage<span class="token punctuation">:</span> str<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Assign train/val datasets for use in dataloaders</span>        <span class="token keyword">if</span> stage <span class="token operator">==</span> <span class="token string">"fit"</span><span class="token punctuation">:</span>            mnist_full <span class="token operator">=</span> MNIST<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_dir<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>self<span class="token punctuation">.</span>transform<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>mnist_train<span class="token punctuation">,</span> self<span class="token punctuation">.</span>mnist_val <span class="token operator">=</span> random_split<span class="token punctuation">(</span>                mnist_full<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">55000</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>            <span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Assign test dataset for use in dataloader(s)</span>        <span class="token keyword">if</span> stage <span class="token operator">==</span> <span class="token string">"test"</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>mnist_test <span class="token operator">=</span> MNIST<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_dir<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>self<span class="token punctuation">.</span>transform<span class="token punctuation">)</span>        <span class="token keyword">if</span> stage <span class="token operator">==</span> <span class="token string">"predict"</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>mnist_predict <span class="token operator">=</span> MNIST<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_dir<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>self<span class="token punctuation">.</span>transform<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">train_dataloader</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> DataLoader<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mnist_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">val_dataloader</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> DataLoader<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mnist_val<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_dataloader</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> DataLoader<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mnist_test<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">predict_dataloader</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> DataLoader<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mnist_predict<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span></code></pre><p><code>prepare_data(self)</code></p><p>使用多个进程（分布式设置）下载和保存数据将导致数据损坏。 Lightning 确保仅在CPU上的单个进程中调用<code>prepare_data()</code>，因此您可以<strong>安全</strong>地在其中添加<strong>下载逻辑</strong>。 在多节点训练的情况下，该钩子的执行取决于<code>prepare_data_per_node</code>。 <code>setup()</code> 在<code>prepare_data</code>之后调用，中间有一个屏障，确保一旦数据准备好并可供使用，所有进程都会继续进行设置。</p><p>所以该步骤适合：1下载数据2如nlp中的分词3保存到本地，如将分好的词保存到本地</p><p><code>setup(self, stage: Optional[str] = None)</code></p><p>执行在每个GPU上运行的数据操作。同时接受 <code>stage</code>参数，用来分离设置逻辑如<code>trainer.{fit,validate,test,predict}</code>.</p><p><code>train_dataloader(self)</code></p><p>使用<code>train_dataloader()</code>方法生成训练数据加载器。 通常只需包装在设置中定义的数据集即可。这是Trainer的<code>fit()</code>方法使用的数据加载器。</p><p><code>val_dataloader(self)</code></p><p>同上，是Trainer的<code>fit()</code>和<code>validate()</code>方法使用的数据加载器。</p><p><code>test_dataloader(self)</code></p><p>同上，是Trainer的<code>test()</code>方法使用的数据加载器。</p><p><code>predict_dataloader(self)</code></p><p>同上，是Trainer的<code>predict()</code>方法使用的数据加载器。</p><p><code>transfer_batch_to_device(self, batch, device, dataloader_id)</code></p><p>有了dataloader，框架使用该函数将对应数据送入device。如果DataLoader 返回自定义数据结构中的张量，重写这个钩子函数。默认的<strong>钩子函数（无需重写）</strong>支持的的数据类型包括：<code> torch.Tensor</code> 或任何实现 <code>.to(...)</code>方法的，<code>list dict tuple</code> 。 对于其他任何数据类型，您需要定义如何将<strong>数据移动到目标设备</strong>（CPU、GPU、TPU……)。</p><p><strong>最终dataloader提供给模型的也是打包好的一个batch</strong>。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">transfer_batch_to_device</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> device<span class="token punctuation">,</span> dataloader_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true">#batch (Any) – A batch of data that needs to be transferred to a new device.</span>  <span class="token comment" spellcheck="true">#device (device) – The target device as defined in PyTorch.</span>  <span class="token comment" spellcheck="true">#dataloader_idx (int) – The index of the dataloader to which the batch belongs.</span>    <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> CustomBatch<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># move all tensors in your custom data structure to the device</span>        batch<span class="token punctuation">.</span>samples <span class="token operator">=</span> batch<span class="token punctuation">.</span>samples<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        batch<span class="token punctuation">.</span>targets <span class="token operator">=</span> batch<span class="token punctuation">.</span>targets<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    <span class="token keyword">elif</span> dataloader_idx <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># skip device transfer for the first dataloader or anything you wish</span>        <span class="token keyword">pass</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        batch <span class="token operator">=</span> super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transfer_batch_to_device<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> device<span class="token punctuation">,</span> dataloader_idx<span class="token punctuation">)</span>    <span class="token keyword">return</span> batch</code></pre><p><code>on_before_batch_transfer(self, batch, dataloader_idx)</code></p><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p><p>You can use <code>self.trainer.training/testing/validating/predicting</code> so that you can add different logic as per your requirement.</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">on_before_batch_transfer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> dataloader_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    batch<span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">]</span> <span class="token operator">=</span> transforms<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> batch</code></pre><p>使用datamodule的方式，trainer会自动依次调用<code>prepare_data,setup,train_dataloader</code>等函数，trainer会免除复杂的训练逻辑，不使用trainer也可以手动执行dm的各个方法来自己写训练逻辑</p><pre class=" language-python"><code class="language-python">dm <span class="token operator">=</span> MNISTDataModule<span class="token punctuation">(</span><span class="token punctuation">)</span>model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">,</span> datamodule<span class="token operator">=</span>dm<span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>test<span class="token punctuation">(</span>datamodule<span class="token operator">=</span>dm<span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>validate<span class="token punctuation">(</span>datamodule<span class="token operator">=</span>dm<span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>datamodule<span class="token operator">=</span>dm<span class="token punctuation">)</span></code></pre><h2 id="Model-Lightning-Module"><a href="#Model-Lightning-Module" class="headerlink" title="Model-Lightning Module"></a>Model-Lightning Module</h2><p>首先需要定义自己的网络模块</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> MNIST<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token keyword">import</span> lightning <span class="token keyword">as</span> L<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span> <span class="token operator">*</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>l1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">28</span> <span class="token operator">*</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>l1<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><p>然后可以使用<code>LightningModule</code>来定义模块的交互、配置优化器，使用该模块的优点是<strong>减少了training loop的复杂训练逻辑</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LitAutoEncoder</span><span class="token punctuation">(</span>L<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder    <span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token comment" spellcheck="true"># 训练阶段模块如何交互</span>        <span class="token comment" spellcheck="true"># training_step defines the train loop.</span>        x<span class="token punctuation">,</span> _ <span class="token operator">=</span> batch        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>x_hat<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss <span class="token comment" spellcheck="true">#这个需要返回loss</span>     <span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># this is the validation loop</span>        x<span class="token punctuation">,</span> _ <span class="token operator">=</span> batch        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        val_loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>x_hat<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"val_loss"</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 记录loss就行</span>     <span class="token keyword">def</span> <span class="token function">test_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># this is the test loop</span>        x<span class="token punctuation">,</span> _ <span class="token operator">=</span> batch        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        test_loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>x_hat<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"test_loss"</span><span class="token punctuation">,</span> test_loss<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 记录loss就行</span>    <span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token comment" spellcheck="true"># 定义优化器</span>        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> optimizer</code></pre><p>实际使用时利用<code>Trainer</code>来负责处理所有工程问题，并将所需的所有复杂性抽象化以实现规模扩展。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># model</span>autoencoder <span class="token operator">=</span> LitAutoEncoder<span class="token punctuation">(</span>Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Decoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># train model</span>trainer <span class="token operator">=</span> L<span class="token punctuation">.</span>Trainer<span class="token punctuation">(</span><span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token operator">=</span>autoencoder<span class="token punctuation">,</span> train_dataloaders<span class="token operator">=</span>train_loader<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Trainer代替的就是以下逻辑</span>autoencoder <span class="token operator">=</span> LitAutoEncoder<span class="token punctuation">(</span>Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Decoder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> autoencoder<span class="token punctuation">.</span>configure_optimizers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> batch <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># 自动循环</span>    loss <span class="token operator">=</span> autoencoder<span class="token punctuation">.</span>training_step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#自动反向传播</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 自动更新参数</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 自动梯度清0</span><span class="token comment" spellcheck="true"># test the model</span>trainer<span class="token punctuation">.</span>test<span class="token punctuation">(</span>model<span class="token punctuation">,</span> dataloaders<span class="token operator">=</span>DataLoader<span class="token punctuation">(</span>test_set<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># train with both splits(训练加验证)</span>trainer <span class="token operator">=</span> L<span class="token punctuation">.</span>Trainer<span class="token punctuation">(</span><span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> valid_loader<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#把两个dataloader都加入</span></code></pre><p>上述是模型的<strong>训练部分（训练、验证、测试）</strong>，对于<strong>推理部分</strong>，简单方式如下：</p><pre class=" language-python"><code class="language-python">model <span class="token operator">=</span> LitModel<span class="token punctuation">.</span>load_from_checkpoint<span class="token punctuation">(</span><span class="token string">"best_model.ckpt"</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    y_hat <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><p>上面的推理逻辑还是很复杂，<code>LightningModule</code>提供<code>predict_step</code>来解决：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">predict_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">,</span> dataloader_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 推理如下</span>data_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span>trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span><span class="token punctuation">)</span>predictions <span class="token operator">=</span> trainer<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>model<span class="token punctuation">,</span> data_loader<span class="token punctuation">)</span></code></pre><p>还可以添加更复杂的预处理或后处理逻辑，例如：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LitMCdropoutModel</span><span class="token punctuation">(</span>L<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> mc_iteration<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>mc_iteration <span class="token operator">=</span> mc_iteration    <span class="token keyword">def</span> <span class="token function">predict_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># enable Monte Carlo Dropout</span>        self<span class="token punctuation">.</span>dropout<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># take average of `self.mc_iteration` iterations</span>        pred <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mc_iteration<span class="token punctuation">)</span><span class="token punctuation">]</span>        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> pred</code></pre><p><strong>分布式推理</strong></p><p>通过使用<code>Lightning</code>的<code>predict_step</code>，可以使用分布式推理通过<a href="https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.BasePredictionWriter.html#lightning.pytorch.callbacks.BasePredictionWriter"><code>BasePredictionWriter</code></a>.</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> lightning<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> BasePredictionWriter<span class="token keyword">class</span> <span class="token class-name">CustomWriter</span><span class="token punctuation">(</span>BasePredictionWriter<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> output_dir<span class="token punctuation">,</span> write_interval<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>write_interval<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>output_dir <span class="token operator">=</span> output_dir    <span class="token keyword">def</span> <span class="token function">write_on_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> trainer<span class="token punctuation">,</span> pl_module<span class="token punctuation">,</span> predictions<span class="token punctuation">,</span> batch_indices<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># this will create N (num processes) files in `output_dir` each containing</span>        <span class="token comment" spellcheck="true"># the predictions of it's respective rank</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_dir<span class="token punctuation">,</span> f<span class="token string">"predictions_{trainer.global_rank}.pt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># optionally, you can also save `batch_indices` to get the information about the data index</span>        <span class="token comment" spellcheck="true"># from your prediction data</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>batch_indices<span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_dir<span class="token punctuation">,</span> f<span class="token string">"batch_indices_{trainer.global_rank}.pt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># or you can set `write_interval="batch"` and override `write_on_batch_end` to save</span><span class="token comment" spellcheck="true"># predictions at batch level</span>pred_writer <span class="token operator">=</span> CustomWriter<span class="token punctuation">(</span>output_dir<span class="token operator">=</span><span class="token string">"pred_path"</span><span class="token punctuation">,</span> write_interval<span class="token operator">=</span><span class="token string">"epoch"</span><span class="token punctuation">)</span>trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>accelerator<span class="token operator">=</span><span class="token string">"gpu"</span><span class="token punctuation">,</span> strategy<span class="token operator">=</span><span class="token string">"ddp"</span><span class="token punctuation">,</span> devices<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span><span class="token punctuation">[</span>pred_writer<span class="token punctuation">]</span><span class="token punctuation">)</span>model <span class="token operator">=</span> BoringModel<span class="token punctuation">(</span><span class="token punctuation">)</span>trainer<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>model<span class="token punctuation">,</span> return_predictions<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><h2 id="Coming-up-next"><a href="#Coming-up-next" class="headerlink" title="Coming up next"></a>Coming up next</h2><p>之后将学习<code>Lightning</code>关于<code>Trainer</code>以及如<code>callback</code>、调试方法、模型优化、可视化等涉及<strong>训练逻辑</strong>和<strong>优化方法</strong>的相关技术。</p><h2 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h2><blockquote><p><a href="https://github.com/Lightning-AI/lightning">Lightning github地址</a></p><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html">官方文档</a></p><p><a href="">AEDA代码</a></p><p><a href="https://zhuanlan.zhihu.com/p/353985363">Pytorch Lightning 完全攻略</a></p></blockquote><h2 id="About-Me"><a href="#About-Me" class="headerlink" title="About Me"></a>About Me</h2><p><strong>个人博客：</strong><a href="https://chenghaodong666.github.io/">月源</a></p><p><strong>知乎文章：</strong><a href="https://www.zhihu.com/people/huang-ye-de-ye-92/posts">月源</a></p><p><strong>公众号：</strong><code>月源的算法仙蛊屋</code></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>diffusion model</title>
      <link href="diffusion-model/"/>
      <url>diffusion-model/</url>
      
        <content type="html"><![CDATA[<h3 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h3><p>了解扩散模型之前,首先了解一下变分自编码器,教程如下:</p><p><a href="https://zhuanlan.zhihu.com/p/112513743">https://zhuanlan.zhihu.com/p/112513743</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>sklearn</title>
      <link href="sklearn/"/>
      <url>sklearn/</url>
      
        <content type="html"><![CDATA[<h1 id="Sklearn"><a href="#Sklearn" class="headerlink" title="Sklearn"></a>Sklearn</h1><p>getting started:<a href="https://scikit-learn.org/stable/getting_started.html">https://scikit-learn.org/stable/getting_started.html</a></p><p>user guide:<a href="https://scikit-learn.org/stable/user_guide.html#user-guide">https://scikit-learn.org/stable/user_guide.html#user-guide</a></p><p>sklearn API:<a href="https://scikit-learn.org/stable/modules/classes.html#api-ref">https://scikit-learn.org/stable/modules/classes.html#api-ref</a></p><h2 id="model-selection"><a href="#model-selection" class="headerlink" title="model_selection"></a>model_selection</h2>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sklearn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面经</title>
      <link href="mian-jing/"/>
      <url>mian-jing/</url>
      
        <content type="html"><![CDATA[<h1 id="八股"><a href="#八股" class="headerlink" title="八股"></a>八股</h1><img src="/mian-jing/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20230220155022189.png" alt="image-20230220155020210" style="zoom:80%;"> <h1 id="面经"><a href="#面经" class="headerlink" title="面经"></a>面经</h1><h3 id="第一篇"><a href="#第一篇" class="headerlink" title="第一篇"></a>第一篇</h3><p><a href="https://www.jianshu.com/p/58855c6971e5">面经第一篇</a></p><p><strong>默写交叉熵和softmax:</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">cross_entropy</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> y_hat<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># n = 1e-6</span>    <span class="token comment" spellcheck="true"># return -np.sum(y * np.log(y_hat + n) + (1 - y) * np.log(1 - y_hat + n), axis=1)</span>    <span class="token keyword">assert</span> y<span class="token punctuation">.</span>shape <span class="token operator">==</span> y_hat<span class="token punctuation">.</span>shape    res <span class="token operator">=</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>nan_to_num<span class="token punctuation">(</span>y <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y_hat<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y_hat<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> round<span class="token punctuation">(</span>res<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 防止输入增大时输出为nan,每个数都减去最大值,这样所有的数都&lt;=0,值域&lt;=1</span>    y_shift <span class="token operator">=</span> y <span class="token operator">-</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>y<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    y_exp <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>y_shift<span class="token punctuation">)</span>    y_exp_sum <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>y_exp<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> y_exp <span class="token operator">/</span> y_exp_sum<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    y_hat <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>cross_entropy<span class="token punctuation">(</span>y<span class="token punctuation">,</span> y_hat<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># y = np.array([[1,2,3,4],[1,3,4,5],[3,4,5,6]])</span>    <span class="token comment" spellcheck="true"># print(softmax(y))</span></code></pre><p><strong>求反向传播：</strong></p><p><a href="https://blog.csdn.net/weixin_41722370/article/details/83590276">https://blog.csdn.net/weixin_41722370/article/details/83590276</a></p><p><strong>卷积参数量和计算量:</strong></p><p><a href="https://www.jianshu.com/p/c2a0ba5bb3d1">https://www.jianshu.com/p/c2a0ba5bb3d1</a></p><p>考虑偏置,参数量:$(K\times K \times C_{in}+1)\times C_{out}$</p><p>计算量:MAC(Multiply Accumulate)乘加次数,考虑偏置为:$C_{in}\times K\times K \times H_{out} \times W_{out} \times C_{out}$,解释:每有一个点,就做$C_{in}\times K\times K$次计算,共有$H_{out}\times W_{out}\times C_{out}$,由于一个MAC包括<strong>一次乘一次加</strong>,做卷积计算是做n次乘法,n-1次加法,再加上一次偏置,正好是n次乘法,n次加法</p><p>FLOPs(floating point operations),浮点运算量,指计算量,跟乘加次数有点不同,考虑偏置为:$(C_{in}\times 2\times K\times K)\times H_{out}\times  W_{out} \times C_{out} $,和MAC不同,所以每有一个点,就做$C_{in}\times 2\times K\times K$次计算,</p><p>FLOPs不考虑偏置:$(C_{in}\times 2\times K\times K-1)\times H_{out}\times  W_{out} \times C_{out} $</p><p><strong>Python的多线程,为什么说他是伪多线程,什么时候应该使用？</strong></p><p><a href="https://blog.kamino.link/2021/03/01/Python-Multithreading-in-detail/">https://blog.kamino.link/2021/03/01/Python-Multithreading-in-detail/</a></p><p><a href="https://www.cnblogs.com/luyuze95/p/11289143.html">https://www.cnblogs.com/luyuze95/p/11289143.html</a></p><p><strong>求mIoU:</strong></p><p>目标检测的IoU,会有两个检测框:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#RT:RightTop</span><span class="token comment" spellcheck="true">#LB:LeftBottom</span><span class="token keyword">def</span> <span class="token function">IOU</span><span class="token punctuation">(</span>rectangle A<span class="token punctuation">,</span> rectangleB<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 关键是先求出交集的宽和高</span>    W <span class="token operator">=</span> min<span class="token punctuation">(</span>A<span class="token punctuation">.</span>RT<span class="token punctuation">.</span>x<span class="token punctuation">,</span> B<span class="token punctuation">.</span>RT<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token operator">-</span> max<span class="token punctuation">(</span>A<span class="token punctuation">.</span>LB<span class="token punctuation">.</span>x<span class="token punctuation">,</span> B<span class="token punctuation">.</span>LB<span class="token punctuation">.</span>x<span class="token punctuation">)</span>    H <span class="token operator">=</span> min<span class="token punctuation">(</span>A<span class="token punctuation">.</span>RT<span class="token punctuation">.</span>y<span class="token punctuation">,</span> B<span class="token punctuation">.</span>RT<span class="token punctuation">.</span>y<span class="token punctuation">)</span> <span class="token operator">-</span> max<span class="token punctuation">(</span>A<span class="token punctuation">.</span>LB<span class="token punctuation">.</span>y<span class="token punctuation">,</span> B<span class="token punctuation">.</span>LB<span class="token punctuation">.</span>y<span class="token punctuation">)</span>    <span class="token keyword">if</span> W <span class="token operator">&lt;=</span> <span class="token number">0</span> <span class="token operator">or</span> H <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>    SA <span class="token operator">=</span> <span class="token punctuation">(</span>A<span class="token punctuation">.</span>RT<span class="token punctuation">.</span>x <span class="token operator">-</span> A<span class="token punctuation">.</span>LB<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>A<span class="token punctuation">.</span>RT<span class="token punctuation">.</span>y <span class="token operator">-</span> A<span class="token punctuation">.</span>LB<span class="token punctuation">.</span>y<span class="token punctuation">)</span>    SB <span class="token operator">=</span> <span class="token punctuation">(</span>B<span class="token punctuation">.</span>RT<span class="token punctuation">.</span>x <span class="token operator">-</span> B<span class="token punctuation">.</span>LB<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>B<span class="token punctuation">.</span>RT<span class="token punctuation">.</span>y <span class="token operator">-</span> B<span class="token punctuation">.</span>LB<span class="token punctuation">.</span>y<span class="token punctuation">)</span>    cross <span class="token operator">=</span> W <span class="token operator">*</span> H    <span class="token keyword">return</span> cross<span class="token operator">/</span><span class="token punctuation">(</span>SA <span class="token operator">+</span> SB <span class="token operator">-</span> cross<span class="token punctuation">)</span></code></pre><p><strong>进程和线程的区别？</strong></p><p>看了一遍排在前面的答案，类似”<strong>进程是资源分配的最小单位，线程是CPU调度的最小单位“</strong>这样的回答感觉太抽象，都不太容易让人理解。</p><p>做个简单的比喻：进程=火车，线程=车厢</p><ul><li>线程在进程下行进（单纯的车厢无法运行）</li><li>一个进程可以包含多个线程（一辆火车可以有多个车厢）</li><li>不同进程间数据很难共享（一辆火车上的乘客很难换到另外一辆火车，比如站点换乘）</li><li>同一进程下不同线程间数据很易共享（A车厢换到B车厢很容易）</li><li>进程要比线程消耗更多的计算机资源（采用多列火车相比多个车厢更耗资源）</li><li>进程间不会相互影响，一个线程挂掉将导致整个进程挂掉（一列火车不会影响到另外一列火车，但是如果一列火车上中间的一节车厢着火了，将影响到所有车厢）</li><li>进程可以拓展到多机，进程最多适合多核（不同火车可以开在多个轨道上，同一火车的车厢不能在行进的不同的轨道上）</li><li>进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。（比如火车上的洗手间）－”互斥锁”</li><li>进程使用的内存地址可以限定使用量（比如火车上的餐厅，最多只允许多少人进入，如果满了需要在门口等，等有人出来了才能进去）－“信号量”</li></ul><p>作者：知乎用户<br>链接：<a href="https://www.zhihu.com/question/25532384/answer/411179772">https://www.zhihu.com/question/25532384/answer/411179772</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h3 id="第二篇"><a href="#第二篇" class="headerlink" title="第二篇"></a>第二篇</h3><p><a href="https://www.jianshu.com/p/c713c2022ab0">https://www.jianshu.com/p/c713c2022ab0</a></p><h3 id="第三篇"><a href="#第三篇" class="headerlink" title="第三篇"></a>第三篇</h3><p><a href="https://blog.csdn.net/qq_41375609/article/details/115473923">https://blog.csdn.net/qq_41375609/article/details/115473923</a></p><h3 id="第四篇"><a href="#第四篇" class="headerlink" title="第四篇"></a>第四篇</h3><p><a href="https://www.nowcoder.com/discuss/802751?source_id=discuss_experience_nctrack&amp;channel=-1">https://www.nowcoder.com/discuss/802751?source_id=discuss_experience_nctrack&amp;channel=-1</a></p><p><a href="https://www.nowcoder.com/discuss/765855?source_id=discuss_experience_nctrack&amp;channel=-1">https://www.nowcoder.com/discuss/765855?source_id=discuss_experience_nctrack&amp;channel=-1</a></p><p><a href="https://www.nowcoder.com/discuss/765025?source_id=discuss_experience_nctrack&amp;channel=-1">https://www.nowcoder.com/discuss/765025?source_id=discuss_experience_nctrack&amp;channel=-1</a></p><p><a href="https://www.nowcoder.com/discuss/761647?source_id=discuss_experience_nctrack&amp;channel=-1">https://www.nowcoder.com/discuss/761647?source_id=discuss_experience_nctrack&amp;channel=-1</a></p><p><a href="https://www.nowcoder.com/discuss/781895?source_id=discuss_experience_nctrack&amp;channel=-1">https://www.nowcoder.com/discuss/781895?source_id=discuss_experience_nctrack&amp;channel=-1</a></p><p><a href="https://zhuanlan.zhihu.com/p/86103903">https://zhuanlan.zhihu.com/p/86103903</a></p><p><a href="https://cloud.tencent.com/developer/article/1426223?from=article.detail.1368191">https://cloud.tencent.com/developer/article/1426223?from=article.detail.1368191</a></p><h1 id="刷题"><a href="#刷题" class="headerlink" title="刷题"></a>刷题</h1><h2 id="单调栈"><a href="#单调栈" class="headerlink" title="单调栈"></a>单调栈</h2><p><a href="https://leetcode.cn/problems/maximum-binary-tree/">leetcode 654最大二叉树</a></p><p>使用单调栈来优化时间复杂度的想法非常妙！</p><h2 id="递归与回溯"><a href="#递归与回溯" class="headerlink" title="递归与回溯"></a>递归与回溯</h2><p><a href="https://leetcode.cn/problems/ju-zhen-zhong-de-lu-jing-lcof/submissions/"></a><a href="https://leetcode.cn/problems/ju-zhen-zhong-de-lu-jing-lcof/">剑指 Offer 12. 矩阵中的路径</a></p><ul><li>可以通过临时赋值省去check二维数组的空间</li><li>关键是明白什么时候标记,什么时候回溯取消标记:<strong>进入当前节点,且该节点满足条件,标记;该节点处理完毕之后，回溯取消标记</strong></li><li>可以不使用for循环,而是对于超出边界的在子递归节点中判断</li></ul><h2 id="dp"><a href="#dp" class="headerlink" title="dp"></a>dp</h2><p><a href="https://www.acwing.com/problem/content/902/">acwing整数划分</a></p><pre class=" language-python"><code class="language-python">n <span class="token operator">=</span> int<span class="token punctuation">(</span>input<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>f <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    f<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">1</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        f<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">(</span>f<span class="token punctuation">[</span>i<span class="token number">-1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">+</span>f<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">%</span><span class="token punctuation">(</span><span class="token number">1e9</span><span class="token operator">+</span><span class="token number">7</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#边界条件从递归公式来找,看递推公式可能出现哪个</span>        <span class="token comment" spellcheck="true"># 边界条件</span><span class="token keyword">print</span><span class="token punctuation">(</span>int<span class="token punctuation">(</span>f<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><ul><li><strong>边界条件从递推公式来找</strong></li><li>有时候上边是不行的，就得从头分析最开始是从哪一步递归而来的</li></ul><h1 id="简历"><a href="#简历" class="headerlink" title="简历"></a>简历</h1><p>你要根据投递的公司适当调整你的项目内容</p><p>不是所有的都适合放上去的</p><p>不然容易变成坑自己的点</p><p>暑期的时候没有实习，就把论文展开讲一下</p><p>看一下STAR法则</p><p>看了几个简历分享几个小点可以改改，一个是实习经历或者项目经历不一定要按时间顺序排列，可以把你觉得做的好的有的讲的往前放，面试官都没空看简历的一般从上往下问。然后模板还是简单点好，用超级简历就行了，不要搞的太花哨。然后项目经历啥的用数字来表示效果会更直接，比如某某算法提升了10个点，或者帮公司运营什么，社群增长了1000人这个样子举例子</p><h1 id="投递"><a href="#投递" class="headerlink" title="投递"></a>投递</h1><p>有些部门根本就不招人,投的时候要注意</p><h1 id="国企"><a href="#国企" class="headerlink" title="国企"></a>国企</h1><p><strong>1.第一年薪资30w以上卷王之选：</strong>招商银行总行、中国银联总部、航天一院一部总体/战武、头部券商/公募基金<br><strong>2.WLB+户口+薪资的相对性价比之选：</strong>四大行总行（建总最高、中总最低）、人寿研发/数据中心、农行天研、邮储苏研、中金所技术<br><strong>3.每个月收入与当地房价比大于1的超高性价比之选：</strong>中物院（绵阳9院）、中西部政策行省分（国开门槛最高，进出口招人最少，农发招人最多）</p><h1 id="笔试注意点"><a href="#笔试注意点" class="headerlink" title="笔试注意点"></a>笔试注意点</h1><img src="/mian-jing/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20230328210430945.png" alt="image-20230328210430945" style="zoom: 50%;"><p>赛码网可以通过查看提交记录来看</p><p>耗时和占用内存,优化了半天,最后也没做出来,不如少两个循环,就差点就过了</p><p><strong>携程第三题</strong></p><p>x的处理是对的,但是根据复杂度可以看到还很富裕，所以说y可以直接遍历，其实y遍历的话是很小的，通过直接算的方式反而容易出错，遍历的话就不重不漏了</p><img src="/mian-jing/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20230329212646897.png" alt="image-20230329212646897" style="zoom:67%;"><p><a href="https://www.nowcoder.com/discuss/470695167230595072">https://www.nowcoder.com/discuss/470695167230595072</a></p><p>python读入数据超时的解决办法</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#python 不要用input</span><span class="token comment" spellcheck="true">#用sys.stdin.readlines（）</span><span class="token comment" spellcheck="true">#获取后面的不要split 再map 值</span><span class="token comment" spellcheck="true">#用 int(line[2:-1])</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面经 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django</title>
      <link href="django/"/>
      <url>django/</url>
      
        <content type="html"><![CDATA[<h2 id="Django"><a href="#Django" class="headerlink" title="Django"></a>Django</h2><p><a href="https://docs.djangoproject.com/zh-hans/3.2/">Django官方文档</a></p><p><a href="https://www.bilibili.com/video/BV1vK4y1o7jH?p=1">B站教学视频</a></p><p><a href="https://www.liujiangblog.com/blog/36/">刘江的博客教程</a></p><h4 id="url"><a href="#url" class="headerlink" title="url"></a>url</h4><p><a href="https://www.bilibili.com/video/BV1vK4y1o7jH?p=4">统一资源定位符</a></p><h4 id="请求和相应"><a href="#请求和相应" class="headerlink" title="请求和相应"></a>请求和相应</h4><blockquote><p>请求是指浏览器端通过Http协议发送给服务器端的数据</p><p>响应是指服务器端接收到请求后做相应的处理后再回复给浏览器</p></blockquote><p><strong>请求</strong></p><p>在Django中就是视图函数中的第一个参数,Django接收到http协议的请求后,会<strong>根据请求数据报文创建HttpRequest对象</strong>,该对象<strong>通过属性描述了请求的所有相关信息</strong></p><ul><li><code>path_info</code>:URL字符串</li><li><code>method</code>:字符串，表示HTTP请求方法，常用值: ‘GET’、’POST’</li><li><code>GET</code>:QueryDict查询字典的对象，包含get请求方式的所有数据</li><li><code>POST</code>: QueryDict查询字典的对象，包含post请求方式的所有数据</li><li><code>FILES</code>:类似于字典的对象,包含所有的上传文件信息</li><li><code>COOKIES</code>: Python字典,包含所有的cookie,键和值都为字符串 </li><li><code>session</code>:似于字典的对象,表示当前的会话</li><li><code>body</code>:字符串,请求体的内容(POST或PUT). </li><li><code>scheme</code>:请求协议(‘http’/‘https’)</li><li><code>request.get_full_path()</code>:请求的完整路径</li><li><code>request.META</code>:请求中的元数据(消息头),<code>request.META['REMOTE_ADDR']</code>:客户端IP地址</li></ul><p><strong>响应</strong></p><p>HTTP状态码的英文为<code>HTTP Status Code</code>,常见的响应状态码有:</p><ul><li><code>200</code>-请求成功 <strong>2开头是成功</strong></li><li><code>301</code>-永久重定向-资源(网页等)被永久转移到其它URL-</li><li><code>302</code>-临时重定向 <strong>3开头是重定向</strong></li><li><code>404</code>-请求的资源(网页等)不存在 <strong>4开头是客户端错误</strong></li><li><code>500</code>-内部服务器错误 <strong>5开头是服务端错误</strong></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 向客户端返回响应,同时返回响应内容</span><span class="token comment" spellcheck="true"># 比如响应数据类型是html,浏览器就按照html解析</span>HttpResponse<span class="token punctuation">(</span>content<span class="token operator">=</span>响应体<span class="token punctuation">,</span>             content_type<span class="token operator">=</span>响应体数据类型<span class="token punctuation">,</span>             status<span class="token operator">=</span>状态码<span class="token punctuation">)</span></code></pre><p>常见的content type:</p><ul><li><code>'text/html’</code>(默认的，html文件)</li><li><code>'text/plain’</code>(纯文本)</li><li><code>'text/css'</code>(css文件)</li><li><code>'text/javascript’</code>(js文件)</li><li><code>'multipart/form-data’</code>(文件提交)</li><li><code>'application/json'</code>(json传输)</li><li><code>'application/xml’</code>(xml文件)</li></ul><p><strong>GET和POST</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Django处理GET请求,查询字符串–少量数据传递</span><span class="token keyword">if</span> request<span class="token punctuation">.</span>method <span class="token operator">==</span> <span class="token string">'GET'</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span><span class="token comment" spellcheck="true"># Django处理POST请求,专用于浏览器提交数据</span><span class="token keyword">elif</span> request<span class="token punctuation">.</span>method <span class="token operator">==</span> <span class="token string">'POST'</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span></code></pre><p><strong>什么是模板</strong></p><h2 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h2><p><a href="https://www.bilibili.com/video/BV14J4114768?p=1">Bilibili视频,讲得很好</a></p><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><p><strong>什么是网页</strong></p><p>网站是指在因特网上根据一定的规则，使用HTML等制作的用于展示特定内容相关的网页集合。<br>网页是网站中的一“页”，通常是HTML格式的文件，它要通过浏览器来阅读。<br>网页是构成网站的基本元素，它通常由图片、链接、文字、声音、视频等元素组成。通常我们看到的网页常见以.htm 或.html后缀结尾的文件，因此将其俗称为HTML文件。</p><p><strong>什么是HTML</strong></p><p><strong>HTML</strong>指的是<strong>超文本标记语言(Hyper Text MarkupLanguage)**，它是用来描述网页的一种语言。HTML不是一种编程语言，而是一种标记语言</strong>(markuplanguage)<strong>. 标记语言是一套标记标签(markup tag).<br>**所谓超文本，有2层含义∶</strong><br>1.它可以加入图片、声音、动画、多媒体等内容（超越了文本限制)。<br>2.它还可以从一个文件跳转到另一个文件，与世界各地主机的文件连接（超级链接文本)。</p><p><strong>浏览器内核</strong></p><p>浏览器内核（渲染引擎)︰负责读取网页内容，整理讯息，计算网页的显示方式并显示页面。</p><table><thead><tr><th>浏览器</th><th>内核</th><th>备注</th></tr></thead><tbody><tr><td>IE</td><td>Trident</td><td>IE、猎豹安全、360极速浏览器、百度浏览器</td></tr><tr><td>firefox</td><td>Gecko</td><td>火狐浏览器内核</td></tr><tr><td>Safari</td><td>Webkit</td><td>苹果浏览器内核</td></tr><tr><td>chrome/Opera</td><td>Blink</td><td>chrome / opera浏览器内核。Blink其实是 WebKit的分支。</td></tr></tbody></table><p><strong>Web标准</strong></p><p>Web标准是由W3C组织和其他标准化组织制定的一系列标准的集合。W3C(万维网联盟）是国际最著名的标准化组织。<br>遵循Web标准除了可以让不同的开发人员写出的页面更标准、更统一。</p><p>Web标准主要包括**结构(Structure )、表现( Presentation )和行为(Behavior )**三个方面:</p><ul><li>结构用于对网页元素进行整理和分类，现阶段主要学的是HTML。</li><li>表现用于设置网页元素的版式、颜色、大小等外观样式，主要指的是CSS</li><li>行为是指网页模型的定义及交互的编写，现阶段主要学的是Javascript</li></ul><p>Web标准提出的最佳体验方案:结构、样式、行为相分离。简单理解∶<strong>结构写到HTML文件中，表现写到CSS文件中，行为写到JavaScript文件中。</strong></p><p>双标签:<code>&lt;html&gt;&lt;/html&gt;</code></p><p>单标签:<code>&lt;br /&gt;</code></p><h3 id="基本标签"><a href="#基本标签" class="headerlink" title="基本标签"></a>基本标签</h3><pre class=" language-html"><code class="language-html"><span class="token comment" spellcheck="true">&lt;!--标题标签,从h1-h6--&gt;</span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h6</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h6</span><span class="token punctuation">&gt;</span></span><span class="token comment" spellcheck="true">&lt;!--段落标签,paragraph--&gt;</span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">&gt;</span></span><span class="token comment" spellcheck="true">&lt;!--换行标签,break--&gt;</span><span class="token comment" spellcheck="true">&lt;!--&lt;br /&gt;标签只是简单地开始新的一行，跟段落不一样，段落之间会插入一些垂直的间距。--&gt;</span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>br</span> <span class="token punctuation">/&gt;</span></span># 文本格式化# 加粗<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>strong</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>strong</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>b</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>b</span><span class="token punctuation">&gt;</span></span># 倾斜<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>em</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>em</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>i</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>i</span><span class="token punctuation">&gt;</span></span># 删除线<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>del</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>del</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>s</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>s</span><span class="token punctuation">&gt;</span></span># 下划线<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ins</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ins</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>u</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>u</span><span class="token punctuation">&gt;</span></span># 盒子,无语义,用来装内容 division分割,分区 span跨度,跨距<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">&gt;</span></span> # 是一个大盒子,一个独占一行<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">&gt;</span></span># 小盒子,一行可以放多个span# 图像标签,属性之间部分先后顺序,空格分隔,# alt:图片显示不出来时显示的文字,title:鼠标放到图片上时显示出来的文字 # width:设定宽度 height:设定高度 border:设定边框粗细<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>图像URL<span class="token punctuation">"</span></span> <span class="token attr-name">alt</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span><span class="token punctuation">"</span></span> <span class="token attr-name">title</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span><span class="token punctuation">"</span></span> <span class="token attr-name">width</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>400<span class="token punctuation">"</span></span> <span class="token attr-name">height</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>500<span class="token punctuation">"</span></span> <span class="token attr-name">border</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>15<span class="token punctuation">"</span></span><span class="token punctuation">/&gt;</span></span><span class="token comment" spellcheck="true">&lt;!--超链接标签 anchor--&gt;</span># 1.外部链接:"http://www.baidu.com"# _self:当前窗口打开 _blank:新窗口打开# 2.内部链接:"a.html"# 3.空链接:"#"# 4.下载链接:"img.zip" "img.exe"# 5.网页元素链接:<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://www.baidu.com<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>img.jpg<span class="token punctuation">"</span></span><span class="token punctuation">/&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">&gt;</span></span># 6.锚点链接,快速定位到页面中的某个位置,<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>#two<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>第二集<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">&gt;</span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h3</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>two<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>第二集<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h3</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>跳转目标<span class="token punctuation">"</span></span> <span class="token attr-name">target</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>目标窗口的弹出方式<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>文本或图像<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">&gt;</span></span># 空格,No-Break Space<span class="token entity" title="&nbsp;">&amp;nbsp;</span># 列表标签,用来布局,列表最大的特点就是整齐、整洁、有序，它作为布局会更加自由和方便。# 无序列表<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ul</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">&gt;</span></span>列表项1,li之间相当于一个容器,可以放任何元素<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">&gt;</span></span>列表项2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">&gt;</span></span>列表项3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">&gt;</span></span>    ...<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ul</span><span class="token punctuation">&gt;</span></span># 有序列表<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ol</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">&gt;</span></span>列表项1,li之间相当于一个容器,可以放任何元素<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">&gt;</span></span>列表项2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">&gt;</span></span>列表项3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">&gt;</span></span>    ...<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ol</span><span class="token punctuation">&gt;</span></span># 自定义列表<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dl</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dt</span><span class="token punctuation">&gt;</span></span>名词1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dt</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dd</span><span class="token punctuation">&gt;</span></span>名词1解释1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dd</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dd</span><span class="token punctuation">&gt;</span></span>名词1解释2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dd</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dl</span><span class="token punctuation">&gt;</span></span># 表单标签,使用表单的目的是为了收集用户信息# 在HTML中,一个完整的表单通常由表单域、表单控件(也称为表单元素)和提示信息3个部分构成。</code></pre><img src="/django/image-20211120165652952.png" alt="image-20211120165652952" style="zoom:80%;"><pre class=" language-html"><code class="language-html"># 表单域,<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>form</span><span class="token punctuation">&gt;</span></span>会把它范围内的表单元素信息提交给服务器<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>form</span><span class="token punctuation">&gt;</span></span>action="用于指定接收并处理表单数据的服务器程序的url地址" method="提交方式,get或post" name="表单域名称"<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>form</span><span class="token punctuation">&gt;</span></span># 表单标签# 输入标签&lt;input type="text/password/radio单选框,可以多选/checkbox复选框/submit提交按钮,表单域的信息发送给服务器/reset重置按钮/button点击按钮,用于通过js启动脚本/file文件域,上传文件使用" name="表单名字,比如radio必须有相同的名字才能实现单选,复选框要有相同的值" value="默认值" checked="单选框和复选框默认打√" maxlength="正整数,规定输入字符长度的最大值" accet="只能与<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>input</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>file<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>配合使用。它规定能够通过文件上传进行提交的文件类型。image/gif,image/jpeg,如果不限制图像的格式，可以写为:image/*" &gt;# label标签,经常与表单标签搭配使用# <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>label</span><span class="token punctuation">&gt;</span></span>标签用于绑定一个表单元素当点击<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>label</span><span class="token punctuation">&gt;</span></span>标签内的文本时，浏览器就会自动将焦点(光标)转到或者选择对应的表单元素上,用来增加用户体验.# for和id必须对应<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>label</span> <span class="token attr-name">for</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>sex<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>男&lt;/ label&gt;&lt;input type="radio" name="sex"id="sek" /&gt;# 下拉表单<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>select</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>option</span> <span class="token attr-name">selected</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>selected<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>山东<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>option</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>option</span><span class="token punctuation">&gt;</span></span>北京<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>option</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>option</span><span class="token punctuation">&gt;</span></span>天津<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>option</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>option</span><span class="token punctuation">&gt;</span></span>火星<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>option</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>select</span><span class="token punctuation">&gt;</span></span># 文本域,定义多行文本输入<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>textarea</span> <span class="token attr-name">cols</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>一行多少字<span class="token punctuation">"</span></span> <span class="token attr-name">rows</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>有几行,实际上不会使用,都是用CSS来改变大小<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>pink老师,我知道这个反馈留言是textarea来做的<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>textarea</span><span class="token punctuation">&gt;</span></span></code></pre><h2 id="CSS3"><a href="#CSS3" class="headerlink" title="CSS3"></a>CSS3</h2><p><a href="https://www.bilibili.com/video/BV14J4114768?p=1">Bilibili视频,讲得很好</a></p><p><strong>Cascading Style Sheets,层叠样式表</strong></p><h3 id="基础知识-1"><a href="#基础知识-1" class="headerlink" title="基础知识"></a>基础知识</h3><p>CSS主要用于设置HTML页面中的文本内容(字体、大小、对齐方式等)、图片的外形(宽高、边框样式边距等)以及版面的布局和外观显示样式。CSS让我们的网页更加丰富多彩,布局更加灵活自如。简单理解:CSS可以美化HTML,让HTML更漂亮让页面布局更简单。</p><p><strong>CSS 规则由两个主要的部分构成:选择器以及一条或多条声明。</strong></p><p><img src="/django/image-20211122102343607.png" alt="image-20211122102343607"></p><pre class=" language-html"><code class="language-html"># 在<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>head</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>head</span><span class="token punctuation">&gt;</span></span>里写<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">&gt;</span></span><span class="token style language-css">    <span class="token comment" spellcheck="true">/*选择器{样式]*/</span>    <span class="token comment" spellcheck="true">/*给谁改样式{改什么样式}*/</span>    <span class="token selector">p </span><span class="token punctuation">{</span>        <span class="token property">color</span><span class="token punctuation">:</span> red<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/*虽然大写COLOR: RED也行,但提倡小写*/</span>        <span class="token property">font-size</span><span class="token punctuation">:</span> <span class="token number">12</span>px<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/*文字大小为12像素*/</span>    <span class="token punctuation">}</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">&gt;</span></span></code></pre><p><strong>CSS选择器的作用</strong></p><p>选择器(选择符)就是<strong>根据不同需求把不同的标签选出来</strong>这就是选择器的作用。简单来说，就是选择标签用的。</p><p>CSS做了两件事:</p><ol><li>找到所有的h1标签。选择器(<strong>选对人</strong>)。</li><li>设置这些标签的样式,比如颜色为红色(<strong>做对事</strong>)。</li></ol><p>选择器分为<strong>基础选择器和复合选择器</strong>两个大类，我们这里先讲解一下基础选择器。<br>基础选择器是由<strong>单个选择器</strong>组成的,基础选择器又包括:<strong>标签选择器、类选择器、id选择器和通配符选择器。</strong></p><pre class=" language-css"><code class="language-css"><span class="token selector"># 标签选择器(元素选择器)是指用HTML标签名称作为选择器,按标签名称分类,为页面中某一类标签指定统一的CSS样式。# 能快速为页面中同类型的标签统一设置样式。# 不能设计差异化样式,只能选择全部的当前标签。&lt;style&gt;    p </span><span class="token punctuation">{</span>        <span class="token property">color</span><span class="token punctuation">:</span> red<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/*虽然大写COLOR: RED也行,但提倡小写*/</span>        <span class="token property">font-size</span><span class="token punctuation">:</span> <span class="token number">12</span>px<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/*文字大小为12像素*/</span>    <span class="token punctuation">}</span><span class="token selector">&lt;/style&gt;# 类选择器,如果想要差异化选择不同的标签,单独选一个或者某几个标签,可以使用类选择器。.类名</span><span class="token punctuation">{</span>    属性<span class="token property">1</span><span class="token punctuation">:</span>属性值<span class="token number">1</span><span class="token punctuation">;</span>    <span class="token number">...</span><span class="token punctuation">}</span><span class="token selector">&lt;style&gt;    <span class="token class">.red</span> </span><span class="token punctuation">{</span>        <span class="token property">color</span><span class="token punctuation">:</span> red<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token selector">&lt;/style&gt;&lt;div class= 'red'&gt;变红色&lt;/div&gt;# 多类名,(1)在标签class属性中写多个类名(2)多个类名中间必须用空格分开# 可以调用公共的类,也可以调用自己独有的类&lt;div class="red font20"&gt;亚瑟&lt;ldiv&gt;# id选择器,只能调用一次,别人切勿使用&lt;style&gt;    <span class="token id">#pink</span> </span><span class="token punctuation">{</span>        <span class="token property">color</span><span class="token punctuation">:</span> pink<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token selector">&lt;/style&gt;&lt;div id="pink"&gt;迈克尔·杰克逊&lt;/div&gt;# 通配符选择器,在CSS中,通配符选择器使用“*”定义,它表示选取页面中所有元素（标签)。# 通配符选择器不需要调用,自动就给所有的元素使用样式&lt;style&gt;    * </span><span class="token punctuation">{</span>        <span class="token property">color</span><span class="token punctuation">:</span>red<span class="token punctuation">;</span>    <span class="token punctuation">}</span>&lt;/style&gt;</code></pre><p><strong>CSS的引入方式</strong></p><p>按照CSS样式书写的位置(或者引入的方式),CSS样式表可以分为三大类:</p><ol><li>行内样式表(行内式)</li><li>内部样式表(嵌入式)</li><li>外部样式表(链接式）</li></ol><p><strong>内部样式表</strong></p><p>内部样式表(内嵌样式表)是写到html页面内部。是将所有的CSS代码抽取出来,单独放到一个<code>&lt;style&gt;</code>标签中。</p><p><code>&lt;style&gt;</code>标签理论上可以放在HTML文档的任何地方，但一般会放在文档的<code>&lt;head&gt;</code>标签中。通过此种方式,可以方便控制<strong>当前整个页面</strong>中的元素样式设置。</p><p>代码结构清晰,但是并没有实现结构与样式完全分离。使用内部样式表设定CSS，通常也被称为嵌入式引入，这种方式是我们练习时常用的方式。</p><p><strong>行内样式表</strong></p><p>行内样式表(内联样式表)是在元素标签内部的style属性中设定CSS样式,适合于修改简单样式。</p><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">color</span><span class="token punctuation">:</span> red<span class="token punctuation">;</span> <span class="token property">font-size</span><span class="token punctuation">:</span> <span class="token number">12</span>px<span class="token punctuation">;</span></span><span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>青春不常在，抓紧谈恋爱<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">&gt;</span></span></code></pre><p><strong>外部样式表</strong></p><p>实际开发都是外部样式表。适合于样式比较多的情况,核心是:<strong>样式单独写到CSS文件中,之后把CSS文件引入到HTML页面中使用</strong>。</p><p>引入外部样式表分为两步:<br>1.新建一个后缀名为.css 的样式文件,把所有CSS代码都放入此文件中。</p><p>2.在HTML页面中,使用<code>&lt;link&gt;</code>标签引入这个文件。<code>&lt;link rel="定义当前文档与被链接文档之间的关系,stylesheet" href="定义所链接外部样式表文件的URL,css文件路径"&gt;</code></p><p><strong>Chrome调试工具</strong></p><p>打开Chrome浏览器，按下F12键或者右击页面空白处→检查。</p><p>调试工具太强了<del>~</del></p><h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><p><a href="https://developer.mozilla.org/zh-CN/">MDN文档</a></p><h3 id="基础知识-2"><a href="#基础知识-2" class="headerlink" title="基础知识"></a>基础知识</h3><p><strong>WebAPIs</strong></p><p>Js 基础学习ECMAScript基础语法为后面作铺垫,Web APIs 是 Js 的应用,大量使用js基础语法做交互效果.</p><p>Web API是浏览器提供的一套操作浏览器功能和页面元素的API( BOM和DOM)。</p><p>现阶段我们主要针对于浏览器讲解常用的API,主要针对浏览器做交互效果。比如我们想要浏览器弹出一个警示框,直接使用alert(弹出’)</p><p><strong>什么是DOM</strong></p><p>文档对象模型(Document Object Model,简称DOM),是W3C组织推荐的处理可扩展标记语言(HTML或者XML )的标准编程接口。W3C已经定义了一系列的DOM接口,通过这些DOM接口可以改变网页的内容、结构和样式。</p><p><strong>DOM树</strong></p><p><img src="/django/image-20211122153608301.png" alt="image-20211122153608301"></p><ul><li>文档:一个页面就是一个文档,DOM中使用document表示</li><li>元素:页面中的所有标签都是元素,DOM中使用element表示</li><li>节点∶网页中的所有内容都是节点(标签、属性、文本、注释等),DOM中使用node表示</li></ul><p><strong>获取元素</strong></p><pre class=" language-javascript"><code class="language-javascript"><span class="token comment" spellcheck="true">// 使用getElementByld()方法可以获取带有ID的元素对象。</span><span class="token operator">&lt;</span>script<span class="token operator">&gt;</span>    <span class="token comment" spellcheck="true">// 1.因为我们文档页面从上往下加载，所以先得有标签所以我们script写到标签的下面</span>    <span class="token comment" spellcheck="true">// 2.驼峰命名法</span>    <span class="token comment" spellcheck="true">// 3.参数id是大小写敏感的字符串</span>    <span class="token comment" spellcheck="true">// 4.返回的是一个对象</span>    <span class="token keyword">var</span> timer <span class="token operator">=</span> document<span class="token punctuation">.</span><span class="token function">getElementById</span><span class="token punctuation">(</span><span class="token string">'time'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>timer<span class="token punctuation">)</span><span class="token punctuation">;</span>    console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token keyword">typeof</span> timer<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 5.查看里面的属性和方法</span>    console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token function">dir</span><span class="token punctuation">(</span>timer<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">&gt;</span><span class="token operator">&lt;</span>script<span class="token operator">&gt;</span>    <span class="token comment" spellcheck="true">// 1.返回的是获取过来元素对象的集合以伪数组的形式存储的</span>    <span class="token keyword">var</span> lis <span class="token operator">=</span> document<span class="token punctuation">.</span><span class="token function">getElementsByTagName</span><span class="token punctuation">(</span><span class="token string">'li'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>lis<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">var</span> i <span class="token operator">=</span> o<span class="token punctuation">;</span> i<span class="token operator">&lt;</span> lis<span class="token punctuation">.</span>length<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>        console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>lis<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 2.如果页面中只有一个li返回的还是伪数组的形式</span>    <span class="token comment" spellcheck="true">// 3.如果页面中没有这个元素返回的空的伪数组的形式</span>    <span class="token comment" spellcheck="true">// 5. element.getElementsByTagName('标签名');</span>    <span class="token keyword">var</span> ol <span class="token operator">=</span> document<span class="token punctuation">.</span><span class="token function">getElementsByTagName</span><span class="token punctuation">(</span><span class="token string">'ol'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// [ol]</span>    console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>ol<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">getElementsByTagName</span><span class="token punctuation">(</span><span class="token string">'li'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">&gt;</span># 获取body和html元素<span class="token operator">&lt;</span>script<span class="token operator">&gt;</span>    <span class="token comment" spellcheck="true">//1.获取body元素</span>    <span class="token keyword">var</span> bodyEle <span class="token operator">=</span> document<span class="token punctuation">.</span>body<span class="token punctuation">;</span>    console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>bodyEle<span class="token punctuation">)</span><span class="token punctuation">;</span>    console<span class="token punctuation">.</span><span class="token function">dir</span><span class="token punctuation">(</span>bodyEle<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//2.获取html元素</span>    <span class="token keyword">var</span> htmlEle <span class="token operator">=</span> document<span class="token punctuation">.</span>documentElement<span class="token punctuation">;</span>    console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>htmlEle<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">&gt;</span></code></pre><p><strong>事件基础</strong></p><p>JavaScript使我们有能力创建动态页面,而事件是可以被JavaScript侦测到的行为。简单理解∶触发—响应机制。网页中的每个元素都可以产生某些可以触发JavaScript的事件,例如,我们可以在用户点击某按钮时产生一个事件,然后去执行某些操作。</p><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>body</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>button</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>btn<span class="token punctuation">"</span></span><span class="token punctuation">&gt;</span></span>唐伯虎<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>button</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span><span class="token punctuation">&gt;</span></span><span class="token script language-javascript">    <span class="token comment" spellcheck="true">//点击一个按钮，弹出对话框</span>    <span class="token comment" spellcheck="true">//1．事件是有三部分组成Ⅰ事件源﹑事件类型事件处理程序―我们也称为事件三要素1/(1）事件源事件被触发的对象谁按钮</span>    <span class="token keyword">var</span> btn <span class="token operator">=</span> document<span class="token punctuation">.</span><span class="token function">getElementById</span><span class="token punctuation">(</span> ' btn " <span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//(2）事件类型﹑如何触发什么事件比如鼠标点击(onclick)还是鼠标经过还是键盘按下</span>    <span class="token comment" spellcheck="true">//(3）事件处理程序通过一个函数赋值的方式完成</span>    btn<span class="token punctuation">.</span>onclick <span class="token operator">=</span> <span class="token keyword">function</span><span class="token punctuation">(</span><span class="token punctuation">)</span>i    <span class="token function">alert</span><span class="token punctuation">(</span><span class="token string">'点秋香'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    </span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">&gt;</span></span>&lt;/ body&gt;</code></pre><p><strong>节点操作</strong></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenMMLab</title>
      <link href="openmmlab/"/>
      <url>openmmlab/</url>
      
        <content type="html"><![CDATA[<h3 id="网络训练流程"><a href="#网络训练流程" class="headerlink" title="网络训练流程"></a>网络训练流程</h3><img src="/openmmlab/image-20211027182440653.png" alt="image-20211027182440653" style="zoom: 50%;"><h2 id="MMCV"><a href="#MMCV" class="headerlink" title="MMCV"></a>MMCV</h2><p><a href="https://mmcv.readthedocs.io/en/latest/understand_mmcv/registry.html">mmcv文档</a></p><h3 id="Registry"><a href="#Registry" class="headerlink" title="Registry"></a>Registry</h3><p> a mapping that maps a class to a string</p><p><a href="https://www.cnblogs.com/573177885qq/p/14307875.html">注册器机制教学</a></p><h3 id="MMDetection"><a href="#MMDetection" class="headerlink" title="MMDetection"></a>MMDetection</h3><img src="/openmmlab/image-20211027184428549.png" alt="image-20211027184428549" style="zoom: 67%;"><img src="/openmmlab/image-20211027184717357.png" alt="image-20211027184717357" style="zoom: 50%;"><h3 id="MMSegmentation"><a href="#MMSegmentation" class="headerlink" title="MMSegmentation"></a>MMSegmentation</h3><p><a href="https://mmsegmentation.readthedocs.io/zh_CN/latest/get_started.html">中文教程</a></p><p><a href="https://mmsegmentation.readthedocs.io/en/latest/">英文教程</a></p><p><a href="https://blog.csdn.net/qq_20549061/article/details/107871736">文件脉络介绍</a></p><img src="/openmmlab/image-20211027192829394.png" alt="image-20211027192829394" style="zoom: 67%;"><img src="/openmmlab/image-20211027192937764.png" alt="image-20211027192937764" style="zoom: 33%;"><img src="/openmmlab/image-20211027195123179.png" alt="image-20211027195123179" style="zoom:67%;"><img src="/openmmlab/image-20211027195312664.png" alt="image-20211027195312664" style="zoom: 67%;"><p>辅助解码头:结合底层特征的解码头</p><p>级联解码头:级联好多个解码头</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>办公软件笔记</title>
      <link href="ban-gong-ruan-jian-bi-ji/"/>
      <url>ban-gong-ruan-jian-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="Word"><a href="#Word" class="headerlink" title="Word"></a>Word</h2><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><h3 id="Axmath"><a href="#Axmath" class="headerlink" title="Axmath"></a>Axmath</h3><p><img src="/ban-gong-ruan-jian-bi-ji/image-20210606102006147.png" alt="image-20210606102006147"></p><p>170%,公式缩放的面板，<strong>ctrl+鼠标滑轮</strong>调整</p><hr><img src="/ban-gong-ruan-jian-bi-ji/image-20210606102226084.png" alt="image-20210606102226084" style="zoom:67%;"><p>符号面板，<strong>鼠标滑轮</strong>可以选择页面，可以为<strong>单独一个符号设置快捷键</strong>。</p><hr><p><strong>ctrl+tab</strong> 转换为latex面板</p><p>公式编辑处输入<strong>\</strong>可以像输入法一样,<strong>上下键</strong>寻找</p><hr><img src="/ban-gong-ruan-jian-bi-ji/image-20210606102938299.png" alt="image-20210606102938299" style="zoom:80%;"><p>笔记，<strong>ctrl+c</strong>用来记笔记，设置自己的常用公式</p><p>磁贴，内置好的公式</p><p>参考书，是参考书里的公式</p><hr><p><img src="/ban-gong-ruan-jian-bi-ji/image-20210606103310248.png" alt="image-20210606103310248"></p><p><strong>B/I</strong>,<strong>作用于将要输入的</strong></p><p><strong>ctrl+1</strong>是正常，<strong>ctrl+2</strong>是斜体，<strong>ctrl+3</strong>是加粗，<strong>ctrl+4</strong>是斜体+加粗，<strong>作用于已经输入的</strong></p><hr><p><img src="/ban-gong-ruan-jian-bi-ji/image-20210606103559903.png" alt="image-20210606103559903"></p><p>自动着色，按编辑器预设好的规则进行着色</p><hr><img src="/ban-gong-ruan-jian-bi-ji/image-20210606103810042.png" alt="image-20210606103810042" style="zoom:67%;"><p>word默认是<strong>基线对齐</strong>，可以利用word中的<strong>清除格式</strong>使其<strong>底部对齐</strong></p><hr><p><img src="/ban-gong-ruan-jian-bi-ji/image-20210606103923751.png" alt="image-20210606103923751"></p><p>插入公式后，<strong>行距变得不正常了</strong>。</p><img src="/ban-gong-ruan-jian-bi-ji/image-20210606104019869.png" alt="image-20210606104019869" style="zoom:67%;"><p>然后<strong>文档网格$\to$无网格</strong>，就可以恢复正常的行距了。</p><hr><p> <img src="/ban-gong-ruan-jian-bi-ji/image-20210606104213608.png" alt="image-20210606104213608"></p><p>四种对齐方式，<strong>居中对齐、靠左对齐、靠右对齐、等号对齐</strong></p><hr><img src="/ban-gong-ruan-jian-bi-ji/image-20210606104650395.png" alt="image-20210606104650395" style="zoom:50%;"><p><strong>连等式</strong>，多次使用多行连等</p><hr><h2 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h2><p>ALT+F5,使用演讲者视图</p><h2 id="Foxit-PDF"><a href="#Foxit-PDF" class="headerlink" title="Foxit PDF"></a>Foxit PDF</h2><h2 id="OneNote"><a href="#OneNote" class="headerlink" title="OneNote"></a>OneNote</h2><p><strong>Alt+= 键入新公式,在按一次就会退出</strong></p><p>输入markdown<strong>数学符号后按空格</strong></p><h2 id="Zotero"><a href="#Zotero" class="headerlink" title="Zotero"></a>Zotero</h2><img src="/ban-gong-ruan-jian-bi-ji/image-20210606113422587.png" alt="image-20210606113422587" style="zoom:67%;"><p>打标签，方便查找</p><hr><img src="/ban-gong-ruan-jian-bi-ji/image-20210606115650777.png" alt="image-20210606115650777" style="zoom:67%;"><p><em>Source Folder for Attaching new Files</em> 这个选项会监控选定的文件夹，在该文件夹下新出现的 PDF 文件就可以通过右键“Attach New File”来添加。一些教程把这个目录设置成了 Zotero 的数据存储位置，但这样其实并不实用（反正被拖入的 PDF 文件也会自动检索元数据，最后生成一个新的数据子目录，ZotFile就检测不到了）。所以我把这个目录设置为了浏览器的下载目录。</p><hr><p><strong>从剪贴板快速导入文献数据</strong></p><p>使用Bibtex等导入</p><p><strong>通过标识符添加文献原数据</strong></p><img src="/ban-gong-ruan-jian-bi-ji/image-20210606120631622.png" alt="image-20210606120631622"><p>输入<strong>ISBN,DOI，PMID或arXiv ID</strong>以添加到您的文献库。</p><hr><h2 id="Typora"><a href="#Typora" class="headerlink" title="Typora"></a>Typora</h2><h4 id="无序列表如何退出？"><a href="#无序列表如何退出？" class="headerlink" title="无序列表如何退出？"></a>无序列表如何退出？</h4><p>左上角的选项很迷，可以使用源代码模式退出，还可以按一下退格两下enter退出</p><h4 id="公式块编辑常见命令？"><a href="#公式块编辑常见命令？" class="headerlink" title="公式块编辑常见命令？"></a>公式块编辑常见命令？</h4><blockquote><p>版权声明：本文为博主原创文章，遵循<a href="http://creativecommons.org/licenses/by-sa/4.0/"> CC 4.0 BY-SA </a>版权协议，转载请附上原文出处链接和本声明。</p><p>本文链接：<a href="https://blog.csdn.net/mingzhuo_126/article/details/82722455">https://blog.csdn.net/mingzhuo_126/article/details/82722455</a> </p></blockquote><p><a href="https://blog.csdn.net/u013914471/article/details/82973812">https://blog.csdn.net/u013914471/article/details/82973812</a></p><p><a href="https://www.jianshu.com/p/25f0139637b7">https://www.jianshu.com/p/25f0139637b7</a></p><p>$\in$ 属于 $\notin$ 不属于  ${ }$ 打大括号 $\sim$打波浪线 $\partial$ 偏导 $\to$箭头 $\times$ 乘号 $\kappa$  $\otimes$ 带圈乘 $\bigotimes$ $\epsilon$  $\eta$ $\gets$  $\sim$<br>$$<br>change\line<br>$$</p><p><img src="/ban-gong-ruan-jian-bi-ji/image-20201125205741314.png" alt="数学模式重音符"></p><p>$\lfloor x \rfloor$ 向下取整  $\lceil x \rceil$向上取整  $\begin{cases} \end{cases}$分段函数大花括号  $\varepsilon$ </p><h4 id="emoji表情md编写？"><a href="#emoji表情md编写？" class="headerlink" title="emoji表情md编写？"></a>emoji表情md编写？</h4><p>参考大神博客：<a href="https://sunhwee.com/posts/a927e90e.html">https://sunhwee.com/posts/a927e90e.html</a></p><h4 id="Markdown设置？"><a href="#Markdown设置？" class="headerlink" title="Markdown设置？"></a>Markdown设置？</h4><p>可以在偏好设置的Markdown处设置内联公示、代码行号、公式序号</p><h4 id="括号问题"><a href="#括号问题" class="headerlink" title="括号问题"></a>括号问题</h4><p>只有中文括号可以显示在博客中</p><h2 id="Overleaf"><a href="#Overleaf" class="headerlink" title="Overleaf"></a>Overleaf</h2>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 办公软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cpp朝花夕拾</title>
      <link href="cpp-zhao-hua-xi-shi/"/>
      <url>cpp-zhao-hua-xi-shi/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zh.cppreference.com/w/%E9%A6%96%E9%A1%B5">cppreference</a></p><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><h4 id="结构struct"><a href="#结构struct" class="headerlink" title="结构struct"></a>结构struct</h4><pre class=" language-cpp"><code class="language-cpp"><span class="token keyword">struct</span> debts <span class="token comment" spellcheck="true">//定义结构</span><span class="token punctuation">{</span>  <span class="token keyword">char</span> name<span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">]</span><span class="token punctuation">;</span>  <span class="token keyword">double</span> amount<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span></code></pre><p>C89中，debts称为标签名（tag），struct tag合起来才是<strong>类型名</strong>.由此使用时要<strong>struct + tag</strong>。<br>可以<strong>象楼上说的，用typedef创建类型名。</strong><br><strong>C99中，可以直接用tag做类型名</strong>。<br><strong>C++中，tag也可以直接用tag做类型名</strong></p><h4 id="动态分配malloc"><a href="#动态分配malloc" class="headerlink" title="动态分配malloc"></a>动态分配malloc</h4><p><code>S=(Stack)malloc(sizeof(struct SNode))</code></p><p>赋值给指针要把malloc分配的内存空间转成对应的指针类型。</p><p>用完之后要<code>free(L1)</code>,完事之后L1不会变为NULL,会变为野指针,所以要注意<code>L1=NULL</code>。</p><h4 id="auto"><a href="#auto" class="headerlink" title="auto"></a>auto</h4><p>auto的原理就是根据后面的值，来自己推测前面的类型是什么。</p><pre class=" language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">// auto作用就是迭代容器中所有的元素，每一个元素的临时名字就是x，等同于下边代码。</span><span class="token keyword">for</span> <span class="token punctuation">(</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">&gt;</span><span class="token operator">::</span>iterator iter <span class="token operator">=</span> nums<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> iter <span class="token operator">!=</span> nums<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> iter<span class="token operator">++</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">// b为一个容器,效果是利用a遍历并获得b容器中的每一个值,但是a无法影响到b容器中的元素。</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> a<span class="token operator">:</span>b<span class="token punctuation">)</span><span class="token comment" spellcheck="true">// 加了引用符号,可以对容器中的内容进行赋值,即可通过对a赋值来做到容器b的内容填充。</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">auto</span> <span class="token operator">&amp;</span>a<span class="token operator">:</span>b<span class="token punctuation">)</span></code></pre><p><strong>单双引号的区别</strong></p><p>在C++中单引号表示字符，双引号表示字符串。</p><p><strong>反向迭代器</strong></p><p><code>vector&lt;int&gt;(res.rbegin(),res.rend())</code>,生成一个反向的vector</p><p><code>vector&lt;vector&lt;bool&gt;&gt; st(rows, vector&lt;bool&gt;(cols))</code>,<code>st</code>包含<code>rows</code>个值为<code>vector&lt;bool&gt;</code>的元素,<code>vector&lt;bool&gt;(cols)</code>表示<code>cols</code>个执行了值初始化的对象</p>]]></content>
      
      
      <categories>
          
          <category> cpp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cpp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git学习</title>
      <link href="git-xue-xi/"/>
      <url>git-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="git学习"><a href="#git学习" class="headerlink" title="git学习"></a>git学习</h2><h4 id="查看本地git安装位置"><a href="#查看本地git安装位置" class="headerlink" title="查看本地git安装位置"></a>查看本地git安装位置</h4><p>git安装时没有注意git的安装位置，使用的默认的安装位置，当在idea中配置版本控制器Version Control时，需要填写git.exe文件的位置，我用的windows系统。我们可以利用win+R，然后输入cmd,回车进入界面。</p><p>然后输入:where git 回车，就可以查看git安装位置。</p><h3 id="廖雪峰教程"><a href="#廖雪峰教程" class="headerlink" title="廖雪峰教程"></a>廖雪峰教程</h3><blockquote><p>廖雪峰git教程 <a href="https://www.liaoxuefeng.com/wiki/896043488029600">https://www.liaoxuefeng.com/wiki/896043488029600</a></p></blockquote><h4 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h4><pre class=" language-shell"><code class="language-shell"># 安装git后首先要设置的是设置用户名和email,这是非常重要的,因为每个git提交都会使用该信息,它被永远嵌入到你的提交中。$ git config --global user.name "Your Name"$ git config --global user.email "email@example.com"# 可以使用以下命令来查看设置的email和name$ git config user.email$ git config user.name# 加上--global 是对所有仓库使用email和name设置，去掉后只对特定git仓库使用，前提是必须是一个git仓库，即已经git init过了。#下面的命令可以列出所有的git的配置信息git config --list/-l# 显示系统配置git config --system --list# 用户自己配的配置git config --global --list</code></pre><p>因为Git是分布式版本控制系统，所以，==每个机器都必须自报家门：你的名字和Email地址==。</p><p>注意<code>git config</code>命令的<code>--global</code>参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。</p><p>Git相关配置文件:</p><ol><li>Git\etc\gitconfig</li><li>C:\Users\Administrator(不同人的名字不同)\.gitconfig</li></ol><p>可以在本地修改</p><img src="/git-xue-xi/image-20210927163502413.png" alt="关系" style="zoom:67%;"><p>Working Directory:工作区,平时存放项目代码的地方</p><p>Index/Stage:暂存区,用于临时存放你的改动,事实上它只是一个文件</p><p>Repository:仓库区(或本地仓库),就是安全存放数据的位置,这里面有你提交到所有版本的数据,其中HEAD指向最新放入仓库的版本。</p><p>Remote:远程仓库,托管代码的服务器,可以简单的认为是你项目组中的一台电脑用于远程数据交换。</p><p><img src="/git-xue-xi/image-20210927164322246.png" alt="关系"></p><h4 id="创建版本库"><a href="#创建版本库" class="headerlink" title="创建版本库"></a>创建版本库</h4><pre class=" language-shell"><code class="language-shell"># 首先创建一个空目录$ mkdir learngit$ cd learngit$ pwd/Users/michael/learngit# 通过git init命令把这个目录变成Git可以管理的仓库：$ git init# 执行后可以看到,仅仅在本地多了一个.git文件夹,关于版本等的所有信息都存放在这个目录里面。# 另一个方式是克隆远程目录，将远程服务器上的仓库完全镜像一份至本地。$ git clone [url]</code></pre><p>不要用windows自带的记事本打开任何文本文件！</p><p>现在编写了一个readme.txt文件，</p><pre class=" language-shell"><code class="language-shell">#git add 告诉git,把文件添加到仓库$ git add readme.txt#用命令git commit告诉Git，把文件提交到仓库：$ git commit -m "wrote a readme file"#-m后面输入的是本次提交的说明，可以输入任意内容，当然最好是有意义的，这样你就能从历史记录里方便地找到改动记录。</code></pre><p>git add先将文件放到车里，可以往车里放好几次东西，git commit把一车的东西运往其他城市。</p><p>Git的<code>commit id</code>不是1，2，3……递增的数字，而是一个SHA1计算出来的一个非常大的数字，用十六进制表示。为什么<code>commit id</code>需要用这么一大串数字表示呢？因为Git是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。</p><p>git管理的文件有三种状态:已修改(modified),已暂存(staged),已提交(committed)</p><img src="/git-xue-xi/image-20210927164622525.png" alt="工作流程" style="zoom:67%;"><pre class=" language-shell"><code class="language-shell">git add 文件夹/ 添加整个文件夹及内容git add *.文件类型 添加目录中所有此文件类型的文件</code></pre><h4 id="时光机穿梭"><a href="#时光机穿梭" class="headerlink" title="时光机穿梭"></a>时光机穿梭</h4><h5 id="文件状态"><a href="#文件状态" class="headerlink" title="文件状态"></a>文件状态</h5><ul><li><strong>Untracked:</strong> <strong>未跟踪, 此文件在文件夹中</strong>, 但<strong>并没有加入到git库</strong>,不参与版本控制.通过<code>git add</code>状态变为Staged.</li><li><strong>Unmodify:</strong> 文件已经入库, 未修改, 即版本库中的文件快照内容与文件夹中完全一致. 这种类型的文件有两种去处, 如果它被修改, 而变为Modified. 如果使用<code>git rm --cached &lt;file&gt;</code>移出版本库, 则成为Untracked文件</li><li><strong>Modified:</strong> 文件已修改, 仅仅是修改, 并没有进行其他的操作. 这个文件也有两个去处, 通过<code>git add</code>可进入暂存staged状态, 使用<code>git checkout/restore</code>则丢弃修改过, 返回到unmodify状态, 这个<code>git checkout/restore</code>即从库中取出文件, 覆盖当前修改 !</li><li><strong>Staged:</strong> 暂存状态. 执行<code>git commit</code>则将修改同步到库中, 这时库中的文件和本地文件又变为一致, 文件为Unmodify状态. 执行<code>git reset HEAD filename</code>取消暂存, 文件状态为Modified.</li></ul><pre class=" language-shell"><code class="language-shell">#查看指定文件状态$git status [filename]#查看所有文件状态$git status</code></pre><p>继续修改readme.txt文件，改成如下内容：</p><pre class=" language-shell"><code class="language-shell"># 想要查看具体修改了什么内容,使用git diff命令查看$ git diff readme.txt # git stash # 本地保存当前工作进度</code></pre><h5 id="忽略文件"><a href="#忽略文件" class="headerlink" title="忽略文件"></a>忽略文件</h5><p>有些时候我们不想把某些文件纳入版本控制中，比如<strong>数据库文件，临时文件，设计文件</strong>等</p><p>在主目录下建立**”.gitignore”**文件，此文件有如下规则：</p><ol><li>忽略文件中的<strong>空行或以井号（#）开始的行</strong>将会被忽略。</li><li>可以使用<strong>Linux通配符</strong>。例如：星号（*）代表任意多个字符，问号（？）代表一个字符，方括号（[abc]）代表可选字符范围，大括号（{string1,string2,…}）代表可选的字符串等。</li><li>如果名称的最前面有一个<strong>感叹号（!），表示例外规则</strong>，将不被忽略。</li><li>如果名称的<strong>最前面是一个路径分隔符（/）</strong>，表示要<strong>忽略的文件</strong>在此目录下，而子目录中的文件不忽略。</li><li>如果名称的<strong>最后面是一个路径分隔符（/）</strong>，表示要<strong>忽略的是此目录下该名称的子目录</strong>，<strong>而非文件</strong>（<strong>默认文件或目录都忽略</strong>）。</li></ol><pre class=" language-shell"><code class="language-shell">#为注释*.txt        #忽略所有 .txt结尾的文件,这样的话上传就不会被选中！!lib.txt     #但lib.txt除外/temp        #仅忽略项目根目录下的TODO文件,不包括其它目录tempbuild/       #忽略build/目录下的所有文件doc/*.txt    #会忽略 doc/notes.txt 但不包括 doc/server/arch.txt</code></pre><h5 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h5><p>不断对文件进行修改，然后不断提交修改到版本库里，就好比玩RPG游戏时，每通过一关就会自动把游戏状态存盘，如果某一关没过去，你还可以选择读取前一关的状态。有些时候，在打Boss之前，你会手动存盘，以便万一打Boss失败了，可以从最近的地方重新开始。Git也是一样，每当你觉得文件修改到一定程度的时候，就可以“保存一个快照”，这个快照在Git中被称为<code>commit</code>。一旦你把文件改乱了，或者误删了文件，还可以从最近的一个<code>commit</code>恢复，然后继续工作，而不是把几个月的工作成果全部丢失。</p><pre class=" language-shell"><code class="language-shell">$ git log# 显示从最近到最远的提交日志,如果嫌输出信息太多，看得眼花缭乱的，可以试试加上--pretty=oneline参数：</code></pre><p>好了，现在我们启动时光穿梭机，准备把<code>readme.txt</code>回退到上一个版本，也就是<code>add distributed</code>的那个版本，怎么做呢？</p><p>首先，Git必须知道当前版本是哪个版本，在Git中，用<code>HEAD</code>表示当前版本，也就是最新的提交<code>1094adb...</code>（注意我的提交ID和你的肯定不一样），上一个版本就是<code>HEAD^</code>，上上一个版本就是<code>HEAD^^</code>，当然往上100个版本写100个<code>^</code>比较容易数不过来，所以写成<code>HEAD~100</code>。</p><pre class=" language-shell"><code class="language-shell">$ git reset --hard HEAD^ #有三种 hard soft mixed 其中hard清除暂存区和工作目录的内容HEAD is now at e475afc add distributed</code></pre><p>最新的那个版本<code>append GPL</code>已经看不到了！好比你从21世纪坐时光穿梭机来到了19世纪，想再回去已经回不去了，肿么办？</p><p>办法其实还是有的，只要上面的命令行窗口还没有被关掉，你就可以顺着往上找啊找啊，找到那个<code>append GPL</code>的<code>commit id</code>是<code>1094adb...</code>，于是就可以指定回到未来的某个版本：</p><pre class=" language-shell"><code class="language-shell">$ git reset --hard 1094aHEAD is now at 83b0afe append GPL</code></pre><p>版本号没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了。</p><p>Git的版本回退速度非常快，因为Git在内部有个指向当前版本的<code>HEAD</code>指针，当你回退版本的时候，Git仅仅是把HEAD从指向<code>append GPL</code>：</p><img src="/git-xue-xi/image-20210505145945408.png" alt="image-20210505145945408" style="zoom:50%;"><p>现在，你回退到了某个版本，关掉了电脑，第二天早上就后悔了，想恢复到新版本怎么办？找不到新版本的<code>commit id</code>怎么办？</p><p>在Git中，总是有后悔药可以吃的。当你用<code>$ git reset --hard HEAD^</code>回退到<code>add distributed</code>版本时，再想恢复到<code>append GPL</code>，就必须找到<code>append GPL</code>的commit id。Git提供了一个命令<code>git reflog</code>用来记录你的每一次命令：</p><pre class=" language-shell"><code class="language-shell">$ git refloge475afc HEAD@{1}: reset: moving to HEAD^1094adb (HEAD -&gt; master) HEAD@{2}: commit: append GPLe475afc HEAD@{3}: commit: add distributedeaadf4e HEAD@{4}: commit (initial): wrote a readme file</code></pre><h4 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h4><h5 id="SSH公钥和私钥"><a href="#SSH公钥和私钥" class="headerlink" title="SSH公钥和私钥"></a>SSH公钥和私钥</h5><p><a href="https://zhuanlan.zhihu.com/p/134349361">非对称加密</a></p><p>使用Github作为免费的Git远程仓库。</p><p>由于你的本地Git仓库和GitHub仓库之间的传输是通过SSH加密的，所以，需要一点设置：</p><p>第1步：创建SSH Key。在用户主目录<code>C:\Users\dch</code>下，看看有没有.ssh目录,如果有,再看看这个目录下有没有<code>id_rsa</code>和<code>id_rsa.pub</code>这两个文件,如果已经有了,可直接跳到下一步。如果没有,打开Shell(Windows下打开Git Bash),创建SSH Key:</p><pre class=" language-shell"><code class="language-shell">$ ssh-keygen -t rsa -C dchlab# -t 使用的不同的加密算法# -C The email is only optional field to identify the key用于识别这个密钥的注释# 所以这个注释你可以输入任何内容，很多网站和软件用这个注释作为密钥的名字</code></pre><p>输入命令之后<strong>一路回车</strong>，使用默认值即可。</p><p>如果一切顺利的话，可以在用户主目录里找到<code>.ssh</code>目录，里面有<code>id_rsa</code>和<code>id_rsa.pub</code>两个文件，这两个就是SSH Key的秘钥对,<code>id_rsa</code>是私钥,不能泄露出去,<code>id_rsa.pub</code>是公钥，可以放心地告诉任何人。</p><p>第2步：登陆GitHub，打开“Account settings”，“SSH Keys”页面：</p><p>然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴<code>id_rsa.pub</code>文件的内容：</p><p>为什么GitHub需要SSH Key呢？因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送。</p><p>当然，GitHub允许你添加多个Key。假定你有若干电脑，你一会儿在公司提交，一会儿在家里提交，只要把每台电脑的Key都添加到GitHub，就可以在每台电脑上往GitHub推送了。</p><p>最后友情提示，在GitHub上免费托管的Git仓库，任何人都可以看到喔（但只有你自己才能改）。所以，不要把敏感信息放进去。</p><p>如果你不想让别人看到Git库，有两个办法，一个是交点保护费，让GitHub把公开的仓库变成私有的，这样别人就看不见了（不可读更不可写）。另一个办法是自己动手，搭一个Git服务器，因为是你自己的Git服务器，所以别人也是看不见的。这个方法我们后面会讲到的，相当简单，公司内部开发必备。</p><p>确保你拥有一个GitHub账号后，我们就即将开始远程仓库的学习。</p><h5 id="本地库关联到远程库"><a href="#本地库关联到远程库" class="headerlink" title="本地库关联到远程库"></a>本地库关联到远程库</h5><pre class=" language-shell"><code class="language-shell"># 查看本地所有分支$git branch# 查看远程所有分支$git branch -r# 查看本地和远程所有分支$git branch -a# 一般当前本地分支前带有“*”号且为绿色，远程分支为红色，‘*’号表示当前分支# 修改git仓库远程分支名# 直接在github改就行</code></pre><p>首先，登陆GitHub，然后，在右上角找到“Create a new repo”按钮，创建一个新的仓库；</p><p>在Repository name填入<code>learngit</code>，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库；</p><p>目前，在GitHub上的这个<code>learngit</code>仓库还是空的，GitHub告诉我们，可以从这个仓库克隆出新的仓库，也可以</p><p>把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库:现在，我们根据GitHub的提示，在本地的<code>learngit</code>仓库下运行命令：</p><pre class=" language-shell"><code class="language-shell"># 关联一个远程库#   远程库  添加 仓库名 仓库来源 远程库的名字是origin,Git的默认叫法git remote add origin https://github.com/chenghaoDong666/1.git# 重命名一个分支git branch -M main# 把当前分支推送到远程# -u把本地分支与远程分支关联起来,以后就不需要-u了git push -u origin main# 查看远程库信息$ git remote -v# 根据名字删除远程库# 此处的“删除”其实是解除了本地和远程的绑定关系，并不是物理上删除了远程库。# 远程库本身并没有任何改动。要真正删除远程库，需要登录到GitHub，在后台页面找到删除按钮再删除。$ git remote rm origin</code></pre><h5 id="远程库拉取到本地库"><a href="#远程库拉取到本地库" class="headerlink" title="远程库拉取到本地库"></a>远程库拉取到本地库</h5><p>上次我们讲了先有本地库，后有远程库的时候，如何关联远程库。</p><p>现在，假设我们从零开发，那么最好的方式是先创建远程库，然后，从远程库克隆。</p><p>首先，登陆GitHub，创建一个新的仓库，名字叫<code>gitskills</code>;我们勾选<code>Initialize this repository with a README</code>，这样GitHub会自动为我们创建一个<code>README.md</code>文件。创建完毕后，可以看到<code>README.md</code>文件;</p><p>现在，远程库已经准备好了，下一步是用命令<code>git clone</code>克隆一个本地库：</p><pre class=" language-shell"><code class="language-shell">$ git clone git@github.com:michaelliao/gitskills.git</code></pre><p>如果有多个人协作开发，那么每个人各自从远程克隆一份就可以了。</p><p>你也许还注意到，GitHub给出的地址不止一个，还可以用<code>https://github.com/michaelliao/gitskills.git</code>这样的地址。</p><p>实际上，Git支持多种协议，<strong>默认的git使用<code>ssh</code>，</strong>但也可以使用<code>https</code>等其他协议。使用**<code>https</code>除了速度慢<strong>以外，还有个最大的麻烦是</strong>每次推送都必须输入口令**，但是在某些只开放http端口的公司内部就无法使用<code>ssh</code>协议而只能用<code>https</code>。</p><p>克隆之后有新的更改需要拉取的话使用<code>git fetch</code>或<code>git pull</code></p><p>区别:</p><blockquote><p>git在本地会保存两个版本的仓库，分为本地仓库和远程仓库。<br>1、本地仓库就是我们平时 add、commit 的那个仓库。<br>2、远程仓库可以用git remote -v查看（这里的远程仓库是保存在本地的远程仓库，等同于另一个版本，不是远程的远程仓库）。</p><p>说说 fetch 和 pull 的不同:</p><p>fetch 只能更新远程仓库的代码为最新的，本地仓库的代码还未被更新，我们需要通过 git merge origin/master 来合并这两个版本，你可以把它理解为合并分支一样的。</p><p>pull 操作是将本地仓库和远程仓库（本地的）更新到远程的最新版本。</p><p>如果想要更加可控一点的话推荐使用fetch + merge。<br>原文链接：<a href="https://blog.csdn.net/qq_37420939/article/details/89736567">https://blog.csdn.net/qq_37420939/article/details/89736567</a></p></blockquote><h4 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h4><img src="/git-xue-xi/image-20230523220805879.png" alt="image-20230523220805879" style="zoom: 67%;"><p>分支在实际中有什么用呢？假设你准备开发一个新功能，但是需要两周才能完成，第一周你写了50%的代码，如果立刻提交，由于代码还没写完，不完整的代码库会导致别人不能干活了。如果等代码全部写完再一次提交，又存在丢失每天进度的巨大风险。</p><p>现在有了分支，就不用怕了。你创建了一个属于你自己的分支，别人看不到，还继续在原来的分支上正常工作，而你在自己的分支上干活，想提交就提交，直到开发完毕后，再一次性合并到原来的分支上，这样，既安全，又不影响别人工作。</p><h5 id="创建与合并分支"><a href="#创建与合并分支" class="headerlink" title="创建与合并分支"></a>创建与合并分支</h5><p>在<a href="https://www.liaoxuefeng.com/wiki/896043488029600/897013573512192">版本回退</a>里，你已经知道，每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫主分支，即<code>master</code>分支。<code>HEAD</code>严格来说不是指向提交，而是指向<code>master</code>，<code>master</code>才是指向提交的，所以，<code>HEAD</code>指向的就是当前分支。</p><p>一开始的时候，<code>master</code>分支是一条线，Git用<code>master</code>指向最新的提交，再用<code>HEAD</code>指向<code>master</code>，就能确定当前分支，以及当前分支的提交点：</p><img src="/git-xue-xi/image-20210505160536491.png" alt="image-20210505160536491" style="zoom:67%;"><p>每次提交，<code>master</code>分支都会向前移动一步，这样，随着你不断提交，<code>master</code>分支的线也越来越长。</p><p>当我们创建新的分支，例如<code>dev</code>时，Git新建了一个指针叫<code>dev</code>，指向<code>master</code>相同的提交，再把<code>HEAD</code>指向<code>dev</code>，就表示当前分支在<code>dev</code>上：</p><img src="/git-xue-xi/image-20210505160558181.png" alt="image-20210505160558181" style="zoom: 50%;"><p>你看，Git创建一个分支很快，因为除了增加一个<code>dev</code>指针，改改<code>HEAD</code>的指向，工作区的文件都没有任何变化！</p><p>不过，从现在开始，对工作区的修改和提交就是针对<code>dev</code>分支了，比如新提交一次后，<code>dev</code>指针往前移动一步，而<code>master</code>指针不变：</p><img src="/git-xue-xi/image-20210505160621512.png" alt="image-20210505160621512" style="zoom:50%;"><p>假如我们在<code>dev</code>上的工作完成了，就可以把<code>dev</code>合并到<code>master</code>上。Git怎么合并呢？最简单的方法，就是直接把<code>master</code>指向<code>dev</code>的当前提交，就完成了合并；所以Git合并分支也很快！就改改指针，工作区内容也不变！</p><p>合并完分支后，甚至可以删除<code>dev</code>分支。删除<code>dev</code>分支就是把<code>dev</code>指针给删掉，删掉后，我们就剩下了一条<code>master</code>分支；</p><p>首先，我们创建<code>dev</code>分支，然后切换到<code>dev</code>分支：</p><pre class=" language-shell"><code class="language-shell"># 新建一个分支,但依然停留在当前分支$ git branch [branch-name]# 新建一个分支,并切换到该分支$ git checkout -b dev# 合并指定分支到当前分支$ git merge dev# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch]</code></pre><p>注意到上面的<code>Fast-forward</code>信息，Git告诉我们，这次合并是“快进模式”，也就是直接把<code>master</code>指向<code>dev</code>的当前提交，所以合并速度非常快。当然，也不是每次合并都能<code>Fast-forward</code>，我们后面会讲其他方式的合并。</p><p>如果同一个文件在合并分支时都被修改了则会引起冲突：解决的办法是我们可以修改冲突文件后重新提交！选择要保留他的代码还是你的代码！</p><p>==master主分支应该非常稳定，用来发布新版本，一般情况下不允许在上面工作，工作一般情况下在新建的dev分支上工作，工作完后，比如上要发布，或者说dev分支代码稳定后可以合并到主分支master上来。==</p><p>Git鼓励大量使用分支</p><p>实际上，切换分支这个动作，用<code>switch</code>更科学。因此，最新版本的Git提供了新的<code>git switch</code>命令来切换分支：</p><p>创建并切换到新的<code>dev</code>分支，可以使用：</p><pre class=" language-shell"><code class="language-shell">$ git switch -c dev</code></pre><p>直接切换到已有的<code>master</code>分支，可以使用：</p><pre class=" language-shell"><code class="language-shell">$ git switch master</code></pre><p>使用新的<code>git switch</code>命令，比<code>git checkout</code>要更容易理解。</p><h5 id="解决冲突"><a href="#解决冲突" class="headerlink" title="解决冲突"></a>解决冲突</h5><p><code>master</code>分支和<code>feature1</code>分支各自都分别有新的提交，变成了这样：</p><img src="/git-xue-xi/image-20210505162330416.png" alt="image-20210505162330416" style="zoom:50%;"><p>这种情况下，Git无法执行“快速合并”，只能试图把各自的修改合并起来，但这种合并就可能会有冲突，我们试试看：</p><pre class=" language-shell"><code class="language-shell">$ git merge feature1Auto-merging readme.txtCONFLICT (content): Merge conflict in readme.txtAutomatic merge failed; fix conflicts and then commit the result.# 存在冲突,必须手动解决冲突# git status可以告诉我们冲突的文件</code></pre><p>我们可以直接查看readme.txt的内容：</p><pre class=" language-shell"><code class="language-shell">Git is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files.&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADCreating a new branch is quick &amp; simple.=======Creating a new branch is quick AND simple.&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1</code></pre><p>Git用<code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>，<code>=======</code>，<code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>标记出不同分支的内容，我们修改如下后保存：</p><pre><code>Creating a new branch is quick and simple.</code></pre><p>再提交：</p><pre class=" language-shell"><code class="language-shell">$ git add readme.txt $ git commit -m "conflict fixed"[master cf810e4] conflict fixed</code></pre><p>用带参数的<code>git log</code>也可以看到分支的合并情况：</p><pre class=" language-shell"><code class="language-shell">$ git log --graph --pretty=oneline --abbrev-commit*   cf810e4 (HEAD -&gt; master) conflict fixed|\  | * 14096d0 (feature1) AND simple* | 5dc6824 &amp; simple|/  * b17d20e branch test* d46f35e (origin/master) remove test.txt* b84166e add test.txt* 519219b git tracks changes* e43a48b understand how stage works* 1094adb append GPL* e475afc add distributed* eaadf4e wrote a readme file</code></pre><p>最后，删除<code>feature1</code>分支：</p><pre class=" language-shell"><code class="language-shell">$ git branch -d feature1Deleted branch feature1 (was 14096d0).</code></pre><h5 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h5><p>通常，合并分支时，如果可能，Git会用<code>Fast forward</code>模式，但这种模式下，删除分支后，会丢掉分支信息。</p><p>如果要强制禁用<code>Fast forward</code>模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。</p><p>准备合并<code>dev</code>分支，请注意<code>--no-ff</code>参数，表示禁用<code>Fast forward</code>：</p><pre class=" language-shell"><code class="language-shell">$ git merge --no-ff -m "merge with no-ff" devMerge made by the 'recursive' strategy. readme.txt | 1 + 1 file changed, 1 insertion(+)</code></pre><p>因为本次合并要创建一个新的commit，所以加上<code>-m</code>参数，把commit描述写进去。</p><p>可以看到，不使用<code>Fast forward</code>模式，merge后就像这样：</p><img src="/git-xue-xi/image-20210505163256977.png" alt="image-20210505163256977" style="zoom:50%;"><p>在实际开发中，我们应该按照几个基本原则进行分支管理：</p><p>首先，<code>master</code>分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；</p><p>那在哪干活呢？干活都在<code>dev</code>分支上，也就是说，<code>dev</code>分支是不稳定的，到某个时候，比如1.0版本发布时，再把<code>dev</code>分支合并到<code>master</code>上，在<code>master</code>分支发布1.0版本；</p><p>你和你的小伙伴们每个人都在<code>dev</code>分支上干活，每个人都有自己的分支，时不时地往<code>dev</code>分支上合并就可以了。</p><p>所以，团队合作的分支看起来就像这样：</p><img src="/git-xue-xi/image-20210505163109667.png" alt="image-20210505163109667" style="zoom:67%;"><h5 id="Bug分支"><a href="#Bug分支" class="headerlink" title="Bug分支"></a>Bug分支</h5><p>当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支<code>issue-101</code>来修复它，但是，等等，当前正在<code>dev</code>上进行的工作还没有提交；并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？</p><p>幸好，Git还提供了一个<code>stash</code>功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作：</p><pre class=" language-shell"><code class="language-shell">$ git stash</code></pre><p>现在，用<code>git status</code>查看工作区，就是干净的（除非有没有被Git管理的文件），因此可以放心地创建分支来修复bug。</p><p>首先确定要在哪个分支上修复bug，假定需要在<code>master</code>分支上修复，就从<code>master</code>创建临时分支,修复bug，提交，合并，删除；刚才的工作现场存到哪去了？用<code>git stash list</code>命令看看：</p><pre class=" language-shell"><code class="language-shell">$ git stash liststash@{0}: WIP on dev: f52c633 add merge</code></pre><p>工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，有两个办法：</p><p>一是用<code>git stash apply</code>恢复，但是恢复后，stash内容并不删除，你需要用<code>git stash drop</code>来删除；</p><p>另一种方式是用<code>git stash pop</code>，恢复的同时把stash内容也删了：</p><pre class=" language-shell"><code class="language-shell">$ git stash popOn branch devChanges to be committed:  (use "git reset HEAD <file>..." to unstage)    new file:   hello.pyChanges not staged for commit:  (use "git add <file>..." to update what will be committed)  (use "git checkout -- <file>..." to discard changes in working directory)    modified:   readme.txtDropped refs/stash@{0} (5d677e2ee266f39ea296182fb2354265b91b3b2a)</file></file></file></code></pre><p>再用<code>git stash list</code>查看，就看不到任何stash内容了：</p><pre class=" language-shell"><code class="language-shell">$ git stash list</code></pre><p>你可以多次stash，恢复的时候，先用<code>git stash list</code>查看，然后恢复指定的stash，用命令：</p><pre class=" language-shell"><code class="language-shell">$ git stash apply stash@{0}</code></pre><p>在master分支上修复的bug，想要合并到当前dev分支，可以用<code>git cherry-pick &lt;commit&gt;</code>命令，把bug提交的修改“复制”到当前分支，避免重复劳动。</p><h5 id="Feature分支"><a href="#Feature分支" class="headerlink" title="Feature分支"></a>Feature分支</h5><p>开发一个新feature，最好新建一个分支；</p><p>如果要丢弃一个没有被合并过的分支，可以通过<code>git branch -D &lt;name&gt;</code>强行删除。</p><h5 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h5><p>当你从远程仓库克隆时，实际上Git自动把本地的<code>master</code>分支和远程的<code>master</code>分支对应起来了，并且，远程仓库的默认名称是<code>origin</code>。</p><p>要查看远程库的信息，用<code>git remote</code>：</p><pre class=" language-shell"><code class="language-shell">$ git remoteorigin</code></pre><p>或者，用<code>git remote -v</code>显示更详细的信息：</p><pre class=" language-shell"><code class="language-shell">$ git remote -vorigin  git@github.com:michaelliao/learngit.git (fetch)origin  git@github.com:michaelliao/learngit.git (push)</code></pre><p>推送分支，就是把该分支上的所有本地提交推送到远程库。推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上：</p><pre class=" language-shell"><code class="language-shell">$ git push origin master</code></pre><p>如果要推送其他分支，比如<code>dev</code>，就改成：</p><pre class=" language-shell"><code class="language-shell">$ git push origin dev</code></pre><p>但是，并不是一定要把本地分支往远程推送，那么，哪些分支需要推送，哪些不需要呢？</p><ul><li><code>master</code>分支是主分支，因此要时刻与远程同步；</li><li><code>dev</code>分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步；</li><li>bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug；</li><li>feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。</li></ul><p>总之，就是在Git中，分支完全可以在本地自己藏着玩，是否推送，视你的心情而定！</p><p>多人协作的工作模式通常是这样：</p><ol><li>首先，可以试图用<code>git push origin &lt;branch-name&gt;</code>推送自己的修改；</li><li>如果推送失败，则因为远程分支比你的本地更新，需要先用<code>git pull</code>试图合并；</li><li>如果合并有冲突，则解决冲突，并在本地提交；</li><li>没有冲突或者解决掉冲突后，再用<code>git push origin &lt;branch-name&gt;</code>推送就能成功！</li></ol><p>如果<code>git pull</code>提示<code>no tracking information</code>，则说明本地分支和远程分支的链接关系没有创建，用命令<code>git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;</code>。</p><p>这就是多人协作的工作模式，一旦熟悉了，就非常简单。</p><h5 id="Rebase"><a href="#Rebase" class="headerlink" title="Rebase"></a>Rebase</h5><p>在上一节我们看到了，多人在同一个分支上协作时，很容易出现冲突。即使没有冲突，后push的童鞋不得不先pull，在本地合并，然后才能push成功。每次合并再push后，分支看上去很乱，有强迫症的童鞋会问：为什么Git的提交历史不能是一条干净的直线？</p><p>其实是可以做到的！</p><p>Git有一种称为rebase的操作，有人把它翻译成“变基”。</p><p>假设Git目前只有一个分支master。开发人员的工作流程是</p><ul><li>git clone master branch</li><li>在自己本地checkout -b local创建一个本地开发分支</li><li>在本地的开发分支上开发和测试</li><li>阶段性开发完成后（包含功能代码和单元测试），可以准备提交代码<ul><li>首先切换到master分支，git pull拉取最新的分支状态</li><li>然后切回local分支</li><li>通过git rebase -i 将本地的多次提交合并为一个，以简化提交历史。本地有多个提交时,如果不进行这一步,在git rebase master时会多次解决冲突(最坏情况下,每一个提交都会相应解决一个冲突)</li><li>git rebase master 将master最新的分支同步到本地，这个过程可能需要手动解决冲突(如果进行了上一步的话,只用解决一次冲突)</li><li>然后切换到master分支，git merge将本地的local分支内容合并到master分支</li><li>git push将master分支的提交上传</li></ul></li><li>本地开发分支可以灵活管理</li></ul><h4 id="标签管理"><a href="#标签管理" class="headerlink" title="标签管理"></a>标签管理</h4><p>发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。</p><p>Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。</p><p>Git有commit，为什么还要引入tag？</p><p>“请把上周一的那个版本打包发布，commit号是6a5819e…”</p><p>“一串乱七八糟的数字不好找！”</p><p>如果换一个办法：</p><p>“请把上周一的那个版本打包发布，版本号是v1.2”</p><p>“好的，按照tag v1.2查找commit就行！”</p><p>所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。</p><h5 id="创建标签"><a href="#创建标签" class="headerlink" title="创建标签"></a>创建标签</h5><p>在Git中打标签非常简单，首先，切换到需要打标签的分支上,然后，敲命令<code>git tag &lt;name&gt;</code>就可以打一个新标签：</p><pre class=" language-shell"><code class="language-shell">$ git tag v1.0</code></pre><p>可以用命令<code>git tag</code>查看所有标签：</p><pre class=" language-shell"><code class="language-shell">$ git tagv1.0</code></pre><h4 id="使用Github"><a href="#使用Github" class="headerlink" title="使用Github"></a>使用Github</h4><p>一定要从自己的账号下clone仓库，这样你才能推送修改。如果从bootstrap的作者的仓库地址<code>git@github.com:twbs/bootstrap.git</code>克隆，因为没有权限，你将不能推送修改。</p><p>Bootstrap的官方仓库<code>twbs/bootstrap</code>、你在GitHub上克隆的仓库<code>my/bootstrap</code>，以及你自己克隆到本地电脑的仓库，他们的关系就像下图显示的那样：</p><img src="/git-xue-xi/image-20210505165906120.png" alt="image-20210505165906120" style="zoom:50%;"><p>如果你想修复bootstrap的一个bug，或者新增一个功能，立刻就可以开始干活，干完后，往自己的仓库推送。</p><p>如果你希望bootstrap的官方库能接受你的修改，你就可以在GitHub上发起一个pull request。当然，对方是否接受你的pull request就不一定了。</p><h4 id="gitignore"><a href="#gitignore" class="headerlink" title=".gitignore"></a>.gitignore</h4><p><a href="https://blog.csdn.net/nyist_zxp/article/details/119887324">https://blog.csdn.net/nyist_zxp/article/details/119887324</a></p><h2 id="Github学习"><a href="#Github学习" class="headerlink" title="Github学习"></a>Github学习</h2><h3 id="基础了解"><a href="#基础了解" class="headerlink" title="基础了解"></a>基础了解</h3><p>先 fork 别人的仓库，相当于拷贝一份，相信我，不会有人直接让你改修原仓库的</p><p>clone 到本地分支，做一些 bug fix</p><p>发起 pull request 给原仓库，让他看到你修改的 bug</p><p>原仓库 review 这个 bug，如果是正确的话，就会 merge 到他自己的项目中</p><p>Pull request:简而言之,**求拉!**让有权限的人review后merge进去。</p><p>可以将pull request链接到issue，以表示正在进行修复，并在merge pull request时自动关闭该issue。</p><p>您可以通过在pull request的description或commit message中使用受支持的关键字将pull requesr链接到issue(请注意,pull request必须位于默认分支上)。</p><ul><li>close</li><li>closes</li><li>closed</li><li>fix</li><li>fixes</li><li>fixed</li><li>resolve</li><li>resolves</li><li>resolved</li></ul><h3 id="Github-Desktop"><a href="#Github-Desktop" class="headerlink" title="Github Desktop"></a>Github Desktop</h3><p>首先复制仓库,然后commit changes到master,然后commit pull:</p><img src="/git-xue-xi/image-20210502213920230.png" alt="基本操作" style="zoom:67%;"><p>history可以查看pull历史,右键可以revert来撤销:</p><p><img src="/git-xue-xi/image-20210502213807069.png" alt="revert"></p><h2 id="pycharm版本控制"><a href="#pycharm版本控制" class="headerlink" title="pycharm版本控制"></a>pycharm版本控制</h2><p><a href="https://www.cnblogs.com/feixuelove1009/p/5955332.html">在Pycharm中使用GitHub进行版本控制</a></p><p><a href="https://blog.csdn.net/u013088062/article/details/50350520">Git用法</a></p><p>从六七条开始看</p><p><img src="/git-xue-xi/image-20210506224704082.png" alt="image-20210506224704082"></p><p>Reset Current Branch to Here,本地和远程都回退到某一版本：</p><img src="/git-xue-xi/image-20210506224828227.png" alt="image-20210506224828227" style="zoom: 50%;"><p>Revert Commit：</p><img src="/git-xue-xi/image-20210506224906415.png" alt="image-20210506224906415" style="zoom:50%;"><p>Checkout是用来选择branch的，注意别选择到某个commit，会出现detached head问题！</p><h2 id="服务器上的git"><a href="#服务器上的git" class="headerlink" title="服务器上的git"></a>服务器上的git</h2><p><a href="https://git-scm.com/book/zh/v2/%E8%B5%B7%E6%AD%A5-%E5%85%B3%E4%BA%8E%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6">另外一本很好的git教程</a></p><h3 id="四种协议"><a href="#四种协议" class="headerlink" title="四种协议"></a>四种协议</h3><p>本地协议（Local），HTTP 协议，SSH（Secure Shell）协议及 Git 协议。</p><p>架设 Git 服务器时<strong>常用 SSH 协议作为传输协议</strong>。 因为大多数环境下服务器已经支持通过 SSH 访问 —— 即使没有也很容易架设。 SSH 协议也是一个验证授权的网络协议；并且，因为其普遍性，架设和使用都很容易。</p><p>首先介绍SSH协议。</p><p>SSH是一种网络协议，我们常说的 ssh 一般指其实现，即 OpenSSH，<strong>在 shell 中，也就是 ssh 命令</strong></p><p>**Secure Shell(安全外壳协议，简称SSH)**是一种加密的网络传输协议，可在不安全的网络中为网络服务提供安全的传输环境。SSH通过在网络中建立安全隧道来实现SSH客户端与服务器之间的连接。</p><p>SSH 的原理跟 HTTPS 差不多，都是基于 TCP 和 非对称加密进行的应用层协议。它跟 HTTPS 的不同之处在于 HTTPS 通过 数字证书 和 数字证书认证中心 来防止中间人攻击，而 ssh 服务器的公钥没有人公证，只能通过其公钥指纹来人工确定其身份。</p><p>如下图所示,我们第一次使用 ssh 登陆某台服务器时,ssh 会提示我们验证服务器的公钥指纹。</p><p><img src="/git-xue-xi/image-20210531222346370.png" alt="image-20210531222346370"></p><h3 id="取巧的解决"><a href="#取巧的解决" class="headerlink" title="取巧的解决"></a>取巧的解决</h3><p>由于服务器上从github git clone 项目整不明白，所以可以先将github上的项目下载到本地，然后再手动上传到服务器进行覆盖。<span class="github-emoji"><span>😢</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>人太菜没办法</p>]]></content>
      
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文撰写</title>
      <link href="lun-wen-zhuan-xie/"/>
      <url>lun-wen-zhuan-xie/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.overleaf.com/">在线Latex工具Overleaf</a></p><p><a href="https://www.overleaf.com/learn">overleaf文档</a></p><p><a href="https://blog.csdn.net/gentleman_qin/article/details/79963396">LaTeX教程</a></p><h3 id="Latex学习"><a href="#Latex学习" class="headerlink" title="Latex学习"></a>Latex学习</h3>]]></content>
      
      
      <categories>
          
          <category> 论文撰写 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文撰写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semantic-Segmentation-3</title>
      <link href="semantic-segmentation-3/"/>
      <url>semantic-segmentation-3/</url>
      
        <content type="html"><![CDATA[<h2 id="Nonlocal和Transformer的区别？"><a href="#Nonlocal和Transformer的区别？" class="headerlink" title="Nonlocal和Transformer的区别？"></a>Nonlocal和Transformer的区别？</h2><p>==区别在于把注意力当做辅助模块还是核心模块==</p><p>无论是non-local还是CCNet,都是认为CNN整体上已经非常好了,但是缺乏一些长距离建模的能力,所以我们稀疏的插入一下注意力模块,帮助CNN做这件事。</p><p>另外的全注意力或者类Transformer的网络,是让网络的主要部分都是用注意力结构。==卷积和注意力的地位已经颠倒过来了==,由卷积去负责一些注意力可能不擅长的事情,比如低层的特征提取,而剩下绝大部分的建模都交给注意力.</p><p>此外，全注意力网络的工作也可以分为两类：</p><p>==一类是保留原有CNN的训练流程，通过修改注意力模块的结构适应CV任务==，比如AAConv、SASA、GSA-Net；<br>==另一类则保留原本的Transformer架构，通过修改训练流程来适应CV任务==，比如ViT、DETR。<br>这两类工作相比，目前看来第二类工作的改动更彻底、效果也更好。==第一类工作往往因为需要处理高分辨率的特征图并且引入相对位置表征，导致在GPU、TPU上效率不高。==第二类工作遇到的主要问题则是，==如果完全抛弃卷积，由于缺乏平移不变性的先验，往往对数据量和训练长度要求很高，不能高效地学习。如果不使用超大规模数据进行预训练，则一般还是需要保留一部分卷积层。==</p><h3 id="关于attention的思考"><a href="#关于attention的思考" class="headerlink" title="关于attention的思考"></a>关于attention的思考</h3><p>对于深层的一个Channel，特征图的每一个点都是之前的</p><h2 id="AANet"><a href="#AANet" class="headerlink" title="AANet"></a>AANet</h2><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>19ICCV Google Brain  <strong>用自注意力增强卷积：这是新老两代神经网络的对话</strong></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>卷积运算的弱点:局部邻域运行 缺少全局信息。本文使用自注意力作为卷积的替代。</p><p>我们介绍了一种<strong>新颖的二维相对自注意力机制</strong>，该机制<strong>证明了在取代卷积作为图像分类的独立计算基元方面具有竞争力。</strong></p><p>==我们在控制实验中发现，将卷积和自注意力结合起来可获得最佳结果。==</p><p>因此，我们建议通过将<strong>卷积特征图与通过自我注意产生的一组特征图进行级联</strong>，以利用这种<strong>自我注意机制来增强卷积算子。</strong></p><p>广泛的实验表明，注意力增强技术可在许多不同的模型和规模（包括ResNets和SOTA的移动受限网络）上，在ImageNet上的图像分类和COCO上的对象检测方面带来一致的改进，同时<strong>保持参数数量相似。</strong></p><p>比SENet好</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>==卷积层的设计强加了 1.有限感受野的局部性 2.权重共享的平移等变性translation equivariance==</p><p>==self attention的关键思想是从hidden units中计算得出值的加权平均。The key idea behind self-attention is to produce a weighted average of values computed from hidden units.==</p><p>==与池化和卷积算子不同，加权平均运算中(self attention)使用的权重是通过hidden units之间的相似度函数动态生成的。作为结果，输入信号之间的相互作用取决于信号本身，而不是像卷积一样由它们的相对位置预先确定。 特别地，这使得self attention能捕捉远程交互，而无需增加参数的数量。==</p><p>应用self attention替代conv</p><p>我们开发了一种新颖的二维相对self attention机制[37]，该机制在注入相对位置信息的同时保持平移等变性，使其非常适合图像。</p><p>我们的self-attention 显示出了完全取代conv的竞争力,但是我们在对照实验中发现，==将两者结合使用可获得最佳效果==。</p><p>因此，我们没有完全放弃卷积的概念，而是建议<strong>使用self attention机制来增强卷积</strong>。 这是通过将强制局部性的卷积特征图与能够建模更长距离依赖项的self attention特征图（请参见图2）进行级联来实现的。</p><p><img src="/semantic-segmentation-3/image-20210417151606466.png" alt="Attention Argument"></p><p>实验还表明,fullyself attention模型(Attention Argument的一种特例)仅比ImageNet上的完全卷积模型稍差，这表明self attention是图像分类的强大独立计算基元。</p><p><strong>这个图真的有大问题,不知道它画的什么,这也能入选顶会？？？就离谱</strong></p><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="2-1-Convolutional-networks"><a href="#2-1-Convolutional-networks" class="headerlink" title="2.1. Convolutional networks"></a>2.1. Convolutional networks</h4><p>的确，用于基于卷积图元设计体系结构的<strong>自动搜索策略</strong>可在跨多种任务的大规模图像分类任务中实现最先进的准确性[55，21]。</p><h4 id="2-2-Attention-mechanisms-in-networks"><a href="#2-2-Attention-mechanisms-in-networks" class="headerlink" title="2.2. Attention mechanisms in networks"></a>2.2. Attention mechanisms in networks</h4><p>针对视觉任务提出了多种注意力机制，以解决卷积的弱点[17、16、7、46、45、53]。 例如，Squeeze and Excitation [17]和Gather-Excite [16]使用从整个特征图汇总的信号<strong>重新分配特征通道的权重</strong>，而BAM [31]和CBAM[46] (18ECCV)在<strong>通道和空间维度</strong>上独立地完善卷积特征。</p><p>但是，non-local block<strong>仅在ImageNet预训练之后才添加到体系结构</strong>中，并以<strong>不破坏预训练的方式</strong>进行初始化。</p><p>我们的在整个架构中采用self attention</p><p>==The use of multi-head attention allows the model to attend jointly to both spatial and feature subspaces.==</p><p>另外，我们通过将relative self attention[37，18]扩展到二维输入，从而增强了self attention对图像的表示能力，从而使我们能够以一种<strong>有原则的方式对平移等变性transparent equivariance</strong>进行建模。</p><p>最终，我们的方法产生了附加的特征图，而不是<strong>通过加法[45、53]或门控(权重相乘的形式</strong>)[17、16、31、46]重新校准卷积特征。==此属性使我们可以灵活地调整注意力通道的比例，并考虑从完全卷积到完全注意力模型的一系列体系结构。==</p><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><blockquote><p><strong>不同空间下的表示可以做attention吗？</strong></p><p>我觉得显然可以。我们对注意力机制最朴素、直观的认识就是，它学习的是一种对齐关系（alignment）。而这种机制<strong>并不</strong>假定query和memory同属于一个所谓语义空间，而是假定两个空间存在某种可以靠注意力机制学习的对齐关系。</p><p>在注意力较早应用的机器翻译应用中，你也很难说源语言和目标语言二者属于同一个语义空间，但是可以大胆假定两门语言存在某种对齐关系，简单的例如词汇之间的对齐，复杂的例如模式之间的对齐，在Transformer中分别对应低层表示的cross-attention和高层的cross-attention。</p><p>在跨模态场景中，显然你也总是可以找到特征之间的某种对齐关系，例如图像和文本之间广泛存在的对应。抛开DL的理论实践，光是看人类的认知，当你阅读小说时，看到『天空』，你可能会想象到一片蓝色；看到『bobo』，你可能会想象到一颗猥琐的猫头。这就是图像和文本之间固有的对齐关系，因为语言习得过程就需要我们将特定文字和特定意象（包括图像、声音）联系，这正是近期众多跨模态工作（包括著名的CLIP，DALL-E）得以成功的原因。引入跨空间的注意力即是学习这种对齐关系的一种方法。</p><p>希望能帮助你理解手头的论文。</p><p>作者：TniL<br>链接：<a href="https://www.zhihu.com/question/452498914/answer/1814362500">https://www.zhihu.com/question/452498914/answer/1814362500</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>可以类比CNN中同时使用<strong>多个滤波器</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征/信息。</strong></p><p>论文中是这么说的：</p><p>Multi-head attention allows the model to jointly attend to information from <strong>different representation subspaces</strong> at different positions.</p><p>我觉得也可以把多头注意力看作是一种ensemble，模型内部的集成。不过另外的答主也提到了，多头注意力的机理还不是很清楚。事实上，注意力机制本身如何工作，这个可解释性工作也还没有完成，目前的一些解释都还只是intuition，除了seq2seq中起到一种alignment的作用外，在许多模型中加入注意力以后如何起作用，还是有一点争议的。</p></blockquote><p>$$<br>Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><p>Q、K获得数值为0-1的mask矩阵(可以理解为attention score矩阵)，V表示的是输入线性变换后的特征,mask矩阵乘上V就获得过滤后的V特征</p><p>$H$特征图的高度 $W$宽度 $F_{in}$输入维度  $N_h$:头数 $d_v$:values的深度**(深度指channels吧)**  $d_k$:query和keys的深度 $d_v^h$ $d_k^h$第h个头的深度</p><h4 id="3-1-Self-attention-over-images"><a href="#3-1-Self-attention-over-images" class="headerlink" title="3.1. Self-attention over images"></a>3.1. Self-attention over images</h4><p>input:$(H,W,F_{in})\to X\in R^{HW\times F_{in}}$</p><p>single head h:<br>$$<br>O_h=softmax(\frac{(XW_q)(XW_k)^T}{\sqrt{d_k^h}})(XW_v)\<br>W_q,W_k\in R^{F_{in}\times d_k^h},W_u\in R^{F_{in}\times d_v^h}<br>$$<br>W是学到的线性变换,将X映射到queries:$Q=XW_q$,keys:$K=XW_k$,values:$V=XW_v$</p><img src="/semantic-segmentation-3/image-20210420110944276.png" alt="示意图" style="zoom:67%;"><img src="/semantic-segmentation-3/image-20210420111339597.png" alt="示意图2" style="zoom:50%;"><p>==所以说,应用到视觉上,是将二维的图像展开,展开后类似nlp==</p><p>The outputs of all heads are then concatenated 连接and projected 投影again as follows:<br>$$<br>MHA(X)=Concat[O_1,O_2,…,O_{Nh}]W^O\<br>W^O\in R^{d_v\times d_v}<br>$$<br>W是学到的线性变换,MHA(X)然后reshaped into $(H,W,d_v)$</p><p>We note that multi-head attention incurs招致 ==a complexity of== $O((HW)^2d_k)$ and ==a memory cost== of $O((HW)^2Nh) $as it requires to store attention.</p><p>==时间复杂度推导:==</p><p>假设现在有两个矩阵$A:m\times n$  $B:n\times l$</p><p>对于矩阵$A$第一行的第一个元素,需要进行$l$次运算,一行需要进行$n\times l$次运算,$m$行需要进行$m\times n \times l$次运算</p><p>根据上面的推导,$multi.head$的时间复杂度为:<br>$$<br>O(multihead)=O(HW\times F_{in}\times d_k + HW\times d_k\times HW)\<br>=O((HW)^2d_k)<br>$$</p><p>==空间复杂度推导:==</p><p>举例:</p><pre class=" language-python"><code class="language-python">int i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>int j <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span><span class="token operator">+</span><span class="token operator">+</span>i<span class="token punctuation">;</span>j<span class="token operator">+</span><span class="token operator">+</span><span class="token punctuation">;</span>int m <span class="token operator">=</span> i <span class="token operator">+</span> j<span class="token punctuation">;</span></code></pre><p>只分配了三个变量,i、j、m,所以空间复杂度$S(n)=O(1)$</p><pre class=" language-cpp"><code class="language-cpp"><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> m <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token keyword">int</span><span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span> i<span class="token operator">&lt;=</span>n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">{</span>   j <span class="token operator">=</span> i<span class="token punctuation">;</span>   j<span class="token operator">++</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>第一行分配了n个元素的数组,之后的循环操作都没有再分配空间,所以说空间复杂度为$S(n)=O(n)$</p><p>对于$multi.head$的空间复杂度为:<br>$$<br>O(multihead)=O((HW)^2N_h)<br>$$<br>每个head,分配的最大空间就是那个$HW\times HW$,$N_h$个头就是$(HW)^2N_h$</p><p><strong>3.1.1 Two-dimensional Positional Encodings</strong><br>==如果没有关于位置的明确信息，则self attention是permutation equivariant置换等变的：==<br>$$<br>MHA(\pi(X))=\pi(MHA(X))<br>$$<br>对于像素位置的任何排列$\pi$,使其对<strong>建模高度结构化的数据(如图像)无效</strong>。</p><p>已经提出了<strong>使用显式空间信息来增强激活图的多种位置编码以缓解相关问题。</strong>Transformer[32]将在original Transformer[43]中引入的<strong>正弦波扩展到二维输入</strong>,而CoordConv[29]将<strong>positional channels连接到激活图。</strong></p><p>这些编码对我们进行图像分类和目标检测的实验没有帮助（请参见第4.5节）</p><p>==我们猜想是因为这样的位置编码虽然不是置换等变的，但不满足平移等变，而平移等变在处理图像时是理想的特性。==</p><p>作为解决方案，我们建议将<strong>relative position encodings[37]**的使用扩展到二维，并提出一种基于</strong>Music Transformer** [18]的内存高效实现。</p><p><strong>Relative positional encodings:</strong></p><p>[37]引入位置编码,防止置换等变的同时实现平移等变。我们通过独立地添加相对高度信息和相对宽度信息来实现二维相对自我关注。</p><p>piexl$i=(i_x,i_y)$,这里的$i_x$和$i_y$是指在spatial时候的坐标， $j=(j_x,j_y)$与$i$类似，他俩的相对注意力计算公式如下:<br>$$<br>l_{i,j}=\frac{q^T_i}{\sqrt{d_k^h}}(k_j+r^W_{j_x-i_x}+r^H_{j_y-i_y})<br>$$<br>$q_i$指query $Q$的第$i$行,也就是第$i$个点,$k_j$就是key $K$的第$j$行,$r^W_{j_x-i_x}$是learned embeddings for 相对宽度$j_x-i_x$,$r^H_{j^y-i^y}$是相对宽度,</p><p>output of head h now becomes:<br>$$<br>O_h=Softmax(\frac{QK^T+S^{rel}_H+S^{rel}_W}{\sqrt{d^h_k}})V<br>$$</p><p>$S^{rel}_H,S^{rel}_W\in R^{HW\times HW}$</p><p>$S^{rel}_H[i,j]=q^T_ir^H_{j_y-i_y}$</p><p>$S^{rel}_W[i,j]=q^T_ir^W_{j_x-i_x}$</p><p>[37]中的相对注意机制带来额外的内存开销为$O((HW)^2d^k_h)$,   太大了 ,不带位置注意的才$O((HW)^2N_h)$,并且一般来说$N_h&lt;d^h_k$，实在得不偿失，==本文提出的只有==$O(HWd_k^h)$</p><p>相对位置嵌入$r^H$和$r^W$在头之间共享而不是在层之间共享,对每个layer,建模高度和宽度的相对位置距离增加了$(2(H+W)-2)d^h_k$的参数量。</p><h4 id="3-2-Attention-Augmented-Convolution"><a href="#3-2-Attention-Augmented-Convolution" class="headerlink" title="3.2. Attention Augmented Convolution"></a>3.2. Attention Augmented Convolution</h4><p>SENet[17] GENet[16]channelwise reweighing</p><p>BAM[31] CBAM[46] reweigh both channels and spatial positions ==independently==. </p><p>与这些方法相比,我们</p><ol><li>使用一种<strong>可以同时注意空间和特征子空间(每个头部对应一个特征子空间)的注意力机制</strong></li><li>引入<strong>额外的特征图而不是refining them</strong></li></ol><p><strong>Concatenating convolutional and attentional feature maps:</strong></p><p>形式上,考虑一个kernel size=k，$F_{in}$input filters $F_{out}$ output filters.相应的AA conv can be written as:<br>$$<br>AAConv(X)=Concat[Conv(X),MHA(X)]<br>$$</p><p>$v=\frac{d_v}{F_{out}}$表示the ratio of attentional channels to number of original output filter</p><p>$\kappa=\frac{d_k}{F_{out}}$the ratio of key depth to number of original output filters</p><p>与卷积类似,AAC:</p><ol><li>is equivariant to translation 平移等变</li><li>可以很容易地对不同spatial尺寸的输入进行操作</li></ol><p><strong>Effect on number of parameters：</strong></p><p>参数量计算先不看了,反正这篇文章奇奇怪怪的啊</p><p><strong>Attention Augmented Convolutional Architectures:</strong></p><p>增强卷积之后是BN,==BN can learn to scale the contribution of the convolution feature maps and the attention feature maps可以学习放缩卷积特征层和注意力特征层的贡献/作用==   与SENet等相同,对每个残差块应用一次增强卷积 </p><p>由于内存成本$O(N_h(HW)^2)$,对于较大的空间维度是不行的,==所以我们从最后一层开始加,直到内存受不了==就tm离谱</p><p>为了减少内存占用,选用较少的$batchsize$，同时开始的尺寸较大的时候用$downsample$，使用3x3,stride=2的平均池化</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h2 id="SASA"><a href="#SASA" class="headerlink" title="SASA"></a>SASA</h2><h3 id="Info-1"><a href="#Info-1" class="headerlink" title="Info"></a>Info</h3><h2 id="Relationship"><a href="#Relationship" class="headerlink" title="Relationship"></a>Relationship</h2><h3 id="Info-2"><a href="#Info-2" class="headerlink" title="Info"></a>Info</h3><p>2020 ICLR oral On the relationship between self-attention and convolutional layers</p><h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><p>==将注意力机制整合到视觉的趋势使研究人员重新考虑卷积层作为主要构建块的优势==</p><p>除了==帮助CNN处理远程依赖关系==外，Ramachandran等人还提出了解决方案表明==注意力可以完全取代卷积，并在视觉任务上达到最先进的性能。==</p><p>==问题:学到的注意力层的运作方式与卷积层的运作方式类似吗？==</p><p>这项工作提供了证据表明==注意力层可以进行卷积，实际上，他们经常在实践中学会这样做。== 具体来说，我们证明<strong>具有足够数量的头部的多头自我注意层至少与任何卷积层一样具有表现力</strong>。 然后，我们的数值实验表明，<strong>自注意力层与CNN层相似地参与了像素网格模式</strong>，从而证实了我们的分析。</p><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p><strong>transformer</strong> 到底是何方神圣？处处都有他</p><p>最初在==神经机器翻译中引入注意力机制就是为了更好地处理长期依赖==，</p><h2 id="CANet"><a href="#CANet" class="headerlink" title="CANet"></a>CANet</h2><h3 id="Info-3"><a href="#Info-3" class="headerlink" title="Info"></a>Info</h3><p>21CVPR </p><h3 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h3><p>Mobile network设计的最新研究表明,<strong>通道注意(如SENet)对提升模型性能</strong>具有显著效果,但他们==通常会忽略位置信息，这对生成空间选择性注意图非常重要。==</p><p>本文为mobile networks 提出了一种新颖的注意机制,==通过将位置注意力嵌入到通道注意力中，我们将其称为”coordinate attention坐标注意力”.==</p><p>与通过2D全局池化将<strong>特征张量</strong>转化为<strong>单个特征向量</strong>的channel attention不同,coordinate attention将channel attention分解成两个1D特征编码的过程，分别沿两个空间方向聚合特征。==这样，可以沿一个空间方向捕获远程依赖关系，同时可以沿另一空间方向保留精确的位置信息。==然后将<strong>生成的特征图分别编码为一对方向感知和位置敏感</strong>的注意图，可以将其互补地应用于输入特征图，以增强关注对象的表示。</p><p>coordinate attention很简单,可以灵活地插入到经典的移动网络中,如<strong>MobileNetV2,MobileNeXt和EfficientNet，而且几乎没有计算开销computational overhead。</strong></p><h3 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h3><p>**被用来告诉模型注意“什么”和注意“哪里”的注意力机制，已经得到了广泛的研究,**并广泛用于提高现代深度神经网络的性能[18，44，3，25，10， 14] SENet  CBAM GCNet SCNet DANet SPM。==但是，它们在移动网络(尺寸有限)中的应用明显落后于大型网络[36、13、46]VGG ResNet RexNeXt。这主要是因为大多数注意力机制带来的计算开销对于移动网络而言是无法承受的。==考虑到其有限的计算能力，现在最受欢迎的还是SENet。但SENet只考虑通道信息，忽略了位置信息。</p><p><img src="/semantic-segmentation-3/image-20210418215647978.png" alt="SEnet CBAM CANet"></p><p>后来的工作,例如BAM和CBAM,试图通过减小输入张量的通道尺寸然后利用卷积计算空间注意力来利用位置信息,如图(b).</p><p>卷积只能捕获局部关系,无法捕获长远依赖。</p><p>在本文中，beyond the first works，我们还提出了一种新颖而有效的注意力机制，==即将位置信息嵌入到频道注意力中，从而使移动网络可以注意较大的区域，同时避免产生大量的计算开销。==</p><p><strong>为了减轻2D全局池化引起的位置信息丢失，我们将channel attention分解为两个并行的1D特征编码过程，以将空间坐标信息有效地集成到生成的注意力图中。</strong></p><p>具体而言，我们的方法利<strong>用两个1D全局池化操作将沿垂直方向和水平方向的输入要素分别聚合为两个单独的方向感知要素图。</strong> </p><p><strong>然后将这两个具有嵌入的特定于方向的信息的特征图分别编码为两个注意图，每个注意图都沿一个空间方向捕获输入特征图的长期依赖性。</strong></p><p> <strong>位置信息因此可以被保存在所生成的注意力图中。</strong></p><p> 然后通过<strong>乘法</strong>将两个注意图都应用于输入特征图，以强调感兴趣的表示。</p><p> 我们将提出的注意力方法称为坐标注意力，<strong>因为其操作可以区分空间方向（即坐标）并生成可感知坐标的注意力图。</strong></p><p>坐标注意力具有以下优势:</p><ul><li>==它不仅捕获cross-channel信息，还捕获方向感知direction-aware和位置敏感position-sensitive信息，这有助于模型更准确地定位和识别感兴趣的对象==</li><li>方法灵活，轻巧，可以轻松地插入到经典的移动网络构建块中,如MobileNetV2中提出的inverted residual block反向残差块和MobileNeXt中的sandglass块，通过强调信息表达来增强特征</li><li>第三，作为一种预先训练的模型，我们的坐标注意力可以为移动网络的下游任务带来显著的性能提升，尤其是对于那些密集预测(例如语义分割)的任务，我们将在实验部分进行展示。</li></ul><p><img src="/semantic-segmentation-3/image-20210421100216540.png" alt="效果对比"></p><p>效果看着确实不错</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="2-1-Mobile-Network-Architectures"><a href="#2-1-Mobile-Network-Architectures" class="headerlink" title="2.1. Mobile Network Architectures"></a>2.1. Mobile Network Architectures</h4><p>最近的SORT的移动网络主要基于深度可分离卷积depthwise separable convolutions</p><blockquote><p>Mobilenets: Efficient convolu- tional neural networks for mobile vision applications.</p></blockquote><p>和反向残差块inverted residual block </p><blockquote><p>Mobilenetv2: Inverted residuals and linear bottlenecks. </p></blockquote><p>HBONet [20]在每个反向残差块内部引入了下采样操作，以对典型的空间信息进行建模。</p><p>ShuffleNetV2 [27]在反向残差块之前和之后使用channel split module和channel shuffle module。</p><p>后来，==MobileNetV3 [15]与neural architecture search algorithms [50]相结合，以搜索最佳激活函数和不同深度的反向残差块的扩展率。==</p><p>此外，MixNet [39]，EfficientNet [38]和ProxylessNAS [2]也==采用不同的搜索策略来搜索最优的深度可分离卷积或标量以控制网络权重==，例如扩展比，输入分辨率， 网络深度和宽度。</p><p>最近，Zhou等人 [49]重新考虑了利用深度可分离卷积的方法，提出了MobileNeXt,该网络为mobile networks采用了一个经典的bottleneck 结构。</p><h4 id="2-2-Attention-Mechanisms"><a href="#2-2-Attention-Mechanisms" class="headerlink" title="2.2. Attention Mechanisms"></a>2.2. Attention Mechanisms</h4><p>SENet首先成功，其次CBAM扩展了它的思想，后来的作品，如GENet [17]，GALA [22]，AA [1]和TA [28]，通过采用不同的空间注意力机制或设计高级注意力块来扩展了这一思想。后来的作品，如GENet [17]，GALA [22]，AA [1]和TA [28]，通过采用不同的空间注意力机制或设计高级注意力块来扩展了这一思想。</p><p>non-local/self-attention网络由于其建立spatial或cahnnel关注的能力而近来非常受欢迎。 典型示例包括NLNet [43]，GCNet [3]，$A^2Net$ [7]，SCNet [25]，GSoP-Net [11]或CC-Net [19]，所有这些都利用nonlocal来获取不同类型的空间信息。 然而，==由于self-attention模块内部的大量计算，它们通常在大型模型中使用[13，46]，但不适用于移动网络。==</p><p>与这些利用昂贵且沉重的non-local或self attention blocks的方法不同，我们的方法考虑了一种捕获positional information和channel-wise relationships以增强移动网络特征表示的更有效方法。 通过将2D全局池化操作分解为两个一维编码过程，我们的方法比其他具有轻量级属性的注意方法（例如SENet [18]，CBAM [44]和TA [28]）执行效果要好得多。</p><h3 id="Coordinate-Attention"><a href="#Coordinate-Attention" class="headerlink" title="Coordinate Attention"></a>Coordinate Attention</h3><p>corordinate attention block即插即用模块,对于任意中间层特征张量$X=[x_1,x_2,…,x_C]\in R^{C\times H\times W}$作为输入，输出具有扩展表示的变换张量$Y=[y_1,y_2,…,y_C]$,与$X$具有相同的size。</p><h4 id="3-1-Revisit-SENet"><a href="#3-1-Revisit-SENet" class="headerlink" title="3.1. Revisit SENet"></a>3.1. Revisit SENet</h4><p>==标准卷积本身很难对通道关系进行建模。明确建立渠道的相互依存关系可以提高模型对信息通道的敏感性，而信息通道对最终分类决策的贡献更大==</p><h4 id="3-2-Coordinate-Attention-Blocks"><a href="#3-2-Coordinate-Attention-Blocks" class="headerlink" title="3.2. Coordinate Attention Blocks"></a>3.2. Coordinate Attention Blocks</h4><p><img src="/semantic-segmentation-3/image-20210418215647978.png" alt="SEnet CBAM CANet"></p><p>坐标注意力同时对channel关系和带精确位置信息的long-range依赖进行编码,通过以下两个步骤:</p><p><strong>coordinate information embedding and coordinate attention generation.</strong> </p><p><strong>3.2.1</strong></p><p><strong>Coordinate Information Embedding</strong><br>$$<br>z_c=\frac{1}{H\times W}\sum^H_{i=1}\sum^W_{j=1}x_c(i,j)<br>$$<br>上面的是SENet，==直接利用全局池化，很难保留位置信息，所以用于目标分类任务，本文提的坐标注意力考虑了位置信息，所以可以用于语义分割。==</p><p><img src="/semantic-segmentation-3/image-20210426095205170.png" alt="缩放成一个点"></p><p>将上面的分解为一对1D feature encoding operations。</p><p>具体来说，给定输入X，我们使用池化核的两个空间范围(H,1)或(1,W)分别沿水平坐标和垂直坐标对每个通道进行编码。因此,第c个通道的高度为h处的输出可以表示为:<br>$$<br>z_c^h(h)=\frac{1}{W}\sum_{0\le i&lt;W}x_c(h,i)<br>$$<br><img src="/semantic-segmentation-3/image-20210426095226968.png" alt="缩放成一个竖条"></p><p>相似的,第c个通道的宽度为w处的输出可以表示为:<br>$$<br>z_c^w(w)=\frac{1}{H}\sum_{0\le j&lt;H}x_c(j,w)<br>$$<br><img src="/semantic-segmentation-3/image-20210426095248067.png" alt="缩放成一个横条"></p><p>上面的两个变换分别沿两个空间方向聚合了特征，从而生成了一对方向感知的特征图。</p><p>==这两个变换还使我们的注意力块能够捕获沿一个空间方向的远距离依赖关系，并沿另一空间方向保留精确的位置信息，这有助于网络更准确地定位感兴趣的对象。==</p><p>==冥冥之中感觉拆分还可以改进==</p><p><strong>3.2.2 Coordinate Attention Generation</strong></p><p>上面两个式子具有全局感受野并编码了精确的位置信息,为了利用由此产生的表现形式,我们提出了第二种转换,<strong>称为坐标注意力生成</strong>。</p><p>我们的设计参考以下三个标准:</p><ul><li>==新的转换应当尽可能简单和开销小,考虑到移动应用环境==</li><li>==充分利用捕获的位置信息,从而可以准确的突出显示感兴趣的区域==</li><li>==有效捕获取channel间的关系==</li></ul><p>对于之前生成的两个特征图,我们首先将他们连接起来,然后将他们发送到共享的1x1卷积变换函数F1,产生<br>$$<br>f=\delta(F_1([z^h,z^w]))<br>$$<br>$[.,.]$表示沿空间维度的concat操作,$\delta$是一个非线性激活函数,$f\in R^{C/r(H+W)}$是在<strong>水平方向和垂直方向上对空间信息进行编码的中间特征图。</strong></p><p>然后，我们沿着空间维将$f$拆分为两个单独的张量:$f^h\in R^{C/r\times H}$ $f^w\in R^{C/r\times W}$</p><p>另外两个1x1卷积变换$F_h$和$F_w$被使用来分别使$f^h$和$f^w$和输入$X$具有相同的channel:<br>$$<br>g^h=\sigma(F_h(f^h))\<br>g^w=\sigma(F_w(f^w))<br>$$<br>$\sigma$是sigmoid function</p><p>$g^h$和$g^w$然后将被扩展并用做注意权重,最终输出$Y$如下:<br>$$<br>y_c(i,j)=x_c(i.j)\times g^h_c(i)\times g^w_c(j)<br>$$<br><strong>Discussion:</strong></p><p>==两个注意力图中的每个元素反映了感兴趣的对象是否存在于相应的行和列中。这种编码过程使我们的注意力集中在更准确地定位感兴趣对象的确切位置上，从而帮助整个模型更好地识别。==</p><h4 id="3-3-Implementation"><a href="#3-3-Implementation" class="headerlink" title="3.3. Implementation"></a>3.3. Implementation</h4><p>采用两种经典的轻量级体系结构,他们具有不同的类型的残差块,即:MobileNetV2和MobieNeXt</p><p>下图3显示了如何将注意力块插入MobileNetV2中的inverted residual block和MobileNeXt中的sandglass block。</p><img src="/semantic-segmentation-3/image-20210421161046801.png" alt="Figure 3" style="zoom: 80%;"><h3 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="4-1-Experiment-Setup"><a href="#4-1-Experiment-Setup" class="headerlink" title="4.1. Experiment Setup"></a>4.1. Experiment Setup</h4><p>PyTorch  SGD decay $momentum =0.9$ 权重衰减设置为$4\times 10^{-5}$ 余弦衰减策略  </p><p>$NVIDIA \quad GPU\quad  batchsize=256$</p><p>$baseline=MobileNetV2$    $epoch=200$</p><h4 id="4-2-Ablation-Studies"><a href="#4-2-Ablation-Studies" class="headerlink" title="4.2. Ablation Studies"></a>4.2. Ablation Studies</h4><h2 id="HaloNet"><a href="#HaloNet" class="headerlink" title="HaloNet"></a>HaloNet</h2><h3 id="Info-4"><a href="#Info-4" class="headerlink" title="Info"></a>Info</h3><p>21CVPR oral</p><blockquote><p>Scaling Local Self-Attention for Parameter Efficient Visual Backbones</p></blockquote><h3 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h3><p>==Self-attention: parameter-independent scaling of receptive fields &amp;content-dependent interactions==</p><p>==convolutions: parameter-dependent scaling &amp;content-independent interactions==</p><p>相比卷积基线模型,self attention模型在==准确性-参数折中==方面就有令人鼓舞的改进</p><p>==旨在开发自我注意力模型，该模型不仅可以胜过规范的基线模型，而且可以胜过高性能的卷积模型。==</p><p>我们提出了两种self attention的扩展，与self attention的更有效实现相结合，可以提高这些模型的速度，内存使用率和准确性。 </p><p>我们利用这些改进来开发新的self attention模型系列HaloNets，该系列在ImageNet分类基准的参数受限设置上达到了最先进的精度。</p><p>在初步的转移学习实验中，我们发现HaloNet模型的性能优于更大的模型，并且具有更好的推理性能。 在诸如对象检测和实例分割之类的较艰巨的任务上，我们简单的local self attention和卷积混合算法在非常强大的基线上显示出改进。==These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutional models.==</p><h3 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h3><p>另一方面,Transformer[57]表明,==自我注意是捕获句子中单词之间整体相互作用的一种有效且计算效率高的机制。==</p><p>==self-attention具有使其非常适合视觉的几个特性:==</p><ul><li><strong>基于内容的交互，而不是与内容无关的卷积交互</strong></li><li><strong>参数无关的感受野尺寸缩放,而不是卷积的参数依赖缩放</strong></li><li><strong>捕获用于较大图像的长期依赖关系的经验能力</strong></li><li><strong>处理和整合视觉中出现的多种类型数据的灵活性，例如像素，点云，序列条件信息和图形</strong></li></ul><p>self attention也可以被认为是一种适应性非线性，可与计算机视觉中非线性处理技术的悠久历史相提并论，例如双边滤波[39]</p><blockquote><p>Bilateral filtering: Theory and applications.</p></blockquote><p>和非局部手段[4]。</p><blockquote><p>A non-local algorithm for image denoising.</p></blockquote><p>我们引入了non-centered版本的local attention,它可以通过haloing高效的映射到现有硬件。</p><p>==尽管我们的公式破坏了平移等变性,但是与SASA中使用的centered local self-attention相比,既提高了吞吐量,又提高了准确性。==</p><p>我们还介绍了用于多尺度特征提取的跨步self-attention下采样操作。</p><p>我们利用之前的技术开发了一个新的local self-attention模型家族,HaloNet,可在不同参数范围内达到最先进的性能</p><p>==我们进行了详细的研究，以揭示self attention和conv模型如何以不同的方式scale。我们最后讨论了将self attention应用于视觉的当前局限性和未来工作的构想。==</p><h3 id="Models-and-Methods"><a href="#Models-and-Methods" class="headerlink" title="Models and Methods"></a>Models and Methods</h3><p>尽管我们的模型使用self attention代替conv来捕获像素之间的空间相互作用，但它们==采用了现代卷积神经网络（CNN）的一些重要架构特征==</p><p>计算多尺度特征层次结构，==为此,开发了一个strided self-attention, a natural extension of strider convolutions==</p><p>==为了处理较大分辨率下的计算成本问题(global attention是不可行的),我们遵循局部处理这一相当普遍的原则,并使用spatially restricted forms of self-attention 并使用空间受限形式的self-attention==</p><p>但是与SASA不同,==我们放弃了强制的平移等变性而代之以更好的硬件利用率,从而提高了速度-精度的权衡==</p><p>另外,尽管我们使用local self attention,==但每个像素的感受野非常大(up to 18x18),并且 SEction 4.2.2表明较大的感受野有助于处理较大的图像。==</p><h4 id="2-1-Self-attention-can-generate-spatially-varying-convolutional-filter"><a href="#2-1-Self-attention-can-generate-spatially-varying-convolutional-filter" class="headerlink" title="2.1. Self-attention can generate spatially varying convolutional filter"></a>2.1. Self-attention can generate spatially varying convolutional filter</h4><p>最近的工作[8]</p><blockquote><p>On the relationship between self-attention and convolutional layers</p></blockquote><p>表明，==具有足够数量的头部的self attention和正确的几何偏差可以模拟卷积，这表明自注意力和卷积之间存在更深的关系==。 </p><p>==The perspective that we discuss in this section is one that views self-attention as generating spatially varying filters, in contrast to the reuse of the same filter across every spatial location in standard convolutions [14].卷积是相同滤波器的重用,而self-attention是生成性的空间变换滤波==</p><p>为了观察这一点,我们将self-attention和conv作为一般空间池化函数的特定实例进行编写,</p><p>To observe this, we write self-attention and convolution as specific instances of a general spatial pooling function.</p><p>给定$input$  $x\in R^{H\times W\times C_{in}}$ ,定义了一个local 2D pooling 函数,该函数计算位置$(i,j)$的输出,$y_{ij}\in R^{c_{out}}$：<br>$$<br>y_{ij}=\sum_{a,b\in N(i.j)}f(i,j,a,b)x_{ab}<br>$$</p><h2 id="CAA"><a href="#CAA" class="headerlink" title="CAA"></a>CAA</h2><h3 id="Info-5"><a href="#Info-5" class="headerlink" title="Info"></a>Info</h3><h3 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h3><p>==分别计算空间注意力和通道注意力,然后直接将他们融合会导致特征表示冲突conflicting feature representation==</p><p><strong>Channelized Axial Attention通道化轴向注意力</strong>,以==降低计算复杂度并无缝集成通道注意力和轴向注意力axial attention==</p><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>==对于特征图中的每个像素，spatial self attention使它的表示与更接近的像素的表示更相似，而channel attention会在整个特征图中找到重要的通道，并对提取的特征应用不同的权重。==</p><img src="/semantic-segmentation-3/image-20210429144509418.png" alt="image-20210429144509418" style="zoom: 80%;"><p>==分别计算空间注意和通道注意不仅增加了计算复杂度，而且还可能导致特征表示的重要性冲突。例如，对于属于特征图中部分区域的像素，某些channel对于channel attention可能显得很重要，但是spatial attention可能具有自己的视角，这是通过将整个特征图中的相似度相加得出的， 并削弱了channel attention的影响。==</p><p>channel attention可能忽略从整体角度获得的局部区域表示，而那可能是spatial attention所要求的。 因此，将spatial attention结果与channel attention结果直接融合可能会为像素表示产生不正确的重要性权重。==在本文的“experiments”部分中，我们开发了一种方法来可视化特征表示的冲突对最终分割结果的影响.==</p><p>为了以一种<strong>互补的方式无缝有效地</strong>结合空间注意力和通道注意力的优势，我们提出了一种<strong>通道化轴向注意力(CAA)</strong>,它基于重新定义的轴向注意力以减少self attention的计算成本。</p><h3 id="Related-Work-1"><a href="#Related-Work-1" class="headerlink" title="Related Work"></a>Related Work</h3><h3 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods"></a>Methods</h3><h4 id="A-Spatial-Self-Attention"><a href="#A-Spatial-Self-Attention" class="headerlink" title="A.Spatial Self-Attention"></a>A.Spatial Self-Attention</h4><p>$f(x_{i,j}, x_{m,n}) = softmax_{m,n}(θ(x_{i,j})^Tθ(x_{m,n}))$</p><p>这是non-local中的相似度函数,$f$采用该形式,$\theta$表示1x1卷积</p><h4 id="B-Spatial-Axial-Attention"><a href="#B-Spatial-Axial-Attention" class="headerlink" title="B.Spatial Axial Attention"></a>B.Spatial Axial Attention</h4><p>将应用在NLP和全景分割的轴线注意力改造过来！</p><p>在轴线注意力中,spatial attention map分别沿着列轴和行轴计算。</p><p>我们将沿Y轴计算的partial attention map称为“列注意”，将“行注意”沿X轴计算的partial attention map称为“行注意”。</p><p>对于第j-th column attention,当前位置$(i,j)$与其他每个位置$(m,j)$进行self attention。<br>$$<br>A_{col}(x_{i,j},x_{m,j})=softmax_m(\theta(x_{i,j})^T(x_{m,j}))<br>$$<br>每个点和同列上的点计算相似度,所以最终特征图$A_{col}$的维度为:$W\times H\times H$</p><p>同理,$A_{row}$的维度为$H\times W\times W$</p><p>行注意力和列注意力都是基于backbone产生的特征图计算,这与Axial-deeplab不同,==Axial-deeplab的row attention map是基于col attention map计算的==</p><p>通过使用相同的特征作为输入,最终输出$y_{i,j}$对特征$x_{i,j}$的依赖会有很大的加强,而不是像Aixal-deeplab那样使用skip-connection。</p><p>最终输出可以表示为:</p><p><img src="/semantic-segmentation-3/image-20210429204730241.png" alt="image-20210429204730241"></p><p>==就直接两个维度的连续相乘的激活呗==</p><p>为了方便表示,如下：<br>$$<br>\alpha_{i,j,m}=A_{col}(x_{i,j},x_{m,j})g(x_{m,j})\<br>\beta_{i,j,n}=A_{row}(x_{i,j},x_{i,n})\sum_{\forall m}\alpha_{i,j,m}<br>$$<br>式子5可以方便的表示为:</p><p><img src="/semantic-segmentation-3/image-20210429205802880.png" alt="image-20210429205802880"></p><h4 id="C-The-Proposed-Channelized-Axial-Attention"><a href="#C-The-Proposed-Channelized-Axial-Attention" class="headerlink" title="C. The Proposed Channelized Axial Attention"></a>C. The Proposed Channelized Axial Attention</h4><p>为了解决双重注意力的特征冲突问题，并无缝结合空间注意力和通道注意力的优势，我们提出了一种新颖的通道化轴向注意力,该注意力采用了中间结果$\alpha_{i,j,m}$和$\beta_{i,j,n}$</p><p><img src="/semantic-segmentation-3/image-20210429212349474.png" alt="image-20210429212349474"></p><p>就是在这个过程中嵌入了通道注意力。</p><h4 id="D-Grouped-Vectorization"><a href="#D-Grouped-Vectorization" class="headerlink" title="D. Grouped Vectorization"></a>D. Grouped Vectorization</h4><h4 id="E-Going-Deeper-in-Channel-Attention"><a href="#E-Going-Deeper-in-Channel-Attention" class="headerlink" title="E. Going Deeper in Channel Attention"></a>E. Going Deeper in Channel Attention</h4><p>为了进一步提高性能，我们探索了更强大的通道注意力设计。</p><p>就是探索了一下全连接呗,说了等于没说</p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semantic-Segmentation-2</title>
      <link href="semantic-segmentation-2/"/>
      <url>semantic-segmentation-2/</url>
      
        <content type="html"><![CDATA[<h2 id="ParseNet"><a href="#ParseNet" class="headerlink" title="ParseNet"></a>ParseNet</h2><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p> 15arixv Looking Wider to See Better <a href="https://blog.csdn.net/zym19941119/article/details/80859601">CSDN教程</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>最大贡献:使用全局语义信息(Global Context)  使用一层的平均特征来增强每个位置的特征</p><p>此外改进了基线</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><img src="/semantic-segmentation-2/image-20210331155819313.png" alt="ParseNet" style="zoom: 67%;"><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="3-1-Global-Context"><a href="#3-1-Global-Context" class="headerlink" title="3.1 Global Context"></a>3.1 Global Context</h4><p><strong>为什么叫语义分割？为什么要考虑上下文信息呢？就要英语阅读题一样，有时候一个词的词性、词义，都要结合上下文来看，就比如奶，有可能是牛奶，有可能是奶奶，但看他是没有办法确定这个字到底是取哪个意思的，但是联系上下文就很容易确定了。</strong></p><p>对于CNN来说，由于池化层的存在，卷积核的感受野（Receptive Field）可以迅速地扩大，对于最顶层的神经元，其感受野通常能够覆盖整个图片。例如对于VGG的fc7层，其理论上的感受野有404x404大小，而输入的图像也不过224x224，似乎底层的神经元是完全有能力去感知到整个图像的全部信息。但事实却并不是这样的。文章通过实验证明了神经网络实际的感受野要远小于其理论上的感受野，并不足以捕捉到全局语义信息。</p><img src="/semantic-segmentation-2/image-20210331161700120.png" alt="实际感受野" style="zoom:67%;"><p>如上图所示,(a)是原图,(b)是某个神经元输出的Activation map，文章对原图上滑动一个窗口，对这个窗口内部的图像加入随机噪声并观察加噪声后该神经元的输出是否有较大的变化，<strong>当产生较大变化时，代表这个神经元可以感受到这部分图像，并由此得到实际的感受野，如图(d)所示。经过实验发现，实际感受野只有原图的约1/4大小。</strong>在另一篇名为《Object detectors emerge in deep scene cnns》的论文中也得到了类似结论。</p><p>既然有了这样的现象，那很自然得就会想到<strong>加入全局信息去提升神经网络分割的能力</strong>。人们常说，窥一斑而知全豹，但这句话并不总是成立的，如果说你盯着一根杆子使劲看而不去关注它的环境位置顶部底座等信息，同样难以判断出来这根杆子是电线杆还是标志牌或者红绿灯。就如同以下FCN的输出一样，充满了错误的分类结果.</p><img src="/semantic-segmentation-2/image-20210331161943702.png" alt="错误分类的牛" style="zoom:67%;"><h4 id="3-2Early-Fusion-and-Late-Fusion"><a href="#3-2Early-Fusion-and-Late-Fusion" class="headerlink" title="3.2Early Fusion and Late Fusion"></a>3.2Early Fusion and Late Fusion</h4><p>就是全局池化之后，如何融合特征</p><ul><li>Early fusion，对<strong>得到的全局信息进行反池化（Unpool），得到和原特征图同样维度的全局特征，再把两者拼接起来，一起送入分类器中</strong>。由于文章使用的全局池化是平均池化，在反池化的时候，就是把得到的结果复制HxW遍铺成矩形，得到一个HxWxC的特征图。<strong>先融合再分类</strong></li><li>Late fusion，把<strong>得到的两个特征分别送入分类器，将最后的分类结果以某种方式融合起来</strong>，比如加权求和。  <strong>先分类再融合</strong></li><li>无论是early fusion还是late fusion，如果进行合适的归一化，结果并不会有太大的不同。但是在融合的时候一定要注意的一个问题就是不同层特征的尺度不同，这也就是为什么一定要进行归一化的原因。</li></ul><img src="/semantic-segmentation-2/image-20210331162817397.png" alt="Fig3" style="zoom: 80%;"><p>这张图的四种颜色代表了从四个不同深度的卷积层中提取出的特征向量，可以看到底层和顶层特征向量的尺度会有很大的差别，<strong>如果不进行归一化，高层的特征几乎都会被底层的大尺度特征向量所覆盖，无法对分类造成影响。</strong></p><p>BN应该就可以归一化吧。</p><h2 id="Non-Local"><a href="#Non-Local" class="headerlink" title="Non-Local"></a>Non-Local</h2><h3 id="Info-1"><a href="#Info-1" class="headerlink" title="Info"></a>Info</h3><p>18CVPR 从“局部连接”回到“全连接”</p><h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><p>卷积运算和循环运算都是每次处理一个局部邻域的构造块</p><p>本文将<strong>非局部操作作为捕获长期依赖关系</strong>的构建块的一般家族。</p><p>受计算机视觉中<strong>经典的非局部均值方法</strong>的启发，我们的<strong>非局部操作将某一位置的响应计算为所有位置特征的加权和</strong>。</p><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>捕捉长距离依赖关系是深度神经网络的核心问题。以图像数据为例，要想捕捉长距离依赖，<strong>通常的做法是堆积卷积层，随着层数的加深，感受野越来越大，就能把原先非相邻的像素点纳入到一个整体考虑，获取的信息分布广度也越来越高。</strong>这种靠堆叠卷积层得到的感受野提升，需要不断重复卷积过程，而这种重复会带来几个弊端：</p><ul><li>首先，计算效率很低，层的加深意味着更多的参数，更复杂的关系；</li><li>其次，优化困难，需要谨慎设计优化过程；</li><li>最后，建模困难，尤其是对于那些多级依赖项，需要在不同距离位置传递信息。</li></ul><p>本文提出了一个有效、简单、通用的non-local block来捕捉长期依赖，是<strong>经典的非局部均值运算的推广</strong>。</p><img src="/semantic-segmentation-2/image-20210401173458032.png" alt="Figure1" style="zoom:80%;"><p>如图，non-local在计算某个位置$X_i$的响应式，是考虑所有位置features的加权–所有位置可以是空间的，时间的，时空的，意味着non-local适用于图像、序列和视频问题。</p><p>使用non-local block的优点:</p><ul><li>与递归操作和卷积操作的渐进行为不同，non-local 操作通过计算任意两个位置之间的相互作用，而不管它们的位置距离，直接捕获长期依赖关系。</li><li>实验表明，有效，即使就几层non-local block</li><li>可以作为一个组件，和其它网络结构结合</li></ul><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="non-local-means"><a href="#non-local-means" class="headerlink" title="non-local means"></a>non-local means</h4><p>Non-local Means 非局部均值去噪滤波可以视为局部均值滤波的特例，它的目的是使用与当前点纹理类似的区域，对当前点加权。也即加权因子，是基于被加权点与当前点的邻域的相似性产生，即:<br>$$<br>\hat{u}(x)=\sum_{y \in I}w(x,y)*v(y)\<br>d=\frac{||block(x)-block(y)||}{block_size}\<br>w(x,y)=e^{-d^2/h^2}<br>$$<br>h为衰减因子，h越小，加权因子越小，则加权点对当前点的影响越小，一般边缘保持得好但是噪声会严重，反之则边缘保持差图像更加光滑，<strong>用周围相似的点来起到减小噪声的效果，但是如果有加权，肯定会对原图产生模糊效果。</strong></p><p>实际操作中，要更新当前点，先计算出以当前点为中心的<strong>搜索框I所有点的加权因子，取最大的加权因子付给当前点位置</strong>，然后对于这个同搜索框尺寸加权矩阵W进行归一化</p><h4 id="Graphical-models"><a href="#Graphical-models" class="headerlink" title="Graphical models"></a>Graphical models</h4><p>长期依赖关系可以通过图形模型(如条件随机场(CRF))建模。在深度神经网络中，CRF可以用于网络的后处理语义分割预测。The iterative mean-field inference of CRF 可以转化为递归网络进行训练。这些方法(包括我们的)都和<strong>图神经网络</strong>有关系。</p><h3 id="Non-local-Net"><a href="#Non-local-Net" class="headerlink" title="Non-local Net"></a>Non-local Net</h3><h4 id="3-1-Formulation-一般定义"><a href="#3-1-Formulation-一般定义" class="headerlink" title="3.1. Formulation/一般定义"></a>3.1. Formulation/一般定义</h4><p>$$<br>y_i=\frac{1}{C(x)}\sum_{\forall j}f(x_i,x_j)g(x_j)<br>$$</p><p>x是输入信号（图片，队列，视频，通常是特征）,i 是输出位置(在空间、时间或时空中)的索引，它的响应值是通过j枚举所有可能位置来计算的。函数f计算i和所有j之间的相似关系，一元函数g 计算输入信号在位置j的表示。最终的响应值通过响应因子从C(x) 进行标准化处理得到。图像中的每一位置$j$都被考虑到。</p><p><strong>对比卷积的过程</strong>:一个3x3卷积核，中心点是i，包括中心点的所有九个点是j.</p><p><strong>对比循环</strong>:第i时刻的操作只基于当前步和上一步</p><p><strong>对比全连接:</strong></p><ul><li>在non-local operation的公式中，响应值是通过计算不同区域之间的关系得到的，而在全连接层中，是通过赋给每个神经元一个学到的权重。换而言之，在全连接层中，$X_i$和$X_j$的关系不能通过一个函数f得到。<strong>学到的权重可以信任吗？注意力是要学习还是要人为设置值得考量。</strong></li><li>non-local公式支持可变大小的输入，并在输出中保持相应的大小，在全连接层中，<strong>要求固定大小的输入和输出(因为要事先设定权重层的维度)，并且由于被拉伸成一列，丢失了原有的位置信息</strong>。</li><li>在与CNN结合位置来看，non-local operation非常灵活，可以添加到深度神经网络中的前半部分，而全连接层通常被用在最后，这既是一个不同，也给了我们一个启发：能够构建一个更丰富的层次结构，将non-local信息和local信息结合起来。</li></ul><h4 id="3-2-Instantiations"><a href="#3-2-Instantiations" class="headerlink" title="3.2 Instantiations"></a>3.2 Instantiations</h4><p>接下来介绍f和g的集中具体形式。有趣的是，我们将通过实验(表2a)表明，我们的非局部模型对这些选择不敏感，<strong>表明通用的非局部行为是所观察到的改进的主要原因.(不同的形式不重要,重要的是思想)</strong></p><p>简单起见，g可以看做一个线性转化，其中:<br>$$<br>g(x_j)=W_gx_j<br>$$<br>其中，$W_g$是需要学习的权重矩阵，可以通过空间上的1x1卷积实现。<strong>假设一个feature map(H,W,C)=(513,513,64),那么共有513x513个点，每个点的维度是(1,1,64),每个点是一个特征向量，这个特征向量决定它是哪一类，就跟之前的视觉词袋类似，或者说图片搜索比对两个特征向量来判断是否是相同的向量。</strong></p><p>对于f:</p><p><strong>Gaussian</strong><br>$$<br>f(x_i,x_j)=e^{x_i^Tx_j}<br>$$<br>$x^T_ix_j$是点乘相似性，欧式距离也行，但是点乘实现友好。$C(x)=\sum_{\forall j}f(x_i,x_j)$</p><p><a href="https://zhuanlan.zhihu.com/p/159244903">点积相似度、余弦相似度、欧几里得相似度</a><br>$$<br>similarity=cos(\theta)=\frac{A\cdot B}{||A||\cdot ||B||})<br>$$<br><strong>余弦相似度</strong>衡量两个向量在方向上的相似性，而不care两个向量的实际长度，A和B的长度即使是一个超级短一个超级长的情况下，二者的余弦相似性也可能为1(即theta=0，此时两个向量重合)；</p><blockquote><p>存在的问题[1]：<br><strong>余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感。</strong><br>比如用户对内容评分，5分制。A和B两个用户对两个商品的评分分别为A:(1,2)和B:(4,5)。我们分别用两种方法计算相似度。<strong>使用余弦相似度得出的结果是0.98，看起来两者极为相似，但从评分上看A似乎不喜欢这两个东西，而B比较喜欢。造成这个现象的原因就在于，余弦相似度没法衡量每个维数值的差异，对数值的不敏感导致了结果的误差。</strong><br>需要<strong>修正这种不合理性，就出现了调整余弦相似度，即所有维度上的数值都减去一个均值。</strong><br>比如A和B对两部电影评分的均值分别是**(1+4)/2=2.5,(2+5)/2=3.5。那么调整后为A和B的评分分别是:(-1.5,-1.5)和(1.5,2.5)**，再用余弦相似度计算,得到-0.98,相似度为负值,显然更加符合现实.<br>修正的余弦相似度可以说就是对余弦相似度进行归一化处理的算法,公式如下:<br>$$<br>s(i,j)=\frac{\sum_{u\in U}(R_{u,i}-R_u)(R_{u,j}-R_u)}{\sqrt{\sum_{u\in U}(R_{u,i}-R_u)^2}\sqrt{\sum_{u\in U}(R_{u,j}-R_u)^2}}<br>$$</p></blockquote><p><strong>欧几里得相似度:</strong><br>$$<br>d(x,y)=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+···+(x_n-y_n)^2}=\sqrt{\sum^n_{i=1}(x_i-y_i)^2}<br>$$<br><strong>点积相似度：</strong></p><p>点积公式:$a\cdot b=|a||b|cos\theta$</p><p>可以看到，我们在优化点积的时候实际上是在<strong>优化a、b的模长和二者的夹角</strong>，（a和b分别是embedding空间的a向量和b向)，以word2vec为例，使用负采样的方式，a和b在同一个窗口内，其对应标签为1，则模型训练肯定是希望|a|*|b|*costheta 越接近1越好，那么在优化的过程中：</p><p>假设a和b的模长均为1且固定，则theta可能会越来越小（theta越小则costheta越接近于1，则整个a*b越接近于1），那么在embedding空间发生的事情就是，a和b向量越来越接近，这就和我们的目的达成一致.</p><p><strong>反正我的理解就是一定程度上抵消了余弦相似度只看方向不看数值的问题。</strong></p><p><strong>Embedded Gaussian</strong></p><p>基于高斯函数的一个简单扩展,计算嵌入空间中的相似度，即:<br>$$<br>f(x_i,x_j)=e^{\theta(x_i)^T\phi(x_j)}<br>$$<br>$\theta$和$\phi$是两个嵌入，embedding会构建一个映射，将一个空间里的实体抛射到一个线性向量空间里去，这样一来可以在向量空间里计算度量它们的距离，可以表示为：<br>$$<br>\theta (x_i)=W_\theta x_i and\phi(x_j)=W_\phi x_j<br>$$<br>C还是$C(x)=\sum_{\forall j}f(x_i,x_j)$</p><p><strong>Embedded Gaussian操作与self-attention很类似，实际上，self-attention是其一个特例。</strong></p><p><strong>Dot product</strong><br>$$<br>f(x_i,x_j)=\theta(x_i)^T\phi(x_j)<br>$$<br>这里采用的是嵌入版本。C(x)=N,N为x中位置的个数，而不是f的和，这样可以简化梯度计算。归一化是非常必要的，因为inputsize是可变的。</p><p>点击和嵌入高斯的主要区别是嵌入高斯有softmax，起到激活函数作用。</p><p><strong>Concatenation</strong><br>$$<br>f(x_i,x_j)=ReLU(w^T_f[\theta(x_i),\phi(x_j)])<br>$$<br>Relation Network用了拼接，所以它也提供了一个拼接形式。</p><p>[.,.]表示拼接，$w_f$是权重向量将向量转为标量，C(x)=N。</p><h4 id="3-3-Non-local-Block"><a href="#3-3-Non-local-Block" class="headerlink" title="3.3. Non-local Block"></a>3.3. Non-local Block</h4><p>将non-local操作封装到一个non-local block，形式如下:<br>$$<br>z_i=W_zy_i+x_i<br>$$<br>$y_i$就是non-local operation的输出,$x_i$是残差连接,残差连接允许我们插入一个新的non-local block到任何pre-trained model，而不破坏它的初始行为(例如，如果$W_z$初始化为零)。</p><img src="/semantic-segmentation-2/image-20210404100058646.png" alt="non-local block" style="zoom: 67%;"><p><strong>搞不明白DANet为什么能发文章，这不跟non-local基本一样吗？</strong></p><p>非局部块的成对计算在高层次的特征映射中是轻量级的。在上图的block中，一般T=4，H=W=14或者7，矩阵乘法的两两运算复杂度与传统卷积计算不相上下。为了进一步提升模型效率，还进行了如下调整：</p><ul><li><p>将$\theta,\phi,g$的通道数设置为输入feature maps的一半，这样会大大减少计算量。</p></li><li><p>采用抽样的方式，进一步减轻计算量，将non-local改进为如下公式：</p></li><li><p>$$<br>y_i=\frac{1}{c(\hat x)}\sum_{\forall j}f(x_i,\hat x_j)g(\hat x_j)<br>$$</p><p>其中,$\hat x$是x<strong>经过池化后得到，在空间域中执行这个操作，可以将成对计算的数量减少1/4。</strong>这个<strong>技巧不会改变非局部的行为，只会使计算变得更稀疏，实现起来也很简单，只需要在上图中</strong>$\theta$<strong>，g中加一个最大池化层即可。</strong></p></li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h2 id="DANet"><a href="#DANet" class="headerlink" title="DANet"></a>DANet</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>自适应的将局部特征与全局依赖相结合。</p><p>位置注意力模块通过所有位置上的特征的加权总和选择性地聚合每个位置上的特征。相似的特征将彼此相关，而与她们之间的距离无关。</p><p>同时，频道关注模块通过整合所有频道图之间的关联特征来有选择地强调相互依存的频道图</p><img src="/semantic-segmentation-2/image-20210511111105083.png" alt="image-20210511111105083" style="zoom:67%;"><p>以图1中的街景为例。首先,第一排的一些人和红绿灯由于光线和视野的原因是<strong>不显眼或不完整的物体。</strong></p><p>如果采用简单的上下文嵌入方法，则<strong>显著对象(如汽车、建筑)的上下文会对不显著对象的标记造成伤害</strong>。相比之下，我们的<strong>注意模型选择性地聚集不显著对象的相似特征，以突出其特征表征，避免显著对象的影响。</strong></p><p>其次,<strong>车和人的尺度是不同的，识别这些不同的物体需要不同尺度上的上下文信息</strong>。也就是说，<strong>不同尺度的特征应该被平等对待，以代表相同的语义。我们的注意机制模型从全局的角度自适应地集成任何尺度上地相似特征</strong>,在一定程度上可以解决上述问题。</p><p>第三，我们明确地将空间和通道关系考虑在内，以便场景理解可以从长期依赖中受益。</p><p><strong>PAM：一些细节和物体边界通过位置注意模块更加清晰</strong>，如第一排的“极点”和第二排的“人行道”。<strong>局部特征的选择性融合增强了对细节的识别能力。</strong></p><p><strong>CAM：一些错误分类的类别现在得到了正确的分类</strong>，例如第一行和第三行中的“bus”。<strong>通道映射之间的选择性集成有助于捕获上下文信息。语义一致性得到了明显的提高</strong></p><p>==和Non-Local不同的一点是nonlocal用的乘，这个用的加来融合注意==</p><h3 id="Info-amp-Question-Analysis"><a href="#Info-amp-Question-Analysis" class="headerlink" title="Info&amp;Question Analysis"></a>Info&amp;Question Analysis</h3><p>19CVPR <a href="https://blog.csdn.net/wumenglu1018/article/details/95949039">DANet阅读笔记</a></p><p><strong>着眼点:整合局部特征及其全局依赖关系</strong></p><ul><li>位置注意力模块通过<strong>所有位置处的特征的加权和来选择性地聚合每个位置的特征。</strong>无论距离如何，类似的特征都将彼此相关。</li><li>通道注意力模块通过整合所有通道映射之间的相关特征来<strong>选择性地强调存在相互依赖的通道映射</strong></li><li>将两个注意模块的输出相加以进一步改进特征表示，这有助于更精确的分割结果</li></ul><p>为了增强特征表达，</p><ol><li>利用多尺度上下文融合，如结合不同扩展卷积和池操作生成的特征映射来聚合多尺度上下文</li><li>使用分解结构增大卷积核尺寸或在网络顶部引入有效的编码层，来捕获更丰富的全局信息。</li><li>编码器-解码器结构来融合中层和高层语义特征。</li><li>使用RNN捕捉长程依赖关系，从而提高分割精度。</li></ol><p>1、2、3尽管上下文融合有助于捕获不同尺度的对象，但它<strong>不能充分利用全局视图中对象或事物之间的关系</strong>，这也是场景分割的关键。4利用递归神经网络<strong>隐含地捕捉全局关系，其有效性严重依赖于长期记忆的学习结果</strong>。</p><p>在处理复杂多样的场景时，DANet比以往的方法更加有效和灵活。</p><p>以图1中的街景为例。首先,第一排的一些人和红绿灯由于光线和视野的原因是<strong>不显眼或不完整的物体。</strong></p><p>如果采用简单的上下文嵌入方法，则<strong>显著对象(如汽车、建筑)的上下文会对不显著对象的标记造成伤害</strong>。相比之下，我们的<strong>注意模型选择性地聚集不显著对象的相似特征，以突出其特征表征，避免显著对象的影响。</strong></p><p>其次,<strong>车和人的尺度是不同的，识别这些不同的物体需要不同尺度上的上下文信息</strong>。也就是说，<strong>不同尺度的特征应该被平等对待，以代表相同的语义。我们的注意机制模型从全局的角度自适应地集成任何尺度上地相似特征</strong>,在一定程度上可以解决上述问题。</p><p>第三，我们明确地将空间和通道关系考虑在内，以便场景理解可以从长期依赖中受益。</p><p><strong>PAM：一些细节和物体边界通过位置注意模块更加清晰</strong>，如第一排的“极点”和第二排的“人行道”。<strong>局部特征的选择性融合增强了对细节的识别能力。</strong></p><p><strong>CAM：一些错误分类的类别现在得到了正确的分类</strong>，例如第一行和第三行中的“bus”。<strong>通道映射之间的选择性集成有助于捕获上下文信息。语义一致性得到了明显的提高</strong></p><h3 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h3><p>基于self-attention捕捉丰富的上下文相关性。与以往通过多尺度特征融合获取上下文的研究不同，提出了一种双注意网络(DANet)来自适应地整合局部特征及其全局依赖关系。</p><ul><li>位置注意力模块通过所有位置处的特征的加权和来选择性地聚合每个位置的特征。无论距离如何，类似的特征都将彼此相关。</li><li>通道注意力模块通过整合所有通道映射之间的相关特征来选择性地强调存在相互依赖的通道映射</li><li>将两个注意模块的输出相加以进一步改进特征表示，这有助于更精确的分割结果</li></ul><p>在Cityscapes、PASCAL Context、COCO Stuff取得sota</p><h3 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h3><p>为了增强特征表达，</p><ol><li>利用多尺度上下文融合，如结合不同扩展卷积和池操作生成的特征映射来聚合多尺度上下文</li><li>使用分解结构增大卷积核尺寸或在网络顶部引入有效的编码层，来捕获更丰富的全局信息。</li><li>编码器-解码器结构来融合中层和高层语义特征。</li><li>使用RNN捕捉长程依赖关系，从而提高分割精度。</li></ol><p>1、2、3尽管上下文融合有助于捕获不同尺度的对象，但它不能充分利用全局视图中对象或事物之间的关系，这也是场景分割的关键。4利用递归神经网络隐含地捕捉全局关系，其有效性严重依赖于长期记忆的学习结果。</p><p><img src="/semantic-segmentation-2/image-20210323162023746.png" alt="Figure1"></p><p>在处理复杂多样的场景时，DANet比以往的方法更加有效和灵活。</p><p>以图1中的街景为例。首先,第一排的一些人和红绿灯由于光线和视野的原因是不显眼或不完整的物体。</p><p>如果采用简单的上下文嵌入方法，则显著对象(如汽车、建筑)的上下文会对不显著对象的标记造成伤害。相比之下，我们的注意模型选择性地聚集不显著对象的相似特征，以突出其特征表征，避免显著对象的影响。</p><p>其次,车和人的尺度是不同的，识别这些不同的物体需要不同尺度上的上下文信息。也就是说，不同尺度的特征应该被平等对待，以代表相同的语义。我们的注意机制模型从全局的角度自适应地集成任何尺度上地相似特征,在一定程度上可以解决上述问题。</p><p>第三，我们明确地将空间和通道关系考虑在内，以便场景理解可以从长期依赖中受益。</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h3 id="DANet-1"><a href="#DANet-1" class="headerlink" title="DANet"></a>DANet</h3><h4 id="3-1Overview"><a href="#3-1Overview" class="headerlink" title="3.1Overview"></a>3.1Overview</h4><p>因为卷积操作产生的是局部感受野，导致相同标签的像素对应特征可能不同，这种差异会进而导致类内的不一致性，影响识别的准确率。所以本文提出：在特征之间使用注意力机制建立关联以获取全局上下文信息。</p><img src="/semantic-segmentation-2/image-20210323184906645.png" alt="DANet" style="zoom:80%;"><h4 id="3-2PAM"><a href="#3-2PAM" class="headerlink" title="3.2PAM"></a>3.2PAM</h4><p>观察:传统FCNs生成的局部特征会导致objects和stuff的错误分类。</p><p>解决:引入位置注意模块在局部特征上建立丰富的上下文关系，将更广泛的上下文信息编码为局部特征，进而增强他们的表示能力。</p><img src="/semantic-segmentation-2/image-20210323192129272.png" alt="PAM" style="zoom:80%;"><p>公式一中$s_{ji}$表示第j个点对第i个点的影响，两个位置的特征表示越相似，它们之间的相关性就越大。向量相乘就是余弦相似度。</p><p>由公式二，每个位置得到的特征E是所有位置特征与原始特征的加权和。因此，它具有全局语境观，并根据空间注意特征选择性地聚合上下文。相似的语义特征相互受益，提高了类内的紧凑性和语义一致性。相似的语义特征相互受益，提高了类内的紧凑性和语义一致性。</p><p>PAM：每个维度所有点需要注意的</p><p><strong>感觉self-attention就是特征缩放，所有点都对每一个点的特征缩放产生影响，self-attention就是确定这个影响的大小。因为网络是可以自己学习的，所以重点是让谁和谁产生关系，这种关系又是以哪种形式确定的。</strong></p><p><strong>加一个特征衰减函数，那么这个衰减函数为什么不可以去学习得到呢？？？？</strong>学出来的话怎么确定学出来的是注意力呢，是不是需要监督？</p><h4 id="3-3CAM"><a href="#3-3CAM" class="headerlink" title="3.3CAM"></a>3.3CAM</h4><p>观察:每个high level特征的通道图都可以看作是一个特定于类的响应，通过挖掘通道图之间的相互依赖关系，可以突出相互依赖的特征图，提高特定语义的特征表示。</p><p>解决:建立一个通道注意力模块来显式地建模通道之间的依赖关系。</p><img src="/semantic-segmentation-2/image-20210323195612525.png" alt="CAM" style="zoom:80%;"><p>实现了对特征图之间的长程语义依赖关系建模，有助于提高特征的辨别性。</p><p>公式三中，$x_{ji}$表示第j个维度所有点对第i个维度所有点的影响。</p><p>注意，==在计算两个通道的关系之前，我们不使用卷积层来嵌入特征，因为它可以保持不同通道特征图之间的关系。==此外，与最近的作品[27]通过全局池化或编码层探索信道关系不同，我们利用所有对应位置的空间信息来建模信道相关性。</p><p><strong>CAM是,一个点有三个维度，每个维度要注意什么</strong></p><h4 id="3-4Attention-Module-Embedding-with-Networks"><a href="#3-4Attention-Module-Embedding-with-Networks" class="headerlink" title="3.4Attention Module Embedding with Networks"></a>3.4Attention Module Embedding with Networks</h4><p>为了充分利用长程上下文信息，所以将这2个注意力模块的特征进行了聚合。即通过卷积层对两个注意力模块的输出进行转换，并执行一个element-wise的求和来实现特征融合。最后接一个卷积得到最后的预测特征图。</p><p>该注意力模块很简单，可以直接插入到现在的FCN中。而且它们不会增加太多参数，还能有效地增强特征表示。我们不采用级联操作，因为它需要更多的GPU内存。</p><h3 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="/semantic-segmentation-2/image-20210323200616217.png" alt="PAM"></p><p><img src="/semantic-segmentation-2/image-20210323200636411.png" alt="CAM"></p><h4 id="4-2-Results-on-Cityscapes-Dataset"><a href="#4-2-Results-on-Cityscapes-Dataset" class="headerlink" title="4.2. Results on Cityscapes Dataset"></a>4.2. Results on Cityscapes Dataset</h4><p><strong>4.2.1 Ablation Study for Attention Modules</strong></p><p>双重注意力来捕捉长期依赖关系 如Fig4、5</p><p><strong>4.2.2 Ablation Study for Improvement Strategies</strong></p><p>提升策略:</p><p>DA:随机缩放   Multi-grid:(4,8,16) in the last ResNet block <strong>MS: We average the segmentation probability maps from 8 image scales{0.5 0.75 1 1.25 1.5 1.75 2 2.2} for inference</strong></p><p><img src="/semantic-segmentation-2/image-20210407095909026.png" alt="AS for improvement strategies"></p><p><strong>4.2.3 Visualization of Attention Module</strong></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文提出了一种双注意网络(DANet)用于场景分割，该网络利用自注意机制自适应地整合局部语义特征。具体来说，引入了位置注意模块和通道注意模块来分别捕捉空间维度和通道维度的全局相关性。消融实验表明，双注意模块能有效地捕获长时间上下文信息，并给出更精确的分割结果。此外，降低计算复杂度和增强模型的鲁棒性也很重要，这将在以后的工作中进行研究。</p><h2 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>其实就两种注意力，整个特征图，每一个1x1xC的点就是一个特征向量，表征该地方的元素的性质，之前的方法都是探究不同的点之间的关系，即1x1xC和1x1xC的点之间的关系，本文是探究每个1x1xC的C维度之间的关系。利用全局池化获得全局统计描述子，显示建模通道之间的关系，再映射回去。</p><p>其实每一个点都是之前所有的点的特征组合输出啊，一个特征图不同channel之所以能学习到不同的高维特征，根本原因就是之前的不同的卷积核的初始化参数不同，这也就解释了为什么multi-head attention为什么要乘以一个权重矩阵了，因为只有这些矩阵不一样，才能学到不同的特征。</p><p>==这种注意力机制让模型可以更加关注信息量最大的 channel 特征，而抑制那些不重要的 channel 特征。==但到底是如何运作的还是存疑~</p><p>==nonlocal是一个点分别去看所有的点，这个是一个点去看一张图？==</p><p>==人家也是用与图像分类和目标检测的，不要把任务忘了，不同的任务有不同的性质啊！比如图像分类，这样的话就很容易理解了，是要看整个特征图代表什么，而不是看每个点代表什么，所以丢掉位置关系也还好。==</p><img src="/semantic-segmentation-2/image-20210511103349422.png" alt="image-20210511103349422" style="zoom:80%;"><p>加上这张图就好理解了！</p><h3 id="Info-2"><a href="#Info-2" class="headerlink" title="Info"></a>Info</h3><p>18CVPR  <a href="https://www.cnblogs.com/bonelee/p/9030092.html">教程讲的很好</a></p><h3 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h3><p>卷积核作为卷积神经网络的核心，通常被看做是在<strong>局部感受野上，将空间上（spatial）的信息和特征维度上（channel-wise）的信息进行聚合的信息聚合体。</strong>为了提高网络的表示能力，最近的一些方法显示了<strong>增强空间编码</strong>的好处。在这项工作中，我们<strong>关注于通道关系</strong>，并提出了SE block</p><p>动机是希望<strong>显式地建模特征通道之间的相互依赖关系</strong>。另外，我们并不打算引入一个新的空间维度来进行特征通道间的融合，而是采用了一种全新的「特征重标定」策略。具体来说，就是<strong>通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。</strong></p><h3 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h3><p>卷积滤波器期望是<strong>在局部感受野内融合空间和通道信息的信息组合</strong>。通过一系列的非线性和下采样的卷积层，cnn能够捕捉<strong>层次性的全局感受野</strong>作为强大的图像描述。</p><p>明确<strong>embedding learning mechanisms 可以帮助capture spatial correlations</strong>  —–Inception 和空间注意力</p><p>本工作建模<strong>通道关系,提出特征重校准机制，通过该机制，它可以学习使用全局信息来选择性地强调蕴涵信息的特征并抑制不太有用的特征</strong></p><p><strong>痛失一个亿啊</strong></p><img src="/semantic-segmentation-2/image-20210408155437704.png" alt="SE block" style="zoom:80%;"><ul><li>$F_{tr}(·,\theta)$:一个或一组卷积</li><li>$F_{sq}(·)$: Squeeze操作  ==HxW的二维平面压缩成一个实数，作为一个通道描述子，具有全局感受野，并且输出的维度和输入的特征通道数相匹配。它<strong>表征着在通道特征上响应的全局分布</strong>，而且**使得靠近输入的层也可以获得全局的感受野(与传统的堆叠卷积层的方式相比)**，==这一点在很多任务中都是非常有用的。</li><li>$F_{ex}(·,w)$:Exciation操作 一个类似于循环神经网络中门的机制。通过参数 w 来为每个特征通道生成权重，其中参数 w 被学习用来显式地建模特征通道间的相关性</li><li>$F_{scale}(·)$:Reweight操作  ==将 Excitation 的输出的权重看做是经过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的重标定。==</li></ul><p>SE block 根据网络需求充当的角色不同  —早期:</p><p><strong>前中后期都起作用</strong></p><p>还有一个<strong>优点:即插即用</strong>   还有一个<strong>优点:计算开销小</strong></p><h3 id="Related-Work-1"><a href="#Related-Work-1" class="headerlink" title="Related Work"></a>Related Work</h3><p><strong>Deep architectures</strong></p><p>一种是整个网络的研究 一种方向是探索调整网络模块化组件功能形式的方法。 最近以<strong>自动化学习的方式组合网络</strong>的方法表现的很有竞争力。</p><p>如1x1卷积，工作大部分集中在降低模型和计算复杂度的目标上，反映了一种假设，即可以将通道关系表示为<strong>具有局部接受域的实例不可知函数的组合</strong>。 相反，我们倡导为单元提供一种机制，可以<strong>使用全局信息对通道之间的动态，非线性依存关系进行显式建模</strong>，从而可以简化学习过程，并显着增强网络的表示能力。</p><p><strong>Attention and gating mechanisms</strong></p><p>可以将注意力广义地看作是一种工具，将<strong>可用处理资源的分配偏向于输入信号的信息最丰富的部分</strong>[17、18、22、29、32]。</p><h3 id="Squeeze-and-Excitation-Blocks"><a href="#Squeeze-and-Excitation-Blocks" class="headerlink" title="Squeeze-and-Excitation Blocks"></a>Squeeze-and-Excitation Blocks</h3><p>==通道相关性隐式的嵌入在filter里，但是这些相关性与空间相关性纠缠在一起。==</p><p>==我们的目标是确保网络能够提高其对信息功能的敏感度，以便随后的转换可以利用它们，并抑制不那么有用的信息。 我们建议通过显式地建模通道相关性来实现此目的，以在进入下一变换之前的两个步骤（挤压和激励）重新校准滤波器的响应。==</p><h4 id="3-1-Squeeze-Global-Information-Embedding"><a href="#3-1-Squeeze-Global-Information-Embedding" class="headerlink" title="3.1. Squeeze: Global Information Embedding"></a>3.1. Squeeze: Global Information Embedding</h4><p>==Embedding:一个全局池化就是将一个二维平面的信息嵌入到一个实数上==</p><p>卷积核感受野太小，所以用全局池化squeeze全局空间信息。<br>$$<br>z_c=F_{sq}(u_c)=\frac{1}{H \cdot W}\sum^H_{i=1}\sum^W_{j=1}u_c(i,j)<br>$$<br>$u_c$其实可以有很多种形式，==全局池化只是一种办法，格局不要小了，要利用数学公式==</p><h4 id="3-2-Excitation-Adaptive-Recalibration"><a href="#3-2-Excitation-Adaptive-Recalibration" class="headerlink" title="3.2. Excitation: Adaptive Recalibration"></a>3.2. Excitation: Adaptive Recalibration</h4><p>$$<br>s=F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_2\delta(W_1z))<br>$$</p><p>该操作旨在<strong>完全捕获通道之间的相互依赖关系</strong>,必须满足:</p><ul><li>灵活，==能够学习非线性关系==</li><li>==必须学习非互斥的关系，因为我们要确保允许强调多个渠道，而不是one-hot类型，也就是软间隔，可以多种并存，而不是非0即1只能选择一种的==</li></ul><p>综上最后一步是<strong>sigmoid</strong><br>$$<br>\tilde x_c=F_{scale}(u_c,s_c)=s_c\cdot u_c<br>$$</p><p>$s_c$是标量,扩展乘到每个标量</p><p>==Excitation充当适应于输入特定描述符z的通道权重。 在这方面，SE块从本质上引入了以输入为条件的动态特性，从而有助于增强特征的可分辨性,有点自监督的那味了==</p><h4 id="3-3-Exemplars-SE-Inception-and-SE-ResNet"><a href="#3-3-Exemplars-SE-Inception-and-SE-ResNet" class="headerlink" title="3.3. Exemplars: SE-Inception and SE-ResNet"></a>3.3. Exemplars: SE-Inception and SE-ResNet</h4><p>SE块的灵活性意味着它可以直接应用于标准卷积以外的转换。</p><img src="/semantic-segmentation-2/image-20210408203845549.png" alt="SE 架构" style="zoom:67%;"><p>在Inception网络中，Inception块作为$F_{tr}$,</p><p>==两个 Fully Connected 层组成一个 Bottleneck 结构去建模通道间的相关性，并输出和输入特征同样数目的权重。==</p><p>我们首先将特征维度降低到输入的 1/16，然后经过 ReLu 激活后再通过一个 Fully Connected 层升回到原来的维度。这样做比直接用一个 Fully Connected 层的好处在于：<strong>1）具有更多的非线性，可以更好地拟合通道间复杂的相关性；2）极大地减少了参数量和计算量。</strong></p><p>在ResNet网络中，Residual块作为$F_{tr}$</p><p>==在 Addition 前对分支上 Residual 的特征进行了特征重标定。如果对 Addition 后主支上的特征进行重标定，由于在主干上存在 0~1 的 scale 操作，在网络较深 BP 优化时就会在靠近输入层容易出现梯度消散的情况，导致模型难以优化。意思就是越乘越小，导致最后直接消失==</p><h3 id="Model-and-Computational-Complexity"><a href="#Model-and-Computational-Complexity" class="headerlink" title="Model and Computational Complexity"></a>Model and Computational Complexity</h3><p>为了可行，<strong>模型复杂度和性能要有权衡</strong>，缩小率$r$通常设为16</p><img src="/semantic-segmentation-2/image-20210408220409345.png" alt="成本比较" style="zoom: 67%;"><p>所有的模型精度基本都有提高,然而GFLOPs增加很少。</p><h2 id="CBAM"><a href="#CBAM" class="headerlink" title="CBAM"></a>CBAM</h2><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>==首先为什么不用softmax而用sigmoid做门控函数的思考？==</p><p>sigmoid不必保证映射之和为1，而softmax可以保证。</p><p>对于softmax，如果有一个值响应特别大，那么其他的就近乎为0了，相当于一个互斥的了，把别的可能有用的都挤没了。</p><p>而对于sigmoid，如下图所示，</p><img src="/semantic-segmentation-2/image-20210510225230665.png" alt="image-20210510225230665" style="zoom:50%;"><p>0右边相当一部分的特征可以被保留。</p><p>==实验分析，他主要是用在图像分类和目标检测，这些任务和语义分割还是有不同的。==</p><h3 id="Info-3"><a href="#Info-3" class="headerlink" title="Info"></a>Info</h3><p>18ECCV</p><h3 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h3><p>提出卷积块注意力模块(CBAM),是一种用于前馈卷积神经网络的简单而有效的注意力模块。<strong>给定一个中间特征图，我们的模块会沿着两个独立的维度（通道和空间）依次推断注意力图，然后将注意力图与输入特征图相乘以进行自适应特征改进。</strong>轻量，开销可忽略不计(==ps:在那个年代你可以这么说，现在人人都是attention和Transformer,你都不一定算得上轻量吧==) 广泛验证。</p><h3 id="Introduction-4"><a href="#Introduction-4" class="headerlink" title="Introduction"></a>Introduction</h3><p>==为了提高CNN的性能，最近的研究主要研究了网络的三个重要因素：深度depth，宽度width和基数cardinality。==</p><p>VGGNet、ResNet等$\to$depth</p><p>GoogLeNet $\to$width </p><p>[6]提出基于ResNet架构来增加网络的宽度。 他们表明，在CIFAR基准测试中，宽度增加的28层ResNet可以胜过具有1001层的极深的ResNet。</p><p>Xception、ResNeXt$\to$cardinality</p><p>==Xception [11]和ResNeXt [7]提出来增加网络的基数。 他们凭经验表明，基数不仅节省了参数的总数，而且还比其他两个因素（深度和宽度）具有更强的表示能力。==</p><p>除了这些因素,另一方面:<strong>attention.目标是通过使用注意力机制来提高表示能力：关注重要特征并抑制不必要的特征。</strong></p><p>==由于卷积运算通过将cross-channel和spatial信息融合在一起来提取信息特征，因此我们采用我们的模块来强调沿这两个主要维度-channel &amp; spatial的有意义的特征。==</p><p><img src="/semantic-segmentation-2/image-20210418224715746.png" alt="CBAM"></p><p>==先channel,再spatial,先学what再学where==</p><p>==使用grad-CAM[18]可视化训练好的模型.==<a href="https://blog.csdn.net/u013738531/article/details/81322043">grad-CAM博客教程，这个就不看论文了吧,会用就行了</a></p><p>==我们证明，使用CBAM可以同时实现更好的性能和更好的可解释性。 考虑到这一点，我们推测性能提升来自准确的关注accurate attention和无关杂波的降噪noise reduction of irrelevant clutters。==</p><h3 id="Related-Work-2"><a href="#Related-Work-2" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="Network-engineering"><a href="#Network-engineering" class="headerlink" title="Network engineering"></a><strong>Network engineering</strong></h4><h4 id="Attention-mechanism"><a href="#Attention-mechanism" class="headerlink" title="Attention mechanism."></a>Attention mechanism.</h4><p>人类视觉系统的一个重要特性是不会立即处理整个场景。 取而代之的是，人类利用一连串的部分瞥见，并有选择地专注于突出部分，以便更好地捕获视觉结构[26]。</p><p>SENet </p><p>==However, we show that those are suboptimal features in order to infer fine channel attention, and we suggest to use max-pooled features as well.==</p><p>每篇论文都提一嘴人家没有spatial,绝了,每个人都来踩一脚</p><p>BAM与CBAM的区别(竟然是一个团队写的,就离谱)</p><p>BAM: Bottleneck Attention Module 18BMVC</p><p>CBAM: Convolutional Block Attention Module 18ECCV</p><p>==BAM模块放置在网络的每个<strong>bottleneck瓶颈</strong>，而CBAM插入每个<strong>卷积模块</strong>。==</p><img src="/semantic-segmentation-2/image-20210419115713469.png" alt="BAM" style="zoom: 67%;"><p>有趣的是，我们观察到多个BAM构造了类似于人类感知程序的层次化注意力hierarchical attention。 BAM在早期对低级特征（例如背景纹理特征）进行降噪。 然后，BAM逐渐将重点放在确切的目标上，这是一个高级语义。 </p><h3 id="CBAM-1"><a href="#CBAM-1" class="headerlink" title="CBAM"></a>CBAM</h3><img src="/semantic-segmentation-2/image-20210419120542929.png" alt="sub-module" style="zoom: 50%;"><p>overall attention process:$F \in R^{C\times H\times W} M_c\in R^{C\times1\times1} M_s\in R^{1\times H\times W}$<br>$$<br>F’=M_c(F)\otimes F,\<br>F’’=M_s(F’)\otimes F’<br>$$</p><p>$\otimes$代表element-wise乘。<strong>乘的时候是需要扩展的。</strong></p><h4 id="CAM"><a href="#CAM" class="headerlink" title="CAM"></a>CAM</h4><p>==由于特征图的每个通道都被视为特征检测器[32]==</p><blockquote><p>Visualizing and understanding convolutional networks.14ECCV</p></blockquote><p>对于给定输入图像,说==channel attention聚焦于“what”==是有意义的。SENet使用全局平均池化计算==空间统计量spatial statistics==</p><p>我们凭直觉感觉最大池获得另一种信息，于是两个都用了，还真不错……</p><p>==Shared MLP==<br>$$<br>M_c(F)=\sigma(MLP(AvgPool(F))+MLP(MaxPool(F)))\<br>=\sigma(W_1(W_0(F^c_{avg}))+W_1(W_0(F^c_{max})))<br>$$<br>$\sigma$表示sigmoid函数,$W_0\in R^{C/r\times C}$,$W_1\in R^{C\times C/r}$ 是MLP weights，对$inputs$权重共享 $ReLU$激活函数followed by $W_0$</p><h4 id="SAM"><a href="#SAM" class="headerlink" title="SAM"></a>SAM</h4><p>==空间注意力聚焦于’where’==</p><p>==沿通道轴应用avg和max,和DANet还是有些不用的,CBAM和DANet都应用了对称的思想,只不过DANet把nonlocal对称到了channel,CBAM把SENet对称到了spatial==</p><p>沿通道轴应用pool显示出突出信息区域方面是有效的[34]</p><blockquote><p>Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In: ICLR. (2017)</p></blockquote><p>$$<br>M_s(F)=\sigma(f^{7\times7}([Augpool(F);MaxPool(F)]))\<br>=\sigma(f^{7\times7}([F^s_{avf};F^s_{max}]))<br>$$</p><p>$F^s_{avg}\in R^{1\times H\times W}$  $F^s_{max}\in R^{1\times H\times W}$  $\sigma$代表sigmoid function  $f^{7\times7}$代表7x7的卷积核</p><h4 id="Arrangement"><a href="#Arrangement" class="headerlink" title="Arrangement"></a>Arrangement</h4><p>==我们发现顺序排列比并行排列提供更好的结果。 对于顺序过程的安排，我们的实验结果表明，通道优先级比空间优先级略好。==</p><h3 id="Experiments-2"><a href="#Experiments-2" class="headerlink" title="Experiments"></a>Experiments</h3>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>变帅秘籍</title>
      <link href="bian-shuai-mi-ji/"/>
      <url>bian-shuai-mi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="发型"><a href="#发型" class="headerlink" title="发型"></a>发型</h2><p>两边太鼓不好看 头发太厚去薄 头发往后留  </p><p>不适合留太短的头发</p>]]></content>
      
      
      <categories>
          
          <category> 变帅 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 变帅 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN_tricks</title>
      <link href="cnn-tricks/"/>
      <url>cnn-tricks/</url>
      
        <content type="html"><![CDATA[<h2 id="Bag-of-Tricks"><a href="#Bag-of-Tricks" class="headerlink" title="Bag of Tricks"></a>Bag of Tricks</h2><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>19CVPR</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>图像分类研究的最新进展主要归功于<strong>训练过程的改进，如数据增强和优化方法的改变。</strong>然而这些改进大部分作为实验细节提一嘴，要么只在源代码中可见。本文通过消融研究来检验这些改进的影响。通过这些trick集合，能够<strong>显著改进各种CNN模型</strong>，同时<strong>对对象检测和语义分割带来更好的迁移学习性能。</strong></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>改进包括模型架构、损失函数、数据预处理、优化方法，这些改进提高模型精度，但几乎不改变计算复杂度。</p><h3 id="Training-Procedures-基线"><a href="#Training-Procedures-基线" class="headerlink" title="Training Procedures/基线"></a>Training Procedures/基线</h3><h4 id="2-1Baseline-Training-Procedure"><a href="#2-1Baseline-Training-Procedure" class="headerlink" title="2.1Baseline Training Procedure"></a>2.1Baseline Training Procedure</h4><p>采用Resnet作为基线</p><p>训练过程执行而验证过程不执行随机增强。</p><p>卷积层和全连接层的<strong>权值均采用Xavier算法进行初始化</strong>。</p><h3 id="Efficient-Training-训练速度"><a href="#Efficient-Training-训练速度" class="headerlink" title="Efficient Training/训练速度"></a>Efficient Training/训练速度</h3><p>在本节中，我们将回顾在不牺牲模型精度的情况下实现低精度和大批量训练的各种技术。有些技术甚至可以提高准确性和训练速度。</p><h4 id="3-1Large-batch-training"><a href="#3-1Large-batch-training" class="headerlink" title="3.1Large-batch training"></a>3.1Large-batch training</h4><p><strong>分析</strong></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;mid=2649032995&amp;idx=2&amp;sn=28b065415c2d8a11345531f1284413d0&amp;chksm=8712b75eb0653e48f46de857b14d6e029f08a5c0336928fc638eac1f8287aefea01a262be1e2&amp;token=2097035342&amp;lang=zh_CN#rd">AI不惑境的教程</a></p><ul><li>batch数太小，而类别又比较多的时候，真的可能会导致loss函数震荡而不收敛，尤其是在你的网络比较复杂的时候。</li><li>随着batchsize增大，处理相同的数据量的速度越快，跑完一个epoch所需迭代次数变少</li><li>一定范围内，batchsize越大，其确定的下降方向就越准，不容易被noise影响，引起训练振荡就越小</li><li>而batchsize越大的话，显存又可能受不住，模型收敛速度可能变慢，因为每次迭代用时更长了，学习率不变的话，需要更多次迭代</li><li>具体的batch size的选取和训练集的样本数目相关。</li><li>所以实际工程最常用的就是mini-batch，GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16,32,64…往往比整10、整100更优</li></ul><p><strong>Linear scaling learning rate</strong></p><p>大批量的批处理减少了梯度中的噪声，所以我们可以增加学习速率，使梯度方向相反的方向取得更大的进步。<strong>增大batch size，单batch数据中噪声的影响会更小，此时就可以使用大的学习率步长</strong>。比如ResNet-50网络，官方采用的batch size为256，初始学习率为0.1，那么当使用更大的batch size，符号表示为b时，那么可以设置初始学习率为$0.1\cdot \frac{b}{256}$。</p><p><strong>Learning rate warmup</strong></p><p>在训练开始时，所有参数通常都是随机值，因此远离最终解。使用过大的学习率可能会导致数值不稳定。<strong>学习率热启动策略具体操作为，将实验中用的学习率参数从0逐渐增大到初始学习率大小，然后再采用常规的学习率衰减方案，逐渐增大学习率的过程被称作warmup阶段</strong>。</p><p>假设使用前m个batch来预热，初始学习率为l，那么对于第i个批次,$1\le i\le m$,学习率为$l\cdot \frac{i}{m}$</p><p><strong>这种策略的好处：防止训练过程中出现的instablility</strong>。</p><p><strong>Zero</strong> $\gamma$</p><p>Resnet网络结构中包含了多个residual blocks，记某个block的输入为x，那么经过残差求和后的结果为$x+block(x)$,而block的最后一层为batch norm层，batch norm层先将该层的输入数据标准化，记作$\hat{x}$ ,那么batch norm层的输出为$ \gamma \hat{x}+\beta$ ，其中，$\gamma$和$\beta$为可训练参数，因此也需要在模型训练之前做初始化，<strong>通常的做法是将它们分别初始化为</strong>1和0。Zero $\gamma$ 的做法是将它们均初始化为0。</p><p><strong>这种策略的好处：将所有residual blocks中的最后一个batch norm层的$\gamma$和$\beta$参数设置为0，也即residual block的输出和输入相等，可以使模型在初始阶段更容易训练。</strong><br><strong>No bias decay</strong></p><p>权值衰减通常适用于所有可学习参数，包括权值和偏差。这相当于对所有参数应用L2正则化，使它们的值趋近于0。但是为了避免过拟合，建议对权重应用正则化。bias,和BN层中的$\gamma$和$\beta$都不正则化。</p><h4 id="3-2-Low-precision-training"><a href="#3-2-Low-precision-training" class="headerlink" title="3.2 Low-precision training"></a>3.2 Low-precision training</h4><p>通常，计算机使用32-bit浮点精度（FP32）做训练，也就是说，所有的数字都以FP32格式存储，算术运算的输入和输出也是FP32数字。英伟达的部分显卡针对FP16做了定制化优化，能够达到更快的计算速度，比如最新的显卡V100。另外，关于低精度训练相关的算法理论，可以参见笔者之前写的一篇文章。</p><p>建议在FP16中存储所有参数和激活，并使用FP16来计算梯度。同时，所有参数都有一个用于参数更新的FP32的副本。此外，用一个标量与损失相乘以更好地将梯度范围对齐到FP16也是一种实用的解决方案。</p><h4 id="3-3Experiment-Results"><a href="#3-3Experiment-Results" class="headerlink" title="3.3Experiment Results"></a>3.3Experiment Results</h4><p>技巧确实有用</p><h3 id="Model-Tweaks-网络结构"><a href="#Model-Tweaks-网络结构" class="headerlink" title="Model Tweaks/网络结构"></a>Model Tweaks/网络结构</h3><p>A model tweak is a minor adjustment to the network architecture。这样的调整通常几乎不会改变计算复杂度，但可能对模型精度有不可忽视的影响。</p><p>剩余内容见backbone中的Resnet部分。</p><h3 id="Training-Refinement-过程优化"><a href="#Training-Refinement-过程优化" class="headerlink" title="Training Refinement/过程优化"></a>Training Refinement/过程优化</h3><h4 id="5-1-Cosine-Learning-Rate-Decay"><a href="#5-1-Cosine-Learning-Rate-Decay" class="headerlink" title="5.1 Cosine Learning Rate Decay"></a>5.1 Cosine Learning Rate Decay</h4><p>广泛使用的策略是指数衰减的学习速率。何的ResNet每30个周期以0.1降低速率，我们称之为“阶跃衰减step decay”。</p><p>$η_t=\frac{1}{2}(1+cos(\frac{t\pi}{T}))η$</p><p>T为batch总数(不包括预热),t为当前批次，η为初始速率。与步长衰减相比，余弦衰减从一开始就开始衰减学习，但一直很大，直到步长衰减使lr降低了10倍，从而潜在地提高了训练进度。</p><h4 id="5-2-Label-Smoothing"><a href="#5-2-Label-Smoothing" class="headerlink" title="5.2 Label Smoothing"></a>5.2 Label Smoothing</h4><p>$z_i$表示对class i的预测分数，则$q=softmax(z)$,即:<br>$$<br>q_i=\frac{exp(z_i)}{\sum_{j=1}^Kexp(z_j)}<br>$$<br>假设真实label为y,则可以构造真实概率分布，$p_i=1\quad if\quad i=y$,否则为0，交叉熵为:<br>$$<br>l(p,q)=-\sum_{i=1}^Kp_ilogq_i\<br>=-logp_y\<br>=-z_y+log(\sum_{i=1}^Kexp(z_i))<br>$$<br>最优解是$z_y = inf$同时保持其他值足够小。换句话说，它鼓励输出分数显著不同，这可能导致过拟合。</p><p>标签平滑改变了真实概率的构造:<br>$$<br>q_i=c(u)=\begin{cases}1-\varepsilon \quad  if \quad i=y,\\varepsilon/(K-1) \quad otherwise \end{cases}<br>$$<br>$\varepsilon$是一个小常数</p><h4 id="5-3-Knowledge-Distillation"><a href="#5-3-Knowledge-Distillation" class="headerlink" title="5.3 Knowledge Distillation"></a>5.3 Knowledge Distillation</h4><p>Knowledge Distillation，可以翻译成知识蒸馏，最早由Hinton在2015年提出，它包含了一个”教师网络”和一个”学生网络“。其中，“教师网络”通常采用预训练好的模型，而“学生网路”中包含了待学习的参数。记标签的真实概率分布为p ，“学生网络”的预测概率分布为z ，“教师网络”的预测概率分布为r，优化的目标是，让“学生网络”在学习标签的真实概率分布的同时，还要学习“教师网络”的先验知识，公式表示如下:<br>$$<br>l(p,softmax(z))+T^2l(softmax(\frac{r}{T}),softmax(\frac{z}{T}))<br>$$<br>其中，前者表示学习标签的真实概率分布，后者表示学习“教师网络”的先验知识，T为超参数。</p><p>为什么学习的目标是这样子呢？</p><p><strong>标签的真实概率分布提供了hard label，准确率高但是信息熵很低，而“教师网络”提供了soft label，准确率相对较低但是信息熵高。这里的信息熵怎么理解呢，比如一副马的图片，可以想象的到它看上去也有点像牛，而hard label给的标签是[1, 0]，soft label给的标签是[0.7, 0.3]，显然，soft label提供了类别之间的关联，提供的信息量更大，有助于模型在学习时增强类间区分度，hard label和soft label结合等价于在学习真实标签的同时，补充类间信息。</strong></p><h4 id="5-4-Mixup-Training"><a href="#5-4-Mixup-Training" class="headerlink" title="5.4 Mixup Training"></a>5.4 Mixup Training</h4><p>mixup的数据增强方法。每次随机抽取两个样本$(x_i,y_i)$和$x_j,y_j$,然后由这两个样本生成新样本:</p><p>$\hat{x}=\lambda x_i+(1-\lambda)x_j$ $\hat{y}=\lambda y_i+(1-\lambda)y_j$    $\lambda \in [0,1] from\quad Beta(\alpha,\alpha) $</p><p>在mixup training,只使用$(\hat{x},\hat{y})$</p><h2 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h2><p><a href="https://www.zhihu.com/question/41631631">https://www.zhihu.com/question/41631631</a></p><p><a href="https://zhuanlan.zhihu.com/p/376068083">https://zhuanlan.zhihu.com/p/376068083</a></p><p><a href="https://zhuanlan.zhihu.com/p/24720954">https://zhuanlan.zhihu.com/p/24720954</a></p><h3 id="1-超参数"><a href="#1-超参数" class="headerlink" title="1.超参数"></a>1.超参数</h3><p>超参数最好沿用前人的,比如不同损失加起来的比例:$loss=\lambda_1loss_1+\lambda_2loss_2$,<strong>乱改极有可能某一个损失主导整个损失!</strong></p><p><strong>2.GAN</strong></p><p>不管generator的能力如何，discriminator的loss在前期一定是快速下降的，</p><p><strong>Adam的优化效率对于GAN来说很显著</strong></p><p>3.模型训练出现nan</p><p>一般为梯度爆炸</p><p><strong>4.预训练的Normailize(找这个bug找了好久~~)</strong></p><p>对图片做了Normalize,后续生成的特征图也是基于这个Normalize的,因为算损失是利用经过预训练后的图片算损失，所以对于图像转换问题,生成的转换图片也是经过这个Normalize的,必须逆过程才能生成正常图片，如果这个时候的分割模型的预训练模型不是用的相同的均值和标准差直接那这个生成的转换图片,就会导致有一个很大的域gap,就会导致效果非常的不好！</p><p><strong>5.损失的权重</strong></p><img src="/cnn-tricks/image-20220421105204158.png" alt="image-20220421105204158" style="zoom:67%;"><p>原来损失的权重能影响到这么大</p><h2 id="Diagram"><a href="#Diagram" class="headerlink" title="Diagram"></a>Diagram</h2><p>1.Weakly Supervised Object Localization as Domain Adaption</p><p><img src="/cnn-tricks/image-20220328162933528.png" alt="image-20220328162933528"></p><img src="/cnn-tricks/image-20220328163009191.png" alt="image-20220328163009191" style="zoom:67%;"><p>2.t-SNE</p><p><a href="https://blog.csdn.net/hustqb/article/details/78144384">https://blog.csdn.net/hustqb/article/details/78144384</a></p><p><a href="https://zhuanlan.zhihu.com/p/81400277">https://zhuanlan.zhihu.com/p/81400277</a></p><p>3.umap</p><p><a href="https://umap-learn.readthedocs.io/en/latest/basic_usage.html">官方文档</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>backbone</title>
      <link href="backbone/"/>
      <url>backbone/</url>
      
        <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>LeNet：5层轻量级网络，一般用来验证小型数据；经典的手写数字识别模型</p><p>AlexNet/VGGNet：把网络层数加深；</p><p>GoogLeNet/Inception：结合1x1卷积并采用带有不同kernel和池化的多分支策略进行特征提取；</p><p>ResNet：Residual block，使训练更深层的网络变得可能；</p><p>RexNeXt：引入组卷积，在精度基本不降的情况下速度超过ResNet；</p><p>DenseNet：主要是特征复用的思想，参数量虽小计算量不敢恭维；</p><p>Res2Net：基于ResNet引入多尺度；</p><p>SENet：基于通道矫正，强化重要特征，抑制非重要特征，重点是轻便可以随意嵌入；</p><p>SKNet：引入特征图注意力，使卷积核的感受野能够自适应</p><p>DCNet：引入可变形卷积，提高了泛化能力；</p><p>SqueezeNet、ShuffleNet、MobileNet：轻量级网络；</p><p>CSPNet：利用跨阶段特征融合策略和截断梯度流来增强不同层次特征的可变性解决冗余梯度信息，提高推理速度；</p><p>EfficientNet：E0-E7的进化之路号称无人能敌，配合谷歌刚出的Lite，实现精度、延迟两不误的移动端新SOTA；</p><p>RegNet：FBAI力作，号称超越EfficientNet，GPU上提速5倍的神作；</p><p>ResNeSt：刚出来的Backbone，乍眼一看是一个ResNeXt和SKNet的结合体，论文写着刷爆各大榜单；具体效果还未使用不从得知，直观感觉是个好的神器，留待时间去考证</p><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><img src="/backbone/image-20210324170556751.png" alt="LeNet" style="zoom:80%;"><p>网络结构如上，基本不会用到，了解即可</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><ol><li>成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。</li><li>训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。</li><li>在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。</li><li>出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。(后来好像被证明没用)</li><li>用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算。AlexNet使用了两块GTX 580 GPU进行训练，单个GTX 580只有3GB显存，这限制了可训练的网络的最大规模。因此作者将AlexNet分布在两个GPU上，在每个GPU的显存中储存一半的神经元的参数。因为GPU之间通信方便，可以互相访问显存，而不需要通过主机内存，所以同时使用多块GPU也是非常高效的。同时，AlexNet的设计让GPU之间的通信只在网络的某些层进行，控制了通信的性能损耗。</li><li>数据增强，随机地从256<em>256的原始图像中截取224</em>224大小的区域（以及水平翻转的镜像），相当于增加了2*(256-224)^2=2048倍的数据量。如果没有数据增强，仅靠原始的数据量，参数众多的CNN会陷入过拟合中，使用了数据增强后可以大大减轻过拟合，提升泛化能力。进行预测时，则是取图片的四个角加中间共5个位置，并进行左右翻转，一共获得10张图片，对他们进行预测并对10次结果求均值。同时，AlexNet论文中提到了会对图像的RGB数据进行PCA处理，并对主成分做一个标准差为0.1的高斯扰动，增加一些噪声，这个Trick可以让错误率再下降1%。</li><li><img src="/backbone/image-20210324170958652.png" alt="AlexNet" style="zoom:67%;"></li></ol><p>Group convolution 分组卷积，最早在AlexNet中出现，由于当时的硬件资源有限，训练AlexNet时卷积操作不能全部放在同一个GPU处理，因此作者把feature maps分给多个GPU分别进行处理，最后把多个GPU的结果进行融合。</p><h2 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h2><p><img src="/backbone/image-20210226194943546.png" alt="VGG6种结构配置"></p><ul><li>16个权重层，也是VGG16名称的来源</li><li>conv3-64表示，kernel size为3x3，64个filter，stride=1，padding=same，池化层均为2</li></ul><p><img src="/backbone/image-20210226195521085.png" alt="VGG16模块图"></p><p><img src="/backbone/image-20210226195459799.png" alt="VGG16流程图"></p><p>参数很多很大！</p><p>size一直除2，channel一直乘2</p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>随着网络层数增加，会出现退化degradation现象，loss饱和后反而增加了   </p><blockquote><p>当网络退化时，浅层网络能够达到比深层网络更好的训练效果，这时如果我们把低层的特征传到高层，那么效果应该至少不比浅层的网络效果差，或者说如果一个VGG-100网络在第98层使用的是和VGG-16第14层一模一样的特征，那么VGG-100的效果应该会和VGG-16的效果相同。所以，我们可以在VGG-100的98层和14层之间添加一条直接映射（Identity Mapping）来达到此效果。</p><p>从信息论的角度讲，由于DPI（数据处理不等式）的存在，在前向传输的过程中，随着层数的加深，Feature Map包含的图像信息会逐层减少，而ResNet的直接映射的加入，保证了 <img src="https://www.zhihu.com/equation?tex=l+1" alt="[公式]"> 层的网络一定比 <img src="https://www.zhihu.com/equation?tex=l" alt="[公式]"> 层包含更多的图像信息。</p></blockquote><p>$x_{l+1}=x_l+F(x_l,W_l)$</p><img src="/backbone/image-20210227125454086.png" alt="残差块" style="zoom:67%;"><p><img src="/backbone/image-20210227151941237.png" alt="结构表"></p><p><img src="/backbone/image-20210227152012707.png" alt="building block"></p><p>左边的叫Basic block，右边的叫Bottleneck block</p><img src="/backbone/15074510-faee46ef496b76bf.webp" alt="网络结构图"><p>这张图tql太厉害了！！！</p><img src="/backbone/image-20210319221135225.png" alt="resnet50结构" style="zoom:67%;"><p>ResNet由一个输入端、四个后续阶段和输出层组成。</p><p><strong>输入端:</strong></p><p>减少宽和高为原来的1/4，并增加通道大小为64.</p><p><strong>四个后续阶段Stage1、Stage2、Stage3、Stage4:</strong></p><p>从stage2开始，先是一个下采样块，然后剩下的是残差块。每个bottleneck最后一个卷积的输出通道都是前两个的四倍。</p><p>人们可以在每个阶段改变剩余残差块的数量，以获得不同的ResNet模型，如ResNet-50和ResNet-152，其中的数字表示网络中卷积层的数量。</p><img src="/backbone/image-20210319222556310.png" alt="tweak" style="zoom:80%;"><h4 id="ResNet-B"><a href="#ResNet-B" class="headerlink" title="ResNet-B"></a>ResNet-B</h4><p>修改了下采样顺序。<strong>将下采样操作放到非1 × 1卷积层。1 × 1卷积层等价于对输入feature maps沿着channel维度做加权求和，因此设置stride为2会导致丢失3/4的特征信息。对于3 × 3的卷积层，设置stride为2不会丢失特征信息</strong></p><h4 id="ResNet-C"><a href="#ResNet-C" class="headerlink" title="ResNet-C"></a>ResNet-C</h4><p>1个7x7的卷积核参数量是1个3x3卷积核的5.4倍。所以用三个3x3卷积核替代一个7x7卷积核显然更好。</p><h4 id="ResNet-D"><a href="#ResNet-D" class="headerlink" title="ResNet-D"></a>ResNet-D</h4><p>受ResNet-B的启发，我们注意到下采样块路径B中的1 × 1卷积也忽略了3/4的输入特征映射。从经验上看，我们发现在卷积前添加一个stride=2的2×2平均池化层，将1x1卷积stride改为1，在实际应用中效果很好，对计算成本的影响很小。</p><h2 id="GoogleNet-InceptionV3"><a href="#GoogleNet-InceptionV3" class="headerlink" title="GoogleNet/InceptionV3"></a>GoogleNet/InceptionV3</h2><p><a href="https://zhuanlan.zhihu.com/p/36878362">https://zhuanlan.zhihu.com/p/36878362</a></p><p>在Inception网络中，作者提出利用2个3×3卷积核的组合比1个5×5卷积核的效果更佳，同时参数量（3×3×2+1 VS 5×5×1+1）被降低，因此后来3×3卷积核被广泛应用在各种模型中。</p><p><strong>怎样才能减少卷积层参数量？– Bottleneck 1x1卷积核</strong></p><p><strong>每层卷积只能用一种尺寸的卷积核？– Inception结构</strong></p><h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h2><p><strong>卷积操作时所有通道都只能用同一个过滤器吗？– DepthWise操作</strong></p><h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p><a href="https://blog.csdn.net/u014380165/article/details/75142664">https://blog.csdn.net/u014380165/article/details/75142664</a></p><p><a href="https://zhuanlan.zhihu.com/p/43057737">https://zhuanlan.zhihu.com/p/43057737</a></p><h2 id="DCNet"><a href="#DCNet" class="headerlink" title="DCNet"></a>DCNet</h2><p><strong>卷积核形状一定是矩形吗？– Deformable convolution 可变形卷积核</strong></p><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><p><strong>通道间的特征都是平等的吗？ – SEnet</strong></p><h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><h2 id="MobileNeXt"><a href="#MobileNeXt" class="headerlink" title="MobileNeXt"></a>MobileNeXt</h2>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java核心基础</title>
      <link href="java-he-xin-ji-chu/"/>
      <url>java-he-xin-ji-chu/</url>
      
        <content type="html"><![CDATA[<h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p>处理器：Intel(R) Core(TM)i5-7300HQ CPU @ 2.50GHz 2.50 GHz</p><p>Core 酷睿   i5-7300 i5 i7 指不同类型  第一个7指第7代  HQ指标压 2.5GHZ相当于<strong>每秒2.5G个脉冲</strong>，时钟速度越快，在给定时间内执行指令越多</p><p>系统类型：64位系统，基于x64的处理器</p><p>64位系统，指处理器能同时处理64位    x64：x86架构的64位扩展</p><h4 id="持久化存储"><a href="#持久化存储" class="headerlink" title="持久化存储"></a>持久化存储</h4><p>内存中的数据一断电就没了，所以要存储到磁盘中</p><h4 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h4><p><a href="https://www.cnblogs.com/maskwolf/p/9972982.html">基本数据类型和引用数据类型</a></p>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VQA</title>
      <link href="vqa/"/>
      <url>vqa/</url>
      
        <content type="html"><![CDATA[<h2 id="Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering"><a href="#Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering" class="headerlink" title="Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"></a>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</h2><p><a href="https://cloud.tencent.com/developer/article/1471855">一篇博客，写的不错</a></p><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>18CVPR 2017 VQA Challenge冠军</p><p>image captioning:图像描述 VQA:视觉问答</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>提出了一种结合自底向上和自顶向下的注意机制，使注意能够在物体和其他显著图像区域的水平上计算。</p><p>自下而上的机制(基于更快的R-CNN)提出图像区域，每个区域都有一个相关的特征向量，而自上而下的机制决定特征权重。</p><p>评价标准：CIDEr   SPICE   BLEU-4</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>可以使用预训练的自底向上的注意力特征而不是CNN特征</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>图像与语言理解相结合-计算机视觉和自然语言处理结合。这需要细粒度的视觉处理，或者甚至需要多个步骤的推理来生成高质量的输出。因此，视觉注意机制在图像字幕和VQA中被广泛采用，其通过学习关注图像中突出的区域来提高性能。</p><p>在人类的视觉系统中，注意力可以由当前任务所决定的自上而下的信号(如寻找某物)自愿地集中起来，也可以由与意想不到的、新奇的或显著的刺激相关的自下而上的信号自动地集中起来。在本文中，我们采用类似的术语，将由非视觉或任务特定背景驱动的注意机制称为“自上而下”，而纯粹的前馈注意机制称为“自下而上”。</p><p>如图1， 自顶向下的注意力机制决定不同特征的贡献度，一般的CNN提取特征都是差不多相同的方格块，而自下向上的注意力机制选择了需要注意的区域，从而有很多大小不一的物体级别的方格。自顶向下得到显著图像区域，每个区域是一组特征向量，由Faster R-CNN实现；自顶向下机制利用特定任务上下文预测图像区域的注意分布。然后计算所关注的特征向量作为图像特征在所有区域的加权平均</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>确定区域的数量和区域的位置很重要。之前显著区域关注的比较少，从概念上讲，优势应该类似于在ImageNet上训练前的视觉表示，并利用显著更大的跨领域知识。</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>自顶向下只使用了最简单的one-pass一遍过模型，而没有使用类似堆叠、多头或者双向等复杂的注意力模型。</p><h4 id="Bottom-Up-Attention-Model"><a href="#Bottom-Up-Attention-Model" class="headerlink" title="Bottom-Up Attention Model"></a>Bottom-Up Attention Model</h4><p>使用 Faster R-CNN，表达区域没有使用之前空间图像特征V的定义，而是使用了bounding box.</p>]]></content>
      
      
      <categories>
          
          <category> VQA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark学习</title>
      <link href="spark-xue-xi/"/>
      <url>spark-xue-xi/</url>
      
        <content type="html"><![CDATA[<p><a href="http://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html">spark scala api</a></p><p><a href="https://www.scala-lang.org/api/current/">scala api</a></p><p><a href="https://www.runoob.com/scala/scala-tutorial.html">菜鸟教程scala</a></p><p><a href="https://my.oschina.net/joymufeng/blog/863823">scala下划线的使用</a></p><p><a href="https://www.cnblogs.com/wjunge/p/10043079.html">scala=&gt;的使用</a></p><p><a href="https://blog.csdn.net/wz_TXWY/article/details/100860862">scala方法调用方式</a> 其中注意无括号调用法，如果没有参数，可以省略括号</p><p><a href="https://blog.csdn.net/u010256841/article/details/53467905">scala中为什么不建议使用return</a>  简而言之就是更符合函数式编程，且不影响类型推断等</p><p><a href="https://blog.csdn.net/xianpanjia4616/article/details/81143146">scala中的:: , +:, :+, :::, +++, 等操作的含义</a></p><p><a href="https://www.jianshu.com/p/6eeaa4511068">讲解了很多scala集合操作，可以来这查找</a></p><p><code>collect()</code>返回一个包含此RDD中所有元素的数组。</p><p><img src="/spark-xue-xi/image-20201212132112158.png" alt="collect函数"></p><pre class=" language-scala"><code class="language-scala"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span><span class="token punctuation">{</span>SparkConf<span class="token punctuation">,</span> SparkContext<span class="token punctuation">}</span><span class="token comment" spellcheck="true">//SparkConf  Spark应用程序的配置。用于将各种Spark参数设置为键值对。</span><span class="token comment" spellcheck="true">//Spark功能的主入口点。SparkContext表示到Spark集群的连接，可用于在该集群上创建RDDs、累加器和广播变量</span><span class="token keyword">val</span> sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"Association Rule Mining Demo"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//new SparkConf()创建了一个从系统属性和类路径加载默认值的SparkConf对象  </span><span class="token comment" spellcheck="true">//setAppName()为你的应用设置一个名字</span><span class="token keyword">val</span> sc <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span><span class="token keyword">val</span> transactions <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span>transactionsFileName<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//textFile(path: String, minPartitions: Int = defaultMinPartitions)</span><span class="token comment" spellcheck="true">//从HDFS、本地文件系统(在所有节点上都可用)或hadoop支持的文件系统URI中读取文本文件，并以字符串的RDD形式返回。</span><span class="token comment" spellcheck="true">//minPartitions:生成的RDD的最小分区数</span><span class="token comment" spellcheck="true">//cache():使用默认的存储级别(MEMORY_ONLY)来支持这个RDD。</span><span class="token keyword">val</span> transactionSize <span class="token operator">=</span> transactions<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//count():返回RDD中的元素数量。</span><span class="token keyword">def</span> toList<span class="token punctuation">(</span>transaction<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span> List<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span> <span class="token keyword">val</span> list <span class="token operator">=</span> transaction<span class="token punctuation">.</span>trim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toList list<span class="token punctuation">}</span><span class="token comment" spellcheck="true">//trim():从指定字符串列的两端修剪指定字符。没带参数就是去掉空格</span><span class="token comment" spellcheck="true">//split():将提供的字符序列围绕','匹配项拆分,返回Array[String]</span><span class="token comment" spellcheck="true">//toList():将Array[String]转换为List[Strings]</span><span class="token keyword">def</span> findSortedCombinations<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">(</span>elements<span class="token operator">:</span> List<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token keyword">implicit</span> B<span class="token operator">:</span> Ordering<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span> <span class="token keyword">val</span> result <span class="token operator">=</span> elements<span class="token punctuation">.</span>sorted<span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token punctuation">.</span>toSet<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">.</span>subsets<span class="token punctuation">.</span>map<span class="token punctuation">(</span>_<span class="token punctuation">.</span>toList<span class="token punctuation">)</span><span class="token punctuation">.</span>toList result<span class="token punctuation">}</span><span class="token comment" spellcheck="true">//sorted(B):根据顺序对这个数组进行排序。是稳定排序</span><span class="token comment" spellcheck="true">//toSet[T]():转换为Set</span><span class="token comment" spellcheck="true">//subsets():遍历该集合所有子集的迭代器。</span><span class="token keyword">def</span> removeOneItem<span class="token punctuation">(</span>list<span class="token operator">:</span> List<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span> List<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>list <span class="token operator">==</span> <span class="token keyword">null</span><span class="token punctuation">)</span> <span class="token operator">||</span> list<span class="token punctuation">.</span>isEmpty<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> list    <span class="token punctuation">}</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">||</span> <span class="token punctuation">(</span>i <span class="token operator">&gt;</span> <span class="token punctuation">(</span>list<span class="token punctuation">.</span>size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> list    <span class="token punctuation">}</span>    <span class="token keyword">val</span> cloned <span class="token operator">=</span> list<span class="token punctuation">.</span>take<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">++</span> list<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>    cloned<span class="token punctuation">}</span><span class="token comment" spellcheck="true">//take(i):选择前i个元素</span><span class="token comment" spellcheck="true">//++：连接两个集合</span><span class="token comment" spellcheck="true">//drpo(i+1):选择除前i+1个元素之外的所有元素。</span><span class="token comment" spellcheck="true">//所以每次去掉了1个元素，如果初始数组传入0，就去掉第一个元素</span><span class="token keyword">val</span> itemsets <span class="token operator">=</span> transactions<span class="token punctuation">.</span>map<span class="token punctuation">(</span>toList<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>findSortedCombinations<span class="token punctuation">(</span>_<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>x <span class="token keyword">=&gt;</span> x<span class="token punctuation">)</span><span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>x <span class="token keyword">=&gt;</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1L</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//map():通过对RDD的所有元素应用一个函数来返回一个新的RDD</span><span class="token comment" spellcheck="true">//flatMap(x =&gt; x):</span><span class="token comment" spellcheck="true">//filter(_.size &gt; 0):过滤，过滤掉size=0的,也就是说去掉空集</span><span class="token comment" spellcheck="true">//</span><span class="token keyword">val</span> minSup <span class="token operator">=</span> <span class="token number">0.4</span><span class="token keyword">val</span> combined <span class="token operator">=</span> itemsets<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>x <span class="token keyword">=&gt;</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>_2<span class="token punctuation">,</span> x<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>toDouble <span class="token operator">/</span> transactionSize<span class="token punctuation">.</span>toDouble<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_2 <span class="token operator">&gt;=</span> minSup<span class="token punctuation">)</span><span class="token punctuation">.</span>cache<span class="token comment" spellcheck="true">//reduceByKey(_ + _):Key值相同的Value值相加</span><span class="token comment" spellcheck="true">//(key,(value1,value2))</span><span class="token comment" spellcheck="true">//value2&gt;=minSup的过滤一下</span><span class="token keyword">val</span> subitemsets <span class="token operator">=</span> combined<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>itemset <span class="token keyword">=&gt;</span> <span class="token punctuation">{</span>    <span class="token keyword">val</span> list <span class="token operator">=</span> itemset<span class="token punctuation">.</span>_1    <span class="token keyword">val</span> frequency <span class="token operator">=</span> itemset<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_1    <span class="token keyword">val</span> support <span class="token operator">=</span> itemset<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_2    <span class="token keyword">var</span> result <span class="token operator">=</span> List<span class="token punctuation">(</span><span class="token punctuation">(</span>list<span class="token punctuation">,</span> <span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>frequency<span class="token punctuation">,</span> support<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>list<span class="token punctuation">.</span>size <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        result    <span class="token punctuation">}</span>     <span class="token keyword">else</span> <span class="token punctuation">{</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span>i <span class="token keyword">&lt;-</span> <span class="token number">0</span> until list<span class="token punctuation">.</span>size<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">val</span> listX <span class="token operator">=</span> removeOneItem<span class="token punctuation">(</span>list<span class="token punctuation">,</span> i<span class="token punctuation">)</span>            <span class="token keyword">val</span> listY <span class="token operator">=</span> list<span class="token punctuation">.</span>diff<span class="token punctuation">(</span>listX<span class="token punctuation">)</span>            result <span class="token operator">++</span><span class="token operator">=</span> List<span class="token punctuation">(</span><span class="token punctuation">(</span>listX<span class="token punctuation">,</span> <span class="token punctuation">(</span>listY<span class="token punctuation">,</span> <span class="token punctuation">(</span>frequency<span class="token punctuation">,</span> support<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">}</span>        result    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cache<span class="token comment" spellcheck="true">//diff(listX):A-A∩B</span><span class="token comment" spellcheck="true">//所以listY就是那个被去除掉的</span><span class="token keyword">val</span> rules <span class="token operator">=</span> subitemsets<span class="token punctuation">.</span>groupByKey<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//groupByKey():和reduceByKey()好像差不多</span><span class="token keyword">val</span> assocRules <span class="token operator">=</span> rules<span class="token punctuation">.</span>map<span class="token punctuation">(</span>in <span class="token keyword">=&gt;</span> <span class="token punctuation">{</span>    <span class="token keyword">val</span> listX <span class="token operator">=</span> in<span class="token punctuation">.</span>_1    <span class="token keyword">val</span> listYLists <span class="token operator">=</span> in<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>toList    <span class="token keyword">val</span> countX <span class="token operator">=</span> listYLists<span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">val</span> newListYLists <span class="token operator">=</span> listYLists<span class="token punctuation">.</span>diff<span class="token punctuation">(</span>List<span class="token punctuation">(</span>countX<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>newListYLists<span class="token punctuation">.</span>isEmpty<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">val</span> result <span class="token operator">=</span> List<span class="token punctuation">(</span><span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">,</span> List<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.0D</span><span class="token punctuation">,</span> <span class="token number">0.0D</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        result    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>        <span class="token keyword">val</span> result <span class="token operator">=</span> newListYLists<span class="token punctuation">.</span>map<span class="token punctuation">(</span>t2 <span class="token keyword">=&gt;</span> <span class="token punctuation">(</span>listX<span class="token punctuation">,</span> t2<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> t2<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_1<span class="token punctuation">.</span>toDouble<span class="token operator">/</span>countX<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_1<span class="token punctuation">.</span>toDouble<span class="token punctuation">,</span>                             t2<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_2<span class="token punctuation">)</span><span class="token punctuation">)</span>        result    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//最小置信度</span><span class="token keyword">val</span> minConf <span class="token operator">=</span> <span class="token number">0.7</span><span class="token comment" spellcheck="true">//必须要大于置信度</span><span class="token keyword">val</span> finalResult <span class="token operator">=</span> assocRules<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>x <span class="token keyword">=&gt;</span> x<span class="token punctuation">)</span><span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_3 <span class="token operator">&gt;=</span> minConf<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//保存txt文件到输出路径</span>finalResult<span class="token punctuation">.</span>saveAsTextFile<span class="token punctuation">(</span>outputPath<span class="token punctuation">)</span>System<span class="token punctuation">.</span>exit<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程软件trick</title>
      <link href="bian-cheng-ruan-jian-trick/"/>
      <url>bian-cheng-ruan-jian-trick/</url>
      
        <content type="html"><![CDATA[<h2 id="jupyter-notebook"><a href="#jupyter-notebook" class="headerlink" title="jupyter notebook"></a>jupyter notebook</h2><p>jupyter notebook使用：<a href="https://zhuanlan.zhihu.com/p/32320214">https://zhuanlan.zhihu.com/p/32320214</a></p><p>在jupyter notebook中，可直接调用<code>arr.dtype</code> <code>arr.shape</code>显示np相应的属性</p><p><code>from tqdm.notebook import tqdm</code></p><p><img src="/bian-cheng-ruan-jian-trick/image-20211105235206404.png" alt="image-20211105235206404"></p><p>tqdm中简单一行的进度条</p><p><strong>整体缩进</strong></p><p>1.选中要缩进的代码。</p><p>2.按Tab键,这样就可以实现整体缩进了。(Shift+Tab是向前缩进)。</p><h4 id="py文件运行"><a href="#py文件运行" class="headerlink" title="py文件运行"></a>py文件运行</h4><p>在jupyter notebook中，运行 %run name.py</p><h3 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h3><p><a href="https://ipython.readthedocs.io/en/stable/">ipython api</a></p><p><code>%</code>和<code>!</code>都用于命令行操作,区别在于:</p><p><code>!</code>开一个新的progress,执行完之后立即终止；provided by the Jupyter<br><code>%</code>的操作会持续；provided by the IPython kernel</p><pre class=" language-python"><code class="language-python"><span class="token operator">%</span>load_ext autoreload<span class="token operator">%</span>autoreload <span class="token number">2</span><span class="token comment" spellcheck="true"># 在执行用户代码之前重新加载模块的 IPython 扩展。</span><span class="token comment" spellcheck="true"># autoreload在输入执行在 IPython 提示符下键入的代码之前自动重新加载模块。</span><span class="token comment" spellcheck="true"># 这使得例如以下工作流程成为可能：</span><span class="token operator">%</span>load_ext autoreload<span class="token operator">%</span>autoreload <span class="token number">2</span><span class="token keyword">from</span> foo <span class="token keyword">import</span> some_functionsome_function<span class="token punctuation">(</span><span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">42</span><span class="token comment" spellcheck="true"># open foo.py in an editor and change some_function to return 43</span>some_function<span class="token punctuation">(</span><span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">43</span><span class="token comment" spellcheck="true"># 模块在没有显式重新加载的情况下被重新加载，并且导入的对象也被更新。</span><span class="token operator">%</span>autoreload <span class="token number">0</span> <span class="token comment" spellcheck="true"># 禁用自动重新加载。</span><span class="token operator">%</span>autoreload <span class="token number">1</span> <span class="token comment" spellcheck="true"># 每次执行键入的 Python 代码之前，重新加载所有使用 %aimport 导入的模块。</span><span class="token operator">%</span>autoreload <span class="token number">2</span> <span class="token comment" spellcheck="true"># 每次执行键入的 Python 代码之前，重新加载所有模块（%aimport排除的模块除外）。</span><span class="token operator">%</span>aimport <span class="token comment" spellcheck="true"># 列出要自动导入或不导入的模块。</span><span class="token operator">%</span>aimport foo <span class="token comment" spellcheck="true"># Import module ‘foo’ and mark it to be autoreloaded for %autoreload 1</span><span class="token operator">%</span>aimport <span class="token operator">-</span>foo <span class="token comment" spellcheck="true"># Mark module ‘foo’ to not be autoreloaded.</span></code></pre><p>使用argparse模块</p><img src="/bian-cheng-ruan-jian-trick/image-20220510222023478.png" alt="image-20220510222023478" style="zoom: 67%;"><p>jupyter notebook训练的话</p><p>可能会把中间结果全保留,需要使用<code>torch.cuda.empty_cache()</code></p><h2 id="pycharm"><a href="#pycharm" class="headerlink" title="pycharm"></a>pycharm</h2><p><a href="https://blog.csdn.net/qq_35473473/article/details/106320708">pycharm配置anaconda环境</a></p><p>注释:Ctrl+/</p><p>取消注释:Ctrl+/</p><p><strong>ctrl+左键就会跳到函数定义处</strong>，还有查找函数在哪里使用的</p><p><strong>鼠标中键Find Usages</strong></p><p>把光标放在要查询的对象上，打开视图菜单，quick definition查看对象的定义，quick documentation 快速文档，这个是jet brains自己对python的解释文档，第三个external documentation 外部文档，这个是python 的官方帮助文档，调转到网页帮助文档中。</p><p>在Run/Debug Configurations中,<strong>Parameters传入参数:</strong></p><p><img src="/bian-cheng-ruan-jian-trick/image-20210405111246850.png" alt="参数"></p><p><strong>Environment variables传入环境变量:</strong></p><p><img src="/bian-cheng-ruan-jian-trick/image-20210405111316647.png" alt="环境变量"></p><p><a href="https://blog.csdn.net/weixin_45670912/article/details/103919401">教育邮箱申请pycharm专业版</a></p><p><a href="https://www.cnblogs.com/shengulong/p/10171386.html">PyCharm中Python代码提示：Shadows name from outer scope</a></p><p>函数内部的变量，如果和函数被调用的外部的变量一样的话，就被PyCharm中叫做shadows name</p><p>这样的话，容易引发不容易觉察到的，由于函数内部和外部的变量名一致而引发的一些问题：</p><p>比如：内部函数名引用时不小心写错了时，就会导致其实调用了外部变量名，从而导致逻辑错乱。</p><p>所以解决办法是：</p><p>确保函数内部和外部的变量名不要重复，这样就不会导致可能由此导致的错误了。</p><p><strong>配置python编译器</strong></p><p><a href="https://www.cnblogs.com/houzp/p/9887870.html">配置编译器</a></p><h2 id="notepad"><a href="#notepad" class="headerlink" title="notepad++"></a>notepad++</h2><p>编辑-&gt;行操作-&gt;删除空行，可以一键去除空行</p><h2 id="VSCode"><a href="#VSCode" class="headerlink" title="VSCode"></a>VSCode</h2><p><strong>远程连接过程试图写入的管道不存在</strong></p><p>打开本地的Known_hosts,将对应的要连接的服务器的记录删掉,再重连就可以了</p><p>ALT+左键,后退;ALT+右键,前进。</p><p>空格和Tab不能混用,否则会报错,红色的波浪下划线</p><p><strong>解决vscode 获取扩展失败，XHR failed</strong></p><p>使用代理的问题,把vpn关掉就行了</p><p><strong>vscode调试</strong></p><p><code>launch.json</code>配置调试环境:</p><pre class=" language-python"><code class="language-python"><span class="token punctuation">{</span>    <span class="token operator">//</span> 使用 IntelliSense 了解相关属性。     <span class="token operator">//</span> 悬停以查看现有属性的描述。    <span class="token operator">//</span> 欲了解更多信息，请访问<span class="token punctuation">:</span> https<span class="token punctuation">:</span><span class="token operator">//</span>go<span class="token punctuation">.</span>microsoft<span class="token punctuation">.</span>com<span class="token operator">/</span>fwlink<span class="token operator">/</span>?linkid<span class="token operator">=</span><span class="token number">830387</span>    <span class="token string">"version"</span><span class="token punctuation">:</span> <span class="token string">"0.2.0"</span><span class="token punctuation">,</span>    <span class="token string">"configurations"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">{</span>            <span class="token string">"name"</span><span class="token punctuation">:</span> <span class="token string">"Python: 当前文件"</span><span class="token punctuation">,</span>            <span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"python"</span><span class="token punctuation">,</span>            <span class="token string">"request"</span><span class="token punctuation">:</span> <span class="token string">"launch"</span><span class="token punctuation">,</span>            <span class="token string">"program"</span><span class="token punctuation">:</span> <span class="token string">"${file}"</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 假设是python tools/run.py,那么program就是指的tools/runpy</span>            <span class="token string">"console"</span><span class="token punctuation">:</span> <span class="token string">"integratedTerminal"</span><span class="token punctuation">,</span>            <span class="token comment" spellcheck="true"># 执行python文件的命令函参数</span>            <span class="token string">"args"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>                <span class="token string">"--num-steps"</span><span class="token punctuation">,</span> <span class="token string">"48320"</span><span class="token punctuation">,</span>                 <span class="token string">"--help-info"</span><span class="token punctuation">,</span> <span class="token string">"替换为dce"</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token comment" spellcheck="true"># 设置当前工作目录为当前目录</span>            <span class="token comment" spellcheck="true">#"cwd":""</span>            <span class="token comment" spellcheck="true"># </span>            <span class="token string">"cwd"</span><span class="token punctuation">:</span> <span class="token string">"${fileDirname}"</span><span class="token comment" spellcheck="true"># 设置当前的工作目录</span>        <span class="token punctuation">}</span>    <span class="token punctuation">]</span><span class="token punctuation">}</span></code></pre><p>在一侧的监视:</p><p><img src="/bian-cheng-ruan-jian-trick/image-20211127150020601.png" alt="image-20211127150020601"></p><p>可以设置要监视的变量</p><p>在vscode中,命令行操作输入部分后<strong>按Tab可以自动补全文件名</strong></p><p>调试ddp分布式代码</p><pre class=" language-python"><code class="language-python"><span class="token punctuation">{</span>    <span class="token operator">//</span> 使用 IntelliSense 了解相关属性。     <span class="token operator">//</span> 悬停以查看现有属性的描述。    <span class="token operator">//</span> 欲了解更多信息，请访问<span class="token punctuation">:</span> https<span class="token punctuation">:</span><span class="token operator">//</span>go<span class="token punctuation">.</span>microsoft<span class="token punctuation">.</span>com<span class="token operator">/</span>fwlink<span class="token operator">/</span>?linkid<span class="token operator">=</span><span class="token number">830387</span>    <span class="token string">"version"</span><span class="token punctuation">:</span> <span class="token string">"0.2.0"</span><span class="token punctuation">,</span>    <span class="token string">"configurations"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">{</span>            <span class="token string">"name"</span><span class="token punctuation">:</span> <span class="token string">"Python: 当前文件"</span><span class="token punctuation">,</span>            <span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"python"</span><span class="token punctuation">,</span>            <span class="token string">"request"</span><span class="token punctuation">:</span> <span class="token string">"launch"</span><span class="token punctuation">,</span>            <span class="token comment" spellcheck="true"># 设置使用torch分布式包中的launch.py文件来作为启动脚本</span>            <span class="token string">"program"</span><span class="token punctuation">:</span> <span class="token string">"/home/dch/anaconda3/lib/python3.8/site-packages/torch/distributed/launch.py"</span><span class="token punctuation">,</span>            <span class="token string">"console"</span><span class="token punctuation">:</span> <span class="token string">"integratedTerminal"</span><span class="token punctuation">,</span>            <span class="token string">"args"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>                <span class="token string">"--nproc_per_node=2"</span><span class="token punctuation">,</span>                 <span class="token string">"/home/dch/ProCA/train_src.py"</span><span class="token punctuation">,</span>                 <span class="token string">"-cfg"</span><span class="token punctuation">,</span> <span class="token string">"configs/gta5/deeplabv2_r101_src.yaml"</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token string">"cwd"</span><span class="token punctuation">:</span> <span class="token string">"${fileDirname}"</span>        <span class="token punctuation">}</span>    <span class="token punctuation">]</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据集标注和评价指标</title>
      <link href="shu-ju-ji-biao-zhu/"/>
      <url>shu-ju-ji-biao-zhu/</url>
      
        <content type="html"><![CDATA[<h3 id="数据集标注"><a href="#数据集标注" class="headerlink" title="数据集标注"></a>数据集标注</h3><p><a href="https://baike.baidu.com/reference/735928/e1c09zpIxso79ckYFsufW97lGFgDuI-Mkxj7Qi6HYi-OVg50jiLd_3hXeWnola-o_uuaI6DtXtCZNK-NMJpSJxM">开放数据标注工具浅析</a></p><p>包括：labelme labelImg VIA 精灵标注  LabelHub</p><p>ps：ps标注可以选择使用磁性套索或者其他工具，标注完之后就涂色就可以了，自己设计一种颜色，比如urban street scene现在多此采用cityscapes的涂色方案。然后在使用的时候，把不同的rgb映射到类别空间，也就是灰度空间就可以了。一般，rgb是为了更好的结果类别的展示，训练的时候使用的是灰度图像。</p><h3 id="labelme标注详解"><a href="#labelme标注详解" class="headerlink" title="labelme标注详解"></a>labelme标注详解</h3><p><a href="https://github.com/wkentaro/labelme">Labelme的github地址</a>     <a href="https://blog.csdn.net/u014061630/article/details/88756644">Labelme教程1</a>      <a href="https://blog.csdn.net/fengxin1995/article/details/80511227">Labelme教程2</a></p><h4 id="1下载安装labelme"><a href="#1下载安装labelme" class="headerlink" title="1下载安装labelme"></a>1下载安装labelme</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">##################</span><span class="token comment" spellcheck="true">## for Python 3 ##</span><span class="token comment" spellcheck="true">##################</span><span class="token comment" spellcheck="true">#使用Anaconda创建虚拟环境</span>conda create <span class="token operator">-</span><span class="token operator">-</span>name<span class="token operator">=</span>labelme python<span class="token operator">=</span><span class="token number">3.6</span>source activate labelmepip install pyqt5  <span class="token comment" spellcheck="true"># pyqt5 can be installed via pip on python3</span>pip install labelme</code></pre><h4 id="2打开labelme"><a href="#2打开labelme" class="headerlink" title="2打开labelme"></a>2打开labelme</h4><p>打开anaconda Prompt</p><img src="/shu-ju-ji-biao-zhu/image-20210116145819793.png" alt="image-20210116145819793" style="zoom:80%;"><h4 id="3标注"><a href="#3标注" class="headerlink" title="3标注"></a>3标注</h4><p>使用open或opendir打开一张图片或一个文件夹</p><p>使用create polygons创建多边形标注即可</p><h4 id="4转换"><a href="#4转换" class="headerlink" title="4转换"></a>4转换</h4><p><code>start labelme_json_to_dataset 1.json</code></p><p>py文件在路径:envs\labelme\Lib\site-packages\labelme\cli</p><p><a href="https://blog.csdn.net/qq_39397024/article/details/93487845">labelme打不开，不显示主界面</a>,之前使用扩展屏幕的原因，憨憨的</p><p>labelme_json_to_dataset.exe转换json生成的文件夹为空，原因是json内存储的文件路径的原因！！！！</p><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p><a href="https://blog.csdn.net/qq_28418387/article/details/95662415">F1、IoU等计算方式理解与代码实现</a>.</p><p><a href="https://blog.csdn.net/u012370185/article/details/94409933">mIoU</a></p><h4 id="PA"><a href="#PA" class="headerlink" title="PA"></a>PA</h4><p>$$<br>PA=\frac{TP+TN}{TP+TN+FP+FN}\<br>PA=\frac{\sum_{i=0}^kp_{ii}}{\sum_{i=0}^k\sum_{j=0}^kp_{ij}}<br>$$</p><p>共有$k+1$个类，其中包含一个空类或背景，$p_{ij}$表示GT为i预测为j</p><p>Mean Pixel Accuracy(MAP,均像素精度):是PA的一种简单提升，<strong>计算每个类内被正确分类像素数的比例，之后求所有类的平均</strong>。<br>$$<br>MPA=\frac{1}{k+1}\sum^k_{i=0}\frac{p_{ii}}{\sum^k_{j=0}p_{ij}}<br>$$</p><h4 id="mIoU"><a href="#mIoU" class="headerlink" title="mIoU"></a>mIoU</h4><p>Why?mIoU的优点</p><p><a href="https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2">https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2</a></p><img src="/shu-ju-ji-biao-zhu/image-20210403154123769.png" alt="直观理解" style="zoom:50%;"><p><strong>在每个类上计算IoU,然后直接取平均</strong></p><p>所以如果某个几个特定类的效果很差的话,会影响很大！<br>$$<br>MIoU=\frac{1}{k+1}\sum^k_{i=0}\frac{TP}{FN+FP+TP}\<br>MIoU=\frac{1}{k+1}\sum^k_{i=0}\frac{p_{ii}}{\sum^k_{j=0}p_{ij}+\sum^k_{j=0}p_{ji}-p_{ii}}<br>$$</p><p>来自AdaptSeg的源码</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> json<span class="token keyword">import</span> argparse<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image<span class="token keyword">from</span> os<span class="token punctuation">.</span>path <span class="token keyword">import</span> join<span class="token keyword">def</span> <span class="token function">fast_hist</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 获得布尔数组,只计算0-18,不计算255</span>    k <span class="token operator">=</span> <span class="token punctuation">(</span>a <span class="token operator">&gt;=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>a <span class="token operator">&lt;</span> n<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 每一行代表一类，该行对角线上的全是正确预测的,其余都是错误预测的</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>bincount<span class="token punctuation">(</span>n <span class="token operator">*</span> a<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>int<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">,</span> minlength<span class="token operator">=</span>n <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>n<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">per_class_iu</span><span class="token punctuation">(</span>hist<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 分子是第i类所有预测正确的,分母是第i类所有预测正确的+将第i类预测为其它类的+将其他类预测为第i类的，就是每一类的IoU</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>hist<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>hist<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> hist<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>hist<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">label_mapping</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> mapping<span class="token punctuation">)</span><span class="token punctuation">:</span>    output <span class="token operator">=</span> np<span class="token punctuation">.</span>copy<span class="token punctuation">(</span>input<span class="token punctuation">)</span>    <span class="token keyword">for</span> ind <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>mapping<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        output<span class="token punctuation">[</span>input <span class="token operator">==</span> mapping<span class="token punctuation">[</span>ind<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> mapping<span class="token punctuation">[</span>ind<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>output<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">compute_mIoU</span><span class="token punctuation">(</span>gt_dir<span class="token punctuation">,</span> pred_dir<span class="token punctuation">,</span> devkit_dir<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Compute IoU given the predicted colorized images and     """</span>    <span class="token comment" spellcheck="true"># 加载json数据</span>    <span class="token keyword">with</span> open<span class="token punctuation">(</span>join<span class="token punctuation">(</span>devkit_dir<span class="token punctuation">,</span> <span class="token string">'info.json'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>        info <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>fp<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 19</span>    num_classes <span class="token operator">=</span> np<span class="token punctuation">.</span>int<span class="token punctuation">(</span>info<span class="token punctuation">[</span><span class="token string">'classes'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Num classes'</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 类别标签</span>    name_classes <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>info<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>str<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 原始标签到训练标签的映射</span>    mapping <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>info<span class="token punctuation">[</span><span class="token string">'label2train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># (19,19)</span>    hist <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 获取gt图片和pred图片路径的列表</span>    image_path_list <span class="token operator">=</span> join<span class="token punctuation">(</span>devkit_dir<span class="token punctuation">,</span> <span class="token string">'zurich_val.txt'</span><span class="token punctuation">)</span>    label_path_list <span class="token operator">=</span> join<span class="token punctuation">(</span>devkit_dir<span class="token punctuation">,</span> <span class="token string">'label_zurich.txt'</span><span class="token punctuation">)</span>    gt_imgs <span class="token operator">=</span> open<span class="token punctuation">(</span>label_path_list<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span>    gt_imgs <span class="token operator">=</span> <span class="token punctuation">[</span>join<span class="token punctuation">(</span>gt_dir<span class="token punctuation">,</span> x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> gt_imgs<span class="token punctuation">]</span>    pred_imgs <span class="token operator">=</span> open<span class="token punctuation">(</span>image_path_list<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span>    pred_imgs <span class="token operator">=</span> <span class="token punctuation">[</span>join<span class="token punctuation">(</span>pred_dir<span class="token punctuation">,</span> x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> pred_imgs<span class="token punctuation">]</span>    <span class="token keyword">for</span> ind <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>gt_imgs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 获取预测图片和标签图片</span>        pred <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>pred_imgs<span class="token punctuation">[</span>ind<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        label <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>gt_imgs<span class="token punctuation">[</span>ind<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 原始标签到训练标签的映射</span>        label <span class="token operator">=</span> label_mapping<span class="token punctuation">(</span>label<span class="token punctuation">,</span> mapping<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 拉成一维比较,确认维数是否一致,原始都是(1080,1920),是'L' mode</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>label<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">!=</span> len<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Skipping: len(gt) = {:d}, len(pred) = {:d}, {:s}, {:s}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>len<span class="token punctuation">(</span>label<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                                                                  len<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                                                                     gt_imgs<span class="token punctuation">[</span>ind<span class="token punctuation">]</span><span class="token punctuation">,</span>                                                                                  pred_imgs<span class="token punctuation">[</span>ind<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">continue</span>        <span class="token comment" spellcheck="true"># 计算IoU</span>        hist <span class="token operator">+=</span> fast_hist<span class="token punctuation">(</span>label<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> pred<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># if ind &gt; 0 and ind % 10 == 0:</span>        <span class="token comment" spellcheck="true">#     print('{:d} / {:d}: {:0.2f}'.format(ind, len(gt_imgs), 100*np.mean(per_class_iu(hist))))</span>    <span class="token comment" spellcheck="true"># 计算mIou</span>    mIoUs <span class="token operator">=</span> per_class_iu<span class="token punctuation">(</span>hist<span class="token punctuation">)</span>    <span class="token keyword">for</span> ind_class <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_classes<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'===&gt;'</span> <span class="token operator">+</span> name_classes<span class="token punctuation">[</span>ind_class<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">':\t'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>round<span class="token punctuation">(</span>mIoUs<span class="token punctuation">[</span>ind_class<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'%'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'===&gt; mIoU: '</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>round<span class="token punctuation">(</span>np<span class="token punctuation">.</span>nanmean<span class="token punctuation">(</span>mIoUs<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'%'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> mIoUs<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>    compute_mIoU<span class="token punctuation">(</span>args<span class="token punctuation">.</span>gt_dir<span class="token punctuation">,</span> args<span class="token punctuation">.</span>pred_dir<span class="token punctuation">,</span> args<span class="token punctuation">.</span>devkit_dir<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># GT路径、预测路径、存放不同数据集文件名索引的文件夹</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--gt_dir'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">'./path/to/Dark_Zurich_val_anon/'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span>                        help<span class="token operator">=</span><span class="token string">'directory which stores CityScapes val gt images'</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--pred_dir'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">'./result/dannet_PSPNet'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span>                        help<span class="token operator">=</span><span class="token string">'directory which stores CityScapes val pred images'</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--devkit_dir'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">'./dataset/lists'</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'base directory of cityscapes'</span><span class="token punctuation">)</span>    args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>    main<span class="token punctuation">(</span>args<span class="token punctuation">)</span></code></pre><h4 id="Dice-coefficient"><a href="#Dice-coefficient" class="headerlink" title="Dice coefficient"></a>Dice coefficient</h4><p>和mIou的区别<a href="https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou/276144#276144">https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou/276144#276144</a></p><p><a href="https://blog.csdn.net/JMU_Ma/article/details/97533768">https://blog.csdn.net/JMU_Ma/article/details/97533768</a></p><p><a href="https://zhuanlan.zhihu.com/p/269592183">https://zhuanlan.zhihu.com/p/269592183</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">dice_loss</span><span class="token punctuation">(</span>target，predictive，ep<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    intersection <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>predictive <span class="token operator">*</span> target<span class="token punctuation">)</span> <span class="token operator">+</span> ep    union <span class="token operator">=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>predictive<span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>target<span class="token punctuation">)</span> <span class="token operator">+</span> ep    loss <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> intersection <span class="token operator">/</span> union    <span class="token keyword">return</span> loss</code></pre><h4 id="Hausdorff-distance"><a href="#Hausdorff-distance" class="headerlink" title="Hausdorff distance"></a>Hausdorff distance</h4><p><a href="https://blog.csdn.net/lijiaqi0612/article/details/113925215/">https://blog.csdn.net/lijiaqi0612/article/details/113925215\</a><br>$$<br>H(A,B)=max(h(A,B),h(B,A))<br>$$<br>可以直接用scipy的函数计算</p><p><code>scipy.spatial.distance.directed_hausdorff</code></p><h3 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h3><h4 id="PASCAL-VOC2010"><a href="#PASCAL-VOC2010" class="headerlink" title="PASCAL VOC2010"></a>PASCAL VOC2010</h4><p><strong>Pascal Visual Object Classes,简称Pascal VOC数据集</strong></p><p><img src="/shu-ju-ji-biao-zhu/image-20210426201733971.png" alt="VOC 2010"></p><p><strong>JPEGImages</strong>是原图,<strong>SegmentationClass</strong>是标准出每一个像素的类别,也就是<strong>语义分割</strong>的label,<strong>SegmentationObject</strong>是标注出每一个像素属于哪一个物体,也就是<strong>实例分割</strong>的label,<strong>Annotations</strong>是<strong>目标检测</strong>的标注</p><p><img src="/shu-ju-ji-biao-zhu/image-20210426204731505.png" alt="image-20210426204731505"></p><p>ImageSets是划分,跟我们相关的主要是Segmentation，</p><p><img src="/shu-ju-ji-biao-zhu/image-20210426204828552.png" alt="image-20210426204828552"></p><p>里面是train,val和trainval，<strong>trainval表示train和val合在一起</strong></p><h4 id="Pascal-Context"><a href="#Pascal-Context" class="headerlink" title="Pascal Context"></a>Pascal Context</h4><p><a href="https://blog.csdn.net/qq_28869927/article/details/93379892">Pascal数据集介绍</a></p><p>PASCAL Context数据集[1]由两部分组成：</p><ol><li>PASCAL VOC 2010 语义分割数据集；</li><li>Context 标注。</li></ol><p>总共有459个类别,包含 10103 张图像，<strong>其中 4998 用于训练集，5105 用于验证集。</strong></p><p>现在最广泛地用法是使用其中<strong>出现频率最高的 59 个类别最为语义标签，其余类别标记为背景即background。</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#59个类别的数字索引</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">259</span><span class="token punctuation">,</span> <span class="token number">260</span><span class="token punctuation">,</span> <span class="token number">415</span><span class="token punctuation">,</span> <span class="token number">324</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">258</span><span class="token punctuation">,</span> <span class="token number">144</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">397</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">284</span><span class="token punctuation">,</span> <span class="token number">158</span><span class="token punctuation">,</span> <span class="token number">159</span><span class="token punctuation">,</span> <span class="token number">416</span><span class="token punctuation">,</span> <span class="token number">33</span><span class="token punctuation">,</span> <span class="token number">162</span><span class="token punctuation">,</span> <span class="token number">420</span><span class="token punctuation">,</span> <span class="token number">454</span><span class="token punctuation">,</span> <span class="token number">295</span><span class="token punctuation">,</span> <span class="token number">296</span><span class="token punctuation">,</span> <span class="token number">427</span><span class="token punctuation">,</span> <span class="token number">44</span><span class="token punctuation">,</span> <span class="token number">45</span><span class="token punctuation">,</span> <span class="token number">46</span><span class="token punctuation">,</span> <span class="token number">308</span><span class="token punctuation">,</span> <span class="token number">59</span><span class="token punctuation">,</span> <span class="token number">440</span><span class="token punctuation">,</span> <span class="token number">445</span><span class="token punctuation">,</span> <span class="token number">31</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">65</span><span class="token punctuation">,</span> <span class="token number">354</span><span class="token punctuation">,</span> <span class="token number">424</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span> <span class="token number">326</span><span class="token punctuation">,</span> <span class="token number">72</span><span class="token punctuation">,</span> <span class="token number">458</span><span class="token punctuation">,</span> <span class="token number">34</span><span class="token punctuation">,</span> <span class="token number">207</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">355</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">347</span><span class="token punctuation">,</span> <span class="token number">220</span><span class="token punctuation">,</span> <span class="token number">349</span><span class="token punctuation">,</span> <span class="token number">360</span><span class="token punctuation">,</span> <span class="token number">98</span><span class="token punctuation">,</span> <span class="token number">187</span><span class="token punctuation">,</span> <span class="token number">104</span><span class="token punctuation">,</span> <span class="token number">105</span><span class="token punctuation">,</span> <span class="token number">366</span><span class="token punctuation">,</span> <span class="token number">189</span><span class="token punctuation">,</span> <span class="token number">368</span><span class="token punctuation">,</span> <span class="token number">113</span><span class="token punctuation">,</span> <span class="token number">115</span><span class="token punctuation">]</span></code></pre><p><strong>PASCAL in Detail API</strong></p><p>到此，原始图片和标注数据都已获取完毕，那么如何实现上述的 4998+5105 的训练/验证划分呢？</p><p>这里需要使用CVPR’17 <a href="https://sites.google.com/view/pasd/dataset">PASCAL in Detail Challenge</a>中实现的Detail接口。其github地址为：<a href="https://github.com/ccvl/detail-api">https://github.com/ccvl/detail-api</a></p><p>具体使用方法可以参考其提供的文档，这里不再赘述。</p><h4 id="PASCAL-VOC2012-Aug"><a href="#PASCAL-VOC2012-Aug" class="headerlink" title="PASCAL VOC2012 Aug"></a>PASCAL VOC2012 Aug</h4><h4 id="ADE-20K"><a href="#ADE-20K" class="headerlink" title="ADE 20K"></a>ADE 20K</h4><h4 id="CityScape"><a href="#CityScape" class="headerlink" title="CityScape"></a>CityScape</h4><p><a href="https://blog.csdn.net/zz2230633069/article/details/84591532">cityscape数据集介绍</a><br>List of cityscapes labels:</p><pre><code>                 name |  id | trainId |       category | categoryId | hasInstances | ignoreInEval|        color--------------------------------------------------------------------------------------------------            unlabeled |   0 |     255 |           void |          0 |            0 |            1 |         (0, 0, 0)          ego vehicle |   1 |     255 |           void |          0 |            0 |            1 |         (0, 0, 0) rectification border |   2 |     255 |           void |          0 |            0 |            1 |         (0, 0, 0)           out of roi |   3 |     255 |           void |          0 |            0 |            1 |         (0, 0, 0)               static |   4 |     255 |           void |          0 |            0 |            1 |         (0, 0, 0)              dynamic |   5 |     255 |           void |          0 |            0 |            1 |      (111, 74, 0)               ground |   6 |     255 |           void |          0 |            0 |            1 |       (81, 0, 81)                 road |   7 |       0 |           flat |          1 |            0 |            0 |    (128, 64, 128)             sidewalk |   8 |       1 |           flat |          1 |            0 |            0 |    (244, 35, 232)              parking |   9 |     255 |           flat |          1 |            0 |            1 |   (250, 170, 160)           rail track |  10 |     255 |           flat |          1 |            0 |            1 |   (230, 150, 140)             building |  11 |       2 |   construction |          2 |            0 |            0 |      (70, 70, 70)                 wall |  12 |       3 |   construction |          2 |            0 |            0 |   (102, 102, 156)                fence |  13 |       4 |   construction |          2 |            0 |            0 |   (190, 153, 153)           guard rail |  14 |     255 |   construction |          2 |            0 |            1 |   (180, 165, 180)               bridge |  15 |     255 |   construction |          2 |            0 |            1 |   (150, 100, 100)               tunnel |  16 |     255 |   construction |          2 |            0 |            1 |    (150, 120, 90)                 pole |  17 |       5 |         object |          3 |            0 |            0 |   (153, 153, 153)            polegroup |  18 |     255 |         object |          3 |            0 |            1 |   (153, 153, 153)        traffic light |  19 |       6 |         object |          3 |            0 |            0 |    (250, 170, 30)         traffic sign |  20 |       7 |         object |          3 |            0 |            0 |     (220, 220, 0)           vegetation |  21 |       8 |         nature |          4 |            0 |            0 |    (107, 142, 35)              terrain |  22 |       9 |         nature |          4 |            0 |            0 |   (152, 251, 152)                  sky |  23 |      10 |            sky |          5 |            0 |            0 |    (70, 130, 180)               person |  24 |      11 |          human |          6 |            1 |            0 |     (220, 20, 60)                rider |  25 |      12 |          human |          6 |            1 |            0 |       (255, 0, 0)                  car |  26 |      13 |        vehicle |          7 |            1 |            0 |       (0, 0, 142)                truck |  27 |      14 |        vehicle |          7 |            1 |            0 |        (0, 0, 70)                  bus |  28 |      15 |        vehicle |          7 |            1 |            0 |      (0, 60, 100)              caravan |  29 |     255 |        vehicle |          7 |            1 |            1 |        (0, 0, 90)              trailer |  30 |     255 |        vehicle |          7 |            1 |            1 |       (0, 0, 110)                train |  31 |      16 |        vehicle |          7 |            1 |            0 |      (0, 80, 100)           motorcycle |  32 |      17 |        vehicle |          7 |            1 |            0 |       (0, 0, 230)              bicycle |  33 |      18 |        vehicle |          7 |            1 |            0 |     (119, 11, 32)        license plate |  -1 |      -1 |        vehicle |          7 |            0 |            1 |       (0, 0, 142)</code></pre><p>Example usages:<br>ID of label ‘car’: 26<br>Category of label with ID ‘26’: vehicle<br>Name of label with trainID ‘0’: road</p><p>这张图片很重要,有些虽然有类别id,但是训练的时候trainId是255,不考虑,真正开始考虑的是road</p><h4 id="COCO-2017"><a href="#COCO-2017" class="headerlink" title="COCO 2017"></a>COCO 2017</h4><p><a href="https://blog.csdn.net/Tiao_12/article/details/120270913">COCO数据集的介绍和使用</a></p><img src="/shu-ju-ji-biao-zhu/image-20211105170711982.png" alt="image-20211105170711982" style="zoom:80%;"><p><strong>pycocotools</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> pycocotools<span class="token punctuation">.</span>coco <span class="token keyword">import</span> COCO<span class="token keyword">print</span><span class="token punctuation">(</span>dir<span class="token punctuation">(</span>COCO<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 下面是一些主要的方法</span><span class="token string">'annToMask'</span><span class="token punctuation">,</span> <span class="token string">'annToRLE'</span><span class="token punctuation">,</span> <span class="token string">'createIndex'</span><span class="token punctuation">,</span> <span class="token string">'download'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 获取满足给定过滤条件的annid。</span>COCO<span class="token punctuation">.</span>getAnnIds<span class="token punctuation">(</span>self<span class="token punctuation">,</span> imgIds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># get anns for given imgs</span>               catIds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># get anns for given cats</span>               areaRng<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># get anns for given area range (e.g. [0 inf])</span>               iscrowd<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># get anns for given crowd label (False or True)</span><span class="token string">'getCatIds'</span><span class="token punctuation">,</span> <span class="token string">'getImgIds'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># Print information about the annotation file.</span>COCO<span class="token punctuation">.</span>info<span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用指定的id加载anns。</span>COCO<span class="token punctuation">.</span>loadAnns<span class="token punctuation">(</span>self<span class="token punctuation">,</span> ids<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token string">'loadCats'</span><span class="token punctuation">,</span> <span class="token string">'loadImgs'</span><span class="token punctuation">,</span> <span class="token string">'loadNumpyAnnotations'</span><span class="token punctuation">,</span> <span class="token string">'loadRes'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 显示指定的annotations。</span>COCO<span class="token punctuation">.</span>showAnns<span class="token punctuation">(</span>self<span class="token punctuation">,</span> anns<span class="token punctuation">)</span></code></pre><p><img src="/shu-ju-ji-biao-zhu/image-20211106103213053.png" alt="pycocotools中的三个py文件"></p><h4 id="Run-Length-Code"><a href="#Run-Length-Code" class="headerlink" title="Run Length Code"></a>Run Length Code</h4><p><a href="http://blog.sina.com.cn/s/blog_3ef2364d0102wp0v.html">RLE</a>(Run- Length Encoding 行程长度编码)压缩算法是Windows 系统中使用的一种图像文件压缩方法, 其基本思想是: 将一扫描行中颜色值相同的相邻像素用两个字节来表示, 第一个字节是一个计数值, 用于指定像素重复的次数; 第二个字节是具体像素的值[2]。主要通过压缩除掉数据中的冗余字节或字节中的冗余位,从而达到减少文件所占空间的目的。例如, 有一表示颜色像素值的字符串RRRRRGGBBBBBB,用RLE压缩方法压缩后可用 5R2G6B 来代替,显然后者的串长度比前者的串长度小得多。译码时按照与编码时采用的相同规则进行, 还原后得到的数据与压缩前的数据完全相同。因此,RLE是无损压缩技术。</p><p><img src="/shu-ju-ji-biao-zhu/image-20211105230029786.png" alt="image-20211105230029786"></p><p>竞赛中的run length code是<code>(start,length)</code>形式,前面的是图片id,每个id有好几条数据,每个数据的标签是<code>cell_type</code>,经过我自己的一番操作发现好像每张图片只有一种细胞类型!</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">rle2mask</span><span class="token punctuation">(</span>rle<span class="token punctuation">,</span> img_w<span class="token punctuation">,</span> img_h<span class="token punctuation">)</span><span class="token punctuation">:</span>    array <span class="token operator">=</span> np<span class="token punctuation">.</span>fromiter<span class="token punctuation">(</span>rle<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> np<span class="token punctuation">.</span>uint<span class="token punctuation">)</span>    array <span class="token operator">=</span> array<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T    array<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> array<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token comment" spellcheck="true"># 每个点的位置都减1,可能是因为mask数组是从0开始</span>    starts<span class="token punctuation">,</span> lenghts <span class="token operator">=</span> array    mask_decompressed <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>s<span class="token punctuation">,</span> s <span class="token operator">+</span> l<span class="token punctuation">,</span> dtype <span class="token operator">=</span> np<span class="token punctuation">.</span>uint<span class="token punctuation">)</span> <span class="token keyword">for</span> s<span class="token punctuation">,</span> l <span class="token keyword">in</span> zip<span class="token punctuation">(</span>starts<span class="token punctuation">,</span> lenghts<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 获得在一维数组的mask坐标</span>    msk_img <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>img_w <span class="token operator">*</span> img_h<span class="token punctuation">,</span> dtype <span class="token operator">=</span> np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>    msk_img<span class="token punctuation">[</span>mask_decompressed<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment" spellcheck="true"># 所有的mask位置设成1,background设成0</span>    msk_img <span class="token operator">=</span> msk_img<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># resize成图片大小</span>    msk_img <span class="token operator">=</span> np<span class="token punctuation">.</span>asfortranarray<span class="token punctuation">(</span>msk_img<span class="token punctuation">)</span>    <span class="token keyword">return</span> msk_img</code></pre><h3 id="SDD"><a href="#SDD" class="headerlink" title="SDD"></a>SDD</h3><h4 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h4><p>15年一个皮肤分割数据集&lt;&lt;A Skin Detection Dataset for Training and Assessment of Human Skin Classifiers&gt;&gt;</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>超过20000张图像  非常精确：<strong>专业图形工具  三元划分</strong>  与SFA比较 <strong>定性且定量</strong>表明更好</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p><strong>数据库组织良好</strong>：因此使用时只和方法有关，而和数据集无关</p><p><strong>不同光照和成像条件下捕获</strong></p><p>不像许多其他数据集与半自动GT标签，在SDD，<strong>地面真相注释非常精确，感谢专业图形工具的使用和三元划分的想法。</strong></p><p>使用后者，处理皮肤和非皮肤区域边界上的繁琐点会更加方便和容易。</p><p>对于该数据集的评价，由于单直方图方法的沉默特性，无论是定性还是定量都选择了单直方图方法，结果表明使用SDD方法比SFA更有效。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><h4 id="Skin-Classification"><a href="#Skin-Classification" class="headerlink" title="Skin Classification"></a>Skin Classification</h4><p>例如，有些数据库认为人类嘴唇是皮肤的一部分，而有些数据库则忽略了它。</p><p>目前数据库中的地面真值图像往往太不准确(本文将对此进行说明)。</p><p>事实上，数据集有时是手工注释的，但是，由于这项任务既无聊又费时，它可能被漫不经心地完成。</p><p>在其他数据库中，GT图像使用半自动程序生成，以减少注释时间。然而，ground truth图像还不够精确，甚至可能比人工标记的图像更差。</p><p>在一些数据库中，皮肤像素被标记为非皮肤像素，但它们甚至不靠近皮肤区域，在某些情况下，非皮肤像素被标记为皮肤像素，这在一定程度上降低了性能。</p><p>在一些数据库中，皮肤像素被标记为非皮肤像素，在某些情况下，非皮肤像素被标记为皮肤像素，这在一定程度上降低了性能。</p><p>此外，训练集和测试集的数量和多样性对分类器性能的影响也很明显。然而，在一些工作中，这一因素往往被忽略。有很多作品在报告统计结果时没有考虑到照片集的大小和多样性的影响。一些数据集是在特定的成像和照明条件下编译的。这在评估和培训步骤中都可能出现问题。在评估中，如果数据集不够一般化，算法的结果要么太好，要么太差，都是不现实的。在训练中，使用非通用数据库仅根据特定类型的皮肤像素对系统进行优化，会严重影响算法的性能。有些数据库太小，无法用于各种方法的培训。例如，考虑Jones et al.[17]开发的Bayesian方法，在一种情况下，无论皮肤直方图还是非皮肤直方图，LUT (look-up table)的大小都达到了2×2563个细胞。要对该算法进行适当的训练，需要一个比可能的单元格整个大小大几倍的数据集。在某些情况下，即使数据库相对较大，其大多数映像也会重复多次。此外，一些数据集既不能用于公共用途，也不能免费用于非商业学术用途。基于上述问题，本文的目标是编制一个半理想的应用数据集，可用于训练和评估皮肤检测方法</p><h4 id="Previous-Databases"><a href="#Previous-Databases" class="headerlink" title="Previous Databases"></a>Previous Databases</h4><p>一些皮肤检测数据集最初是为人脸检测、手跟踪和人脸识别问题而开发的。最重要的皮肤数据库组是那些专门为皮肤分类器的训练和评估而设计的，而不是其他生物识别应用。</p><p>康柏是一个相对较老的数据库，其图像质量太低，不再值得信赖。新一代的成像设备已经被开发出来并用于商业用途，它们与以前的设备有很大的不同。另一个问题是康柏没有明确的划分，即不同的作者在测试和评估过程中可能使用不同的图像，这将影响性能。半监督标注，非常不准确</p><p>ECU图像保证了背景场景、光照条件和皮肤类型的多样性。照明条件包括室内照明和室外照明;皮肤类型包括白色，棕色，黄色和黑色皮肤。一些作品已经用这个数据库进行了评估。虽然图像的多样性足以用于评估一般的皮肤检测系统，但数据集的大小还不够大。此外，处理注释还存在一个特殊的问题，即所有图像中，特别是那些质量较低或拥挤的图像中的某些区域，注释并不是一项简单的任务。例如，图3中红色箭头表示的点是毛发周围的像素点，或者其他物体往往会带来额外的困难。这实际上是GTs手工标记方案的错误来源</p><p>基于神经网络皮肤分类器的最佳拓扑和阈值，将UCI和SFA进行了比较，发现在评估皮肤检测器[35]时，SFA的准确性略高于UCI。然而，SFA主要包括半护照图像，不适合用于评估目的。另外，SFA中的GT注释也不一致，即有很多图像没有进行准确的标注。此外，在一些图像中，人脸被2-3像素的厚白色边界包围。这将导致不同颜色空间皮肤簇的构建不准确。图4绘制了GTs对应的SFA图像;它们代表了所有的图像。这个数据集是公开的。</p><h4 id="SDD-1"><a href="#SDD-1" class="headerlink" title="SDD"></a>SDD</h4><p>据作者所知，它是文献中引入的用于训练和评估皮肤分类器性能的最大数据库。图像是在不同的光照条件下拍摄的，使用不同的成像设备，从世界各地不同肤色的人。一些图像是在线视频和电影的快照，而一些是从流行的人脸识别/跟踪/检测数据集中获取的静态图像[40-43]。SDD中图像的高度多样性使得它可以用于一般系统的训练和评估，即当一种方法被SDD评估时，结果是可靠的，而当一种方法被SDD训练时，它可以达到其最大的潜在性能。</p><p>如前所述,所有图片中有一些像素要么skinness的有一个问题(比如眉毛和眼睛周围地区,嘴唇和鼻子孔,等等)或者是位于皮肤和non-skin的边界,在这两种情况下,很难区分skinness和一些图片,要花很多的时间来注释。将GT图像划分为3个不重叠区域;皮肤像素、非皮肤像素以及在评估和训练步骤中都被考虑的像素(不关心点)</p><p>用的Photoshop CS5</p><p>$$<br>F1=\frac{2\cdot Recall\cdot Precision}{Recall+Precision}<br>$$</p><p>$$<br>IoU=\frac{tp}{tp+fn+fp}<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 肤色分级 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 肤色分级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow学习</title>
      <link href="tensorflow-xue-xi/"/>
      <url>tensorflow-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="Tensorflow1-x"><a href="#Tensorflow1-x" class="headerlink" title="Tensorflow1.x"></a>Tensorflow1.x</h2><p><a href="https://tensorflow.google.cn/install/source_windows#cpu">与python的版本对应关系</a></p><h3 id="tf-keras"><a href="#tf-keras" class="headerlink" title="tf.keras"></a>tf.keras</h3><h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p><code>load_weights</code></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 从TensorFlow或HDF5文件加载所有层权重。</span>load_weights<span class="token punctuation">(</span>    filepath<span class="token punctuation">,</span> by_name<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p><code>summary</code></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 打印网络的字符串摘要</span>summary<span class="token punctuation">(</span>    line_length<span class="token operator">=</span>None<span class="token punctuation">,</span> positions<span class="token operator">=</span>None<span class="token punctuation">,</span> print_fn<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p><code>fit_generator</code></p><pre class=" language-python"><code class="language-python">fit_generator<span class="token punctuation">(</span>    generator<span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 生成器函数,输出应该是形为（inputs,target）或者（inputs,targets,sample_weight）的元组,</span>    <span class="token comment" spellcheck="true"># 生成器会在数据集上无限循环</span>    steps_per_epoch<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 在宣布一个epoch完成并开始下一个epoch之前从生成器产生的总步数（样本批次）。</span>    <span class="token comment" spellcheck="true">#它通常应等于数据集的样本数除以batch_size。</span>    epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># </span>    verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    callbacks<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># List of callbacks to be called during training.callbacks类的函数</span>    validation_data<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 验证集数据</span>    validation_steps<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># </span>    validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 多少个epoch验证一次</span>    class_weight<span class="token operator">=</span>None<span class="token punctuation">,</span> max_queue_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> workers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> use_multiprocessing<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 没多少个epoch运行一次验证</span>    initial_epoch<span class="token operator">=</span><span class="token number">0</span><span class="token comment" spellcheck="true"># 开始训练的时期（用于恢复之前的训练）</span><span class="token punctuation">)</span></code></pre><h3 id="tf-keras-callbacks"><a href="#tf-keras-callbacks" class="headerlink" title="tf.keras.callbacks"></a>tf.keras.callbacks</h3><p>Callbacks: utilities called at certain points during model training.在训练模型的过程中可能要做一些事。</p><p><code>ModelCheckpoint</code></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Save the model after every epoch.</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>ModelCheckpoint<span class="token punctuation">(</span>    filepath<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 需要监视的值,通常为:val_acc或val_loss或acc或 loss</span>    verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 信息展示模式，0或1。为1表示输出epoch模型保存信息，默认为0表示不输出该信息</span>    <span class="token comment" spellcheck="true"># 信息形如：Epoch 00001: val_acc improved from -inf to 0.49240, saving model to</span>    <span class="token comment" spellcheck="true"># /xxx/checkpoint/model_001-0.3902.h5</span>    save_best_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 当设置为True时，将只保存在验证集上性能最好的模型</span>    save_weights_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#如果为True,只保存weights,否则保存整个模型</span>    mode<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 在save_best_only=True时决定性能最佳模型的评判准则</span>    <span class="token comment" spellcheck="true"># 例如,当监测值为val_acc时,模式应为max,当检测值为val_loss时,模式应为min。</span>    <span class="token comment" spellcheck="true"># 在auto模式下，评价准则由被监测值的名字自动推断。</span>    save_freq<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span>    <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre><h2 id="Tensorflow2-x"><a href="#Tensorflow2-x" class="headerlink" title="Tensorflow2.x"></a>Tensorflow2.x</h2><p><a href="https://www.tensorflow.org/api_docs/python/tf?hl=hr">tensorflowAPI</a></p><p><a href="https://keras-cn.readthedocs.io/en/latest/other/callbacks/">keras教程</a></p><p><a href="https://github.com/aymericdamien/TensorFlow-Examples">https://github.com/aymericdamien/TensorFlow-Examples</a></p><p><a href="https://github.com/instillai/TensorFlow-Course">https://github.com/instillai/TensorFlow-Course</a></p><p><a href="https://github.com/sjchoi86/Tensorflow-101">https://github.com/sjchoi86/Tensorflow-101</a></p><p><a href="https://github.com/terryum/TensorFlow_Exercises">https://github.com/terryum/TensorFlow_Exercises</a></p><p>keras是对tensorflow的再封装，是tensorflow的高级api</p><h3 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h3><p>*表示点乘   矩阵乘tf.matmul</p><p><a href="https://www.jianshu.com/p/30b40b504bae">tf.reduce_sum</a>这个博客太强了，把axis彻彻底底讲明白了   </p><p>本来维度是(2,3) axis=0 完事就变成了(3)   如果keepdims=True，就变成了(1,3)  </p><p>如果axis是一个元组(1,2,3)，就是先加维度1，加完加2，然后加3，如果没有传axis，默认所有的加起来</p><h4 id="基本函数"><a href="#基本函数" class="headerlink" title="基本函数"></a>基本函数</h4><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dtype<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#张量数据类型转换</span><span class="token comment" spellcheck="true">#x:要转换的数据</span><span class="token comment" spellcheck="true">#dtype:目标数据类型</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>input<span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">,</span>out_type<span class="token operator">=</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将矩阵的维度输出为一个维度矩阵</span><span class="token comment" spellcheck="true">#input:张量</span><span class="token comment" spellcheck="true">#name:op的名字，用于tensorboard中</span><span class="token comment" spellcheck="true">#out_type:输出类型</span>tf<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> paddings<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'CONSTANT'</span><span class="token punctuation">,</span> constant_values<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对张量在各个维度进行填充</span><span class="token comment" spellcheck="true">#tensor:待填充的张量</span><span class="token comment" spellcheck="true">#paddings:对哪个维度进行填充，上下左右填充多少,例如[[1,2],[3,4]],上填充一行，下2行，左3列，右4列</span><span class="token comment" spellcheck="true">#mode 填充方式,有’CONSTANT’'REFLECT''SYMMETRIC'</span><span class="token comment" spellcheck="true">#constant_values:constant填充值</span><span class="token comment" spellcheck="true">#name：操作的名字</span>dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>function<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对dataset中的每一个tf.Tensor执行该函数操作</span>tf<span class="token punctuation">.</span>fill<span class="token punctuation">(</span>dims<span class="token punctuation">,</span>value<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#创建一个充满标量值的张量。</span>tf<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#把所有是1的维度都去除，捏一下，挤压一下，把水分挤出来</span></code></pre><h4 id="维度"><a href="#维度" class="headerlink" title="维度"></a>维度</h4><p><strong>tensorflow的维度情况是NHWC</strong></p><h4 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h4><p>tf的API是不同的<code>Module</code>嵌套,<code>Module</code>里包括<code>Class</code>和<code>Function</code>,<code>Class</code>包括<code>Arguments</code>参数介绍,<code>Attributes</code>属性,<code>Methods</code>方法</p><h4 id="tf-data-Dataset"><a href="#tf-data-Dataset" class="headerlink" title="tf.data.Dataset"></a>tf.data.Dataset</h4><ol><li>从输入数据创建源数据集。</li><li>应用数据集转换对数据进行预处理。</li><li>遍历数据集并处理元素。</li></ol><p><strong>Source Datasets:</strong></p><p>从列表：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> element <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>  <span class="token keyword">print</span><span class="token punctuation">(</span>element<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tf.Tensor(1, shape=(), dtype=int32)</span><span class="token comment" spellcheck="true">#tf.Tensor(2, shape=(), dtype=int32)</span><span class="token comment" spellcheck="true">#tf.Tensor(3, shape=(), dtype=int32)</span></code></pre><p><strong>as_numpy_iterator</strong></p><p>返回一个迭代器，它将数据集的所有元素转换为numpy。</p><p>使用as_numpy_iterator检查数据集的内容。要查看元素的形状和类型，请直接打印数据集元素，而不是使用as_numpy_iterator</p><p><strong>batch</strong></p><pre class=" language-python"><code class="language-python">batch<span class="token punctuation">(</span>    batch_size<span class="token punctuation">,</span> drop_remainder<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p>将连续数据组合为batch，drop_remainder表示如果最后的数据不够一批，是否删除</p><p>组合完的数据会多一个额外的维度batch_size,如下：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>range<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">for</span> element <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>element<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tf.Tensor([0 1 2], shape=(3,), dtype=int64)</span><span class="token comment" spellcheck="true">#tf.Tensor([3 4 5], shape=(3,), dtype=int64)</span><span class="token comment" spellcheck="true">#tf.Tensor([6 7], shape=(2,), dtype=int64)</span></code></pre><p><strong>padded_batch</strong></p><pre class=" language-python"><code class="language-python">padded_batch<span class="token punctuation">(</span>    batch_size<span class="token punctuation">,</span> padded_shapes<span class="token operator">=</span>None<span class="token punctuation">,</span> padding_values<span class="token operator">=</span>None<span class="token punctuation">,</span> drop_remainder<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p>也是生成batch的，不同的是如果有变长如何填充：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Components of nested elements can be padded independently.</span>elements <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_generator<span class="token punctuation">(</span>    <span class="token keyword">lambda</span><span class="token punctuation">:</span> iter<span class="token punctuation">(</span>elements<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Pad the first component of the tuple to length 4, and the second</span><span class="token comment" spellcheck="true"># component to the smallest size that fits.</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>padded_batch<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>    padded_shapes<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    padding_values<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>list<span class="token punctuation">(</span>dataset<span class="token punctuation">.</span>as_numpy_iterator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#[(array([[ 1,  2,  3, -1],</span><span class="token comment" spellcheck="true">#         [ 4,  5, -1, -1]], dtype=int32),</span><span class="token comment" spellcheck="true">#  array([[ 10, 100],</span><span class="token comment" spellcheck="true">#         [ 11,  12]], dtype=int32))]</span></code></pre><p><strong>map</strong></p><p>Maps <code>map_func</code> across the elements of this dataset.</p><p><strong>prefetch（buffer_size）</strong></p><p>这允许在处理当前元素时准备后面的元素。这通常会提高延迟和吞吐量，但代价是使用额外的内存来存储预先获取的元素。</p><p><strong>cache</strong></p><p><strong>shuffle</strong> </p><p>随机打乱此数据集的元素。</p><p>这个数据集用buffer_size元素填充一个缓冲区，然后从这个缓冲区随机抽取元素，用新元素替换选中的元素。为了实现完美的洗牌，需要一个大于或等于数据集的完整大小的缓冲区。</p><p>例如，如果数据集包含10,000个元素，但buffer_size被设置为1,000，那么shuffle最初将从缓冲区中的前1,000个元素中随机选择一个元素。一旦一个元素被选中，它在缓冲区中的空间将被下一个元素(即1001 -st)替换，保持缓冲区的1,000个元素。</p><p><strong>prefetch</strong></p><h4 id="流水线"><a href="#流水线" class="headerlink" title="流水线"></a>流水线</h4><p><a href="https://blog.csdn.net/jackhh1/article/details/102763999">https://blog.csdn.net/jackhh1/article/details/102763999</a></p><p>加速器执行训练步骤N时，CPU正在为步骤N+1准备数据，这样可以大大减少CPU和GPU的空闲时间</p><h4 id="回调函数"><a href="#回调函数" class="headerlink" title="回调函数"></a>回调函数</h4><p><a href="https://keras-cn.readthedocs.io/en/latest/other/callbacks/">https://keras-cn.readthedocs.io/en/latest/other/callbacks/</a></p><h3 id="术语表"><a href="#术语表" class="headerlink" title="术语表"></a>术语表</h3><p>对数(logits)</p><p>分类模型生成的原始(非标准化)预测向量,通常会传递给标准化函数.如果模型要解决多类别分类问题,则对数通常变成 <a href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2">softmax 函数</a>的输入.之后,softmax 函数会生成一个(标准化)概率向量,对应于每个可能的类别.</p><h3 id="tf-compat"><a href="#tf-compat" class="headerlink" title="tf.compat"></a>tf.compat</h3><p>提供兼容性功能</p><p>tf.compat.v1.reset_default_graph()   #清除默认图形堆栈并重置全局默认图形。</p><p>具体例子看这个博客<a href="https://blog.csdn.net/duanlianvip/article/details/98626111%EF%BC%8C%E6%88%91%E7%9A%84%E7%90%86%E8%A7%A3%E6%9A%82%E6%97%B6%E5%B0%B1%E6%98%AF%E9%87%8D%E6%96%B0%E5%BC%80%E5%A7%8B%EF%BC%8C%E9%98%B2%E6%AD%A2%E5%9C%A8%E4%BB%A5%E5%89%8D%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E7%94%9F%E6%88%90%E6%96%B0%E8%8A%82%E7%82%B9">https://blog.csdn.net/duanlianvip/article/details/98626111，我的理解暂时就是重新开始，防止在以前的基础上生成新节点</a></p><h3 id="tf-io"><a href="#tf-io" class="headerlink" title="tf.io"></a>tf.io</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">process_example_paths</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#先读取再按照对应格式解码</span>            <span class="token keyword">return</span> <span class="token punctuation">{</span><span class="token string">'feature'</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>io<span class="token punctuation">.</span>decode_jpeg<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>io<span class="token punctuation">.</span>read_file<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    <span class="token string">'label'</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>io<span class="token punctuation">.</span>decode_png<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>io<span class="token punctuation">.</span>read_file<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">#获得如下</span><span class="token comment" spellcheck="true">#tf.Tensor([600 394   3], shape=(3,), dtype=int32)</span><span class="token comment" spellcheck="true">#tf.Tensor([600 394   1], shape=(3,), dtype=int32)</span></code></pre><h3 id="tf-keras-1"><a href="#tf-keras-1" class="headerlink" title="tf.keras"></a>tf.keras</h3><p>tensorflow的高级API</p><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow <span class="token keyword">import</span> keras<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> layers</code></pre><h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p><code>Input()</code>被用来实例化一个Keras tensor。 </p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>    shape<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#一个shape tuple(int),不包括batch size.例如,shape =(32,)表示期望的输入将是batches of 32-dim vectors.</span>    <span class="token comment" spellcheck="true">#该元组的元素可以为None;“None”元素代表形状未知的尺寸。</span>    batch_size<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#optional static batch size (integer).</span>    name<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#一个可选的名字string for the layer.应该unique,如果不指定会被自动生成</span>    dtype<span class="token operator">=</span>None<span class="token punctuation">,</span> sparse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> tensor<span class="token operator">=</span>None<span class="token punctuation">,</span>    ragged<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>A tensor</code></pre><p>Keras张量是一个TensorFlow symbolic tensor object，我们使用某些属性对其进行了扩充，这些属性使我们仅通过了解模型的输入和输出即可构建Keras模型。</p><p>例如,如果<code>a</code>,<code>b</code>,<code>c</code>是Keras Tensor,那么可能:<code>model=Model(input=[a,b],output=c)</code></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># this is a logistic regression in Keras</span>x <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>y <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>model <span class="token operator">=</span> Model<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span></code></pre><p>Note that even if <strong>eager execution is enabled</strong>, <code>Input</code> produces <strong>a symbolic tensor (i.e. a placeholder)</strong>. This symbolic tensor can be used with other TensorFlow ops。</p><h4 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h4><p><code>Model</code>将layers组合成一个具有training和inference特征的对象. </p><p><a href="https://keras-cn.readthedocs.io/en/latest/legacy/models/model/">keras教程</a></p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">(</span>    <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre><p>Model的<code>model.__call__()</code>的参数:</p><ul><li>inputs:模型的输入,一个<code>keras.Input</code>对象或一个<code>keras.Input</code>对象的列表</li><li>outputs:模型的输出</li><li>name:String,模型的名字</li></ul><p>有两种方式来实例化一个<code>Model</code>:</p><p><strong>1</strong> 使用”Functional API”,从<code>input</code>开始,通过连接起来的layer的calls来指定模型的前向传播,并且最终从输入和输出创建你的模型。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tfinputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>x <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>inputs<span class="token punctuation">,</span> outputs<span class="token operator">=</span>outputs<span class="token punctuation">)</span></code></pre><p><strong>2</strong> By subclassing the <code>Model</code> class:这种情况下,你将你的layers定义在<code>__init__</code>并且你应该实现模型的前向传播在<code>__call__</code></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>dense1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>dense2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>dense1<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>dense2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>你可以选择性的设立一个<code>training</code>argument(boolean) 在<code>call</code>,你可以使用这个参数在训练阶段和推断阶段指定不同的行为:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> training<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>dense1<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>    <span class="token keyword">if</span> training<span class="token punctuation">:</span>      x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">,</span> training<span class="token operator">=</span>training<span class="token punctuation">)</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>dense2<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><p>模型一旦建立,你可以为你的模型配置losses和metrics 通过<code>model.compile()</code>,训练模型通过<code>model.fit()</code>,或者使用<code>model.predict()</code>预测模型。</p><p><code>compile</code></p><p>配置模型用于训练</p><pre class=" language-python"><code class="language-python">compile<span class="token punctuation">(</span>    optimizer<span class="token operator">=</span><span class="token string">'rmsprop'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#String(name of optimizer)or optimizer instance.See tf.keras.optimizers.</span>    loss<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># String(name of objective function),objective function or tf.keras.losses.Loss 实例.</span>    metrics<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#List of metrics在训练和测试阶段被评估.里面的每一个可以是a string(name of a built-in function),</span>    <span class="token comment" spellcheck="true">#function,或者a tf.keras.metrics.Metric实例.</span>    loss_weights<span class="token operator">=</span>None<span class="token punctuation">,</span>    weighted_metrics<span class="token operator">=</span>None<span class="token punctuation">,</span> run_eagerly<span class="token operator">=</span>None<span class="token punctuation">,</span> steps_per_execution<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对于loss,An objective function is any callable with the signature</span>loss <span class="token operator">=</span> fn<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span>y_pred<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#y_true为GT,shape=[batch_size, d0,...,dN] y_pred为预测值,shape=[batch_size, d0,...,dN]</span><span class="token comment" spellcheck="true">#它返回一个加权损失浮点张量.如果使用自定义Loss实例,并且将reduce设置为NONE,则返回值的形状为[batch_size,d0,...,dN-1],即每个样本或每个时间步的loss;否则,它是一个scalar.如果模型具有多个输出，则可以通过传递dict或a list of losses来在每个输出上使用不同的loss.然后,模型将minimized的loss将是所有单个loss的总和。</span><span class="token comment" spellcheck="true">#对于metrics,经典的你可以使用</span>metrics <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#A function is any callable with the signature</span>result <span class="token operator">=</span> fn<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对于多输出模型的不同输出指定不同metrics,你也可以传递一个dict,例如</span>metrics<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'output_a'</span> <span class="token punctuation">:</span> <span class="token string">'accuracy'</span><span class="token punctuation">,</span> <span class="token string">'output_b'</span> <span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">,</span> <span class="token string">'mse'</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">#你也可以传递一个list(len = len(outputs))</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">,</span> <span class="token string">'mse'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">,</span> <span class="token string">'mse'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#当你传递string例如'accuracy'或'acc',我们将它们one of</span><span class="token comment" spellcheck="true">#tf.keras.metrics.BinaryAccuracy,</span><span class="token comment" spellcheck="true">#tf.keras.metrics.CategoricalAccuracy,</span><span class="token comment" spellcheck="true">#tf.keras.metrics.SparseCategoricalAccuracy</span><span class="token comment" spellcheck="true">#基于使用的损失函数和模型输出的shape选择一个,我们对'crossentropy'和'ce'做类似的转换.</span></code></pre><p><code>fit</code></p><p>为模型训练固定的epoch。</p><pre class=" language-python"><code class="language-python">fit<span class="token punctuation">(</span>    x<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#Input data</span>    y<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#Target data</span>    batch_size<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#int或None,如果未指定,默认为batch_size=32,如果x是datasets,generators或者keras.util.Swquence</span>    <span class="token comment" spellcheck="true">#无需指定,(因为他们自己生成batches)</span>    epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#int</span>    verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#0,1,2.详细模式, 0=silent,1=progress bar,2=one line per epoch.</span>    <span class="token comment" spellcheck="true">#请注意,progress bar在logged到文件时不是特别有用,因此,如果不以交互方式运行(例如,在生产环境中),建议使用verbose = 2.</span>    callbacks<span class="token operator">=</span>None<span class="token punctuation">,</span>    validation_split<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span>None<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> class_weight<span class="token operator">=</span>None<span class="token punctuation">,</span>    sample_weight<span class="token operator">=</span>None<span class="token punctuation">,</span> initial_epoch<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> steps_per_epoch<span class="token operator">=</span>None<span class="token punctuation">,</span>    validation_steps<span class="token operator">=</span>None<span class="token punctuation">,</span> validation_batch_size<span class="token operator">=</span>None<span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    max_queue_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> workers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> use_multiprocessing<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对于input data,可以是</span><span class="token comment" spellcheck="true">#A Numpy array(or array-like),或者a list of arrays(如果模型有多个输入)</span><span class="token comment" spellcheck="true">#A Tensorflow tensor,或者a list of tensors(如果模型有多个输入)</span><span class="token comment" spellcheck="true">#A dict 映射input names到相应的array/tensors,如果模型有named inputs</span><span class="token comment" spellcheck="true">#A tf.data dataset,其应该返回要么是(inputs,targets)要么是(inputs, targets, sample_weights)</span><span class="token comment" spellcheck="true">#A generator or keras.util.Sequence返回(inputs, targets)或者(inputs, targets, sample_weights)</span><span class="token comment" spellcheck="true">#对于target_data,与x类似,可以是Numpy arrays或者Tensorflow tensor(s),但是必须与x保持一致.当x是dataset,generator或者keras.utils.Sequence时,y不应该被指定.</span></code></pre><p><code>evalute</code></p><p>返回测试模式下的loss value &amp; metrics values.</p><pre class=" language-python"><code class="language-python">evaluate<span class="token punctuation">(</span>    x<span class="token operator">=</span>None<span class="token punctuation">,</span> y<span class="token operator">=</span>None<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>None<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> sample_weight<span class="token operator">=</span>None<span class="token punctuation">,</span> steps<span class="token operator">=</span>None<span class="token punctuation">,</span>    callbacks<span class="token operator">=</span>None<span class="token punctuation">,</span> max_queue_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> workers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> use_multiprocessing<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    return_dict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>Scalar test loss<span class="token comment" spellcheck="true">#(如果模型只有一个输出且没有metrics)</span><span class="token operator">-</span><span class="token operator">&gt;</span><span class="token operator">or</span> list of scalars<span class="token comment" spellcheck="true">#(如果模型有多个输出且有metrics)</span><span class="token comment" spellcheck="true">#属性modle.metrics_name将为你展示标量输出的标签.</span></code></pre><p><code>predict</code></p><p>生成输入样本的输出预测。</p><p>计算是分批进行的。 此方法专为在大规模输入中的性能而设计。</p><p> 对于适合one batch的少量输入，建议您直接使用<code>__call__</code>来加快执行速度,例如,如果您有<code>tf.keras.layers.BatchNormalization</code>这样的在inference中的行为有所不同的layers，则可以使用<code>model(x)</code>或<code>model(x, training = False)</code>. 另外，请注意，test loss不受诸如noise和dropout之类的regularization layers的影响。</p><pre class=" language-python"><code class="language-python">predict<span class="token punctuation">(</span>    x<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>None<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> steps<span class="token operator">=</span>None<span class="token punctuation">,</span> callbacks<span class="token operator">=</span>None<span class="token punctuation">,</span> max_queue_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>    workers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> use_multiprocessing<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>Numpy array<span class="token punctuation">(</span>s<span class="token punctuation">)</span> of predictions<span class="token punctuation">.</span></code></pre><h4 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h4><p><code>Sequential</code>组合一个layers的线性堆叠成一个 <code>tf.keras.Model</code>。</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>    layers<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#Optional list of layers to add to the model.</span>    name<span class="token operator">=</span>None<span class="token comment" spellcheck="true">#</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#eg:</span><span class="token comment" spellcheck="true"># Optionally, the first layer can receive an `input_shape` argument:</span>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Afterwards, we do automatic shape inference:</span>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#注意,您也可以省略“ input_shape”参数.在这种情况下,直到第一次调用training/evaluation方法(由于尚未构建),模型才具有任何权重.</span><span class="token comment" spellcheck="true">#这个input_shape是指单个元素的维度,不算batch_size那个维度.</span></code></pre><h4 id="tf-keras-backend"><a href="#tf-keras-backend" class="headerlink" title="tf.keras.backend"></a>tf.keras.backend</h4><p><a href="https://keras-cn.readthedocs.io/en/latest/backend/">https://keras-cn.readthedocs.io/en/latest/backend/</a></p><p>keras后端API，有Theano和Tensorflow</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#结束当前的TF计算图，并新建一个。有效的避免模型/层的混乱</span></code></pre><h4 id="tf-keras-datasets"><a href="#tf-keras-datasets" class="headerlink" title="tf.keras.datasets"></a>tf.keras.datasets</h4><p><strong>Overview</strong></p><ul><li>boston_housing</li><li>cifar10</li><li>cifar100</li><li>fashion_mnist</li><li>imdb</li><li>mnist</li><li>reuters</li></ul><p><strong>mnist</strong></p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>    path<span class="token operator">=</span><span class="token string">'mnist.npz'</span> <span class="token comment" spellcheck="true">#本地缓存的名字,不设置的话默认就是这个了</span><span class="token punctuation">)</span></code></pre><h4 id="tf-keras-models"><a href="#tf-keras-models" class="headerlink" title="tf.keras.models"></a>tf.keras.models</h4><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>load_model<span class="token punctuation">(</span>filepath<span class="token punctuation">,</span> custom_objects<span class="token operator">=</span>None<span class="token punctuation">,</span> compile<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> options<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#可选</span><span class="token comment" spellcheck="true">#compile:布尔值，是否在加载后编译模型</span><span class="token comment" spellcheck="true">#options:可选</span></code></pre><p><a href="https://blog.csdn.net/sjtuxx_lee/article/details/80399514">keras保存和加载模型的方法</a></p><h4 id="tf-keras-utils"><a href="#tf-keras-utils" class="headerlink" title="tf.keras.utils"></a>tf.keras.utils</h4><p>tf.keras.utils.plot_model </p><h4 id="tf-keras-layers"><a href="#tf-keras-layers" class="headerlink" title="tf.keras.layers"></a>tf.keras.layers</h4><p><code>tf.keras.layers.Conv2D</code></p><p>该层创建a conv kernel，该conv kernel与该层input进行卷积以产生a tensor of outpus. 如果<code>use_bias</code>为True，则会创建一个bias vector并将其添加到outputs中。 最后，如果<code>activation</code>不为<code>None</code>，则它也将应用于输出。</p><p>当使用该层作为model的第一个layer时,要提供关键的<code>input_shape</code>(tuple of integers),eg,<code>input_shape=(128,128,3)</code>对于128x128 RGB图片如果<code>data_format="channels_last"</code></p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>    filters<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#卷积过滤器的数量,对应输出的维度</span>    kernel_size<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span>    data_format<span class="token operator">=</span>None<span class="token punctuation">,</span> dilation_rate<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>None<span class="token punctuation">,</span>    use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span>    bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>None<span class="token punctuation">,</span>    bias_regularizer<span class="token operator">=</span>None<span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span>None<span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span>None<span class="token punctuation">,</span>    bias_constraint<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>A tensor of rank <span class="token number">4</span><span class="token operator">+</span> representing activation<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> kernel<span class="token punctuation">)</span> <span class="token operator">+</span> bias<span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># The inputs are 28x28 RGB images with `channels_last` and the batch</span><span class="token comment" spellcheck="true"># size is 4.</span>input_shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>x <span class="token operator">=</span> tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>input_shape<span class="token punctuation">)</span>y <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span>input_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#不用batch</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></code></pre><p>$input_shape$</p><p>4+D tensor with shape: <code>batch_shape + (channels, rows, cols)</code> if <code>data_format='channels_first'</code> </p><p>or 4+D tensor with shape: <code>batch_shape + (rows, cols, channels)</code> if <code>data_format='channels_last'</code>.</p><p>$output_shape$</p><p>4+D tensor with shape: <code>batch_shape + (filters, new_rows, new_cols)</code> if <code>data_format='channels_first'</code> </p><p>or 4+D tensor with shape: <code>batch_shape + (new_rows, new_cols, filters)</code> if <code>data_format='channels_last'</code>. </p><p><code>rows</code> and <code>cols</code> values might have changed due to padding.</p><p><code>tf.keras.layers.Dense</code></p><p>全连接</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>    units<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#正整数,输出空间的维数</span>    activation<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#激活函数</span>    use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#是否使用bias vector</span>    kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span>    bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>None<span class="token punctuation">,</span>    bias_regularizer<span class="token operator">=</span>None<span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span>None<span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span>None<span class="token punctuation">,</span>    bias_constraint<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre><p><code>Dense</code>实现了这个操作:<code>output = activation(dot(input, kernel) +bias)</code>其中<code>activation</code>是element-wise激活函数作为<code>activation</code>参数传递进来,<code>kernel</code>是被layer创建的权重矩阵,<code>bias</code>是由layers创建的bias vecvor(只在<code>use_bias</code>是<code>True</code>的时候)</p><p>此外,layer被调用一次之后,属性便无法更改(可训练属性除外).</p><p>$input_shape$</p><p>N维张量的shape:<code>(batch_size,...,input_dim)</code>.最常见的情况是<code>(batch_size,input_dim)</code>的输入.</p><p>$output_shape$</p><p>N维张量的shape:<code>(batch_size,...,units)</code>.例如,一个2D输入的shape:<code>(batch_size,input_dim)</code>,输入可能有shape:<code>(batch_size,units)</code></p><p><code>tf.keras.layers.Flatten</code></p><p>重新格式化数据$(B,H,W,C)\to(B,H\times W\times C)$   $(B,)\to(B,1)$</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span>    data_format<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#data_format有channel_last(默认)和channel_first</span>    <span class="token operator">**</span>kwargs<span class="token comment" spellcheck="true">#</span><span class="token punctuation">)</span></code></pre><p><code>tf.keras.layers.Softmax</code></p><p>Softamx激活函数.</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>    axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#Integer,or list of Integers,softmax应用的维度</span>    <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre><h4 id="tf-keras-losses"><a href="#tf-keras-losses" class="headerlink" title="tf.keras.losses"></a>tf.keras.losses</h4><p>Built-in loss functions.</p><p><code>tf.keras.losses.SparseCategoricalCrossentropy</code></p><p>计算labels和predictions之间的crossentropy。当有两个或者更多label类别时使用。我们期望labels是integers。如果你想让labels使用<code>one-hot</code>方式,请使用</p><p><code>CategoricalCrossentropy</code>损失.</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>    from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#y_pred是否应该是一个logits tensor.</span>    <span class="token comment" spellcheck="true">#Using from_logits=True may be more numerically stable.</span>    reduction<span class="token operator">=</span>losses_utils<span class="token punctuation">.</span>ReductionV2<span class="token punctuation">.</span>AUTO<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#(Optional) Type of tf.keras.losses.Reduction to apply to loss. </span>    <span class="token comment" spellcheck="true">#默认值是AUTO,意味着reduction操作会根据使用的上下文决定。绝大多数情况默认为SUM_OVER_BATCH_SIZE.</span>    name<span class="token operator">=</span><span class="token string">'sparse_categorical_crossentropy'</span><span class="token comment" spellcheck="true">#可选的名字对于该op</span><span class="token punctuation">)</span></code></pre><p>对于<code>y_pred</code>应该有<code># classes</code>个浮点数每个特征,而对于<code>y_true</code>一个浮点数每个特征。</p><p><code>y_true</code>shape=<code>[batch_size]</code>,而<code>y_pred</code>shape=<code>[batch_size, num_classes]</code></p><pre class=" language-python"><code class="language-python">y_true <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>y_pred <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">0.95</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># Using 'auto'/'sum_over_batch_size' reduction type.</span>scce <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span><span class="token punctuation">)</span>scce<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><code>__call__</code></p><pre class=" language-python"><code class="language-python">__call__<span class="token punctuation">(</span>    y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">,</span> sample_weight<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络结构解密</title>
      <link href="wang-luo-jie-gou-jie-mi/"/>
      <url>wang-luo-jie-gou-jie-mi/</url>
      
        <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h3><p><a href="https://zhuanlan.zhihu.com/p/31561570">深度学习基础</a></p><p><strong>representation learning 表示学习</strong></p><p><strong>transfer learning 迁移学习</strong> </p><p><strong>multi-task learning 多任务学习</strong> 低层特征共享，产生分支完成各自的任务</p><p> <strong>end-to-end learning 端到端学习</strong> 一步到位，就是一种表示学习</p><h3 id="计算机视觉四大基本任务"><a href="#计算机视觉四大基本任务" class="headerlink" title="计算机视觉四大基本任务"></a>计算机视觉四大基本任务</h3><p><a href="https://zhuanlan.zhihu.com/p/31727402">计算机视觉四大基本任务</a></p><p><a href="https://zhuanlan.zhihu.com/p/34142321">目标检测入门</a></p><h3 id="计算机视觉其他应用"><a href="#计算机视觉其他应用" class="headerlink" title="计算机视觉其他应用"></a>计算机视觉其他应用</h3><p><a href="https://zhuanlan.zhihu.com/p/31727405">计算机视觉其他应用</a></p><h2 id="统计学习知识和网络细节"><a href="#统计学习知识和网络细节" class="headerlink" title="统计学习知识和网络细节"></a>统计学习知识和网络细节</h2><h3 id="常识"><a href="#常识" class="headerlink" title="常识"></a>常识</h3><p>通俗理解的话，<strong>离散即分类，连续即回归</strong>，回归是有误差度量的，比如5和6差为1,5和7差为2，分类除了分类为5为对其他均为错，误差度量离散二值化了</p><p>李宏毅GAN教程:</p><p>**Regression-**output a scalar **Classification-**output a “class” **Structured Learning/Prediction-**output a sequence, a matrix , a graph, a tree…</p><p><strong>output is composed of components with dependency 带有依赖的元素的组合</strong></p><h4 id="Head-Neck-Backbone"><a href="#Head-Neck-Backbone" class="headerlink" title="Head/Neck/Backbone"></a>Head/Neck/Backbone</h4><p>输入-&gt;主干-&gt;脖子-&gt;头-&gt;输出。主干网络提取特征，脖子提取一些更复杂的特征，然后头部计算预测输出</p><p><strong>backbone：</strong>主干网络，就代表其是网络的一部分，那么是哪部分呢？这个主干网络大多时候指的是提取特征的网络，其作用就是提取图片中的信息，共后面的网络使用。这些网络经常使用的是resnet、VGG等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对他进行微调，使得其更适合于我们自己的任务。</p><p><strong>head：</strong>head是获取网络输出内容的网络，利用之前提取的特征，head利用这些特征，做出预测。</p><p>**neck:**是放在backbone和head之间的，是为了更好的利用backbone提取的特征。</p><p><strong>bottleneck:**在ResNet中,bottleneck主要是指</strong>每个block进来一个特征图现有一个压缩的步骤**,然后3x3卷积,然后再扩张。<strong>如果bottleneck是指行为的话,就是指压缩这一行为,如果bottleneck是指特征图的话,就是指压缩后或扩张前的通道数较少的特增图。</strong></p><p><strong>GAP：</strong>Global Average Pool全局平均池化，就是将某个通道的特征取平均值，经常使用AdaptativeAvgpoold(1),在pytorch中，这个代表自适应性全局平均池化，说人话就是将某个通道的特征取平均值。</p><p><strong>Embedding</strong>:深度学习方法都是利用使用线性和非线性转换对复杂的数据进行自动特征抽取，并将特征表示为“向量”（vector），这一过程一般也称为“嵌入”（embedding）</p><p><strong>False Positive</strong>:假阳性,其实这个说法就已经揭示了如何分辨,就比如一个试纸,测出来是阳性，但其实是假的,换句话说就是网络预测出来是真的,其实是假的</p><p>3x3的卷积有padding一般是1,5x5的卷积有padding一般是2,7x7的卷积有padding一般是3</p><h3 id="归一-标准-正则"><a href="#归一-标准-正则" class="headerlink" title="归一/标准/正则"></a>归一/标准/正则</h3><p><a href="https://maristie.com/2018/02/Normalization-Standardization-and-Regularization">Differences between Normalization, Standardization and Regularization</a></p><p><a href="https://www.zhihu.com/question/20455227/answer/370658612">归一化和标准化的作用于区别</a></p><p>翻译成中文是归一化，标准化，正则化，前两个是特征缩放，最后一个是减少过拟合</p><h4 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h4><img src="/wang-luo-jie-gou-jie-mi/image-20210317213036238.png" alt="归一化的作用" style="zoom: 67%;"><h4 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h4><p>特征缩放，在许多学习算法中，标准化是一种<strong>广泛使用的预处理步骤，其目的是将特征缩放到零均值和单位方差:</strong><br>$$<br>x^{’}=\frac{x-u}{\sigma}<br>$$<br>也是特征缩放,但是更<strong>加动态和具有弹性,与整体样本的分布有关,加速收敛</strong></p><p><strong>当整体较为集中时,方差更小,标准化后就会更宽松;如果较为宽松,方差更大,标准化后就会变紧。</strong></p><p>用这些大数据集的均值和方差,代替真实的均值和方差</p><p><strong>coco数据集的均值和方差（三分量顺序是RGB）</strong></p><p><code>mean = [0.471, 0.448, 0.408]</code><br><code>std = [0.234, 0.239, 0.242]</code></p><p><strong>ImageNet数据集的均值和方差（三分量顺序是RGB）</strong></p><p><code>mean = [0.485, 0.456, 0.406]</code><br><code>std = [0.229, 0.224, 0.225]</code></p><p>注意,经过数据标准化之后图像的范围可能就不在[0-1]范围内了</p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>正则化,减少过拟合，损失函数后加正则项</p><p><strong>贝叶斯学派观点来看，正则项是在模型训练过程中引入了某种模型参数</strong>的先验分布[后验由数据得出，先验就是理性推断，不拘束于数据。</p><p><strong>范数</strong>$L_P-norm$</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201125202729072.png" alt="Lp范数"></p><p>p=1，<strong>曼哈顿距离</strong>，p=2，<strong>欧式距离</strong>，p为无穷，<strong>无穷范数或最大范数</strong>。</p><p>做正则项时，称为Lp-正则项。L1-正则项也叫<strong>LASSO</strong>正则项，L2-正则项也叫<strong>Tikhonov或Ringe正则项</strong></p><p><a href="https://liam.page/2017/03/30/L1-and-L2-regularizer/">详解</a></p><p>L1正则化主要是在损失函数中添加权重参数的绝对值的和，得到的<strong>参数通常比较稀疏，常用于特征选择</strong>。L2正则化则是在损失函数中添加权重参数的平方项，<strong>得到的模型参数通常比较小</strong>。</p><p>在这里，我们需要关注的最主要是范数的「非负性」。我们刚才讲，损失函数通常是一个有下确界的函数。而这个性质保证了我们可以对损失函数做最优化求解。如果我们要保证目标函数依然可以做最优化求解，那么我们就必须让正则项也有一个下界。非负性无疑提供了这样的下界，而且它是一个下确界——由齐次性保证[当 c=0 时]</p><p>因此，我们说，范数的性质使得它天然地适合作为机器学习的正则项。而范数需要的向量，则是<strong>机器学习的学习目标——参数向量</strong>。</p><p>L0范数表示向量中非零元素的个数 </p><p><strong>我们考虑这样一种普遍的情况，即：预测目标背后的真是规律，可能只和某几个维度的特征有关；而其它维度的特征，要不然作用非常小，要不然纯粹是噪声。在这种情况下，除了这几个维度的特征对应的参数之外，其它维度的参数应该为零。若不然，则当其它维度的特征存在噪音时，模型的行为会发生预期之外的变化，导致过拟合。</strong>引入L0范数</p><p>L0范数不好，又引入L1范数，L1范数也可以在梯度更新时使得参数趋于0.</p><p>L1使得参数稀疏化，L2使得参数稠密的趋近于0</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201125205147790.png" alt="稀疏和稠密的原因"></p><p><strong>提前停止</strong></p><p>提前停止可看做是<strong>时间维度上的正则化</strong>。直觉上，随着迭代次数的增加，如梯度下降这样的训练算法倾向于学习愈加复杂的模型。在实践维度上进行正则化有助于控制模型复杂度，提升泛化能力。在实践中，<strong>提前停止一般是在训练集上进行训练，而后在统计上独立的验证集上进行评估；当模型在验证集上的性能不在提升时，就提前停止训练</strong>。最后，可在测试集上对模型性能做最后测试。</p><p><a href="https://alisure.github.io/2018/04/14/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%93%E6%B7%B7%E6%A6%82%E5%BF%B5%E4%B9%8B%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E4%B8%8E%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9/#:~:text=%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9(Structural%20Risk)%EF%BC%9A,%EF%BC%88%E4%BE%8B%E5%A6%82%E6%AD%A3%E5%88%99%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%89%E3%80%82">期望风险、经验风险、结构风险 简洁</a></p><h3 id="偏差-方差分解"><a href="#偏差-方差分解" class="headerlink" title="偏差-方差分解"></a>偏差-方差分解</h3><p><a href="https://liam.page/2017/03/25/bias-variance-tradeoff/">详解</a></p><p>误差来源有三种：随机误差、偏差和方差</p><p>随机误差是高斯白噪声</p><p>偏差bias描述的是通过学习拟合出来的结果之期望，与真实规律之间的差距，记作$Bias(X)=E[\hat{f}(X)]-f(X)$</p><p>方差variance即是统计学中的定义，描述的是通过学习拟合出来的结果自身的不稳定性，记作$Var(X)=E[(\hat{f}(X)-E[\hat{}f(X)])^2]$</p><p>均方误差可以分解为:$Bias^2+Variance+Random Error$</p><img src="/wang-luo-jie-gou-jie-mi/image-20201125210219206.png" alt="直观图示" style="zoom:67%;"><h3 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h3><p><a href="https://www.jianshu.com/p/903e35e1c95a">期望风险、经验风险、结构风险2深入</a></p><p>损失函数：度量模型一次预测的好坏</p><p>风险函数：度量平均意义下的模型预测好坏</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p><a href="https://zhuanlan.zhihu.com/p/70821070">谁才是合格的激活函数 </a></p><p>激活函数应满足的性质：</p><ul><li>非线性,这是必须的,也是添加激活函数的原因</li><li>几乎处处可微：反向传播中，损失函数要对参数求偏导，如果激活函数不可微，那就无法使用梯度下降方法更新参数了。(ReLU只在零点不可微，但是梯度下降几乎不可能收敛到梯度为0)</li><li>计算简单：神经元(units)越多，激活函数计算的次数就越多，复杂的激活函数会降低训练速度。</li><li>非饱和性：<strong>饱和指在某些区间梯度接近于零，使参数无法更新</strong>。Sigmoid和tanh都有这个问题，而ReLU就没有，所以普遍效果更好。因为是链式求导,所以你一个是0乘以其他的还是0,不好。</li><li>有限的值域：这样可以使网络更稳定，即使有很大的输入，激活函数的输出也不会太大。</li></ul><h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>$$<br>\sigma(x)=\frac{1}{1+e^{-x}}\<br>\sigma’(x)=\sigma(x)[1-\sigma(x)]<br>$$</p><p><strong>易饱和问题:</strong></p><img src="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20210708170146109.png" alt="image-20210708170146109" style="zoom:50%;"><p>$x$在大部分范围的梯度都趋近于0,同时该函数单调递增,所以梯度始终&gt;0</p><p><strong>非零均值问题:</strong></p><p>如果每一层的激活函数都是用$sigmoid$,那么假设有:</p><p>$f(x,w,b)=\sigma(\sum w_ix_i+b)$,$x_i$是上一层的输出,这一层的出入。</p><p>$w_i$的更新为:$w_i-\alpha \frac{\partial L}{\partial w_i} \to w_i$</p><p>$L(x)$对$w_i$求偏导:$\frac{\partial L}{\partial w_i}=\frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial a}\cdot \frac{\partial a}{\partial w_i}$</p><p>$\frac{\partial L}{\partial w_i}=\frac{\partial L}{\partial y}\cdot y(1-y)\cdot x_i$</p><p>$y(1-y)&gt;0$,对于$\frac{\partial L}{\partial y}$,设$L=(y_{true}-y)^2/2$,求完导之后:$y-y_{true}$,是个常数,跟具体的某个$w_i$无关,又因为$x_i$是上一层的输出,那么经过$sigmoid$激活之后,$x_i&gt;0$,所以所有的$w_i$更新的方向一致。所有的权重只能一起变大或变小，这样模型收敛就需要消耗很多的时间,如下图所示。</p><img src="/wang-luo-jie-gou-jie-mi/image-20201217191234682.png" alt="这时的收敛是这样的" style="zoom:50%;"><p><strong>幂运算问题：</strong>幂运算相对较为耗时,这个不算太大的问题。</p><h4 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h4><p>Tanh函数是 0 均值的，因此实际应用中 Tanh 会比 sigmoid 更好。但是仍然存在<strong>梯度饱和</strong>与<strong>exp计算</strong>的问题。<br>$$<br>f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\<br>f(x)’=1-[f(x)]^2<br>$$</p><h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p><strong>ReLU的Dead ReLU问题</strong></p><p>如果某一次权重更新用力过猛，啪的一下，大部分权重都是负的了，下一次前向输出很容易是负的，一激活，就是0，之后就不更新了，就完蛋了</p><p>两个原因：<strong>1初始化太糟糕 2learning rate设太高导致用力过猛</strong></p><p><a href="https://zhuanlan.zhihu.com/p/71882757">不同激活函数的优缺点总结</a></p><p>了解<strong>ELU</strong>和<strong>Maxout</strong>，之前的问题解决了，但计算量和参数量会有损失。</p><h4 id="输出层选择"><a href="#输出层选择" class="headerlink" title="输出层选择"></a>输出层选择</h4><p><strong>输出层的激活函数</strong>和<strong>损失函数</strong>由任务类型决定，见下表。它们与<strong>隐藏层的激活函数</strong>的选择是独立的。</p><img src="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20210708204741317.png" alt="image-20210708204741317"><p>隐藏层一般就是$ReLU$,至于为什么输出层的激活函数不用$ReLU$呢?输出是$0$到正无穷,没办法分类,你**还得自己设置阈值来确定哪个是哪一类!**输入过来的值是负的的话,所有的输出值都是0,那就没办法区分了。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数，就是计算真实分布与预测分布的差异性，差异性越小，损失函数越小，预测的越好</p><h4 id="概率与似然"><a href="#概率与似然" class="headerlink" title="概率与似然"></a>概率与似然</h4><p>概率大家都比较了解,主要是似然的了解。</p><p>在数理统计学中,<strong>似然函数(likelihood function)**是一种关于统计模型中的参数的函数，表示模型参数中的</strong>似然性**（英语：likelihood）。似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。文字意义上，“似然性”与“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“概率”（或然性）有明确的区分：==概率，用于在已知一些参数的情况下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，也就是说已观察到某事件后，对相关参数进行猜测。==</p><p>在这种意义上，似然函数可以理解为条件概率的逆反。在已知某个参数<strong>B</strong>时，事件<strong>A</strong>会发生的概率写作：<br>$$<br>P(A|B)=\frac{P(A,B)}{P(B)}<br>$$<br>利用贝叶斯定理,<br>$$<br>P(B|A)=\frac{P(A|B)P(B)}{P(A)}<br>$$<br>因此，我们可以反过来构造表示似然性的方法：已知有事件<strong>A</strong>发生，运用似然函数$L(B|A)$，我们估计或猜测参数<strong>B</strong>的不同值的可能性。<strong>形式上，似然函数也是一种条件概率函数，但我们关注的变量改变了</strong>：<br>$$<br>b\to P(A|B=b)<br>$$<br>在英语语境里，likelihood 和 probability 的日常使用是可以互换的，都表示对机会 (chance) 的同义替代。但在数学中，probability 这一指代是有严格的定义的，即符合柯尔莫果洛夫公理 (Kolmogorov axioms) 的一种<strong>数学对象</strong>（换句话说，<strong>不是</strong>所有的可以用0到1之间的数所表示的对象都能称为概率），而 likelihood (function) 这一概念是由Fisher提出，他采用这个词，也是为了凸显他所要表述的<strong>数学对象</strong>既和 probability 有千丝万缕的联系，但又不完全一样的这一感觉。中文把它们一个翻译为<strong>概率</strong>一个翻译为<strong>似然</strong>也是独具匠心。</p><p><a href="https://www.zhihu.com/question/54082000/answer/470252492">HiTao知乎回答</a></p><p>概率和似然类似$2^b$和$a^2$,而<code>p(x|θ)</code>也是一个有着两个变量的函数。<strong>如果，你将θ设为常量，则你会得到一个概率函数（关于x的函数）；如果，你将x设为常量你将得到似然函数（关于θ的函数）。</strong></p><p>有一个硬币，它有θ的概率会正面向上，有1-θ的概率反面向上。θ是存在的，但是你不知道它是多少。为了获得θ的值，你做了一个实验：将硬币抛10次，得到了一个正反序列：x=HHTTHTHHHH。</p><p>无论θ的值是多少，这个序列的概率值为 θ⋅θ⋅(1-θ)⋅(1-θ)⋅θ⋅(1-θ)⋅θ⋅θ⋅θ⋅θ = θ⁷ (1-θ)³</p><p>比如，如果θ值为0，则得到这个序列的概率值为0。如果θ值为1/2，概率值为1/1024。</p><p>但是，我们应该得到一个更大的概率值，所以我们尝试了所有θ可取的值，画出了下图：</p><img src="/wang-luo-jie-gou-jie-mi/image-20210601213407084.png" alt="image-20210601213407084" style="zoom:50%;"><p>这个曲线就是θ的似然函数，<strong>通过了解在某一假设下，已知数据发生的可能性，来评价哪一个假设更接近θ的真实值。</strong></p><p>==因为这么多种情况偏偏这种情况发生了,肯定是这种情况发生的概率尽可能大,此时的$\theta$就更可能是真实的$\theta$==</p><p>如图所示，最有可能的假设是在θ=0.7的时候取到。但是，你无须得出最终的结论θ=0.7。事实上，根据贝叶斯法则，0.7是一个不太可能的取值（如果你知道几乎所有的硬币都是均质的，那么这个实验并没有提供足够的证据来说服你，它是均质的）。但是，0.7却是最大似然估计的取值。</p><p>因为这里仅仅试验了一次，得到的样本太少，所以最终求出的最大似然值偏差较大，如果经过多次试验，扩充样本空间，则最终求得的最大似然估计将接近真实值0.5。在<a href="https://link.zhihu.com/?target=http://blog.sina.com.cn/s/blog_e8ef033d0101oa4k.html">这篇博客</a>中有详细的过程，就不再赘述。</p><h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p><a href="https://zhuanlan.zhihu.com/p/70804197">为什么用交叉熵做损失函数？</a></p><p><strong>信息熵是消除不确定性所需信息量的度量。</strong>信息熵就是信息的不确定性程度，信息熵越小，信息越确定。</p><p>信息熵=$\sum_{x=1}^N$(信息$x$发生的概率$\times$验证信息$x$需要的信息量)</p><p>举个列子，比如说：今年中国取消高考了，这句话我们很不确定(甚至心里还觉得这TM是扯淡)，那我们就要去查证了，这样就需要很多信息量(去查证)；反之如果说今年正常高考，大家回想：这很正常啊，不怎么需要查证，这样需要的信息量就很小。从这里我们可以学到：根据信息的<strong>真实分布</strong>，我们能够找到一个最优策略，以<strong>最小的代价消除系统的不确定性</strong>，即<strong>最小信息熵</strong>。</p><p>简而言之，<strong>概率越低，需要越多的信息去验证</strong>，所以<strong>验证真假需要的信息量和概率成反比</strong>。我们需要用数学表达式把它描述出来，推导：</p><img src="/wang-luo-jie-gou-jie-mi/image-20210707193548113.png" alt="image-20210707193548113" style="zoom: 80%;"><p>信息熵即所有信息量的期望:<br>$$<br>H(x)=-\sum_xp(x)log(p(x))=-\sum_{i=1}^Np(x_i)log(p(x_i))<br>$$<br>编码层面的解释:</p><p><strong>一个事件结果的出现概率越低，对其编码的bit长度就越长。以期在整个随机事件的无数次重复试验中，用最少的 bit 去记录整个实验历史。即无法压缩的表达，代表了真正的信息量。</strong></p><p>熵的本质的另一种解释:<strong>最短平均编码长度</strong>。编码方案完美时,最短平均编码长度。编码方案完美就是指一个事件结果的出现概率越低，对齐编码的bit长度就越长。</p><p>假设一个随机变量X的正确分布p如下:</p><p>$A:\frac{1}{2}\quad B:\frac{1}{4}\quad C:\frac{1}{4}$</p><p>可以简单认为，平均每发送4个符号其中就有2个A，1个B，1个C。因为我们知道信道传输是只能传二进制编码的，所以必须对A、B、C进行编码，根据哈夫曼树来实现，如下所示：</p><p><img src="/wang-luo-jie-gou-jie-mi/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%A7%A3%E5%AF%86%5Cimage-20210707203744894.png" alt="image-20210707203744894"></p><p>A被编码为0，B被编码为10，C被编码为11。所以每4个字符的平均编码长度为：<br>$$<br>\frac{2+2+2}{4}=1.5<br>$$<br>那么这个随机变量的信息熵是什么呢？</p><p>$H(x)=-1/2log(1/2)-1/4log(1/4)-1/4log(1/4)=1.5$</p><p>编码的话以2为底。</p><h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h4><p>有了信息熵的概念后，然后就去看看交叉熵的物理含义。假设我们用一个错误的分布q，对随机变量编码，q的分布如下：</p><p>$A:\frac{1}{4}\quad B:\frac{1}{4}\quad C:\frac{1}{2}$</p><p><img src="/wang-luo-jie-gou-jie-mi/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%A7%A3%E5%AF%86%5Cimage-20210707204259161-1625661782418.png" alt="image-20210707204259161"></p><p>也就是说A被编码为11，B被编码为10，C被编码为0。但是实际上，平均每4个字符还是2个A，1个B，1个C。所以在这种情况下，平均编码长度变成了:<br>$$<br>\frac{4+1+2}{4}=1.75<br>$$<br>交叉熵此时等于:</p><p>$H(p,q)=-1/2log(1/4)-1/4log(1/4)-1/4log(1/2)=1.75$</p><h4 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h4><p>有了交叉熵和信息熵，那么相对熵更好理解了。其<strong>实际含义就是用错误分布对随机变量编码时，其产生多余的编码长度</strong>。这里就是 $D_{KL}(p||q)=1.75-1.5=0.25$.</p><p>相对熵衡量两个概率分布之间的差值，如下:<br>$$<br>D_{KL}(p||q)=\sum_{i=1}^Np(x_{i})\cdot (logp(x_i)-logq(x_i))\<br>=E[logp(x)-logq(x)]\<br>=\sum_{i=1}^Np(x_{i})\cdot log\frac{p(x_i)}{q(x_i)}\<br>\quad \quad \quad \quad \quad=\sum_{i=1}^Np(x_{i})logp(x_i)-\sum_{i=1}^Np(x_i)logq(x_i))\<br>=-H(x)+cross_entropy<br>$$<br>在机器学习中，往往$p$用来描述<strong>真实分布</strong>,$q$用来描述<strong>预测分布</strong>。</p><p>第一行为啥是$p(x_i)$？<strong>后面的是他们的信息量，用它们的概率分布，但实际算期望的时候还是要看真实的概率。</strong>实际$A,B,C$出现还是要按照真实的概率出现。</p><p>实际KL散度算的是<strong>预测分布减去真实分布的熵</strong>,其实就是错误分布多使用的信息量。</p><p><strong>交叉熵,第四行和第五行显示了交叉熵，因为真实分布是确定的，所以H(x)是固定的，交叉熵可以替代KL散度</strong>。</p><p>总结:<span class="github-emoji"><span>😄</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p><blockquote><p>信息熵完美编码，<br>交叉熵不完美编码，<br>相对熵是两者的差值，交叉熵减去信息熵。差值即差异，也即KL散度。</p></blockquote><p>以上内容参考:</p><p><a href="https://www.zhihu.com/question/41252833/answer/140950659">如何通俗的解释交叉熵与相对熵？ - 张一山的回答 - 知乎 </a></p><p><a href="https://www.zhihu.com/question/41252833/answer/515095695">如何通俗的解释交叉熵与相对熵？ - erwin的回答 - 知乎</a></p><p><a href="https://www.zhihu.com/question/41252833/answer/182891314">如何通俗的解释交叉熵与相对熵？ - 小锋子Shawn的回答 - 知乎</a></p><h4 id="为什么能用交叉熵作为损失函数？"><a href="#为什么能用交叉熵作为损失函数？" class="headerlink" title="为什么能用交叉熵作为损失函数？"></a>为什么能用交叉熵作为损失函数？</h4><p>在机器学习中，比如分类问题，如果把结果当作是概率分布来看:</p><ul><li><strong>标签表示的就是数据真实的概率分布</strong>，实际所属类别概率为1,其他类别概率为0。</li><li>由softmax函数产生的结果其实是对于数据的预测分布,每个类别有一个预测的概率。</li></ul><p>按照之前的理解,相当于对于某一张图片分类或语义分割的某一个像素:</p><p>真实分布为:$a:1,b:0,c:0,…,z:0$</p><p>预测分布为:$a:p_a,b:p_b,c:p_c,…,z:p_z$</p><h4 id="二元交叉熵解决sigmoid梯度饱和"><a href="#二元交叉熵解决sigmoid梯度饱和" class="headerlink" title="二元交叉熵解决sigmoid梯度饱和"></a>二元交叉熵解决sigmoid梯度饱和</h4><p>与$MSE$对比,对$w$链式求导时:</p><p>$MSE=(y_{true}-y)^2/2,\part MSE/\part w_i=(y-y_{true})\cdot \sigma’\cdot x_i$,结果是包含$\sigma’$,这就会导致梯度饱和；</p><p>$\part BCE/\part w_i=(y-y_{true})\cdot x_i$,二元交叉熵只包含$\sigma$(即$y$),不包含$\sigma’$,所以缓解了$sigmoid$的梯度饱和问题。</p><h4 id="Bootstrapped-Cross-Entropy-Loss"><a href="#Bootstrapped-Cross-Entropy-Loss" class="headerlink" title="Bootstrapped Cross Entropy Loss"></a>Bootstrapped Cross Entropy Loss</h4><h4 id="对数损失函数-负对数似然"><a href="#对数损失函数-负对数似然" class="headerlink" title="对数损失函数/负对数似然"></a>对数损失函数/负对数似然</h4><p><a href="https://blog.csdn.net/silver1225/article/details/88914652">负对数似然函数</a></p><p><strong>概率</strong>用于在已知参数的情况下，预测接下来的观测结果；<strong>似然性</strong>用于根据一些观测结果，估计给定模型的参数可能值。数值上相等，意义上不同</p><p>$L(\theta|x)=f_\theta(x)=P_\theta(X=x)=P(X=x|\theta)=P(X=x;\theta)$</p><p>负对数似然:$L(y)=-log(y)$ 负对数似然常和softmax结合使用,$0&lt;=P&lt;=1$</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201223183034910.png" alt="log"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201223183214140.png" alt="a>1,p"></p><p>我们希望得到的概率越大越好，因此概率越接近于1，则函数整体值越接近于0，即使得损失函数取到最小值。</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201125152106078.png" alt="对数损失函数" style="zoom:67%;"><img src="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20210920113522304.png" alt="image-20210920113522304" style="zoom: 67%;"></p><p>==这里的$P(y_i|x_i)$就是图片为$x_i$,预测出来正好就是正确的对应类别$y_i$的概率。只把这个预测正确的概率加起来。交叉熵损失除了$c_{ii}$为1，别的都为0，就只剩下了$-loga_{ii}$。综上两个都是只把类别预测正确的那个概率加起来了。所以是一致的。==</p><p>右边的关键的一步应该是应用大数定律</p><p><strong>结构风险本质</strong></p><p>结构风险是对经验风险模型复杂度的惩罚</p><p><strong>结构化风险（正则项）其实是加入了模型参数分布的先验知识</strong>，也就是贝叶斯学派为了将模型往人们期望的地方去发展，继而加入了先验分布，由于是人为的先验，因此也就是一个规则项（这也就是正则项名称的由来）。这样一来，风险函数将进一步考虑了被估计量的先验概率分布</p><p><strong>统计学习方法=模型+策略+算法</strong></p><p>由决策函数表示的模型为非概率模型，由条件概率表示的模型为概率模型。</p><p><strong>当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。</strong></p><p><strong>当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计</strong></p><h4 id="logsoftmax"><a href="#logsoftmax" class="headerlink" title="logsoftmax"></a>logsoftmax</h4><p>首先是softmax公式:<br>$$<br>softmax(x)=\frac{e^{x_i}}{\sum_je^{x_i}}<br>$$<br>求导如下:</p><img src="/wang-luo-jie-gou-jie-mi/image-20210601202104509.png" alt="image-20210601202104509" style="zoom: 25%;"><p>logsoftmax()就是在softmax()之后对结果再多一次log操作:<br>$$<br>softmax(x)=log\frac{e^{x_i}}{\sum_je^{x_i}}<br>$$<br>求导:</p><img src="/wang-luo-jie-gou-jie-mi/image-20210601202516351.png" alt="image-20210601202516351" style="zoom: 25%;"><p>为什么要加一次log呢?</p><p>log_softmax能够解决函数上溢出和下溢出的问题,加快运算速度,提高数据稳定性。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210601204136195.png" alt="image-20210601204136195" style="zoom: 50%;"><p>由于是指数操作,所以当输入比较大的时候,可能导致上溢出,为啥下溢出？</p><p>尽管在数学表示式上是对softmax取对数，但在实操中是通过:</p><img src="/wang-luo-jie-gou-jie-mi/image-20210601204816037.png" alt="image-20210601204816037" style="zoom: 50%;"><p>来实现，其中$M=max(x_i),i=1,2,3,…,n$,即$M$为所有$x_i$中最大的值。在加快运算速度的同时，可以保持数值的稳定性。</p><h4 id="L1和L2损失函数"><a href="#L1和L2损失函数" class="headerlink" title="L1和L2损失函数"></a>L1和L2损失函数</h4><p>L1,又称为LAD:<br>$$<br>\sum^n_{i=1}|y_i-f(x_i)|<br>$$<br>L2,又称为LSE：<br>$$<br>\sum^n_{i=1}(y_i-f(x_i))^2<br>$$<br>在大多数情况下,通常首选使用L2损失函数。但是L2收到离群点的影响更大(因为有了平方),所以当出现异常值的时候,L2损失函数的效果就不佳了</p><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><h4 id="Precision-amp-Recall"><a href="#Precision-amp-Recall" class="headerlink" title="Precision&amp;Recall"></a>Precision&amp;Recall</h4><img src="/wang-luo-jie-gou-jie-mi/image-20210707211929357.png" alt="image-20210707211929357" style="zoom: 50%;"><img src="/wang-luo-jie-gou-jie-mi/image-20210707211958788.png" alt="image-20210707211958788" style="zoom:67%;"><p>有时候查准率更重要，有时候查全率更重要，一般两者都是越高越好。</p><h3 id="机器学习问答"><a href="#机器学习问答" class="headerlink" title="机器学习问答"></a>机器学习问答</h3><h4 id="梯度下降or-求导-0"><a href="#梯度下降or-求导-0" class="headerlink" title="梯度下降or 求导=0"></a>梯度下降or 求导=0</h4><p>$$<br>\theta=(X^TX)^{-1}X^TY<br>$$</p><p><strong>使用梯度下降进行线性回归的主要原因是计算复杂性：在某些情况下，使用梯度下降找到解决方案的计算成本较低（较快）。</strong></p><p>$(X^TX)^{-1}X^T$是X 的伪逆</p><p>数据量很大的情况下求$X^TX$的逆是非常<strong>消耗内存以及时间</strong>的。</p><h4 id="梯度下降为什么不用二阶求导？"><a href="#梯度下降为什么不用二阶求导？" class="headerlink" title="梯度下降为什么不用二阶求导？"></a>梯度下降为什么不用二阶求导？</h4><p>a. 牛顿法使用的是目标函数的二阶导数，在高维情况下<strong>这个矩阵非常大，计算和存储都是问题。</strong></p><p>b. 在小批量的情况下，牛顿法对于<strong>二阶导数的估计噪声太大。</strong></p><p>c**.目标函数非凸的时候，牛顿法容易受到鞍点或者最大值点的吸引。**</p><p>最大的问题就是*<em>计算复杂度。二阶一次迭代更新的复杂度是n</em>n，这在高维的时候是不可行的**<br><strong>稳定性。越简单的东西往往越robust，对于优化算法也是这样。</strong><br><strong>二阶求导不易</strong><br>二阶方法能够更快地求得更高精度的解，<strong>但是在神经网络这类深层模型中，不高的精度对模型还有益处，能够提高模型的泛化能力。</strong></p><h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><h4 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h4><p><a href="https://zhuanlan.zhihu.com/p/29895933">指数加权平均</a></p><p>$v_t=\beta v_{t-1} + (1-\beta)\theta_t $</p><p><strong>指数式递减加权   优势</strong>:我们可以看到指数加权平均的求解过程实际上是一个递推的过程，那么这样就会有一个非常大的好处，每当我要求从0到某一时刻（n）的平均值的时候，我并不需要像普通求解平均值的作为，保留所有的时刻值，类和然后除以n。</p><p>而是只需要保留0-(n-1)时刻的平均值和n时刻的温度值即可。也就是每次只需要保留常数值，然后进行运算即可，这对于深度学习中的海量数据来说，是一个很好的<strong>减少内存和空间的做法</strong>。</p><p>$v_t=(1-\beta)\theta_t+(1-\beta)\beta\theta_{t-1}+(1-\beta)\beta^2\theta_{t-1}+(1-\beta)\beta^3\theta_{t-2}+…$</p><p>形成了指数衰减，所以越靠前的影响就越小，所以$\beta$越大，则相当于平均了过去越多天的信息。</p><p>偏差修正:比如$\beta=0.98$,那么$(1-\beta)\times\theta_t$远小于初始值，这会导致刚开始的时候有偏差，刚开始的平均值预测小于实际值，所以需要偏差修正，如下:</p><img src="/wang-luo-jie-gou-jie-mi/image-20210704131948198.png" alt="image-20210704131948198" style="zoom:67%;"><p>应得到绿色曲线，实际得到紫色曲线，偏差修正即$v_t$除以$1-\beta^t$，即$\frac{v_t}{1-\beta^t}$,初始时修正，随着t变大，分母趋向1.</p><p>当然很多人不用修正是因为只要熬过初始偏差就好，但是如果关心初始时期的正确率的话，就要用到偏差修正。</p><h4 id="动量梯度下降法"><a href="#动量梯度下降法" class="headerlink" title="动量梯度下降法"></a>动量梯度下降法</h4><p>momentum:动量</p><p>计算梯度的指数加权平均数，并利用该梯度更新你的权重</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20210705105913587.png" alt="image-20210705105913587"></p><p>On iteration t:</p><p>Compute $dw$,$db$ on current mini-batch.</p><p>$V_{dw}=\beta V_{dw}+(1-\beta)d_w$</p><p>$V_{db}=\beta V_{db}+(1-\beta)d_b$</p><p>$w=w-\alpha V_{dw}$</p><p>$b=b-\alpha V_{db}$</p><p>纵轴的摆动被平均了，而横轴一直向右。</p><p>我倾向于这么理解，$\beta$和$1-\beta$是质量，而$V_{dw}$和$d_w$是速度，$\beta V_{dw}$是原有动量，而$(1-\beta)d_w$是新加的动量，$\beta$越大，惯性越大，不同左右摇摆，而是垂直落入碗中。</p><h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>root mean square prop</p><p>指数加权平均平方的根</p><img src="/wang-luo-jie-gou-jie-mi/image-20210705112839995.png" alt="image-20210705112839995" style="zoom:80%;"><p>On iteration t:</p><p>Compute $d_w,d_b$ on current mini-batch:</p><p>$S_{dw}=\beta_2 S_{dw}+(1-\beta_2)d_w^2$</p><p>$S_{db}=\beta_2 S_{db}+(1-\beta_2)d_b^2$</p><p>$w=w-\alpha \frac{d_w}{\sqrt{S_{dw}+\epsilon}}$</p><p>$b=b-\alpha\frac{d_b}{\sqrt{S_{db}+\epsilon}}$</p><p>上下抖动大，除以大的变小；左右幅度小，除以小的变大。</p><h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p><em>Adaptive Momentum</em> Estimation</p><p>相当于将Momentum和RMSprop结合起来，具体公式如下:</p><p>$V_{dw}=0,S_{dw}=0,V_{db}=0,S_{db}=0$</p><p>On iteration t:</p><p>Compute $d_w,d_b$using current mini-batch</p><p>$V_{dw}=\beta_1V_{dw}+(1-\beta_1)d_w,V_{db}=\beta_1V_{db}+(1-\beta_1)d_b$</p><p>$S_{dw}=\beta_2S_{dw}+(1-\beta_2)d_w^2,S_{db}=\beta_2S_{db}+(1-\beta_2)d_b^2$</p><p>$V_{dw}^{corrected}=\frac{V_{dw}}{1-\beta_1^t},V_{db}^{corrected}=\frac{V_{db}}{1-\beta_1^t}$</p><p>$S_{dw}^{corrected}=\frac{S_{dw}}{1-\beta_2^t},S_{db}^{corrected}=\frac{S_{db}}{1-\beta_2^t}$<br>$$<br>w=w-\alpha\frac{V_{dw}^{corrected}}{\sqrt{S_{dw}}+\epsilon},b=b-\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}}+\epsilon}<br>$$<br>通常$\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$</p><h4 id="学习率设计"><a href="#学习率设计" class="headerlink" title="学习率设计"></a>学习率设计</h4><p><em>learing rate decay</em></p><p><strong>polynominal learning rate</strong></p><p>$base\cdot (1-\frac{iter}{max_iter})^{power}$</p><p> <a href="https://zhuanlan.zhihu.com/p/54756923">函数图像变换的知识点</a></p><img src="/wang-luo-jie-gou-jie-mi/image-20210704124810149.png" alt="image-20210704124810149" style="zoom:50%;"><p>呈多项式曲线下降</p><h4 id="weight-decay"><a href="#weight-decay" class="headerlink" title="weight decay"></a>weight decay</h4><p><em>Weight decay</em> (commonly called $L2$ regularization), might be the most widely-used technique for regularizing parametric machine learning models.</p><p>To penalize the size of the weight vector, we must somehow add $||W||$ to the loss function, but how should the model trade off the standard loss for this new additive penalty? In practice, we characterize this tradeoff via the <em>regularization constant</em>  λ, a non-negative hyperparameter that we fit using validation data:<br>$$<br>L(w,b)+\frac{\lambda}{2}||w||^2<br>$$</p><p>$$<br>\begin{split}\begin{aligned} w_1 &amp;\leftarrow \left(1- \eta\lambda \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\ w_2 &amp;\leftarrow \left(1- \eta\lambda \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}\end{split}<br>$$</p><p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。</p><h4 id="warm-up"><a href="#warm-up" class="headerlink" title="warm up"></a>warm up</h4><p>为什么warm up策略有效?</p><p><strong>warm up策略:在训练最初使用较小的学习率来启动，并很快切换到大学习率而后进行常见的 decay。</strong></p><p><a href="https://www.zhihu.com/question/338066667/answer/771252708">香侬科技的回答</a></p><p>这个问题目前还没有被充分证明，我们只能从直觉上和已有的一些论文[1,2,3]得到推测：</p><ul><li>有助于减缓模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳</li><li>有助于保持模型深层的稳定性</li></ul><p>刚开始模型对数据的“分布”理解为零，或者是说“均匀分布”（当然这取决于你的初始化）；在第一轮训练的时候，每个数据点对模型来说都是新的，<strong>模型会很快地进行数据分布修正，如果这时候学习率就很大，极有可能导致开始的时候就对该数据“过拟合”，后面要通过多轮训练才能拉回来</strong>，浪费时间。当训练了一段时间（比如两轮、三轮）后，模型已经对每个数据点看过几遍了，或者说对当前的batch而言有了一些正确的先验，较大的学习率就不那么容易会使模型学偏，所以可以适当调大学习率。这个过程就可以看做是warmup。那么为什么之后还要decay呢？<strong>当模型训到一定阶段后（比如十个epoch），模型的分布就已经比较固定了，或者说能学到的新东西就比较少了。如果还沿用较大的学习率，就会破坏这种稳定性</strong>，用我们通常的话说，就是<strong>已经接近loss的local optimal了</strong>，为了靠近这个point，我们就要慢慢来。</p><p><strong>mini-batch size较小，样本方差较大。</strong>在训练的过程中，如果有mini-batch内的数据分布方差特别大，这就会<strong>导致模型学习剧烈波动，使其学得的权重很不稳定，这在训练初期最为明显，最后期较为缓解</strong>(所以我们要对数据进行scale也是这个道理)。</p><h4 id="局部最优问题"><a href="#局部最优问题" class="headerlink" title="局部最优问题"></a>局部最优问题</h4><img src="/wang-luo-jie-gou-jie-mi/image-20210705143024838.png" alt="image-20210705143024838" style="zoom:67%;"><p>不太可能困在局部最优，而更有可能是鞍点。</p><p>鞍点会导致平滑段训练速度减慢，这也是优化算法的作用。</p><p><a href="https://zhuanlan.zhihu.com/p/84614490">Transformer中改变LayerNorm的位置甚至可以不同warm up</a></p><h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3><p><a href="https://www.cnblogs.com/shine-lee/p/12069176.html">这篇太优质了 ！！！！</a></p><p><strong>感受野是个相对概念，某层feature map上的元素看到前面不同层上的区域范围是不同的，通常在不特殊指定的情况下，感受野指的是看到输入图像上的区域。</strong></p><p>感受野大小<br>$$<br>r_l=r_{l-1}+(k_l-1)\cdot j_{l-1}\<br> =r_{l-1}+(k_l-1)\cdot \prod_{i=1}^{l-1} s_i<br>$$</p><p><strong>Layer l 一个元素的感受野rl等价于Layer l−1 上k×k 个感受野的叠加；</strong></p><p><strong>Layer l−1 上连续k 个元素的感受野可以看成是，第1个元素看到的感受野加上剩余k−1步扫过的范围</strong></p><img src="/wang-luo-jie-gou-jie-mi/image-20210312012129435.png" alt="感受野计算示例" style="zoom:67%;"><p>如上图,Conv1的感受野是3,Conv2的感受野是5</p><h3 id="所有的卷积"><a href="#所有的卷积" class="headerlink" title="所有的卷积"></a>所有的卷积</h3><p><a href="https://blog.csdn.net/tsyccnh/article/details/87357447">一篇非常好的教程关于卷积和转置卷积</a></p><p><a href="https://zhuanlan.zhihu.com/p/50573160">腾讯云教程卷积和转置卷积关系</a></p><p><a href="https://www.cnblogs.com/ywheunji/p/11887906.html">卷积核的参数量和计算量</a></p><h4 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h4><p><strong>卷积核</strong></p><ol><li><strong>卷积核默认第三维与输入图片的第三维（通道数）一样，并进行多层卷积，产生一个二维结果</strong></li><li><strong>1个卷积核产生一个二维结果，n个卷积核产生第三维为n的三维结果</strong></li><li><strong>卷积核n的个数就是输出的通道数</strong></li><li>卷积核个数通常为奇数</li></ol><p>为什么out_channel要大于in_channel?</p><p>Out_channels要能反映图片的多种特征,<strong>每个kernel对不同的特征有不同的敏感度</strong></p><p><strong>大的卷积核尺寸意味着更大的感受野</strong></p><p><a href="https://github.com/vdumoulin/conv_arithmetic">结合github的图演示看</a></p><p><strong>1 通用公式</strong><br>$$<br>o=\frac{i+2p-k}{s}+1 \<br>D2=K(filters_num)<br>$$</p><p><a href="https://zhuanlan.zhihu.com/p/512123584">https://zhuanlan.zhihu.com/p/512123584</a> 解释，注意o向下取整</p><p><strong>2 没有填充,单位步长</strong><br>$$<br>s=1,p=0,o=i-k+1<br>$$</p><p><strong>3有零填充,单位步长</strong></p><p><strong>3.1same padding</strong></p><p>$$<br>p=0,s=1\<br>o=i-k+1<br>$$<br><strong>3 Half/same padding</strong>         $p=\lfloor \frac{k}{2}\rfloor$<br>$$<br>k=2n+1,n\in N \<br>s=1,p=\lfloor \frac{k}{2}\rfloor=n\<br>o=i<br>$$</p><p><strong>3.2 Full padding</strong>            $p=k-1$<br>$$<br>p=k-1,s=1\<br>o=i+k-1<br>$$<br><strong>4 没有零填充,非单位步长</strong><br>$$<br>p=0,o=\lfloor\frac{i-k}{s}\rfloor+1<br>$$</p><p><strong>5 零填充,非单位步长</strong>     $odd$<br>$$<br>o=\lfloor \frac{i+2p-k}{s}\rfloor+1<br>$$<br><strong>填充方式</strong>有三种：full same  valid,一般为<strong>零填充</strong></p><p>我们平时<strong>所接触的卷积其实是滤波</strong>，不是<strong>真正的数学定义上的卷积</strong>。</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201209205446747.png" alt="步长为2的卷积"></p><p>步长为2的卷积可以<strong>看做步长为1的卷积结果的S=2的采样</strong>，因此可以认为是<strong>下采样</strong>的一种。</p><h4 id="卷积的性质"><a href="#卷积的性质" class="headerlink" title="卷积的性质"></a>卷积的性质</h4><p>普通卷积的计算看起来是挪动，其实不是挪动，效率太低。<strong>计算机会将卷积核转换成等效的矩阵，将输入转换为向量。通过输入向量和卷积核矩阵的相乘获得输出向量。输出的向量经过整形便可得到我们的二维输出特征。</strong></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201220214743847.png" alt="卷积示例"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201220214800536.png" alt="四个位置补0"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201220214820691.png" alt="拉长，然后就可以矩阵相乘"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201220214911375.png" alt="卷积核所展开的全连接层的权重矩阵"></p><p>在CNN提出之前，我们所提到的<strong>人工神经网络应该多数情况下都是前馈神经网络</strong>，两者区别主要在于<strong>CNN使用了卷积层，而前馈神经网络用的都是全连接层</strong>，而这<strong>两个layer的区别又在于全连接层认为上一层的所有节点下一层都是需要的，通过与权重矩阵相乘层层传递，而卷积层则认为上一层的有些节点下一层其实是不需要的，所以提出了卷积核矩阵的概念</strong>。</p><p>如果卷积核的大小是nxm，那么意味着该卷积核认为上一层节点每次映射到下一层节点都只有nxm个节点是有意义的。到这里，有些初学者会认为<strong>全连接层也可以做到，只要让权重矩阵某些权重赋值为0就可以实现了</strong>，例如假设在计算当前层第2个节点时认为上一层的第1个节点我不需要，那么设置w01=0就可以了。其实没错，<strong>卷积层是可以看做全连接层的一种特例,卷积核矩阵是可以展开为一个稀疏的包含很多0的全连接层的权重矩阵</strong>。</p><p><strong>卷积的key ideas:</strong></p><p><strong>局部连接</strong></p><p>卷积神经网络有两种神器可以降低参数数目，局部感知野和权值共享。先来说说局部感知也，一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。即，局部感受野指卷积层的神经元只和上一层map的局部相联系。</p><p><strong>权值共享</strong></p><p>==隐含的原理是：图像中的一部分的统计特性与其他部分是一样的。==</p><p><strong>平移不变形Translation Invariance</strong></p><p><a href="https://zhangting2020.github.io/2018/05/30/Transform-Invariance/">参考1</a> </p><p>不变形意味着即使目标的外观发生了某种变化,但是你依然可以把它识别出来。这对==图像分类来说是一种很好的特性，==因为我们希望图像中目标无论是被平移，被旋转，还是被缩放，甚至是不同的光照条件、视角，都可以被成功地识别出来。</p><p>还有各种不变形,如旋转/视角不变形 Ration/Viewpoint Invariance 尺度不变性Size Invariance 光照不变性illumination Invariance</p><p>==而对于其它问题，比如物体检测(detection)、物体分割(segmentation)来说，这个性质则不应该有，原因是当输入发生平移时，输出也应该相应地进行平移。这种性质又称为平移等价性(translation equivalence)。这两个概念是比较混淆的，但确实是两个不同的东西(敲黑板)。==</p><p><strong>图像在平移后再特征图上的表示也是同样平移的，这就使图像拥有了一定的平移不变性。</strong></p><p>但有人也提出了反驳:</p><blockquote><p>Why do deep convolutional networks generalize so poorly to small image transformations?</p></blockquote><p><a href="https://cloud.tencent.com/developer/article/1661000">看这篇讲解的最全面了</a></p><p>【<strong>平移不变性</strong>对应的有一个概念是<strong>平移同变性（translation equivariance）</strong>，这个是用在图像的目标检测中的，如果输入图像中的目标进行了平移，那么最终检测出来的候选框应该也相应的移动，这就是同时改变。】</p><p>==卷积到底有没有平移不变性？有的说有，有的说没有，我的观点是没有，但两个观点我都会讲明白==</p><p><strong>对于卷积，参数共享的特殊形式使得神经网络层具有==平移等变性(equivariance)==。例如，当处理时间序列数据时，这意味着通过卷积可以得到一个由输入中出现不同特征的时刻所组成的时间轴。如果我们把输入中的一个事件向后延时，在输出中仍然会有完全相同的表示，只是时间延后了。也就是说，卷积对输入数据的时刻是敏感的，在输出中有对应的表示。</strong></p><p>关于池化，无论采用<strong>何种池化函数</strong>，当<strong>输入作出少量平移</strong>时，池化能<strong>帮助输入的表示近似不变(invariant)**。对于平移的不变性是指当我们对输入进行少量平移时，经过</strong>池化函数后的大多数输出并不会发生改变**。这意味着==池化对特征位置不敏感，只有当我们不关心特征具体出现的位置时，池化才是合理的，这正是胶囊网络的动机之一。==</p><p>==总结来说，CNN 中的卷积操作具有平移等变性，但池化操作具有局部平移不变性。两者矛盾地统一于 CNN 中。胶囊网络完全去掉了池化操作，达到了对平移等变性的追求。==</p><p><strong>为什么说近似不变？</strong></p><p><img src="/wang-luo-jie-gou-jie-mi/gs3zt2oucw.gif" alt="微小平移"></p><p>平移小鸟位置的时候,预测结果是有很大的波动</p><img src="/wang-luo-jie-gou-jie-mi/image-20210419212506628.png" alt="shift:0" style="zoom: 67%;"><img src="/wang-luo-jie-gou-jie-mi/image-20210419212547579.png" alt="shift:1" style="zoom:67%;"><p>差距很大</p><p><strong>平移不变性的公式</strong><br>$$<br>U(D(X))=U(D(X_{shift}))<br>$$<br><img src="/wang-luo-jie-gou-jie-mi/image-20210419212739432.png" alt="shift:0" style="zoom:50%;"></p><img src="/wang-luo-jie-gou-jie-mi/image-20210419213109362.png" alt="shifit:1" style="zoom:50%;"><p>回不去了</p><p><strong>如何实现平移不变性:</strong></p><p>==全局平均池化,消除了位置的影响==</p><h4 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h4><p><a href="https://github.com/vdumoulin/conv_arithmetic">结合github的图演示看</a></p><p><strong>1 p=0,s=1</strong><br>$$<br>p=0,s=1\<br>k’=k,s’=s,p’=k-1\<br>o’=i’+(k-1)<br>$$<br><strong>2 s=1</strong><br>$$<br>s=1\<br>p’=k-p-1\<br>o’=i’+(k-1)-2p<br>$$<br><strong>3 Half/same padding</strong><br>$$<br>k=2n+1,n\in N,s=1,p=\lfloor\frac{k}{2}\rfloor=n\<br>k’=k,s’=s,p’=p\<br>o’=i’<br>$$<br><strong>4 Full padding</strong><br>$$<br>s=1,p=k-1\<br>k’=k,s’=s,p’=0\<br>o’=i’-(k-1)<br>$$<br><strong>5 p=0 ,non-unit strides</strong><br>$$<br>p=0,(i-k)%s=0\<br>k’=k,s’=1,p’=k-1\<br>o’=s(\widetilde{i}’-1)+k\<br>\widetilde{i}’ :adding,s-1,zeros,between,each,input,unit<br>$$<br><strong>6 zero-padding,non-unit strides</strong><br>$$<br>(i+2p-k)%s=0\<br>k’=k,s’=1,p’=k-p-1\<br>o’=s(i’-1)+k-2p<br>$$<br><strong>7 zero-padding,non-unit strides odd</strong><br>$$<br>k’=k,s’=1,p’=k-p-1\<br>a=(i+2p-k)%s\<br>o’=s(i’-1)+a+k-2p<br>$$<br>也叫反卷积/分数步长卷积</p><p>先说一下为什么人们很喜欢叫转置卷积为反卷积或逆卷积。首先举一个例子，将一个4x4的输入通过3x3的卷积核在进行普通卷积（无padding, stride=1），将得到一个2x2的输出。而转置卷积将一个2x2的输入通过同样3x3大小的卷积核将得到一个4x4的输出，看起来似乎是普通卷积的逆过程。就好像是加法的逆过程是减法，乘法的逆过程是除法一样，人们自然而然的认为这两个操作<strong>似乎是一个可逆的过程。但事实上两者并没有什么关系，操作的过程也不是可逆的。</strong></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201221160433547.png" alt="转置乘过来"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201221160511466.png" alt="列向量再转化成卷积核"></p><img src="/wang-luo-jie-gou-jie-mi/image-20201221160546890.png" alt="所有的" style="zoom:67%;"><img src="/wang-luo-jie-gou-jie-mi/image-20201221160601788.png" alt="看起来就像大的在小的上滑动" style="zoom:67%;"><p><img src="/wang-luo-jie-gou-jie-mi/image-20201221160626802.png" alt="如图"></p><p><strong>转置卷积公式</strong></p><h4 id="空洞-扩张卷积"><a href="#空洞-扩张卷积" class="headerlink" title="空洞/扩张卷积"></a>空洞/扩张卷积</h4><p><a href="https://blog.csdn.net/m0_37644085/article/details/89480326">空洞卷积总结</a></p><p><img src="/wang-luo-jie-gou-jie-mi/dilation.gif" alt="dilation"></p><img src="/wang-luo-jie-gou-jie-mi/image-20210311181332266.png" alt="空洞卷积增加感受野" style="zoom:67%;"><p>扩张卷积增加了感受野，而不增加核的大小,<strong>因为中间插入的是空格，需要训练的卷积核参数量是不变的</strong>。</p><p>卷积核大小为k,dilation_rate=d,扩张后的卷积大小如下:<br>$$<br>\hat{k} = k + (k − 1)(d − 1)<br>$$<br>则<br>$$<br>o = \lfloor\frac{i+2p-\hat{k}}{s}\rfloor+1<br>$$<br><strong>空洞卷积的作用</strong></p><ul><li><strong>不丢失分辨率的情况下扩大感受野</strong>：在deep net中为了增加感受野且降低计算量，总要进行降采样(pooling或s2/conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。</li><li><strong>调整扩张率获得多尺度上下文信息：</strong>空洞卷积有一个参数可以设置dilation rate，具体含义就是在卷积核中填充dilation rate-1个0，因此，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。<strong>多尺度信息在视觉任务中相当重要啊。</strong></li><li>ps: 空洞卷积虽然有这么多优点，但在实际中不好优化，速度会大大折扣。</li></ul><p><strong>空洞卷积gridding问题</strong></p><ul><li><strong>局部信息丢失</strong>：由于空洞卷积的计算方式类似于棋盘格式，某一层得到的卷积结果，来自上一层的独立的集合，没有相互依赖，因此该层的卷积结果之间没有相关性，即局部信息丢失。</li><li><strong>远距离获取的信息没有相关性</strong>：由于空洞卷积稀疏的采样输入信号，使得远距离卷积得到的信息之间没有相关性，影响分类结果。</li></ul><h4 id="1x1卷积-Network-in-Network"><a href="#1x1卷积-Network-in-Network" class="headerlink" title="1x1卷积/Network in Network"></a>1x1卷积/Network in Network</h4><p><a href="https://zhuanlan.zhihu.com/p/35814486">这篇写的好啊，是我太笨了，不带例子就老觉得读不透彻</a></p><p><strong>feature map是卷积核卷出来的，一个卷积核卷出来一个feature，所有feature构成featuremap，feature map数量就是channels</strong></p><p><strong>作用：</strong></p><ol><li><strong>进行卷积核通道数的降维和升维</strong></li><li><strong>实现跨通道的交互和信息整合</strong></li><li><strong>增加非线性特性</strong></li></ol><p><strong>Inception中：</strong></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201222151157720.png" alt="加入1x1"></p><p>原图feature map:28x28x192  1x1 channel:64 3x3 channel:128 5x5 channel:32</p><p>左:$192 × (1×1×64) +192 × (3×3×128) + 192 × (5×5×32) = 387072$ </p><p>右:$192 × (1×1×64) +（192×1×1×96+ 96 × 3×3×128）+（192×1×1×16+16×5×5×32）= 157184$</p><p>对于右边的池化层，原始的不能降channel，然后就会越来越多，这样也可以用1x1降channel</p><p><strong>ResNet：</strong></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201222154809733.png" alt="ResNet"></p><p>假设输入输出都是256 channel,参数真的差太多了！</p><p>左:$3 × 3 ×256×256×2=1179648$</p><p>右:$ 1×1×256×64 + 3×3×64×64 + 1×1×64×256 = 69632$</p><p><strong>全连接层角度：</strong></p><img src="/wang-luo-jie-gou-jie-mi/image-20201222155618108.png" alt="全连接层角度" style="zoom:67%;"><p>原channel 6,之后channel 5,只需要一个1x1x5的卷积核就行，参数很少，FC至少$w_0\cdot h_0 \cdot 6\cdot w_1\cdot h_1\cdot 5$</p><blockquote><p><em>In Convolutional Nets, there is no such thing as “fully-connected layers”. There are only convolution layers with 1x1 convolution kernels and a full connection table-Yann LeCun</em></p></blockquote><h4 id="Separable-Convlution"><a href="#Separable-Convlution" class="headerlink" title="Separable Convlution"></a>Separable Convlution</h4><p><a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728#5892">可分离卷积</a></p><p>two main types:<em>spatial separable conv</em> and <em>depthwise separable conv</em></p><p><strong>spatial separable conv</strong>:</p><p> illustrates the idea of separating one convolution into two well，but have significant limitations，so not heavily used</p><p>这么命名是因为主要处理spatial dimensions:the <strong>width</strong> and the <strong>height</strong>.The other dimension, the “<strong>depth</strong>” dimension, is the number of channels of each image</p><p>一个kernel分解成两个更小的kernel,eg:</p><p><img src="/wang-luo-jie-gou-jie-mi/%5C%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%A7%A3%E5%AF%86%5Cimage-20210706145221143.png" alt="image-20210706145221143"></p><img src="\网络结构解密\image-20210706145549709.png" alt="image-20210706145549709" style="zoom:67%;"><p>9次乘法下降到6次,网络运行速度更快。</p><p>主要的问题就是并不是所有的卷积核都可以分离，导致的结果就是所有可能分离的卷积中最终只有小部分可以分离。</p><p><strong>depthwise separable conv:</strong></p><p>更适用。This is the type of separable convolution seen in <code>keras.layers.SeparableConv2D</code> or <code>tf.layers.separable_conv2d</code>.</p><blockquote><p>The depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension — the number of channels — as well. An input image may have 3 channels: RGB. After a few convolutions, an image may have multiple channels. You can image each channel as a particular interpretation of that image; in for example, the “red” channel interprets the “redness” of each pixel, the “blue” channel interprets the “blueness” of each pixel, and the “green” channel interprets the “greenness” of each pixel. An image with 64 channels has 64 different interpretations of that image.</p></blockquote><p>和spatial separable conv类似,depthwise separable conv将一个kernel分成两个kernel做两次卷积:<em>the depthwise conv</em> and <em>the pointwise conv</em>.</p><p><em>depthwise conv</em>:</p><img src="\网络结构解密\image-20210706151630767.png" alt="image-20210706151630767" style="zoom:67%;"><p>For an image:12x12x3,to get an output with size 8x8x3.正常的卷积需要5x5x3x3,对于depthwise conv,则需要5x5x1x3,channel之间不交互。</p><p><em>Pointwise conv:</em></p><p>但是如果我们需要8x8x256,那么就需要提升channel了。</p><blockquote><p>The pointwise convolution is so named because it uses a 1x1 kernel, or a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has; in our case, 3. Therefore, we iterate a 1x1x3 kernel through our 8x8x3 image, to get a 8x8x1 image.We can create 256 1x1x3 kernels that output a 8x8x1 image each to get a final image of shape 8x8x256.</p></blockquote><p><strong>What’s the point of creating a depthwise separable convolution?</strong></p><p>参数量:$19200\to843$,乘法次数:$1228800\to53952$</p><h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p><strong>池化一般不涉及零填充</strong><br>$$<br>o=\lfloor\frac{i-k}{s}\rfloor+1<br>$$</p><p><strong>图像中的相邻像素倾向于具有相似的值，因此通常卷积层相邻的输出像素也具有相似的值</strong>。这意味着，卷积层输出中包含的大部分信息都是<strong>冗余</strong>的。</p><p>如果我们使用边缘检测滤波器并在某个位置找到强边缘，那么我们也可能会在距离这个像素1个偏移的位置找到相对较强的边缘。但是它们都一样是边缘，我们并没有找到任何新东西。</p><p>池化层解决了这个问题。这个网络层所做的就是通过<strong>减小输入的大小降低输出值的数量</strong>。</p><p>池化一般通过<strong>简单的最大值、最小值或平均值操作完成</strong>。</p><p><strong>最大池化可以提取特征纹理</strong></p><p><strong>平均池化可以保留背景信息。</strong></p><p><a href="https://firstai.blog.csdn.net/article/details/105299448">综述:最大池化，平均池化，全局最大池化和全局平均池化？区别原来是这样</a></p><h3 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h3><p><a href="https://blog.csdn.net/qq_34919792/article/details/102697817">上采样方法总结</a></p><p>三种方法：</p><ol><li>基于<strong>线性插值</strong>的上采样</li><li>基于<strong>深度学习</strong>的上采样(转置卷积)</li><li><strong>Unpooling</strong>的方法 简单的补0或扩充</li></ol><h4 id="线性插值"><a href="#线性插值" class="headerlink" title="线性插值"></a>线性插值</h4><p><strong>双线性插值？</strong></p><p>应用：</p><ol><li>对数据中的缺失进行合理补偿 </li><li>对数据进行放大或缩小</li><li>其他</li></ol><p><a href="https://www.cnblogs.com/linkr/p/3630902.html">通俗易懂啊</a></p><p><a href="https://www.cnblogs.com/yssongest/p/5303151.html">图像插值的举例说明</a></p><p>根据下式计算目标像素在源图像中的位置：<br>$$<br>srcX=dstX\cdot \frac{srcWidth}{dstWidth}\<br>srcY=dstY\cdot \frac{srcHeight}{dstHeight}<br>$$<br>单线性插值：单变量只有一个x，对y进行插值，线性插值原理是两个点可以确定一条直线，又可以根据x确定y<br>$$<br>\frac{y-y_0}{x-x_0}=\frac{y_1-y_0}{x_1-x_0}\<br>y=\frac{x_1-x}{x_1-x_0}y_0+\frac{x-x_0}{x_1-x_0}y_1\<br>y=k\cdot y0+(1-k)\cdot y_1 \quad \quad<br>$$</p><p>双线性插值：有两个变量x、y，分别在x、y两个方向进行插值</p><p>对于一个目的像素，设置坐标通过反向变换得到的浮点坐标为$(i+u,j+v)$(其中$i、j$均为浮点坐标的整数部分，$u、v$为浮点坐标的小数部分，是取值[0,1)区间的浮点数)，则这个像素得值$f(i+u,j+v)$可由原图像中坐标为$(i,j)(i+1,j)(i,j+1)(i+1,j+1)$所对应的周围四个像素的值决定，即:$f(i+u,j+v) = (1-u)(1-v)f(i,j) + (1-u)vf(i,j+1) + u(1-v)f(i+1,j) + uvf(i+1,j+1)$,<strong>在哪个方向离得近，比如距离是0.3，权重大，是0.7</strong></p><p>现在假如目标图的象素坐标为$(1,1)$，那么反推得到的对应于源图的坐标是$(0.75,0.75)$, 这其实只是一个概念上的虚拟象素,实际在源图中并不存在这样一个象素,那么目标图的象素$（1,1）$的取值不能够由这个虚拟象素来决定，而只能由源图的这四个象素共同决定：$(0,0)(0,1)(1，0)(1,1)$，而由于$(0.75,0.75)$离$(1,1)$要更近一些，那么$(1,1)$所起的决定作用更大一些，这从公式1中的系数$uv=0.75×0.75$就可以体现出来，而$(1,1)$离$(0.75,0.75)$最远，所以$(0,0)$所起的决定作用就要小一些，公式中系数为$(1-u)(1-v)=0.25×0.25$也体现出了这一特点。</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201120155633758.png" alt="很棒！"></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true">#矩阵运算最简单，所以最重要的是坐标如何对应！</span>a<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">0.25</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#注意2和3位置反过来了</span>            <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>c<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.25</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token punctuation">[</span><span class="token number">0.75</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>d<span class="token operator">=</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span>e<span class="token operator">=</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>d<span class="token punctuation">,</span>c<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> input <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> inputtensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># align_corners=False</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.2500</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1.5000</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">2.2500</span><span class="token punctuation">,</span>  <span class="token number">2.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">2.5000</span><span class="token punctuation">,</span>  <span class="token number">2.7500</span><span class="token punctuation">,</span>  <span class="token number">3.2500</span><span class="token punctuation">,</span>  <span class="token number">3.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.2500</span><span class="token punctuation">,</span>  <span class="token number">3.7500</span><span class="token punctuation">,</span>  <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.3333</span><span class="token punctuation">,</span>  <span class="token number">1.6667</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1.6667</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">,</span>  <span class="token number">2.3333</span><span class="token punctuation">,</span>  <span class="token number">2.6667</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">2.3333</span><span class="token punctuation">,</span>  <span class="token number">2.6667</span><span class="token punctuation">,</span>  <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.3333</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.3333</span><span class="token punctuation">,</span>  <span class="token number">3.6667</span><span class="token punctuation">,</span>  <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span></code></pre><p><img src="/wang-luo-jie-gou-jie-mi/image-20201222100731710.png" alt="两种方式"></p><p>第一种，四个角对齐，均匀插值,公式：<br>$$<br>scale = (input_size-1)/(output_size-1)\<br>srcIndex=scale\cdot dstIndex<br>$$<br>第二种，不均匀，出界的只和有原坐标的算，公式：<br>$$<br>scale=input_size/output_size\<br>srcIndex=scale\cdot (dstIndex+0.5)-0.5<br>$$</p><h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><p><strong>PixelShuffle</strong></p><h3 id="Normalization-1"><a href="#Normalization-1" class="headerlink" title="Normalization"></a>Normalization</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p><a href="https://blog.csdn.net/qq_25737169/article/details/79048516">BN教程</a></p><p><a href="https://www.cnblogs.com/shine-lee/p/11989612.html">BN教程2</a></p><p>直译过来就是批归一化。</p><p>激活之前进行BN，增加了两个学习参数scale和shift，再从线性变到非线性</p><p>有轻微的正则化作用，因为使用mint-batch，不同的mini-bath不同，所以会有一些噪音，所以有轻微的正则化效果</p><p>测试时逐样本处理，batch norm从之前训练时的$\mu$和$\sigma$做指数加权平均，方式很多，反正是从训练集得到的</p><h5 id="第一节：Batchnorm主要解决的问题"><a href="#第一节：Batchnorm主要解决的问题" class="headerlink" title="第一节：Batchnorm主要解决的问题"></a>第一节：Batchnorm主要解决的问题</h5><p>深度神经网络主要就是为了学习训练数据的分布，并在测试集上达到很好的泛化效果。但是：</p><ol><li>如果我们<strong>每一个batch输入的数据都具有不同的分布</strong>，显然会给网络的训练带来困难</li><li><strong>数据经过一层层网络计算后，其数据分布也在发生着变化</strong>，此现象称为Internal Covariate Shift.</li></ol><p><strong>1.1 Internal Covariate Shift</strong></p><p>首先<strong>Internal Covariate Shift</strong>是指训练深度网络的时候经常发生训练困难的问题，因为，<strong>每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难（神经网络本来就是要学习数据的分布，要是分布一直在变，学习就很难了）</strong></p><p><strong>1.2 Covariate Shift</strong></p><p><strong>Internal Covariate Shift和Covariate Shift</strong>是不同的，差就差在这个Internal,<strong>一个发生在神经网络内部，一个发生在输入数据上</strong><br>==BatchNorm是归一化的一种手段，减小图像之间的绝对差异，突出相对差异==</p><p>举个例子，如下图所示：</p><img src="/wang-luo-jie-gou-jie-mi/image-20210510212843757.png" alt="image-20210510212843757" style="zoom:50%;"><p>假定当前输入$x1$和$x2$的分布如图中圆点所示，本次更新的方向是将直线$H_1$更新成$H_2$，本以为切分得不错，但是当前面层的权重更新完毕，当前层输入的分布换成了另外一番样子，直线相对输入分布的位置可能变成了$H_3$，下一次更新又要根据新的分布重新调整。<strong>直线调整了位置，输入分布又在发生变化，直线再调整位置，</strong>==就像是直线和分布之间的“追逐游戏”。==<strong>对于浅层模型，比如SVM，输入特征的分布是固定的，即使拆分成不同的batch，每个batch的统计特性也是相近的，因此只需调整直线位置来适应输入分布，显然要容易得多。而深层模型，每层输入的分布和权重在同时变化，训练相对困难。</strong></p><h5 id="第二节：Batchnorm-原理解读"><a href="#第二节：Batchnorm-原理解读" class="headerlink" title="第二节：Batchnorm 原理解读"></a>第二节：Batchnorm 原理解读</h5><p>为了减小Internal Covariate Shift，对神经网络的每一层做归一化不就可以了，假设将每一层输出后的数据都归一化到0均值，1方差，满足正态分布，但是，此时有一个问题，<strong>每一层的数据分布都是标准正态分布，导致其完全学习不到输入数据的特征，</strong>因为，<strong>费劲心思学习到的特征分布被归一化</strong>了，因此，直接对每一层做归一化显然是不合理的。但如果加上可学习的参数就好多了。</p><p>如果batch size为$m$，则在前向传播过程中，网络中每个节点都有$m$个输出，所谓的Batch Normalization，就是对该层每个节点的这$m$个输出进行归一化再输出，具体计算方式如下：</p><img src="/wang-luo-jie-gou-jie-mi/image-20210510213910551.png" alt="image-20210510213910551" style="zoom:50%;"><p><strong>$\gamma$和$\beta$为待学习的scale和shift参数，用于控制$y_i$的方差和均值。</strong>==这样的话就是具有不同均值和方差的正态分布了，有利于权重和分布的相互协调==</p><h5 id="第三节：BatchNorm实现"><a href="#第三节：BatchNorm实现" class="headerlink" title="第三节：BatchNorm实现"></a>第三节：BatchNorm实现</h5><h5 id="第四节：Batchnorm的优点"><a href="#第四节：Batchnorm的优点" class="headerlink" title="第四节：Batchnorm的优点"></a>第四节：Batchnorm的优点</h5><ul><li><strong>可以使用更大的学习率</strong>，训练过程更加稳定，极大提高了训练速度。</li><li><strong>可以将bias置为0</strong>，因为Batch Normalization的Standardization过程会移除直流分量，所以不再需要bias。</li><li><strong>权重初始化不再敏感</strong>，通常权重采样自0均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了Batch Normalization后，对与同一个输出节点相连的权重进行放缩，其标准差σ也会放缩同样的倍数，相除抵消。</li><li><strong>对权重的尺度不再敏感</strong>，理由同上，尺度统一由γ参数控制，在训练中决定。</li><li><strong>深层网络可以使用sigmoid和tanh了</strong>，理由同上，BN抑制了梯度消失。</li><li><strong>Batch Normalization具有某种正则作用，不需要太依赖dropout，减少过拟合</strong>。</li></ul><h5 id="什么时候不能用BN？"><a href="#什么时候不能用BN？" class="headerlink" title="什么时候不能用BN？"></a>什么时候不能用BN？</h5><p><img src="/wang-luo-jie-gou-jie-mi/image-20210510215346460.png" alt="image-20210510215346460"></p><p>![image-20230903160740141](/Users/dongchenghao/Library/Application Support/typora-user-images/image-20230903160740141.png)</p><p>==BN是保留Channel，在N,H,W上做操作==</p><p>肯定是batch size越大，使用BN的效果越好。为什么BN不做channel只做NHW，可能是因为每个channel都对应之前一个卷积核，逐channel代表逐卷积核的输出做归一化。</p><h4 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h4><p><a href="https://zhuanlan.zhihu.com/p/35005794">吴育昕 何恺明的GN教程</a></p><ol><li>BN依赖于batch size，batch size较小时，BN效果不好，有些任务往往batch size只有1-2</li><li>训练，验证，测试这三个阶段存在inconsistency。</li></ol><p>Layer Norm和Instance Norm就是Group Norm的特例，归一化避开了batch。</p><p>为什么工作？个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。</p><h4 id="Instance-Norm"><a href="#Instance-Norm" class="headerlink" title="Instance Norm"></a>Instance Norm</h4><p>Instance Normalization(IN),一种更适合对单个像素有更高要求的场景的归一化算法(IST,GAN等)。IN的算法非常简单,计算归一化统计量时考虑单个样本,单个通道的所有元素。</p><p>对于图像风格迁移任务来说,每个样本的每个信息点都是非常重要的。对于BN对整个batch做归一化,可能造成每个样本独特细节的丢失;对于LN,忽略了不同通道之间的差异。</p><h4 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h4><img src="/wang-luo-jie-gou-jie-mi/image-20220901105559979.png" alt="image-20220901105559979" style="zoom:67%;"><p><strong>均值为0,方差为1</strong>,只对最后的C进行归一化</p><h3 id="网络权重初始化"><a href="#网络权重初始化" class="headerlink" title="网络权重初始化"></a>网络权重初始化</h3><p><a href="https://www.cnblogs.com/shine-lee/p/11908610.html">权重初始化方法总结</a></p><h3 id="如何判断模型收敛"><a href="#如何判断模型收敛" class="headerlink" title="如何判断模型收敛"></a>如何判断模型收敛</h3><h3 id="特征融合"><a href="#特征融合" class="headerlink" title="特征融合"></a>特征融合</h3><p><a href="https://zhuanlan.zhihu.com/p/141685352">知乎教程</a></p><p><a href="https://blog.csdn.net/xys430381_1/article/details/88370733">CSDN类似的教程</a></p><p>很多工作通过融合多层来提升检测和分割的性能，按照融合与预测的先后顺序，分类为<strong>早融合(Early fusion)和晚融合(Late fusion)。</strong></p><h4 id="Early-fusion"><a href="#Early-fusion" class="headerlink" title="Early fusion"></a>Early fusion</h4><p>先融合多层的特征，然后在融合后的特征上训练预测器(<strong>只在完全融合之后，才统一进行检测</strong>)。这类方法也被称为<strong>skip connection，即采用concat、add操作</strong>。这一思路的代表是Inside-Outside Net(ION)和HyperNet。</p><ol><li>concat</li><li>add</li></ol><h4 id="Late-fusion"><a href="#Late-fusion" class="headerlink" title="Late fusion"></a>Late fusion</h4><p>同过<strong>结合不同层的检测结果</strong>改进检测性能(<strong>尚未完成最终的融合之前，在部分融合的层上就开始进行检测，会有多层的检测，最终将多个检测结果进行融合</strong>)。</p><p>代表思路有两种:</p><ol><li><strong>feature不融合，多尺度的feture分别进行预测，然后对预测结果进行综合，</strong>如Single Shot MultiBox Detector (SSD) , Multi-scale CNN(MS-CNN)</li><li><strong>feature进行金字塔融合，融合后进行预测</strong>，如Feature Pyramid Network(<strong>FPN</strong>)等。</li></ol><p>接下来主要归纳晚融合方法</p><h3 id="多尺度设计"><a href="#多尺度设计" class="headerlink" title="多尺度设计"></a>多尺度设计</h3><p><a href="https://www.jianshu.com/p/57cfa4fdd423?from=timeline">AI不惑境教程</a></p><h3 id="判断函数是否收敛？"><a href="#判断函数是否收敛？" class="headerlink" title="判断函数是否收敛？"></a>判断函数是否收敛？</h3><p>两个算法,batch_size为4,epoch为30和batch_size为8,epoch为60,不同的算法是否有对应的大小关系呢?</p><p>主要是看两个，第一个epoch多少是否有影响，<strong>要看函数是否收敛？</strong></p><p>第二个batch_size不同,要看<strong>是否能收敛到最小点</strong>,batch_size小<strong>容易收敛到鞍点</strong>,大的话容易收敛到最小点。</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>back propagation<img src="/wang-luo-jie-gou-jie-mi/new1\jianguoyun_posts\网络结构解密\image-20210707214021643.png" alt="image-20210707214021643">传播,扩展</p><h4 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h4><img src="\网络结构解密\image-20210707221902911.png" alt="image-20210707221902911" style="zoom:67%;"><p>$x_1,x_2$是一个独立样本的两个维度,如果样本的维度是15维，那么MLP输入层的神经元就有15个。</p><p><strong>链式法则-BP的基础</strong></p><p>假设我们现在更新$w_1$,那么就要求$\frac{\partial E}{\partial w_1}$<br>$$<br>\frac{\partial E}{\partial w_1}=\frac{\partial E}{\partial y}\cdot \frac{\partial y}{\partial h_1}\cdot \frac{\partial h_1}{\partial w_1}<br>$$<br>最开始是损失函数,做分子的只能是神经元的值,参数只能做分母.</p><p>这里面还有激活函数,如果$y=\sigma(w_1x+b_1)$直接展开,那么就是$\frac{\partial y}{\partial w_1}$,如果看成是两步$y=\sigma(a),a=w_1x+b_1$,则为$\frac{\partial y}{\partial a}\cdot\frac{\partial a}{\partial w_1}$根据链式法则,算出来是一样的。</p><p>例子:$y=3a^2+1,a=3x+1$,算出来都是$54x+18$(如果是对$x$求偏导)</p><p><strong>为什么现在是对$w$和$b$求导？</strong></p><p>因为现在$x$和$y$数据已知,就像极大似然估计一样,现在是把$w$和$b$看做参数来反推$w$和$b$,所以需要对$w$和$b$求导。</p><h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><h4 id="池化-1"><a href="#池化-1" class="headerlink" title="池化"></a>池化</h4><p>池化层一般对应在卷积层的后面，属于一对一的关系（它只影响当前深度的一个节点），不与其它卷积核做连接，所以没有权重参数需要学习。所以反向传播的时候，只需对输入参数求导，不需要进行权值更新。</p><p>对于最大池化和平均池化，也有不同:<a href="https://blog.csdn.net/kieven2008/article/details/81603994">教程</a></p><img src="/wang-luo-jie-gou-jie-mi/image-20210303003004886.png" alt="最大池化" style="zoom:50%;"><img src="/wang-luo-jie-gou-jie-mi/image-20210303003024968.png" alt="平均池化" style="zoom:50%;"><h3 id="Self-attention与Transformer"><a href="#Self-attention与Transformer" class="headerlink" title="Self attention与Transformer"></a>Self attention与Transformer</h3><p><a href="https://zhuanlan.zhihu.com/p/47282410">知乎教程</a></p><p><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p><p>直接看上面这篇文章</p><p><a href="https://blog.csdn.net/yujianmin1990/article/details/85221271">The Illustrated Transformer的翻译参考</a></p><h4 id="A-High-level-Lokok"><a href="#A-High-level-Lokok" class="headerlink" title="A High-level Lokok"></a>A High-level Lokok</h4><p>Transformer中的编码器是完全结构相同的,但是并不共享参数，每一个编码器都可以拆解成以下两个子部分:</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426100431432.png" alt="Encoder" style="zoom:80%;"><p>解码器同样也有这些子层，但是在两个子层间增加了attention层，该层有助于解码器能够关注到输入句子的相关部分，与 <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">seq2seq model</a> 的Attention作用相似。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426100534638.png" alt="image-20210426100534638" style="zoom:80%;"><h4 id="Bringing-The-Tensors-Into-the-Picture"><a href="#Bringing-The-Tensors-Into-the-Picture" class="headerlink" title="Bringing The Tensors Into the Picture"></a>Bringing The Tensors Into the Picture</h4><p>正如NLP应用的常见例子，先将输入单词使用<a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">embedding algorithm</a>转成向量。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426100735512.png" alt="image-20210426100735512" style="zoom:80%;"><p>词的向量化仅仅发生在最底层的编码器的输入时，<strong>这样每个编码器的都会接收到一个list（每个元素都是512维的词向量）</strong>，只不过其他编码器的输入是前个编码器的输出。list的尺寸是可以设置的超参，==通常是训练集的最长句子的长度==。</p><p>在对输入序列做词的向量化之后，它们流经编码器的如下两个子层:</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426100904390.png" alt="image-20210426100904390" style="zoom:80%;"><p>这里能看到Transformer的一个关键特性，==每个位置的词仅仅流过它自己的编码器路径。在self-attention层中，这些路径两两之间是相互依赖的。前向网络层则没有这些依赖性，但这些路径在流经前向网络时可以并行执行。==</p><h4 id="Now-We’re-Encoding"><a href="#Now-We’re-Encoding" class="headerlink" title="Now We’re Encoding !"></a>Now We’re Encoding !</h4><p>正如之前所提，编码器接收向量的list作输入。然后将其送入self-attention处理，再之后送入前向网络，最后将输入传入下一个编码器。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426101726843.png" alt="image-20210426101726843" style="zoom:80%;"><p>每个位置的词向量被送入self-attention模块，然后是前向网络(对每个向量都是完全相同的网络结构)。</p><h4 id="Self-Attention-at-a-High-Level"><a href="#Self-Attention-at-a-High-Level" class="headerlink" title="Self-Attention at a High Level"></a>Self-Attention at a High Level</h4><p>不要被self-attention这个词迷惑了，看起来好像每个人对它都很熟悉，但是在我读到Attention is All You Need这篇文章之前，我个人都没弄懂这个概念。下面我们逐步分解下它是如何工作的。</p><p>以下面这句话为例，作为我们想要翻译的输入语句“The animal didn’t cross the street because it was too tired”。句子中”it”指的是什么呢？“it”指的是”street” 还是“animal”？对人来说很简单的问题，但是对算法而言并不简单。</p><p>当模型处理单词“it”时，self-attention允许将“it”和“animal”联系起来。==当模型处理每个位置的词时，self-attention允许模型看到句子的其他位置信息作辅助线索来更好地编码当前词。==如果你对RNN熟悉，就能想到RNN的隐状态是如何允许之前的词向量来解释合成当前词的解释向量。Transformer使用self-attention来将相关词的理解编码到当前词中。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210420102549063.png" alt="Scaled Dot-Product Attention" style="zoom:67%;"><h4 id="Self-Attention-in-Detail"><a href="#Self-Attention-in-Detail" class="headerlink" title="Self-Attention in Detail"></a>Self-Attention in Detail</h4><p>我们先看下如何计算self-attention的向量，再看下如何以矩阵方式计算。<br><strong>第一步</strong>，==根据编码器的输入向量，生成三个向量==，比如，对每个词向量，生成$query$, $key$, $value$三个vector</p><p>生成方法为分别乘以三个矩阵，这些<strong>矩阵在训练过程中需要学习</strong>。【注意：==不是每个词向量独享3个matrix，而是所有输入共享3个转换矩阵；==<strong>权重矩阵是基于输入位置的转换矩阵</strong>；有个可以尝试的点，如果每个词独享一个转换矩阵，会不会效果更厉害呢？】<br>注意到这些新向量的维度比输入词向量的维度要小（512–&gt;64），并不是必须要小的，是为了让多头attention的计算更稳定。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426104949349.png" alt="image-20210426104949349" style="zoom: 67%;"><p><strong>第二步</strong>，计算attention就是计算一个分值。对“Thinking Matchines”这句话，对“Thinking”（pos#1）计算attention 分值。我们需要计算每个词与“Thinking”的评估分，这个分决定着编码“Thinking”时（某个固定位置时），每个输入词需要集中多少关注度。<br>==这个分，通过“Thing”对应query-vector与所有词的key-vec依次做点积得到。==所以当我们处理位置#1时，第一个分值是q1和k1的点积，第二个分值是q1和k2的点积。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426105236082.png" alt="image-20210426105236082" style="zoom:67%;"><p><strong>每个是一个Vector,可以用矩阵加速计算</strong></p><p><strong>第三步和第四步</strong>，除以8($d_k$,为query和key向量的维度)，这样梯度会更稳定。然后加上softmax操作，<strong>归一化分值使得全为正数且加和为1。</strong></p><img src="/wang-luo-jie-gou-jie-mi/image-20210426145803111.png" alt="image-20210426145803111" style="zoom: 67%;"><p>softmax分值决定着在这个位置,每个词的表达程度(关注度)。<strong>很明显,这个位置的词应该有最高的归一化分数,但大部分时候总是有助于关注该词的相关的词。</strong></p><p><strong>第五步</strong>，将softmax分值与value-vec按位相乘。==保留关注词的value值，削弱非相关词的value值。==</p><p><strong>第六步</strong>，将所有加权向量加和，产生该位置的self-attention的输出结果。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426145954742.png" alt="image-20210426145954742" style="zoom:67%;"><p>上述就是self-attention的计算过程，生成的向量流入前向网络。在实际应用中，上述计算是以速度更快的矩阵形式进行的。下面我们看下在单词级别的矩阵计算。</p><h4 id="Matrix-Calculation-of-Self-Attention"><a href="#Matrix-Calculation-of-Self-Attention" class="headerlink" title="Matrix Calculation of Self-Attention"></a>Matrix Calculation of Self-Attention</h4><p>第一步是计算Query,Key和Value矩阵。 为此，我们将embeddings堆叠成矩阵X，然后将其乘以我们训练过的权重矩阵（WQ，WK，WV）。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426151301289.png" alt="image-20210426151301289" style="zoom:67%;"><p><strong>X矩阵中的每一行对应于输入句子中的一个单词。</strong></p><p>最后，由于我们要处理矩阵，因此我们可以将步骤2到6压缩成一个公式，以计算self attention layer的输出。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426151533778.png" alt="image-20210426151533778" style="zoom:67%;"><h4 id="The-Beast-With-Many-Heads"><a href="#The-Beast-With-Many-Heads" class="headerlink" title="The Beast With Many Heads"></a>The Beast With Many Heads</h4><p>本文通过添加一种称为“多头”注意力的机制，进一步完善了self attention layer。 这样可以通过两种方式提高attention layer的性能：</p><ol><li>它==扩展了模型专注于不同位置的能力==。在上面的例子中，==z1只包含了其他所有encoding的一点点，但是它很可能由实际该单词本身主导==。如果我们要翻译这样的句子: “The animal didn’t cross the street because it was too tired”,那么我们会想知道”it”指什么,多头注意力很有用。</li><li>它为关注层提供了多个“表示子空间”。正如我们接下来要看到的，在multi-headed attention下有多组Query/Key/Value的权重matrix,而非仅仅一组（论文中使用8-heads）。每一组都是随机初始化，==然后，在训练之后，将每组用于将input embeddings(或 vectors from lower encoders/decoders)投影到不同的表示子空间representation subspace中。==</li></ol><img src="/wang-luo-jie-gou-jie-mi/image-20210426153133207.png" alt="image-20210426153133207" style="zoom:67%;"><p>为每个头维护单独的$W_Q$,$W_K$和$W_V$,与$X$乘完之后获得$Q$,$K$,$V$</p><p>如果我们执行与上面的概述相同的self attention计算，我们最终将得到八个不同的$Z$矩阵</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426153614905.png" alt="image-20210426153614905" style="zoom:67%;"><p>这给我们带来了一些挑战。==前馈层不希望有8个矩阵,它只要一个矩阵(a vector for each word)==。因此,我们需要一种将这8个矩阵压缩为1个矩阵的方法。</p><p>==我们该怎么做?concat 然后乘以额外的权重矩阵==$W_O$</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426153909094.png" alt="image-20210426153909094" style="zoom:67%;"><p>这就是multi-headed self attention的全部。 我知道，矩阵很多。 让我尝试将它们全都放在一个视觉中，以便我们可以在一处查看它们。</p><img src="/wang-luo-jie-gou-jie-mi/image-20210426161648133.png" alt="image-20210426161648133" style="zoom:80%;"><h4 id="Representing-The-Order-of-The-Sequence-Using-Positional-Encoding"><a href="#Representing-The-Order-of-The-Sequence-Using-Positional-Encoding" class="headerlink" title="Representing The Order of The Sequence Using Positional Encoding"></a>Representing The Order of The Sequence Using Positional Encoding</h4><h3 id="扩散模型"><a href="#扩散模型" class="headerlink" title="扩散模型"></a>扩散模型</h3><p>参考:</p><p><a href="https://segmentfault.com/a/1190000043744225">https://segmentfault.com/a/1190000043744225</a></p><p><a href="https://zhuanlan.zhihu.com/p/563661713">https://zhuanlan.zhihu.com/p/563661713</a></p><h4 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h4><p>马尔科夫链定义本身比较简单，它假设某一时刻状态转移的概率只依赖于它的前一个状态。</p><p><strong>$p(z^{m+1}|z^{1},…,z^{m})=p(z^{m+1}|z^{m})$</strong></p><p>具有马尔可夫性的随机序列$X={X_0,X_1,…,X_T,… }$称为<strong>马尔可夫链</strong>，或马尔可夫过程。</p><h5 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h5><p><strong>扩散过程</strong></p><p>总共包含$T$ 步的扩散过程的每一步都是对上一步得到的数据$x_{t-1}$按如下方式增加高斯噪音<br>$$<br>q(x_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t I)<br>$$<br>这边这个分号前的$x_t$应该是意味着是关于$x_t$，后面两个分别是均值和方差。${\beta_t}^{T}_{t=1}$是每一步所采用的方差,介于0-1之间。</p><p>对于扩散模型，我们往往称不同step的方差设定为<strong>variance schedule</strong>或者<strong>noise schedule</strong>，通常情况下，越后面的step会采用更大的方差，即满足$\beta_1&lt;\beta_2&lt;…&lt;\beta_T$。</p><p>在一个设计好的<strong>variance schedule</strong>下，的如果扩散步数$T$足够大，那么最终得到的$X_T$就完全丢失了原始数据而变成了一个随机噪音。 扩散过程的每一步都生成一个带噪音的数据$x_t$，整个扩散过程也就是一个<strong>马尔可夫链</strong>：<br>$$<br>q(x_{1:T}|x_0)=\prod_{t=1}^Tq(x_t|x_{t-1})<br>$$<br>$q(x_2|x_1)\times q(x_1|x_0) = q(x_2|x_0) $,利用条件概率和马尔克夫的定义严格证明我还不会，但是对当前这个条件，$q(x_t|x_{t-1})= N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)$,当前这个确实满足啊~</p><p>上面好像是错误的,$x_{1:T}$好像是代表一个联合分布</p><p>条件概率的一般形式: </p><p>$1:P(A,B,C)=P(C|BA)P(B,A)=P(C|BA)P(B|A)P(A)$</p><p>$2: P(B,C|A)=P(B|A)P(C|A,B)$ </p><p>推导:$P(BC|A)=P(ABC)/P(A)=P(C|AB)P(AB)/P(A)=P(C|AB)\times P(B|A) $</p><p>马尔可夫概率的一般形式:</p><p>$1:P(A,B,C)=P(C|BA)P(B,A)=P(C|B)P(B|A)P(A)$</p><p>$2: P(B,C|A)=P(B|A)P(C|B)$</p><p>扩散过程的一个重要特性是我们可以<strong>直接基于原始数据$x_0$来对任意$t$步的$x_t$进行采样。</strong></p><img src="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20230608163339800.png" alt="image-20230608163339800" style="zoom:80%;"><p>从前一步采样，如何从$x_{t-1}$直接算出来$x_t$呢,这一步公式是关键,$x_t=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon$,相当于对原图像乘以一个值,然后加上一个正态分布,其实相当于对$x_{t-1}$逐点进行高斯采样，采样就是以当前点的值为均值，方差为$\beta_t$,即$x_t\sim N(\sqrt{1-\beta_t}x_{t-1},\beta_tI)$.</p><p>$\sqrt{\bar{a}_t}$和$\sqrt{1-\bar{a}_t}$分别称为<strong>signal_rate</strong>和<strong>noise_rate</strong>,随着$\beta$增大，分别趋向0和1</p><p><strong>反向过程</strong></p><p>去噪声操作的数学形式是怎么样的？怎么让神经网络来学习它呢？数学原理表明，当$\beta_t$足够小时，每一步加噪声的逆操作也满足正态分布。<br>$$<br>x_{t-1} \sim N(\tilde{\mu_t},\tilde{\beta_t}I)<br>$$<br>为了描述所有去噪声操作，<strong>神经网络</strong>应该根据当前的时刻$t$、当前的图像$x_t$，拟合当前时刻的加噪声逆操作的正态分布，也就是拟合当前的均值$\tilde{\mu}_t$和方差$\tilde{\beta_t}$。</p><blockquote><p>加噪声的逆操作不太可能从理论上求得，我们只能用一个神经网络去拟合它。去噪声操作和加噪声逆操作的关系，就是神经网络的预测值和真值的关系。</p></blockquote><p>$$<br>q(x_{t-1}|x_t,x_0)=q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}<br>$$</p><p>推导:<br>$$<br>q(x_{t-1}|x_t,x_0)=\frac{q(x_{t-1},x_t,x_0)}{q(x_t,x_0)}\<br>=\frac{q(x_t|x_{t-1},x_0)\times q(x_{t-1}|x_{0})\times q(x_0)}{q(x_t,x_0)}\<br>=q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}<br>$$<br>原来如此~</p><img src="/wang-luo-jie-gou-jie-mi/网络结构解密\image-20230608203301129.png" alt="image-20230608203301129" style="zoom:80%;"><p>神经网络拟合均值时，$x_t$是已知的（别忘了，图像是一步一步倒着去噪的）。式子里唯一不确定的只有$\epsilon$,所以干脆拟合噪声，所以最终噪声的误差函数可写成:<br>$$<br>L=||\epsilon_t-\epsilon_\theta(x_t,t)||^2<br>$$</p><h4 id="DDIM"><a href="#DDIM" class="headerlink" title="DDIM"></a>DDIM</h4><p>看了半天看不懂,简单总结</p><p>找到了一种能满足DDPM逆向条件，且能减少采样步骤的逆向公式：<br>$$<br>q_{\sigma}(x_{1:T}|x_0)=q_{\sigma}(x_{T}|x_0)\prod^t_{t=2}q_{\sigma}(x_{t-1}|x_t,x_0)\<br>=N(x_{t-1};\sqrt{\bar{\alpha}_{t-1}}x_0+\sqrt{1-\bar{\alpha}_{t-1} -\sigma^2_t}\cdot \frac{x_t-\sqrt{\bar{\alpha}_t}x_0}{\sqrt{1-\bar{\alpha}_t}},\sigma^2_tI)<br>$$<br>或者是这样:</p><img src="/wang-luo-jie-gou-jie-mi/image-20230609152618007.png" alt="image-20230609152618007" style="zoom:67%;"><h4 id="Conditional-Diffusion-Models"><a href="#Conditional-Diffusion-Models" class="headerlink" title="Conditional Diffusion Models"></a>Conditional Diffusion Models</h4><p>训练的时候,sample $(x_0,\tilde{x})\sim q(x_0,\tilde{x})$从成对的数据分布,如一个clean和一个noisy,则在反向过程就可以提供$\tilde{x}$作为输入在反向过程中:<br>$$<br>p_\theta(x_{0:T}|\tilde{x})=p(x_T)\prod^T_{t=1}p_\theta(x_{t-1}|x_t,\tilde{x})<br>$$<br>所以现在对应的优化目标变为$\epsilon_\theta(x_t,\tilde{x},t)$,$x$和$\tilde{x}$在通道维度被concat，所以最终输入图像的channel是C=6</p><p>所以对应的DDIM中的公式为：<br>$$<br>x_{t-1}=\sqrt{\bar{\alpha}_{t-1}}(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\cdot \epsilon_\theta(x_t,\tilde<br>{x},t)}{\sqrt{\bar{\alpha}<em>t}})+\sqrt{1-\bar{\alpha}</em>{t-1}}\cdot \epsilon_\theta(x_t,\tilde{x},t)<br>$$<br>从$x_T\sim N(0,I)$开始</p><h3 id="搜广推相关八股"><a href="#搜广推相关八股" class="headerlink" title="搜广推相关八股"></a>搜广推相关八股</h3><p><strong>auc和roc是什么？</strong></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人像肤色检测-2</title>
      <link href="ren-xiang-pi-fu-jian-ce-2/"/>
      <url>ren-xiang-pi-fu-jian-ce-2/</url>
      
        <content type="html"><![CDATA[<h2 id="Fair-comparison-of-skin-detection-approaches-on-publicly-available-datasets"><a href="#Fair-comparison-of-skin-detection-approaches-on-publicly-available-datasets" class="headerlink" title="Fair comparison of skin detection approaches on publicly available datasets"></a>Fair comparison of skin detection approaches on publicly available datasets</h2><h4 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h4><p>年份：2020</p><p>期刊：EXPERT SYSTEMS WITH APPLICATIONS  JCR分区Q1</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>提出了一个公平比较方法，使用几个不同的数据集</p><p>主要贡献：</p><ol><li>肤色检测方法详尽介绍和一个公平比较方法</li><li>数据集收集和检查</li><li>一个评估和结合不同皮肤检测方法的框架</li><li>集成</li></ol><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>DeepLabv3 +,根据我们的实验是表现最好的独立方法</p><p>预训练、微调和集成都是有效的</p><p><strong>现在皮肤分割的最大问题就是没有数据集，建议收集一个不同地区的人的大数据集</strong></p><p>皮肤颜色检测器-&gt;颜色恒常性的预处理-&gt;形态学算子的后处理</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>用于区分皮肤和非皮肤像素的有用特征是像素颜色;然而，在不同光照、不同种族和不同采集设备下获得肤色一致性是一项非常具有挑战性的任务。此外，皮肤检测作为其他应用程序的初步步骤，需要计算效率高，不受几何变换、局部遮挡或姿态/面部表情变化的影响，对复杂或伪皮肤背景不敏感，对采集设备的质量具有鲁棒性。</p><p>较近的综述就这三篇  和另外两篇比，这篇主要提供公平比较方法</p><p>现有的最近的深度学习：</p><ul><li>Patch-wise skin segmentation of human body parts via deep neural networks 15 JEI</li><li>Combining Convolutional and Recurrent Neural Networks for Human Skin Detection 17 SPL</li><li>Human Skin Segmentation Using Fully Convolutional Neural Networks 18GCCE</li></ul><h4 id="Skin-detection-approaches"><a href="#Skin-detection-approaches" class="headerlink" title="Skin detection approaches"></a>Skin detection approaches</h4><p>影响因素：</p><ol><li>Human characteristics as ethnicity and age</li><li>Acquisition conditions</li><li>Skin painting</li><li>Complex background</li></ol><p>皮肤检测方法分类：</p><ol><li>考虑是否存在预处理步骤，如色彩校正和照明取消或动态适应，以减少不同获取条件的影响</li><li>考虑用于像素分类的颜色空间 A survey of skin-color modeling and detection methods 07PR                                                         <strong>basic models</strong> (i.e. RGB, normalized RGB), <strong>perceptual models</strong> (i.e. HIS, HSV) <strong>perceptual uniform models</strong> (i.e. CIE-Lab, CIE-Luv) and <strong>orthogonal models</strong> (i.e. YCbCr, YIQ) with the finding that orthogonal models are characterized by a reduced redundancy/correlation among channels, therefore they are the most suited for skin detection</li><li>Examining the problem formulation：图像分割出皮肤存在的区域/基于分隔和分类像素而不考虑邻居/基于像素      基于区域的论文很少，最近的一些卷积神经网络也可以看做基于分割这一类</li><li>执行像素分类的不同方法。1基于规则 2基于使用参数化或非参数化方法的机器学习方法估计颜色分布</li><li>机器学习分类器的不同方法  8种</li></ol><p>最近的两个主方向：</p><ul><li>对于一些应用，背景很容易区分，简单的基于规则的方法就可以，这种往往作为别的复杂任务的一个步骤；然后列举了一系列方法 15年名为SKN的新颜色空间</li><li>神经网络的</li></ul><p>12种基本方法：</p><ul><li>Statistical color models with application to skin detection 2002IJCV   GMM Bayes</li><li>Detector adaptation by maximising agreement between independent data sources 2007CVPR SPL 基于像素</li><li>Cheddad</li><li>Chen</li><li>SA1 SA2 SA3</li><li>DYC</li><li>SegNet</li><li>U-Net</li><li>DeepLab Deeplabv3+  ResNet50预训练 batch size32 学习率0.001 最大epoch30</li></ul><p><strong>重点看TABLE1</strong></p><h4 id="Skin-detection-evaluation-Datasets-and-performance-indicators"><a href="#Skin-detection-evaluation-Datasets-and-performance-indicators" class="headerlink" title="Skin detection evaluation: Datasets and performance indicators"></a>Skin detection evaluation: Datasets and performance indicators</h4><p><strong>重点看TABLE2</strong></p><p>SDD 2015 21000张 精确 未开源</p><p>HGR 2014 1558张  手势图片和皮肤mask  精确 开源</p><p>SFA 2013 1118 张 中等精度 开源</p><p>VDM 2013 285张 精确 人类活动识别+光照广泛 开源</p><p>Pratheepan 2012 78张 精确 简单背景+复杂背景 开源</p><p>Feeval 2009 8991张 不精确质量低但多 开源</p><p>Schmugge 2007 845张 精确 三类 开源</p><p>ECU 2005 4000张 精确 未开源</p><h4 id="A-fair-experimental-comparison"><a href="#A-fair-experimental-comparison" class="headerlink" title="A fair experimental comparison"></a>A fair experimental comparison</h4><p><strong>重点看TABLE3、4</strong></p><h2 id="A-survey-on-skin-detection-in-colored-images"><a href="#A-survey-on-skin-detection-in-colored-images" class="headerlink" title="A survey on skin detection in colored images"></a>A survey on skin detection in colored images</h2><h4 id="Info-1"><a href="#Info-1" class="headerlink" title="Info"></a>Info</h4><p>年份：2018</p><p>期刊：ARTIFICIAL INTELLIGENCE REVIEW JCR分区Q1</p><h2 id="A-Comprehensive-Survey-on-Human-Skin-Detection"><a href="#A-Comprehensive-Survey-on-Human-Skin-Detection" class="headerlink" title="A Comprehensive Survey on Human Skin Detection"></a>A Comprehensive Survey on Human Skin Detection</h2><h4 id="Info-2"><a href="#Info-2" class="headerlink" title="Info"></a>Info</h4><p>2016 IJIGSP</p><h4 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h4><h2 id="Real-time-Segmentation-and-Facial-Skin-Tones-Grading"><a href="#Real-time-Segmentation-and-Facial-Skin-Tones-Grading" class="headerlink" title="Real-time Segmentation and Facial Skin Tones Grading"></a>Real-time Segmentation and Facial Skin Tones Grading</h2><h3 id="Info-3"><a href="#Info-3" class="headerlink" title="Info"></a>Info</h3><p>CVPR</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>毛发和面部皮肤分割方法，DCNN，</p>]]></content>
      
      
      <categories>
          
          <category> 肤色分级 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 肤色分级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semantic Segmentation</title>
      <link href="semantic-segmentation/"/>
      <url>semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p><a href="https://cloud.tencent.com/developer/article/1589733">https://cloud.tencent.com/developer/article/1589733</a></p><p><a href="https://blog.csdn.net/ShuqiaoS/article/details/87360693">https://blog.csdn.net/ShuqiaoS/article/details/87360693</a></p><h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h2><p>知乎教程：<a href="https://zhuanlan.zhihu.com/p/22976342?utm_source=tuicool&amp;utm_medium=referral">https://zhuanlan.zhihu.com/p/22976342?utm_source=tuicool&amp;utm_medium=referral</a></p><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>15 CVPR 17TPAMI</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ol><li>不含全连接层(fc)的全卷积(fully conv)网络。可适应任意尺寸输入。将现代分类网络<strong>AlexNet、VGGNet、GoogLeNet</strong>改造为FCN，并微调</li><li>增大数据尺寸的反卷积deconv层。能够输出精细的结果</li><li>结合不同深度层结果的跳级(skip)结构。同时确保鲁棒性和精准性</li></ol><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><strong>语义分割面临语义和位置之间固有的紧张关系:全局信息解决what问题，而局部信息解决where问题。</strong><br>深度特征层次将位置和语义编码在一个非线性的local-to-global的金字塔中。在4.2节中，定义了一个skip架构来利用这个结合了深层、粗糙的语义信息和浅层、精细的外观信息的特征。</p><h3 id="FCN-1"><a href="#FCN-1" class="headerlink" title="FCN"></a>FCN</h3><p>卷积网络是建立在平移不变性的基础上的。它们的基本成分(卷积、池化和激活函数)作用于局部输入区域，仅依赖于相对的空间坐标。</p><h4 id="3-1-Adapting-classifiers"><a href="#3-1-Adapting-classifiers" class="headerlink" title="3.1. Adapting classifiers"></a>3.1. Adapting classifiers</h4><img src="/semantic-segmentation/image-20210324164018584.png" alt="转变" style="zoom:80%;"><p>直接生成heatmap,再加损失</p><h2 id="UNet"><a href="#UNet" class="headerlink" title="UNet"></a>UNet</h2><h3 id="Info-1"><a href="#Info-1" class="headerlink" title="Info"></a>Info</h3><p>15ICM</p><p><a href="https://www.cnblogs.com/PythonLearner/p/14041874.html">UNet详解</a>  最常用….最简单…  一种<strong>U型的网络结构来获取上下文的信息和位置信息</strong></p><p><a href="https://zhuanlan.zhihu.com/p/46251798?edition=yidianzixun&amp;utm_source=yidianzixun&amp;yidian_docid=0KDRTjYB">知乎这篇讲的非常好</a></p><p><strong>前半部分</strong>是<strong>特征提取</strong>部分，<strong>后半部分</strong>是<strong>上采样</strong>,有些文献也把这种结构叫做<strong>编码器-解码器</strong>结构。</p><p>copy and crop：在论文中叫拼接，在UNet有四个拼接操作。如上图所示：有人也叫Skip connect,这一操作的目的是为了<strong>融合特征信息，使深层和浅层的信息融合起来</strong>，在拼接的时候要注意，<strong>不仅图片大小要一致（故要crop,是为了使图片大小一致）</strong>而且<strong>特征的维度（channels）也要一样</strong>，才可以拼接。</p><p><strong>Unet的好处我感觉是：网络层越深得到的特征图，有着更大的视野域，浅层卷积关注纹理特征，深层网络关注本质的那种特征，所以深层浅层特征都是有格子的意义的；另外一点是通过反卷积得到的更大的尺寸的特征图的边缘，是缺少信息的，毕竟每一次下采样提炼特征的同时，也必然会损失一些边缘特征，而失去的特征并不能从上采样中找回，因此通过特征的拼接，来实现边缘特征的一个找回。</strong></p><ol><li><strong>医疗影像的所有特征都很重要，因此低级特征和高级语义特征都很重要，所以U型结构的skip connection结构（特征拼接）更好派上用场</strong></li><li>医学影像数据较少，大网络容易过拟合</li><li><strong>医学影像任务中，往往需要自己设计网络去提取不同的模态特征，因此轻量结构简单的Unet可以有更大的操作空间</strong></li></ol><h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><p>卷积网络的典型用途是分类任务，<strong>其中图像的输出是单个类别标签。</strong> 然而，在许多视觉任务中，<strong>尤其是在生物医学图像处理中，期望的输出应该包括定位，即，应该将类标签分配给每个像素。（也就是分割）</strong></p><p><strong>在U-Net结构中，包括一个捕获上下文信息的收缩路径和一个允许精确定位的对称拓展路径</strong></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>首先要<strong>像素级预测</strong>，其次<strong>训练图像没那么多</strong>，然后<strong>运行要快</strong>，最后<strong>上下文信息和局部位置信息的权衡</strong>。</p><p><a href="https://www.bilibili.com/read/cv8291595">Overlap-tile 重叠-切片</a></p><p>UNet并不是一个完全对称的结构，因为使用的是<strong>不带padding的3x3卷积</strong></p><p><strong>如果想保持尺寸一致该如何做？</strong></p><ol><li>使用插值，插值是不可学习的，会带来一定的误差</li><li>使用转置卷积，会增加参数量，并且模型也不一定能学的很好</li><li>same卷积，即使用padding，但是padding会引入误差，而且模型越深层得到的feature map抽象程度越高，收到padding的影响会呈累积效应 因为填充是0</li><li>Overlap-tile  对称的肯定比0要好 通常需要将图像进行分块的时候才使用</li></ol><p>图像的主要成分是<strong>低频信息</strong>，它形成了<strong>图像的基本灰度等级</strong>，对图像结构的决定作用较小；<strong>中频信息决定了图像的基本结构</strong>，形成了图像的<strong>主要边缘结构</strong>；<strong>高频信息形成了图像的边缘和细节</strong>，是在中频信息上对图像内容的进一步强化。</p><p><a href="https://blog.csdn.net/weixin_38208741/article/details/79823681">图像的高频信息和低频信息</a>简单来说就是，高频就是变化多的细节，低频就是背景板。</p><p>UNet使用<strong>弹性变形</strong>来进行<strong>数据扩充</strong>,这可以使得网络学习到这种<strong>不变性invariance</strong>。</p><p>使用<strong>加权损失</strong>，迫使网络学习<strong>边界像素</strong>。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>由于<strong>没有填充的卷积，输出图像要比输入图像小一个恒定的边界宽度</strong>。</p><p>为了最小化开销并最大限度地利用GPU内存，偏爱大的输入块而不是大的批处理大小，因此将批处理减少到单个图像。</p><p>因此，使用一个高动量(0.99)，以便在当前优化步骤中使用大量之前看到的训练样本来确定更新。</p><h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><p>对于显微镜图像，首先需要<strong>平移和旋转不变性</strong>，以及对<strong>变形和灰度值变化的鲁棒性</strong>。特别是训练样本的<strong>随机变形</strong>似乎是训练带有少量标注图像的分割网络的关键概念。我们在3×3网格上使用<strong>随机位移矢量</strong>生成平滑变形。位移从一个<strong>10像素标准差的高斯分布中采样</strong>。然后使用<strong>双三次插值计算像素位移</strong>。缩路径的最后加入了<strong>Dropout</strong>，隐式的加强了数据增强。</p><h2 id="DeeplabV3"><a href="#DeeplabV3" class="headerlink" title="DeeplabV3+"></a>DeeplabV3+</h2><h3 id="Info-2"><a href="#Info-2" class="headerlink" title="Info"></a>Info</h3><p>18ECCV</p><p><a href="https://zhuanlan.zhihu.com/p/62261970">知乎教程</a></p><img src="/semantic-segmentation/image-20210330225402974.png" alt="总结" style="zoom: 80%;"><h3 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h3><img src="/semantic-segmentation/image-20210331102425870.png" alt="区别" style="zoom:80%;"><p><strong>典型的DilatedFCN</strong></p><p>空洞卷积可以减少下采样率但是又可以保证感受野，但是天下没有免费的午餐，<strong>保持分辨率意味着较大的运算量</strong></p><p>空间金字塔池化SPP和编码器-解码器encoder-decoder</p><p>SPP通过多种感受野池化不同分辨率的特征来挖掘上下文信息。</p><p>Encoder-decoder逐步恢复空间信息来更好的捕捉物体的边缘。</p><p>DeepLabv3+对DeepLabv3进行了拓展，<strong>在encoder-decoder结构上采用SPP模块</strong>。encoder提取丰富的语义信息，decoder细化分割结果，恢复精细的物体边缘。encoder允许在任意分辨率下采用空洞卷积。</p><p>探索了Xception</p><h3 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h3><p>感觉decoder和HDNet的位置距离都是在逐步的探索物体边界</p><p>DeeplabV3采用ASPP PSPNet采用PPM模块</p><p>DeeplabV3+<strong>添加了一个简单有效的解码器模块来恢复对象边界</strong>，扩展了DeeplabV3</p><img src="/semantic-segmentation/image-20210330143251152.png" alt="Fig1" style="zoom:67%;"><p>下采样会丢失对象的边界信息，</p><p>贡献:</p><ul><li>提出了一种采用DeepLabv3作为强大的encoder和简单而有效的decoder模块的新型encoder-decoder结构。</li><li>encoderdecoder结构中可以<strong>通过空洞卷积来平衡精度和运行时间</strong>，现有的encoder-decoder结构是不可行的。</li><li>将<strong>Xception模型</strong>应用于分割任务，并将<strong>depthwise separable convs应用于ASPP模块和decoder模块</strong>，从而实现更快、更强的encoder-decoder网络。</li></ul><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>为了提取上下文信息，提出了一些基于FCN的模型变体，例如:</p><img src="/semantic-segmentation/image-20210330150110244.png" alt="DeeplabV3的Fig2" style="zoom: 50%;"><p>还有采用概率图形模型，例如(Dense CRF),本文主要讨论使用spatial pyramid pooling 和encoder-decoder的模型</p><h4 id="Spatial-pyramid-pooling"><a href="#Spatial-pyramid-pooling" class="headerlink" title="Spatial pyramid pooling"></a>Spatial pyramid pooling</h4><p>PSPNet的PPM和Deeplab的ASPP</p><h4 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h4><p>通常，encoder-decoder包含(1)一个encoder模块，逐步减小特征图并捕获更高的语义信息，(2)一个decoder模块，逐步恢复空间信息。在此基础上，我们建议使用DeepLabv3作为编码器模块，并添加一个简单而有效的解码器模块，以获得更清晰的分割。</p><h4 id="Depthwise-separable-convolution"><a href="#Depthwise-separable-convolution" class="headerlink" title="Depthwise separable convolution"></a>Depthwise separable convolution</h4><p>Depthwise separable convolution 深度可分卷积or group convolution 群卷积 保持性能前提下，有效降低了计算量和参数量。</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><img src="/semantic-segmentation/image-20210330152438360.png" alt="DeeplabV3+结构" style="zoom: 80%;"><h4 id="3-1-Encoder-Decoder-with-Atrous-Convolution"><a href="#3-1-Encoder-Decoder-with-Atrous-Convolution" class="headerlink" title="3.1 Encoder-Decoder with Atrous Convolution"></a>3.1 Encoder-Decoder with Atrous Convolution</h4><p><strong>Depthwise separable convolution</strong></p><p>将一个标准卷积分离成一个depthwise conv，然后是一个point-wise(即1x1 conv)，大大降低了计算复杂性。</p><img src="/semantic-segmentation/image-20210330153636493.png" alt="深度可分离" style="zoom:80%;"><p>(a)DW:为每个输入通道应用一个过滤器 (b)PW:通道之间进行结合 (c):Atrous DW,在该文中,探讨了Atrous DW的效果 ，两个一起叫atrous separable convolution</p><p><strong>DeepLabv3 as encoder</strong></p><p>此处的output stride 表示input_size/output_size(在全局池化或全连接层之前)。</p><p>图像分类任务中，最终feature map的空间分辨率通常是输入图像分辨率的32倍，因此output stride = 32。</p><p>语义分割任务，令outputstride=16or8，通过移除最后1or2个blocks的stride并应用空洞卷积(rate=2and4 for stride=8)来密集提取特征。也就是说不下采样了，防止损失信息，但是通过空洞卷积来增强感受野。</p><p>DeeplabV3使用了ASPP，多尺度，包括image-level features(参见ParseNet)，最终feature map是256通道</p><p><strong>Proposed decoder</strong></p><p>deeplabv3通常output stride=16.DeeplabV3中直接上采样16倍，可能无法恢复对象分割细节。</p><p>Low-level features后加一个1x1，因为他的通道一般挺多，如256或512，必须把它通道数减少，<strong>否则会超过encoder输出的feature map 的重要性</strong>，相连之后，<strong>应用几个3x3来细化特征</strong>，然后<strong>又一个简单的双线性上采样4倍</strong>。</p><h4 id="3-2-Modified-Aligned-Xception"><a href="#3-2-Modified-Aligned-Xception" class="headerlink" title="3.2 Modified Aligned Xception"></a>3.2 Modified Aligned Xception</h4><p>Xception用于<strong>图像分类</strong>，Aligner Xception用于<strong>物体检测</strong>，在Aligned Xception的基础上，将之应用于语义分割，</p><ul><li>更多的层，为了计算量和内存，不对Entry flow网络结构进行修改。</li><li>所有池化层替换为depthwise separable conv，以便采用 atrous separable conv提取任意分辨率的特征。</li><li>类似于MobileNet，在每个3×3后添加额外的BN和ReLU。</li></ul><img src="/semantic-segmentation/image-20210330225235503.png" alt="Modified Aligned Xception" style="zoom:80%;"><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><h2 id="HDNet"><a href="#HDNet" class="headerlink" title="HDNet"></a>HDNet</h2><h3 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h3><p>目前方法分离了<strong>特征图中点之间的关系</strong>，导致分割结果不连续</p><p>本论文提出混合距离网络，从两个方面来度量距离–位置距离和高维特征距离</p><p>在此基础上，提出了一种位置感知注意力模块，利用位置距离对上下文进行有效采样，生成稀疏混合距离关系。<br>它综合每个点的不同上下文，并生成position-wise的注意值，以紧凑的对象级表示。<br>在训练步骤中，高维特征距离损失也作为在特征空间中压缩类别级表示的辅助损失。</p><p>SOTA in Pascal Context, ADE20K, and COCO Stuff 10K</p><h3 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h3><p>ACNet[6] ICCV 2019 证明卷积核主要集中在中心点上，所以滑动窗口过程会产生每个点的隔离。这种隔离会干扰特征学习，导致分割结果的不连续，称为“<strong>隔离问题</strong>”。同时语义分割网络采用逐像素的交叉熵作为损失函数，对于相邻的像素点的监督是独立的，因此该损失函数无法有效地惩罚不连续的分割结果，这 加剧了分割结果中的孤立问题。</p><p>为了解决像素的孤立问题，许多研究在神经网络结构中加入上下文模块，通过在空间维度捕获像素点的上下文信息，增大像素的感受野，缓解像素孤立的问题。</p><ul><li>DeeplabV1  空洞卷积扩大感受野  <strong>但仍将点与全局场景上下文分隔开</strong></li><li>PSPNet、deeplabv2中进一步提多尺度的金字塔模型获得更大范围、更丰富的上下文信息。                                                                                                      <strong>PSPNet    金字塔池化模块PPM      两个点位于同一个池化窗口，产生相同但不准确的上下文信息  两个点相邻位于不同池化窗口，获得完全不同的上下文信息。</strong></li></ul><img src="/semantic-segmentation/image-20210228030113655.png" alt="PSPNet" style="zoom:80%;"><p>使用不同窗口大小的pooling操作，得到不同尺寸的输出，然后缩放到相同的尺寸，再进行特征融合。</p><p>具体来说，包含4个pooling层次，对于原始特征图，分别通过pooling得到大小为 <img src="https://www.zhihu.com/equation?tex=1%5Ctimes1" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=2%5Ctimes2" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=3%5Ctimes3" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=6%5Ctimes6" alt="[公式]"> 。然后分别使用1×1卷积调整通道数至 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D" alt="[公式]"> （ <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 即为pooling层次数，此时 <img src="https://www.zhihu.com/equation?tex=N=4" alt="[公式]"> ）。然后将这些特征图全部上采样，通过双线性插值完成。再将这些pooling特征图输出，以及原始的特征图（跳跃连接），全部concat起来，作为特征输出，可以由此产生分割的预测结果。</p><p>实现过程中，这些pooling层通过AdaptivePooling层完成，根据输入输出大小计算pooling的窗口大小。其中输出大小为 <img src="https://www.zhihu.com/equation?tex=1%5Ctimes1" alt="[公式]"> 时，其实就是Global Average Pooling。</p><ul><li>Non-Local   local operations就是基于局部区域进行操作，长距离依赖则是图像中非相邻像素点之间的关系。该论文中的<strong>非局部操作是将所有位置对一个位置的特征加权和作为该位置的响应值。使用相同策略，远程和短程背景起相同作用，这种关系表示忽略了一对像素点点之间的位置距离的影响，也失去了对象的整体概念 该方法不能解决对象内部的隔离问题，且导致大量的冗余计算</strong> 。</li></ul><p><img src="/semantic-segmentation/image-20210228004156931.png" alt="Non Local公式"></p><p>上述方法在捕捉上下文信息时丢弃了空间信息，这削弱了它们的分割精度。<strong>HDR 结合位置距离和高维特征距离  位置距离表示特征图上一对点之间的相对位置，而高维特征距离表示两个特征向量之间的相似性</strong>。HDR建模在一定范围内的一个点和它的上下文区域之间的关系，位置距离隐含在区域的范围内。</p><p>其次，提出<strong>位置感知注意模块LAA</strong>，对HDR的不同位置距离进行采样，从而捕获点与其不同上下文之间的关系。</p><p>然后由这些HDRs生成一个基于位置的注意值，以融合不同的关系，补充原有的特征图。</p><p>LAA模块引入<strong>稀疏关系连接来关注高维空间中特征的位置距离</strong>，而不是使用以前的方法[12DANet ,13 CCNet]中<strong>每对特征之间的密集连接，这样消耗的计算量较少</strong>。该算法能够<strong>感知物体在中心位置周围的空间关系，提高了分割结果的连续性，降低了计算复杂度</strong>。利用这一特点，我们的方法在中心对象上的特征分布明显比基于非局部的方法更紧凑</p><p>DANet：基于空洞卷积的FCN添加了两种注意力模块：position attention module和channel attention module</p><p>CCNet:Criss-Cross Attention 以十字形交叉的   对Non Local的一种改进，减少计算量</p><img src="/semantic-segmentation/image-20210228023222192.png" alt="CCNet" style="zoom:67%;"><img src="/semantic-segmentation/image-20210228023250154.png" alt="网络结构图image-20210228023250154" style="zoom:67%;"><h3 id="Related-Work-1"><a href="#Related-Work-1" class="headerlink" title="Related Work"></a>Related Work</h3><p>近年来出现许多用于捕获上下文信息的模块</p><h4 id="Spatial-Aggregation-Module空间聚集模块"><a href="#Spatial-Aggregation-Module空间聚集模块" class="headerlink" title="Spatial Aggregation Module空间聚集模块"></a>Spatial Aggregation Module空间聚集模块</h4><p>Deeplab V1:空洞卷积</p><p>Deeplab V2:<strong>atrous spatial pyramid pooling (ASPP)</strong> 空洞空间金字塔池化 与<strong>SPPNet空间金字塔池化网络</strong>中的SPP模块类似，<strong>在给定的输入上以不同采样率的空洞卷积并行采样，相当于以多个比例捕捉图像的上下文</strong></p><img src="/semantic-segmentation/image-20210228025059270.png" alt="空间金字塔池化层" style="zoom:50%;"><img src="/semantic-segmentation/image-20210228025421137.png" alt="ASPP" style="zoom:50%;"><h4 id="Relation-Module-关系模块"><a href="#Relation-Module-关系模块" class="headerlink" title="Relation Module 关系模块"></a>Relation Module 关系模块</h4><p>CCNet:简化Non Local计算</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>三部分 1 HDR Hybrid distance relation  2 LAA   Location Aware Attention 3 HFD loss</p><p><img src="/semantic-segmentation/image-20210303013337764.png" alt="overview"></p><h4 id="1-HDR-Hybrid-Distance-Relation"><a href="#1-HDR-Hybrid-Distance-Relation" class="headerlink" title="1 HDR: Hybrid Distance Relation"></a>1 HDR: Hybrid Distance Relation</h4><p>目前的方法只考虑上下文信息的捕获，而不考虑特征的连续性，不能很好的解决隔离问题。</p><p>HDR的目的:1 产生连续结果 2 避免冗余计算</p><p>$R_k(i,j)=F(x_{i,j},A_k(x_{i,j}))$</p><p>$x_{i,j}$表示i,j处点的特征向量，$A_k(x_{i,j})$表示range为k的上下文计算得出的特征向量，$F(.,.)$计算两个特征向量之间的高维特征距离</p><p><strong>HDR的本质是两个特征向量之间的特征距离</strong>，每个HDR通过聚合函数$Ak(·)$使用采样后的特征图，使点感知周围区域。</p><p><strong>位置距离是指周围区域的范围。</strong></p><p>所以位置距离通过聚合函数$Ak(·)$嵌入到了HDR中，从而使所提出的HDR既考虑了高维特征距离，又考虑了位置距离。</p><p>随着range k的不同，如下:</p><p><img src="/semantic-segmentation/image-20210302222214915.png" alt="改变k的范围"></p><p>换句话说，<strong>中心点可以感知到在二维平面上由于不同的距离k到物体边界的空间距离。</strong></p><p>此外，HDR在捕捉不同k范围的中心点与其上下文区域之间的关系时，可可视化为稀疏连接；Non Local则是使用孤立的一对点之间的密集连接关系(图5a)。并且k为n和k为n-1时的差值代表kn和kn-1间的上下文，这种带孔的HDR增强了上下文的多样性。</p><p><img src="/semantic-segmentation/image-20210302223254591.png" alt="HDR的稀疏连接和带孔的HDR"></p><p><img src="/semantic-segmentation/image-20210302223538080.png" alt="F距离函数"></p><p>F可以简单实现为余弦距离，只关注了点的分布，忽略了点的特征值的大小，[-1,1]方便后续操作</p><p>为了简化聚合过程的计算，使用<strong>池化操作</strong>作为聚合函数$A_k(.)$,将k专门化为<strong>池化窗口的大小</strong>。因此，特定区域内的距离为<strong>正方形</strong>。</p><p>$Ak(·)$采用<strong>平均池化操作</strong>，考虑k范围内的所有点，对于大对象，无论窗口大小，响应一致。此外，我们还采用了<strong>最大池化操作</strong>，可以<strong>捕捉到显著特征</strong>。为了使每个点获得自己的聚合结果，将聚合函数设为位置级函数，步幅设为1。</p><p>由于k值不同，一串$A_k(.)$函数可以视为局部位置级金字塔。</p><p>和PPM的区别:</p><p>PPM是一个全局金字塔，通过这种下行采样策略，相邻的点可能会得到相同或不同的上下文信息，如下图:</p><p><img src="/semantic-segmentation/image-20210302225644534.png" alt="PPM模块"></p><p>这种全局的方式丢弃了特定位置，产生不精确的上下文信息。</p><p>本论文的HDR是每个点的局部金字塔，所以特征图大小为HxW，就有HxW个金字塔，每个点都有它自己的上下文。</p><p>ASPP[14]是另一个捕捉上下文信息的位置金字塔，因此可以将其表述为与HDRs中提出的聚合相同。但它继承了扩张卷积的缺点，即只对一个感受野中的几个点进行采样，而忽略了整个感受野。而池操作可以考虑整个字段，不会导致这种不连续的效果。</p><h4 id="2-LLA-Location-Aware-Attention"><a href="#2-LLA-Location-Aware-Attention" class="headerlink" title="2 LLA: Location Aware Attention"></a>2 LLA: Location Aware Attention</h4><p>LAA模块利用提出的HDR，获取不同范围的上下文信息，增强对对象大小的感知，增强对象级的连续性。<br>LAA有4个子模块:输入/输出变换、多范围HDRs、HDRs结果的交互、对原始feature map的激励</p><p><img src="/semantic-segmentation/image-20210302234656307.png" alt="LLA模块"></p><p><strong>变换子模块：</strong>$\theta$为1x1卷积，减少了信道数量。$\phi$也是1x1卷积，但是前后信道数目不变。</p><p>**交互子模块:**在每一点融合不同的关系，可以表示为:</p><p>$u_{i,j }= ω([R_{k_1}(i, j), R_{k_2}(i, j), · · · , R_{k_n} (i, j)]),$</p><p>不同的k表示不同的范围 ，ω(·)是一种不同关系的位置非线性融合。</p><p>**激励子模块: **      $v_{i,j }= u_{i,j} × x_{i,j}$</p><p><strong>最终结果:</strong>$y_{i,j} = x_{i,j} + ϕ(v_{i,j})$,可表述为位置方面的增强</p><p><img src="/semantic-segmentation/image-20210303001931862.png" alt="反向传播"></p><p>Channel Attention用的就是DANet中的。</p><p>基于稀疏连接和位置注意，我们的注意模块计算复杂度明显低于Non Local。其次，交互模块和激励模块补充了每个点的特征，增强了位置感知。</p><h4 id="3-HFD-High-dimension-Feature-Distance-Loss"><a href="#3-HFD-High-dimension-Feature-Distance-Loss" class="headerlink" title="3 HFD: High-dimension Feature Distance Loss"></a>3 HFD: High-dimension Feature Distance Loss</h4><img src="/semantic-segmentation/image-20210303005022704.png" alt="HFD loss" style="zoom: 80%;"><p>目前的网络[12DANet,13CCNet,18EMANet]使用头部结构来减少特征图的通道来对每个点进行分类，这是一种从高维特征到预测语义类别的有效概述，而潜在特征的监督仅依赖于梯度通过。<br>为了预测类别内的连续结果和类别间的分化结果，高维特征应保持相同的一致性和差异性。</p><p>HFD Loss:减少类别内点的特征距离的方差，增加类别间的方差</p><p>负对数似然(不就是交叉熵函数吗):</p><p>$L_{H,p} = −E_x[log\widetilde{D}p(x)]$</p><p>x为某一点特征，$\widetilde{D}p(x)$为某一点特征与第q类特征中心的归一化距离。与无监督聚类方法类似，HFD loss方法探索了每个类别的特征中心，并迫使每个点靠近相应的中心。为了得到点与类别中心的精确对应，使用ground truth对结果进行监督，这是与聚类方法的主要区别。</p><p><img src="/semantic-segmentation/image-20210303010956280.png" alt="归一化"></p><p>T维度为CxN，C为特征个数，就是每个点有多少个k，N为类别个数，整体是softmax归一化。</p><p><strong>look-up table：</strong></p><p>查找表T是根据生成的特征图对每个类别进行高维表示。因此，在训练过程中，随着特征图的变化，它会不断更新。</p><p>在更新过程中，采用与批处理归一化层相同的指数移动平均策略，通过小批量逼近实数表示。所以更新不需要梯度，而是依赖于输入，如图8中的黑色虚线，公式为:</p><p><img src="/semantic-segmentation/image-20210303012402882.png" alt="查找表T公式"></p><p>初始中心就是加起来求平均，之后采用移动平均策略。</p><p>提出的HFD Loss使得每个点的高维特征更接近ground truth所表示的特征中心Tp，直接促进了高维特征空间中类别的识别。本文提出的HFD Loss作为辅助Loss监督<strong>头部结构</strong>之前的高维特征图。而传统的交叉熵损失也通过最终的分割结果来监督HDNet。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>数据集:PASCAL Context ADE20K COCO Stuff 10K</p><h4 id="4-1-Implements-Details"><a href="#4-1-Implements-Details" class="headerlink" title="4.1. Implements Details"></a>4.1. Implements Details</h4><p><img src="/semantic-segmentation/image-20210303021340412.png" alt="全部损失"></p><p><img src="/semantic-segmentation/image-20210303022652697.png" alt="LAA"></p><h2 id="多尺度空洞卷积"><a href="#多尺度空洞卷积" class="headerlink" title="多尺度空洞卷积"></a>多尺度空洞卷积</h2><h3 id="Info-3"><a href="#Info-3" class="headerlink" title="Info"></a>Info</h3><p>16ICLR</p><p><a href="https://blog.csdn.net/zxfhahaha/article/details/102478092">其他人的论文阅读笔记,很赞</a></p><h3 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h3><p>开发了一个新的卷积网络模块，专门用于密集预测。该模块<strong>使用空洞卷积来系统地聚合多尺度上下文信息而不丢失分辨率</strong>。<br>该架构基于<strong>空洞卷积支持感受野的指数扩展，而不会丧失分辨率或覆盖范围</strong>的事实。我们表明，所提出的上下文模块提高了最先进的语义分割系统的准确性。</p><h3 id="Introduction-4"><a href="#Introduction-4" class="headerlink" title="Introduction"></a>Introduction</h3><p>语义分割具有挑战性，因为它需要结合像素级精度和多尺度上下文推理。</p><p>FCN表明<strong>原本用于图像分类的卷积网络架构可以成功地用于密集预测</strong>。这些重新设计的网络在挑战语义分割基准上的表现大大超过了先前的技术水平。由于图像分类和密集预测的结构差异，这引发了新的问题。重新使用的网络的哪些方面是真正必要的，哪些方面在密集操作时降低了准确性?专为密集预测设计的专用模块能否进一步提高准确性?</p><p>现代图像分类网络通过连续的池化和子采样层集成多尺度上下文信息，从而降低分辨率，直到获得全局预测。相反，密集预测需要多尺度上下文推理与全分辨率输出相结合。</p><p>近年来研究了两种方法来解决<strong>多尺度推理和全分辨率密集预测</strong>的矛盾需求。</p><p>一种方法是反复进行上卷积，目的是恢复丢失的分辨率，同时从下采样层进行全局视角。这就留下了一个问题，即是否真的有必要进行严重的中间下采样。</p><p>例如15年ICCV的&lt;&lt;Learning deconvolution network for semantic segmentation&gt;&gt;</p><p><img src="/semantic-segmentation/image-20210310235530599.png" alt="DeConvNet"></p><p>Encoder-Decoder结构 采用了VGG16</p><p><a href="https://www.jianshu.com/p/6c09ecda592b">通俗教程1</a> <a href="https://blog.csdn.net/u010772289/article/details/69526178">详细教程2</a></p><p>产生足够多的候选区域，然后在每个候选区域用网络获得语义分割图(semantic segmentation maps),然后将所有区域的输入组合起来</p><p>另一种方法包括提供多个重新缩放版本的图像作为网络输入，并结合这些多个输入的预测。同样，目前还不清楚是否真的需要对重新缩放后的输入图像进行单独分析</p><p>在这项工作中，我们开发了一个卷积网络模块，在不丢失分辨率或分析重新缩放图像的情况下聚合多尺度上下文信息。该模块可以插入到任何分辨率的现有体系结构中。与从图像分类中继承过来的金字塔形架构不同，提出的上下文模块是专门为密集预测而设计的。该模块基于空洞卷积，支持感受野的指数扩展，而不会丢失分辨率或覆盖范围</p><h3 id="DILATED-CONVOLUTIONS"><a href="#DILATED-CONVOLUTIONS" class="headerlink" title="DILATED CONVOLUTIONS"></a>DILATED CONVOLUTIONS</h3><p>FCN虽然分析了filter dilation但是没有使用它，deeplabV1使用filter dilation简化了FCN的网络结构</p><img src="/semantic-segmentation/image-20210312013030003.png" alt="Figure1" style="zoom:67%;"><p>$F_{i+1}=F_i\cdot_{2^i}k_i,i=0,1,2\cdots$，则获得$(2^{i+2}-1)\cdot (2^{i+2}-1)$的感受野</p><img src="/semantic-segmentation/image-20210312014137893.png" alt="解析" style="zoom: 80%;"><ul><li>a图对应3x3的空洞率为1的卷积，和普通的卷积操作一样，计算量是9个点。对于(a)这个feature map F1而言，F1是由1-dilated convolution 卷积F0得来的，如果不考虑之前层的感受野，那这个卷积核的感受野大小是3x3，也就是F1的每个元素的感受野都是3x3。</li><li>b图对应3x3的空洞率为2的卷积，实际的计算量还是9个点。对于(b)这个feature map而言，(b)是对(a)空洞卷积而来的，卷积核覆盖的区域大小为5x5（图中蓝框），但是这个时候感受野大小并不是5x5，因为a中的元素的感受野就已经为3x3了，覆盖的5x5区域要往外多加（3-1）/2=1个像素，如(b)中红框所示。即1-dilated和2-dilated堆叠起来就能达到7x7的感受野，而普通卷积需要三层3x3的卷积层堆叠才能达到7x7的感受野。</li><li>c图对应3x3的空洞率为4的卷积，实际的计算量还是9个点。同理，对于©而言，卷积核覆盖的(b)区域大小为9x9（图中蓝框），但是由于(b)中的元素的感受野大小为7x7，因此，在这个9x9的区域大小之外还要扩张出（7-1）/2=3个像素，如©中红框所示。即1-dilated、2-dilated、4-dilated堆叠起来就能达到15x15的感受野。</li><li>注意,这个的stride是1</li></ul><h3 id="MULTI-SCALE-CONTEXT-AGGREGATION"><a href="#MULTI-SCALE-CONTEXT-AGGREGATION" class="headerlink" title="MULTI-SCALE CONTEXT AGGREGATION"></a>MULTI-SCALE CONTEXT AGGREGATION</h3><h4 id="network-architecture"><a href="#network-architecture" class="headerlink" title="network architecture"></a>network architecture</h4><p>上下文模块旨在通过聚合多尺度上下文信息来提高密集预测体系结构的性能。该模块接受通道数为C的特征图作为输入，并生成通道数为C的特征图作为输出。输入和输出具有相同的形式，因此该模块可以插入现有的密集预测体系结构中</p><p><img src="/semantic-segmentation/image-20210312025120461.png" alt="模块"></p><p>从小的局部特征,到大的特征</p><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>卷积网络通常使用随机分布的样本进行初始化。然而，我们发现随机初始化方案对上下文模块并不有效。我们发现一个具有清晰语义的替代初始化更有效:</p><p><img src="/semantic-segmentation/image-20210312031632482.png" alt="初始化公式"></p><p>这种identity初始化设置所有滤波器的值，这样每一层都能将前一层的信息直接传递到下一层。直觉上感到不利于反向传播信息的传递。但实验证明这种担心是多余的。<br>basic 的context module只有64C^2个参数，参数的数量非常少，但实验结果已经表现的非常好了</p><h3 id="Front-End"><a href="#Front-End" class="headerlink" title="Front-End"></a>Front-End</h3>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In-camera image processing pipeline</title>
      <link href="in-camera-image-processing-pipeline/"/>
      <url>in-camera-image-processing-pipeline/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在进行之前先把学姐的汇报弄明白</p><p>首先是raw图像格式：<a href="https://zhuanlan.zhihu.com/p/158088019">https://zhuanlan.zhihu.com/p/158088019</a></p><p>ISP大佬的教程:<a href="https://ridiqulous.com/process-raw-data-using-matlab-and-dcraw/#brightnesscorrection">https://ridiqulous.com/process-raw-data-using-matlab-and-dcraw/#brightnesscorrection</a></p><p><a href="https://zhuanlan.zhihu.com/p/139394687">CMOS</a>:感光元件</p><p><strong>光通量</strong>:指每单位时间内由光源所发出或由被照体所吸收的光能</p><p>使用拜耳阵列,在每个传感器像素井上放置一片滤色镜,使得只有和该滤色镜颜色相同的光才能通过滤色镜,并用四个滤色镜,**红绿绿蓝(RGGB)**组成 2x2 的单元阵进行重复排列,形成的马赛克彩色滤色阵列,这样我们便能得到传感器对不同颜色光的响应情况。</p><img src="/in-camera-image-processing-pipeline/image-20211102123009891.png" alt="image-20211102123009891" style="zoom:67%;"><p><strong>ISP pipeline：</strong></p><p><strong>线性处理</strong>-&gt;<strong>处理黑电平和饱和像素</strong>-&gt;<strong>白平衡</strong>，与颜色恒常性的白平衡不一样，仅仅是对三通道乘以增益系数，以补偿因为三种滤波片具有不同光谱灵敏度带来的影响-&gt;<strong>Demosaicking去马赛克/色彩插值</strong>-&gt;<strong>色彩空间转换</strong> 传感器光谱空间到CIEXYZ到sRGB-&gt;<strong>亮度矫正和伽马矫正</strong></p><img src="/in-camera-image-processing-pipeline/image-20201119091331032.png" alt="颜色空间变换的归一化" style="zoom:67%;"><p><strong>亮度矫正与伽马矫正</strong>之前都是线性变换，因为其实都可以乘一个矩阵将它变换回去，而亮度矫正和伽马矫正是非线性的。如果用Lab空间来描述的话，我们好像只关心ab通道的准确，并不关心L通道，L调合适就行了？不同显示器的物理亮度是不一样的，追求亮度的绝对准确复现没有意义，何况最亮的显示器也远无法复现真实世界里的最大亮度。</p><p><strong>伽马矫正</strong>：</p><p>维基百科：<a href="https://zh.wikipedia.org/wiki/%E4%BC%BD%E7%91%AA%E6%A0%A1%E6%AD%A3">https://zh.wikipedia.org/wiki/%E4%BC%BD%E7%91%AA%E6%A0%A1%E6%AD%A3</a></p><p>LearnOpenGL，这篇讲的挺好：<a href="https://learnopengl-cn.readthedocs.io/zh/latest/05%20Advanced%20Lighting/02%20Gamma%20Correction/">https://learnopengl-cn.readthedocs.io/zh/latest/05%20Advanced%20Lighting/02%20Gamma%20Correction/</a></p><p>简书教程：<a href="https://www.jianshu.com/p/321f39b7fa93">https://www.jianshu.com/p/321f39b7fa93</a></p><p><strong>RGB色彩空间是设备相关的？</strong></p><p><strong>wiki百科色彩空间：</strong><a href="https://zh.wikipedia.org/zh-cn/%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%96%93#cite_note-1">https://zh.wikipedia.org/zh-cn/%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%96%93#cite_note-1</a></p><p>RGB的实现方法有<strong>每原色8位或每原色16位</strong>，实际的RAW image可能是12位或14位，但是为了存储将它扩充到16位。</p><p>每台设备（如显示器或打印机）都有自己的色彩空间并只能生成其色域内的颜色。将图像从某台设备移至另一台设备时，因为每台设备按照自己的色彩空间解释 RGB 或 CMYK 值，所以图像颜色可能会发生变化。为了保证图像在不同设备上显示效果一致，必须使用色彩管理</p><p><strong>颜色匹配实验</strong>：<a href="https://zhuanlan.zhihu.com/p/84897327">https://zhuanlan.zhihu.com/p/84897327</a></p><p><strong>设备无关的颜色空间</strong>：<a href="http://www.doho17.cn/News/507.html">http://www.doho17.cn/News/507.html</a></p><p><strong>与设备有关的颜色空间对应的颜色印象如何取决于生成颜色的设备</strong>。例如：在某台计算机显示器上显示的红色与另一台显示器上显示的红色极有可能不同，这是因为每台显示器根据自己的色彩空间解释色的参数。而RGB、CMYK颜色空间都是与设备有关的颜色模型。</p><p><strong>Lab颜色模型</strong>是由CIE（国际照明委员会）制定的一种色彩模式。自然界中任何一点色都可以在Lab空间中表达出来，它的色彩空间比RGB空间还要大。另外，这种模式是以数字化方式来描述人的视觉感应，与设备无关，所以它弥补了RGB和CMYK模式必须依赖于设备色乡特性的不足。</p><p>这个RGB是指什么？就比如每个摄像机根据自己设备的色彩空间，就是设备相关的。sRGB是设备无关的，暂时理解就是指现在很多显示器都是用sRGB，所以是相对设备无关？？？而CIEXYZ则是完完全全把所有的色域都包括了，它是一个参照量，由它在转换到各个依赖于设备的颜色空间</p><p><strong>颜色空间是什么？就是满足我们的设备我们的显示标准的一个颜色配置标准，比如在a颜色空间1是黄色，但是b颜色空间我们设置2是黄色，大概就是这个意思</strong></p>]]></content>
      
      
      <categories>
          
          <category> Color Constancy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 相机成像 </tag>
            
            <tag> 颜色空间 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人像肤色区域检测</title>
      <link href="ren-xiang-fu-se-jian-ce/"/>
      <url>ren-xiang-fu-se-jian-ce/</url>
      
        <content type="html"><![CDATA[<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>肤色区域检测是肤色定级的前提，只有正确的将图像中人体的肤色区域检测出来，才能探究人体皮肤的肤色感光差异以及分布规律，从而选择更合适的颜色空间并做出更好的划分。</p><p>皮肤检测任务方法类别？</p><p>指标较高的方法和模型？</p><p>肤色定级的不同颜色空间为啥如此定义研究的更深一点？–之后</p><p>学习阶段：现有的算法有哪些可以不全面 ？   可以分为几大类？    看综述怎样分类的？ 指标结果贴出来 </p><p>自己试验：一两个几个重要的算法复现了  前沿的？都没有的话复现最先进  拿数据集跑跑   </p><h3 id="Human-Skin-Detection-Using-RGB-HSV-and-YCbCr-Color-Models"><a href="#Human-Skin-Detection-Using-RGB-HSV-and-YCbCr-Color-Models" class="headerlink" title="Human Skin Detection Using RGB, HSV and YCbCr Color Models"></a>Human Skin Detection Using RGB, HSV and YCbCr Color Models</h3><h4 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h4><p>2017年 引用次数102 会议：ICCASP 怎么感觉查不到这个会议？</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>皮肤颜色具有对方向和大小不变性和处理速度快等特点，常用于人体皮肤检测中。提出了一种新的人体皮肤检测算法。识别皮肤像素的三个主要参数是RGB(红、绿、蓝)、HSV(色调、饱和度、值)和YCbCr(亮度、色度)颜色模型。不仅考虑了三种颜色参数的单独范围，而且考虑了交流计数的组合范围，从而提高了识别给定图像皮肤区域的精度。</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>基于阈值的算法，能够处理不同光照条件下的图像，未来应用可能在脸部、手势识别、皮肤病检测等</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>作为预处理步骤，识别关键是皮肤颜色。其他因素如光照条件也会影响结果，所以常常与纹理、边缘特征等线索相结合。</p><p>检测方式是判断单个像素是否位于我们设定的颜色范围，本文是结合RGB、HSV和YCbCr三个颜色空间设定阈值。</p><h4 id="Literature-Review"><a href="#Literature-Review" class="headerlink" title="Literature Review"></a>Literature Review</h4><p>皮肤检测技术可以大致分为<strong>基于像素的技术</strong>和<strong>基于区域的技术</strong>。<strong>pixel-based</strong> <strong>region-based</strong></p><p>在基于像素的皮肤检测中，每个像素根据一定的条件分别被分为皮肤像素和非皮肤像素。基于颜色值的皮肤检测是基于像素的。</p><p>基于区域的皮肤检测技术考虑<strong>像素点的空间关系</strong>.初始皮肤区域不断判断周围皮肤的属性来增大。</p><h4 id="Color-Spaces"><a href="#Color-Spaces" class="headerlink" title="Color Spaces"></a>Color Spaces</h4><p><strong>基于RGB</strong>：RGB,normalized RGB</p><p>规范化RGB就是一个normalized过程$e.g. r=\frac{R}{R+G+B}$</p><p><strong>基于色调Hue-based</strong>：HSI,HSV,HSL</p><p><strong>基于亮度Luminance-based</strong>:YCbCr,YIQ,YUV</p><p>亮度信息存储为单个分量(Y)，而色度信息存储为两个色差分量(Cb和Cr)。Cb表示蓝色分量与参考值之差。Cr表示红色分量与参考值的差值。</p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201119112418920.png" alt="RGB->YCbCr"></p><h4 id="Proposed-Skin-Detection-Algorithm"><a href="#Proposed-Skin-Detection-Algorithm" class="headerlink" title="Proposed Skin Detection Algorithm"></a>Proposed Skin Detection Algorithm</h4><p>ARGB color model:<a href="https://en.wikipedia.org/wiki/RGBA_color_model">https://en.wikipedia.org/wiki/RGBA_color_model</a></p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201119145342738.png" alt="ARGB32位"></p><p>利用右移操作和与0xff按位与，得到每个通道值，然后根据下图的阈值判断：</p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201119145454111.png" alt="阈值"></p><p>这个阈值如何获得的？</p><h4 id="Experiments-Resuts"><a href="#Experiments-Resuts" class="headerlink" title="Experiments Resuts"></a>Experiments Resuts</h4><img src="/ren-xiang-fu-se-jian-ce/image-20201119145605005.png" alt="实验结果" style="zoom:80%;"><img src="/ren-xiang-fu-se-jian-ce/image-20201119145623528.png" alt="精确率和准确率" style="zoom:80%;"><p>还有一点就不贴图了，即这三个颜色空间所获得的结果是差不多的。</p><h3 id="Combining-Convolutional-and-Recurrent-Neural-Networks-for-Human-Skin-Detection"><a href="#Combining-Convolutional-and-Recurrent-Neural-Networks-for-Human-Skin-Detection" class="headerlink" title="Combining Convolutional and Recurrent Neural Networks for Human Skin Detection"></a>Combining Convolutional and Recurrent Neural Networks for Human Skin Detection</h3><h4 id="Info-1"><a href="#Info-1" class="headerlink" title="Info"></a>Info</h4><p>年份：2017  </p><p>引用次数：72</p><p>期刊：IEEE SIGNAL PROCESSING LETTERS  JCR分区Q2</p><h4 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h4><p>高效的传统手工设计的肤色检测算法需要领域专家的广泛工作</p><p>CNN在像素级标记任务中取得了巨大的成功。CNN的架构不足以建模像素与其邻居之间的关系。引进RNN。FCN层捕获一般的局部特征，RNN层建模图像中的语义上下文依赖。在COMPAO和ECU皮肤数据集上验证了方法的有效性，其中RNN层增强了复杂背景下皮肤检测的识别能力。</p><h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>利用RNN层对图像像素间的语义空间依赖进行建模。</p><h4 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h4><p>在许多常用的颜色空间中，皮肤像素和非皮肤像素之间有明显的重叠。例如，背景中的许多物体，如墙壁、木材和布料，可能与不同类型的人类皮肤有着相似的颜色。在不考虑相邻像素的情况下，很难确定单个像素是皮肤还是非皮肤。此外，皮肤检测的性能还受到多种其他因素的影响(光照不均匀、相机特性、受试者种族、年龄、性别等)。<br>最近的研究集中在：</p><p>不同的颜色空间(如RGB [9], YCbCr [10], CIE-XYZ [11], HSV[12],和SKN[13])</p><p>特征提取(如颜色[14],纹理[15],和空间分布[16])</p><p>分类方法(贝叶斯分类器[9],高斯混合模型[17],支持向量机[18],神经网络[19],随机森林[20]等等)。</p><p>基于cnn的架构不擅长建模像素和它们的邻居之间的关系。最近，Zheng等人[27]在CNN的最后一层引入了<strong>条件随机场</strong>来细化粗略的预测。</p><h4 id="Problem-Statement-amp-Method"><a href="#Problem-Statement-amp-Method" class="headerlink" title="Problem Statement &amp; Method"></a>Problem Statement &amp; Method</h4><h4 id="Experiments-and-Analysis"><a href="#Experiments-and-Analysis" class="headerlink" title="Experiments and Analysis"></a>Experiments and Analysis</h4><p>Matlab环境  什么颜色空间仍然是开放问题，但是有人证明性能在某种程度上是独立于颜色空间的</p><p><strong>实验设置</strong></p><ul><li>FCN初始权值：预训练的FCN-8s </li><li>RNN初始权值：正态分布随机数</li><li>最优化方法：带动量SGD，动量0.9</li><li>学习速率：10^-7固定学习速率，<a href="https://blog.csdn.net/qq_17464457/article/details/101846874?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromBaidu-1.not_use_machine_learn_pai&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromBaidu-1.not_use_machine_learn_pai">bias加倍的学习速率</a></li><li>20个epoch per image 55ms</li></ul><p><strong>数据集</strong></p><ul><li>COMPAQ 只使用了4670皮肤图像 &amp; ECU 1000测试 3000训练</li><li>提供了GT skin masks 和 versatile attributes</li><li>COMPAQ结果相对ECU较差，因为数据集质量较差，包含大量低质量图像和半自动的GT</li></ul><p><strong>实验结果</strong></p><ul><li>混淆矩阵</li><li>the receiver operating characteristics <a href="https://www.jianshu.com/p/2ca96fce7e81">ROC</a> TPR和FPR 两个指标相互有点制衡 ROC相比P-R曲线更稳定，样本数量改变后不会振荡 ROC曲线的<strong>绘制</strong>：调整不同阈值设置，每个阈值在ROC空间上产生一个不同点  ‘*‘表示最佳工作点，由斜率Slope得到</li><li>respective area under curve AUC ROC曲线下的面积 物理意义：</li><li>equal error rate 1-EER</li><li>FCN8s+RNN的最终输出层为2通道的softmax，做二分类，设置阈值，超过阈值为皮肤，否则为非皮肤。</li><li>注意，对现有的alogrithm进行比较是困难的，因为它们要么使用不同的评估指标，要么使用非公共数据集，要么使用任意的操作点(或阈值)。</li></ul><img src="/ren-xiang-fu-se-jian-ce/image-20201120174744122.png" alt="Compaq" style="zoom:50%;"><img src="/ren-xiang-fu-se-jian-ce/image-20201120174807952.png" alt="ECU" style="zoom:50%;"><h3 id="Semi-supervised-Skin-Detection-by-Network-with-Mutual-Guidance"><a href="#Semi-supervised-Skin-Detection-by-Network-with-Mutual-Guidance" class="headerlink" title="Semi-supervised Skin Detection by Network with Mutual Guidance"></a>Semi-supervised Skin Detection by Network with Mutual Guidance</h3><h4 id="Info-2"><a href="#Info-2" class="headerlink" title="Info"></a>Info</h4><p>年份：2019</p><p>会议：ICCV</p><p>Megvii Technology 旷视研究院</p><h4 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h4><p>a single human portrait image-&gt;结合人体作为弱语义指导， 考虑到获取大规模的人体标记皮肤数据通常是昂贵和费时的-&gt;半监督学习策略进行皮肤和身体联合检测的dual-task network-&gt;一个共享编码器，分别用于皮肤和身体的两个解码器，两个解码器可以互相引导</p><h4 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>半监督训练，即不需要一个训练样本存在两种GT</p><h4 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h4><p>之前的方法</p><p>Combining haar feature and skin color based classifiers for face detection 2011 ICASSP</p><p>Adaptive learning of an accurate skin-color model 2004 ICAFGR</p><p>尝试在不同的颜色空间中建模皮肤颜色，并在这些空间中训练皮肤分类器</p><p>缺点：严重依赖于肤色的分布，并且没有涉及到语义信息，因此性能有限。</p><p>基于其他检测任务的DNN的改进受限于皮肤数据</p><p>引入人体检测的两个优势：</p><ul><li>为皮肤位置提供先验信息</li><li>检测到皮肤后，可以过滤掉False positive</li></ul><p>皮肤检测也为人体检测提供了信息</p><p><strong>两个检测器的共享编码器会考虑到两个任务的相似性和网络的紧凑性，从输入图像中提取共同的特征图。这种结构使我们在训练皮肤检测网络时不需要增加带注释的训练数据，而只需要增加一个人体面罩数据集，这更容易获得。由于这两个数据集分别包含两种ground truth类型，即一个数据样本中要么有一个目标skin mask，要么有一个body mask，因此我们采用新设计的loss和自定义的训练策略对网络进行半监督的训练。</strong></p><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p><strong>Skin detection and segmentation</strong></p><p>现有的方法可以分为三类:</p><ol><li>在颜色空间上明确定义边界模型/阈值-划分区域定皮肤像素-皮肤和非皮肤像素存在明显重叠</li><li>,应用传统机器学习技术学习种肤色模型-生成/判别模型预测，可能考虑到纹理等局部特征-学习能力有限，精度较低</li><li>利用深度神经网络学习皮肤分割的端到端模型–MLP-&gt;FCN,需要大规模监督-&gt;18CVPR Normalized cut loss for weakly-supervised cnn segmentation 引入条件随机场，使弱监督成为可能-&gt;本文引用额外数据集提高性能</li></ol><p><strong>Multi-task joint learning</strong></p><p>它通常通过在所有任务之间共享隐藏层来应用，同时保留几个特定于任务的输出层作为分支。</p><p>一些多任务网络通常通过共享编码器来学习共同的特征图，从而潜在地同时提高所有任务的性能。</p><h4 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h4><p>**Dataset and Implementation Details **</p><ul><li>10711 RGB images 5000 skin masks 5711 body masks 网络收集 512x512分辨率 </li><li>随机选择470S和475B，作验证集  随即翻转/调整/裁剪-训练数据增强</li><li>Tensorflow NVIDIA GeForce GTX 1080Ti GPU 训练了12个小时</li></ul><p><strong>Comparison with Existing Methods</strong></p><ul><li>2种传统算法 六种NN算法</li><li>Human skin color clustering for face detection  2003 IEEE RGB和HSV空间定阈值</li><li>Statistical color models with application to skin detection 2002 IJCV 学习GMM预测</li><li>缺陷：缺乏高级特征，对复杂环境和光照变化鲁棒性差</li><li>A skin detector based on neural network 2002 ICCCSWSE</li><li>Combining convolutional and recurrent neural networks for human skin detection 2017 SPL</li><li>U- net: Convolutional networks for biomedical image segmentation 2015</li><li>Deep residual learning for image recognition 2016CVPR</li><li>Encoder-decoder with atrous separable convolution for semantic image segmentation 2018ECCV</li></ul><p>We trained the six networks to convergence收敛 with multiple trials实验 with dataset DS, and selected their best results</p><p>For recall, our method ranks only below the GMM method, which has more false alarms so as to suffer from a poor precision. </p><p>定量比较-Table1 Figure6 平衡和不平衡 此文数据集与Pratheepan IoU IoU Top-1 Precision Recall </p><p>几个典型预测结果-Figure5 Figure1 这几个典型带有各种挑战性 传统方法完全失败 其他CNN不稳定 我们的有效且可靠</p><p><strong>Ablation Studies</strong></p><p><a href="https://www.zhihu.com/question/60170398">ablation study</a></p><p>去掉某些特征 模型 算法 对结果会有什么影响，就是控制变量，为了研究你提出的方法是不是有效 </p><p>根据奥卡姆剃刀法则，简单和复杂的方法能达到一样的效果，那么简单的方法更可靠。</p><p><strong>1-Mutual guidance</strong></p><p>Figure4 显示有无互指导的效果 即使没有互指导 也比单任务最好的好 因为共享编码器从身体数据学到了信息</p><p><strong>2-Weakly supervised losses</strong></p><p>Table2Top虽不显著 确实起到了作用</p><p>Figure3 显示了一个例子 不同的弱监督损失的作用</p><p><strong>3-Unbalanced dataset</strong></p><p>Figure1 若使用 不平衡数据集   IoU下降约%6 但仍比其他的高</p><p><strong>4-Backbone networks</strong></p><p>Table2Bottom Mobile-Net 替换UNet IoU相对较低 但互指导起到的作用更大</p><p><strong>5-Training strategy</strong></p><ul><li>Gradient stopping Figure7 </li><li>Initial guidance  给个先验会好一点</li><li>Finetune Figure4 Top <strong>train-from-scratch</strong> and <strong>finetune versions</strong>  有没有预训练</li></ul><h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><p>双任务FCN 输入一个RGB图像I 输出 skin Os 和 body Ob的概率图 两个解码器Ds Db 编码器E输出I的特征图Ei Os和Ei送到Db 反之亦然  网络结构 Figure2a</p><p><strong>1-Network with Mutual Guidance</strong></p><p>Figue2b 将原始网络解耦为没有循环的两个阶段 X表示Stage1 X‘ Stage2 Xk包括Xs和Xb</p><p>I E Dk 相同 Gk和Gk’不同 第一阶段信息很少而第二阶段有Ok作为指引</p><p>Table1 Figure4共享编码器考虑：</p><ol><li>虽然两个任务GT不同，但具有相似的统计信息</li><li>特征图具有共性，如区分人的前景和非人的背景的鲁棒性</li></ol><p><img src="/ren-xiang-fu-se-jian-ce/image-20201122214758315.png" alt="网络结构"></p><p>es eb 第一阶段的指导 一般设为0 应该是一直为0，不会变     E和Dk的结构采用了标准UNet，包括E中的4个下采样 Dk中的4个上采样 </p><p>输入512x512x3  E-&gt;Dk 的特征映射Ei 32x32x1024  另外应用了一个和E相同结构但是通道数只有一半的编码器给Gk，然后将输出与Ei相连，再喂给Dk</p><p>每个FCN层 kernel size 3x3 后面是一个BatchNorm和一个ReLu层</p><p><strong>2-Learning Algorithm</strong></p><p>对于人体检测，由于广泛研究，数据易得 所以对每个数据对，只提供Ms或Mb 很少提供（I，Ms，Mb）所以是一个半监督任务</p><p>这主要有半监督损失和具体训练细节实现</p><p><strong>2.1Semi-supervised loss</strong></p><p>半监督损失由三部分组成，包括：strongly-supervised and weakly-supervised ones</p><p>再有GT的一侧算Cross-entropy loss 另一侧算CRF loss 两个输出集合起来算WCE loss，</p><p><strong>Cross-entropy loss</strong></p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201123092722193.png" alt="Cross-entropy loss"></p><p>where $L_{ce}(x,y)=x\cdot log(y)+(1-x)\cdot log(1-y)$       $l_k$表示一委托个数据样例是否含有GT$M_k$</p><p><strong>CRF loss</strong></p><p>从 On regularized losses for weakly-supervised cnn segmentation 2018ECCV 引入CRFLoss CRF可以使I中颜色相同且相邻的像素在</p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201123093447500.png" alt="CRF loss"></p><p>where  $L_{crf}=S^TWS$  W是I的Affinity Matrix S是平坦化的Ok的列向量   具体看论文</p><p><strong>WCE loss</strong></p><p>输出之间也有限制，皮肤概率高的话，那么身体概率也应该高，也就是两个分布一致，皮肤概率低的话，就没有限制了，所以用皮肤概率做权重，限定WCE loss 对总体的影响</p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201123100039914.png" alt="WCE Loss"></p><p>$L_{wce}(x,y)=x\cdot L_{ce}(x,y)$</p><p><strong>semi-supervised loss</strong></p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201123100808245.png" alt="semi-supervised loss"></p><p>$\lambda_1=0.0001 \quad \lambda_2=0.001 $</p><p><strong>2.2Training details</strong><br><strong>Dual-task joint learning</strong></p><p>奇数次和偶数次分别喂MB和MS，用Lk来指导算loss</p><p><strong>Finetune</strong></p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201124205344966.png" alt="Stage2 Guidance"></p><p>有GT的就直接用真实的指导就行</p><p>两个阶段的指导变化很大，但是他们使用相同的解码器权重,不然G’反向传播，防止Decoder参数对G‘过拟合，对G欠拟合</p><h3 id="SKINNY-A-LIGHTWEIGHT-U-NET-FOR-SKIN-DETECTION-AND-SEGMENTATION"><a href="#SKINNY-A-LIGHTWEIGHT-U-NET-FOR-SKIN-DETECTION-AND-SEGMENTATION" class="headerlink" title="SKINNY: A LIGHTWEIGHT U-NET FOR SKIN DETECTION AND SEGMENTATION"></a>SKINNY: A LIGHTWEIGHT U-NET FOR SKIN DETECTION AND SEGMENTATION</h3><h4 id="Info-3"><a href="#Info-3" class="headerlink" title="Info"></a>Info</h4><p>2020ICIP</p><h4 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h4><p>lightweight轻量的</p><p>全卷积UNet对图像分割很有效</p><p> 空间上下文对皮肤分隔很重要，对UNet很狭窄是什么意思</p><p>扩展了多尺度分析的范围</p><p>在ECU和HGR测试</p><h4 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Skinny受益与几个架构组件,包括inception和dense blocks，从而更好利用上下文 </p><p>inception和dense很关键，并且不影响速度</p><p>未来：增强上下文像素分析思想，使用扩张卷积，保持参数低数量，减少FP</p><h4 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h4><p>由于皮肤外观的低特异性和高方差，该问题仍具有挑战性</p><h4 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h4><p>图一 UNet没有足够上下文，分类错了，Skinny考虑到更广泛背景，分类正确了</p><p>图二 网络结构和两种变体</p><p> <a href="https://blog.csdn.net/u014380165/article/details/75142664">DenseNet</a>      </p><p>注意语义分割和实例分割的区别，语义分割是人都归为一类，实例分割是不同的人标成不同的类 </p><p> <a href="https://zhuanlan.zhihu.com/p/83496100">全景分割</a>将语义分割和实例分割结合在一起，既能分割背景，也能分割实例   </p><p><a href="https://www.sohu.com/a/284185732_129720">全景分割</a>图像中的内容可以按照是否有固定形状分为 things 类别和 stuff 类别，其中，人，车等有固定形状的物体属于 things 类别（可数名词通常属于 things）；天空，草地等没有固定形状的物体属于 stuff 类别（不可数名词属于 stuff）</p><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><a href="https://www.aiuai.cn/aifarm1159.html">dice-loss</a>  对于二分类问题，GT 分割图是只有 0, 1 两个值的，因此 |X⋂Y| 可以有效的将在 Pred 分割图中未在 GT 分割图中激活的所有像素清零. 对于激活的像素，主要是惩罚低置信度的预测，较高值会得到更好的 Dice 系数.   </p><p>意思就是说主要注重皮肤点预测的怎么样，非皮肤点的预测就不管了</p>]]></content>
      
      
      <categories>
          
          <category> 肤色分级 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 肤色分级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器使用</title>
      <link href="fu-wu-qi-shi-yong/"/>
      <url>fu-wu-qi-shi-yong/</url>
      
        <content type="html"><![CDATA[<p>linux教程：<a href="http://c.biancheng.net/view/4017.html">http://c.biancheng.net/view/4017.html</a></p><h3 id="基本函数"><a href="#基本函数" class="headerlink" title="基本函数"></a>基本函数</h3><pre class=" language-python"><code class="language-python">unzip <span class="token operator">-</span>d des src<span class="token comment" spellcheck="true">#解压到指定目录 解压zip</span>zip <span class="token operator">-</span>r day<span class="token punctuation">.</span>zip day<span class="token operator">/</span><span class="token operator">*</span><span class="token comment" spellcheck="true"># 压缩指定目录为zip文件，会将软链接文件转为硬链接文件</span>zip <span class="token operator">-</span>ry day<span class="token punctuation">.</span>zip day<span class="token operator">/</span><span class="token operator">*</span><span class="token comment" spellcheck="true"># 加个y,会保留软链接文件</span>tar <span class="token operator">-</span>xzvf <span class="token punctuation">.</span>tar<span class="token punctuation">.</span>gz<span class="token comment" spellcheck="true"># 解压.tar.gz</span>tar <span class="token operator">-</span>cvf FileName<span class="token punctuation">.</span>tar DirName<span class="token comment" spellcheck="true"># 使用tar打包</span><span class="token comment" spellcheck="true"># 有可能使用gzip2压缩的，这个时候可以使用file查看文件类型</span>file filename<span class="token punctuation">.</span>tar<span class="token punctuation">.</span>gz<span class="token comment" spellcheck="true"># 如果是用 bzip2 压缩的，而不是 gzip，则使用以下命令</span>tar <span class="token operator">-</span>xjvf data<span class="token punctuation">.</span>tar<span class="token punctuation">.</span>gzgunzip <span class="token comment" spellcheck="true"># 用于解开被 gzip 压缩过的文件，这些压缩文件预设最后的扩展名为 .gz。 解压完会删除</span>gunzip passwd<span class="token punctuation">.</span>gztar <span class="token operator">-</span>xvf file<span class="token punctuation">.</span>tar <span class="token comment" spellcheck="true"># 解压tar包</span>df <span class="token operator">-</span>h<span class="token comment" spellcheck="true"># 查看磁盘空间大小</span>history<span class="token comment" spellcheck="true"># 查看历史命令</span>nvcc <span class="token operator">-</span>V<span class="token comment" spellcheck="true"># 查看当前cuda版本</span>ps aux <span class="token operator">|</span>grep dch<span class="token comment" spellcheck="true"># 查看进程 |gerp是查找对应的字符串</span>kill <span class="token operator">-</span><span class="token number">9</span> pid<span class="token comment" spellcheck="true"># 杀死指定pid的进程,9是信号编号SIGKILL</span>cdcd <span class="token operator">~</span><span class="token comment" spellcheck="true">#都会返回到用户主目录/home/dch</span>sudo <span class="token comment" spellcheck="true">#用来执行需要提升权限(通常是作为root用户)的命令</span><span class="token comment" spellcheck="true"># 显示后20行</span>tail <span class="token operator">-</span>n <span class="token number">20</span> <span class="token number">1.</span>txt<span class="token comment" spellcheck="true"># 显示持续刷新的内容</span>tail <span class="token operator">-</span>f <span class="token number">1.</span>log<span class="token comment" spellcheck="true"># 显示内存使用情况</span>free<span class="token comment" spellcheck="true"># 实时显示进程情况，包括内存等</span>top<span class="token comment" spellcheck="true"># 删除文件</span>rm filename<span class="token comment" spellcheck="true"># 删除文件夹,递归地删除目录</span>rm <span class="token operator">-</span>r dirname<span class="token comment" spellcheck="true"># 如果目录是受写保护的，则会提示是否继续删除目录和目录中的文件作为一个整体。-f 强制删除而不被提示。</span>rm <span class="token operator">-</span>rf dirname<span class="token comment" spellcheck="true"># 复制文件夹， 表示将dir1及其dir1下所包含的文件复制到dir2下</span>cp <span class="token operator">-</span>r dir1 dir2<span class="token comment" spellcheck="true"># 重启命令</span>shutdown <span class="token operator">-</span>r now<span class="token comment" spellcheck="true"># 显示当前工作夹的绝对目录</span>pwd<span class="token comment" spellcheck="true"># 使用requirements格式列出所有已安装的包,包以不区分大小写的排序顺序列出</span>pip freeze <span class="token operator">&gt;</span> requirments<span class="token punctuation">.</span>txt<span class="token comment" spellcheck="true"># 安装requirements.txt的包</span>pip install <span class="token operator">-</span>r requirements<span class="token punctuation">.</span>txt<span class="token comment" spellcheck="true"># 软连接 -s或--symbolic 　对源文件建立符号连接，而非硬连接。</span>ln <span class="token operator">-</span>s 源文件 软连接文件</code></pre><p><img src="/fu-wu-qi-shi-yong/image-20211009172341652.png" alt="image-20211009172341652"></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 创建新文件</span>touch filename<span class="token comment" spellcheck="true"># 以树状图列出目录的内容。</span>tree dir<span class="token comment" spellcheck="true"># 服务器间数据互传 https://blog.csdn.net/zhangjipinggom/article/details/106137809</span>scp <span class="token operator">-</span>r <span class="token operator">/</span>home<span class="token operator">/</span>dch<span class="token operator">/</span>DarkSeg<span class="token operator">/</span>path<span class="token operator">/</span>to<span class="token operator">/</span> dch@<span class="token number">10.112</span><span class="token punctuation">.</span><span class="token number">9.72</span><span class="token punctuation">:</span><span class="token operator">/</span>home<span class="token operator">/</span>dch<span class="token operator">/</span>DarkSeg<span class="token operator">/</span>path<span class="token operator">/</span><span class="token comment" spellcheck="true"># 如果提示端口不对（默认端口是22），改成：</span>scp <span class="token operator">-</span>P port <span class="token operator">-</span>r <span class="token operator">/</span>home<span class="token operator">/</span>dch<span class="token operator">/</span>DarkSeg<span class="token operator">/</span>path<span class="token operator">/</span>to<span class="token operator">/</span> dch@<span class="token number">10.112</span><span class="token punctuation">.</span><span class="token number">9.72</span><span class="token punctuation">:</span><span class="token operator">/</span>home<span class="token operator">/</span>dch<span class="token operator">/</span>DarkSeg<span class="token operator">/</span>path<span class="token operator">/</span><span class="token comment" spellcheck="true"># 显示各用户占用空间大小</span>sudo du <span class="token operator">-</span>sh <span class="token operator">/</span>home<span class="token operator">/</span><span class="token operator">*</span><span class="token comment" spellcheck="true"># nohup及取消输出</span>nohup java <span class="token operator">-</span>cp WEB<span class="token operator">-</span>INF<span class="token operator">/</span>lib<span class="token operator">/</span><span class="token operator">*</span><span class="token punctuation">:</span>WEB<span class="token operator">-</span>INF<span class="token operator">/</span>classes org<span class="token punctuation">.</span>b3log<span class="token punctuation">.</span>solo<span class="token punctuation">.</span>Starter <span class="token operator">&gt;</span><span class="token operator">/</span>dev<span class="token operator">/</span>null <span class="token number">2</span><span class="token operator">&gt;</span><span class="token operator">&amp;</span><span class="token number">1</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true"># &gt;/dev/null 将信息输出到/dev/null</span><span class="token comment" spellcheck="true"># 2&gt;&amp;1 将错误信息重定向到标准输出</span><span class="token comment" spellcheck="true"># 最后一个&amp;符号，表示程序在后台运行</span><span class="token triple-quoted-string string">"""/dev/null:在类 Unix 系统中，/dev/null，或称空设备，是一个特殊的设备文件，它丢弃一切写入其中的数据（但报告写入操作成功），读取它则会立即得到一个 EOF。在程序员行话，尤其是 Unix 系统中，/dev/null 被称为位桶(bit bucket)或者黑洞(black hole)。空设备通常被用于丢弃不需要的输出流，或作为用于输入流的空文件。当你读它的时候，它会提供无限的空字符(NULL, ASCII NUL, 0x00)。"""</span>adduser username 用户名<span class="token comment" spellcheck="true"># linux添加新用户</span>userdel <span class="token operator">-</span>r 用户名</code></pre><h3 id="GPU使用情况查看？"><a href="#GPU使用情况查看？" class="headerlink" title="GPU使用情况查看？"></a>GPU使用情况查看？</h3><p><a href="https://www.jianshu.com/p/ceb3c020e06b">以下来自木子的毛线</a></p><pre class=" language-shell"><code class="language-shell">watch -n 1 nvidia-smi</code></pre><img src="/fu-wu-qi-shi-yong/image-20211009133137665.png" alt="image-20211009133137665" style="zoom:80%;"><ul><li><strong>GPU：</strong>本机中的GPU编号(有多块显卡的时候,从0开始编号)</li><li><strong>Fan：</strong>风扇转速(0%-100%)，N/A表示没有风扇</li><li><strong>Name：</strong>GPU类型</li><li><strong>Temp：</strong>GPU的温度(GPU温度过高会导致GPU的频率下降)</li><li><strong>Perf：</strong>GPU的性能状态,从P0(最大性能)到P12(最小性能),图上是：P2</li><li><strong>Persistence-M：</strong>持续模式的状态,持续模式虽然耗能大,但是在新的GPU应用启动时花费的时间更少,图上显示的是：off</li><li><strong>Pwr：Usager/Cap：</strong>能耗表示,Usage用了多少,Cap总共多少</li><li><strong>Bus-Id：</strong>GPU总线相关显示,domain：bus：device.function</li><li><strong>Disp.A：</strong>Display Active ,表示GPU的显示是否初始化</li><li><strong>Memory-Usage：</strong>显存使用率</li><li><strong>Volatile GPU-Util：</strong>GPU使用率</li><li><strong>Uncorr. ECC：</strong>关于ECC的东西，是否开启错误检查和纠正技术，0/disabled,1/enabled</li><li><strong>Compute M：</strong>计算模式,0/DEFAULT,1/EXCLUSIVE_PROCESS,2/PROHIBITED</li><li><strong>Processes：</strong>显示每个进程占用的显存使用率、进程号、占用的哪个GPU</li></ul><p><strong>将监控结果写入文件，并且指定写入文件的监控字段</strong></p><pre class=" language-shell"><code class="language-shell">nvidia-smi -l 1 --format=csv --filename=report.csv --query-gpu=timestamp,name,index,utilization.gpu,memory.total,memory.used,power.draw,temperature.gpu</code></pre><p><strong>-l：</strong>隔多久记录一次，命令中写的是1</p><p><strong>–format：</strong>结果记录文件格式是csv</p><p><strong>–filename:</strong> 结果记录文件的名字</p><p><strong>–query-gpu：</strong>记录哪些数据到csv文件</p><p><strong>timestamp：</strong>时间戳</p><p><strong>memory.total：</strong>显存大小</p><p><strong>memory.total：</strong>显存使用了多少</p><p><strong>utilization.gpu：</strong>GPU使用率</p><p><strong>power.draw：</strong>显存功耗，对应Pwr：Usage</p><p>**temperature.gpu:**显卡温度</p><h3 id="linux服务器安装conda？"><a href="#linux服务器安装conda？" class="headerlink" title="linux服务器安装conda？"></a>linux服务器安装conda？</h3><p><a href="https://blog.csdn.net/weixin_40258579/article/details/85001218?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">CSDN</a></p><p><a href="https://docs.anaconda.com/anaconda/install/linux/">官方文档</a></p><p><code>source ~/.bashrc</code>环境变量生效</p><h3 id="conda的使用"><a href="#conda的使用" class="headerlink" title="conda的使用"></a>conda的使用</h3><p>菜鸟教程pip使用介绍：<a href="https://www.runoob.com/w3cnote/python-pip-install-usage.html">https://www.runoob.com/w3cnote/python-pip-install-usage.html</a></p><p>pip安装包应该是安装到了<code>Lib/site-packages</code></p><p>windows用命令行运行python文件,利用anaconda prompt,然后 python  *.py</p><p><code>pip show 包</code>   显示包的安装位置</p><pre class=" language-shell"><code class="language-shell"># 查看当前存在哪些虚拟环境conda env list# 创建虚拟环境# your_env_name文件可以在Anaconda安装目录envs文件下找到。conda create -n your_env_name python=X.X # (2.7、3.6等)# 激活虚拟环境conda activate ml_django# 关闭虚拟环境conda deactivate# 删除虚拟环境conda remove -n your_env_name --all</code></pre><h3 id="安装pytorch"><a href="#安装pytorch" class="headerlink" title="安装pytorch?"></a>安装pytorch?</h3><p><a href="https://blog.csdn.net/xzy5210123/article/details/107237037?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">换源解决速度慢教程</a></p><p>建议安装下面这个版本的pytorch,不然咱们实验室的cuda用不了</p><p><code>pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html</code></p><p><a href="https://discuss.pytorch.org/t/rtx-3000-support/98158">https://discuss.pytorch.org/t/rtx-3000-support/98158</a></p><p><a href="https://blog.csdn.net/wuzhiwuweisun/article/details/82753403">包括cuda、cudnn、pytorch的全方位介绍</a></p><hr><p>上面的其实是因为那个版本用的是cuda11.0,实验室的cuda也是11.0</p><p>其实只要pytorch版本可cuda11.对应都可以</p><h3 id="python版本更改？"><a href="#python版本更改？" class="headerlink" title="python版本更改？"></a>python版本更改？</h3><p>还是有大佬的！<a href="https://www.cnblogs.com/chenhuabin/p/10718471.html">https://www.cnblogs.com/chenhuabin/p/10718471.html</a></p><p><img src="/fu-wu-qi-shi-yong/image-20201113190611958.png" alt="这一步是重点"></p><h3 id="Xshell设置"><a href="#Xshell设置" class="headerlink" title="Xshell设置"></a>Xshell设置</h3><p>复制粘贴</p><p><a href="https://www.cnblogs.com/sxdcgaq8080/p/10025759.html">https://www.cnblogs.com/sxdcgaq8080/p/10025759.html</a></p><h4 id="jupyter配置？"><a href="#jupyter配置？" class="headerlink" title="jupyter配置？"></a>jupyter配置？</h4><p>首先，安装jupyter</p><pre class=" language-shell"><code class="language-shell">pip install jupyter  </code></pre><p>生成jupyter配置文件</p><pre class=" language-shell"><code class="language-shell">jupyter notebook --generate-config</code></pre><p>打开ipython，生成密码：</p><pre class=" language-shell"><code class="language-shell"># ipythonIn [1]:  from notebook.auth import passwdIn [2]: passwd()'argon2:$argon2id$v=19$m=10240,t=10,p=8$/ea+nGkJvktsLC2yo2Huzw$fr/mxPU0bp5BZxAxObad2A''sha1:ad2bd4dc8582:9d740abfc74bf994362d0c0dbf866e2b749c4ada'</code></pre><p>会让输入两次密码，输入完成后 复制生成的 秘钥，后面会用到，秘钥带上sha1</p><p>修改配置文件：<a href="https://www.cnblogs.com/logsharing/p/8036893.html">vi中查找</a>   命令模式/字符串</p><pre class=" language-shell"><code class="language-shell">vi /username/.jupyter/jupyter_notebook_config.py #/username/可能要去掉如果在这个目录的话    #改几个地方：c.NotebookApp.ip = 'localhost'  #一定要把注释的#号去掉c.NotebookApp.port = 10001                    即对外提供访问的端口c.NotebookApp.open_browser = False            False即启动不打开浏览器c.NotebookApp.password = u'sha1:XXXXX'   这个就是上面生成的秘钥c.NotebookApp.notebook_dir = u'workplace' 即设置jupyter启动后默认的根目录，这个文件夹要自己创建</code></pre><p><code>mkdir workplace</code></p><p>然后打开xshell，文件-&gt;当前会话属性-&gt;隧道-&gt;添加 #端口号可以自己设置别的</p><p><img src="/fu-wu-qi-shi-yong/image-20201113093744564.png" alt="隧道属性"></p><p>然后打开jupyter</p><pre class=" language-shell"><code class="language-shell">jupyter notebook</code></pre><p>然后在windows上打开localhost:10001,就能在windows上使用linux的jupyter notebook</p><h3 id="安装Cuda和Cudnn"><a href="#安装Cuda和Cudnn" class="headerlink" title="安装Cuda和Cudnn"></a>安装Cuda和Cudnn</h3><p><a href="https://blog.csdn.net/matrix273/article/details/103534991">使用conda创建指定cuda版本环境</a></p><p>上面的tql！！！！</p><pre class=" language-shell"><code class="language-shell"># 首先创建一个虚拟环境conda create –n py3.6tf python=3.6 #创建Python3.6版本的名为py3.6tf的虚拟环境conda activate py3.6tf #激活环境# 2查询支持的cuda版本conda search cudatoolkit </code></pre><img src="/fu-wu-qi-shi-yong/image-20211211001707690.png" alt="image-20211211001707690" style="zoom:67%;"><pre class=" language-shell"><code class="language-shell"># 3 conda search cudnn #查询支持的cudnn版本,需要确保cuda版本一致</code></pre><img src="/fu-wu-qi-shi-yong/image-20211211001756161.png" alt="image-20211211001756161" style="zoom:67%;"><pre class=" language-shell"><code class="language-shell">conda install cudatoolkit=9.0 cudnn=7.3.1 #安装指定版本的cuda,cudnn到当前的虚拟环境中</code></pre><p>然后就可以用了,太强了!!!!!!</p><p>==<strong>Cuda30系不支持cuda10.0,至少也是cuda11.1了,但是tensorflow1.x官网写的是cuda10.0,实际上11.1也能跑</strong>==</p><p>Cuda如何加载，把这个搞明白好吧</p><p>首先取Cuda官网下载对应版本的Cuda Toolkit</p><p>在本地环境变量设置cuda</p><pre class=" language-shell"><code class="language-shell">#可以显示安装的cudals /usr/local#显示如下#bin  cuda  cuda-11.3  etc  games  include  lib  man  sbin  share  src  sunlogin# 将下面这些命令加到.bashrc文件末尾#加CUDA的bin到PATH环境变量export PATH=$PATH:/usr/local/cuda/bin#加CUDA的库文件到系统库搜索路径export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64</code></pre><p>然后<code>source .bashrc</code>，然后可以运行<code>nvcc -V</code>查看版本来检查是否正确，出现下述情况表示正确运行</p><p><img src="/fu-wu-qi-shi-yong/image-20210519204527712.png" alt="image-20210519204527712"></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 安装torch</span>pip install torch<span class="token operator">==</span><span class="token number">1.11</span><span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">+</span>cu113 torchvision<span class="token operator">==</span><span class="token number">0.12</span><span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">+</span>cu113 torchaudio<span class="token operator">==</span><span class="token number">0.11</span><span class="token punctuation">.</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>extra<span class="token operator">-</span>index<span class="token operator">-</span>url https<span class="token punctuation">:</span><span class="token operator">//</span>download<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>org<span class="token operator">/</span>whl<span class="token operator">/</span>cu113<span class="token comment" spellcheck="true"># 这个特别好用</span></code></pre><p><a href="https://zhuanlan.zhihu.com/p/619901627">安装cuda与cudnn</a></p><p><a href="https://www.jianshu.com/p/622f47f94784">Cuda与Cudnn的关系</a></p><p> <strong>CUDA看作是一个工作台，上面配有很多工具，如锤子、螺丝刀等。</strong>cuDNN是基于CUDA的深度学习GPU加速库，有了它才能在GPU上完成深度学习的计算。<strong>它就相当于工作的工具，比如它就是个扳手。但是CUDA这个工作台买来的时候，并没有送扳手。想要在CUDA上运行深度神经网络，就要安装cuDNN，就像你想要拧个螺帽就要把扳手买回来。</strong>这样才能使GPU进行深度神经网络的工作，工作速度相较CPU快很多。</p><p>从官方安装指南可以看出，<strong>只要把cuDNN文件复制到CUDA的对应文件夹里就可以，即是所谓插入式设计</strong>，把cuDNN数据库添加CUDA里，<strong>cuDNN是CUDA的扩展计算库，不会对CUDA造成其他影响。</strong></p><img src="/fu-wu-qi-shi-yong/image-20210519210512044.png" alt="image-20210519210512044" style="zoom:67%;"><img src="/fu-wu-qi-shi-yong/image-20210519210813074.png" alt="image-20210519210813074" style="zoom: 50%;"><p>二者文件不重复，所以不会对原有的Cuda功能造成影响，卸载Cudnn也很简单，只需要把对应的Cudnn文件删除即可</p><h3 id="后台使用jupyter"><a href="#后台使用jupyter" class="headerlink" title="后台使用jupyter"></a>后台使用jupyter</h3><p><a href="https://blog.csdn.net/donaldsy/article/details/96350061">后台运行jupyter教程</a></p><pre class=" language-shell"><code class="language-shell">jupyter notebook &amp; #后台运行，你关掉终端会停止运行nohup jupyter notebook &amp; #后台运行，你关掉终端也会继续运行ps -aux | grep jupyter #查找jupyter命令jobs -l#查看当前有多少在后台运行的命令，加上-l参数可以显示后台运行的pidfg %jobnumber#将后台中的命令调至前台继续运行。%jobnumber是jobs查到的序号，不是pidbg %jobnumber#将一个在后台暂停的命令，变成继续执行。</code></pre><h3 id="opencv安装"><a href="#opencv安装" class="headerlink" title="opencv安装"></a>opencv安装</h3><p><a href="https://www.cnblogs.com/thewaytotheway/p/12847260.html">https://www.cnblogs.com/thewaytotheway/p/12847260.html</a></p><h3 id="tensorflow安装"><a href="#tensorflow安装" class="headerlink" title="tensorflow安装"></a>tensorflow安装</h3><p>pip install tensorflow安装的是CPU版本</p><p>pip install tensorflow-gpu 安装的是gpu版本</p><p><a href="%E8%BF%99%E4%B8%AA%E5%8D%9A%E5%AE%A2%E5%A4%A7%E4%BD%ACtql">https://blog.csdn.net/china_xin1/article/details/109824882</a> 显卡3090只支持CUDA11</p><p>所以要<code>pip install tf-nightly-gpu</code>  尝新版</p><img src="/fu-wu-qi-shi-yong/image-20211117165748691.png" alt="image-20211117165748691" style="zoom:67%;"><p><code>TensorFlow1.x</code>GPU和CPU软件包是分开的,<code>TensorFlow2.x</code>同时支持CPU和GPU。</p><h3 id="bashrc"><a href="#bashrc" class="headerlink" title=".bashrc"></a>.bashrc</h3><p><code>/home/dch/.bashrc</code> 配置文件，保存个人的一些个性化设置，如命令别名、路径、环境变量等</p><p><code>source .bashrc</code>立刻加载修改后的设置，使之生效。</p><p><code>export</code></p><h3 id="查找文件"><a href="#查找文件" class="headerlink" title="查找文件"></a>查找文件</h3><p><a href="https://www.linuxprobe.com/linux-jar.html">https://www.linuxprobe.com/linux-jar.html</a>   find -name 文件</p><h3 id="vim调高亮"><a href="#vim调高亮" class="headerlink" title="vim调高亮"></a>vim调高亮</h3><pre><code>set nocompatible " 关闭 vi 兼容模式syntax on " 自动语法高亮set number " 显示行号set cursorline " 突出显示当前行set ruler " 打开状态栏标尺set shiftwidth=4 " 设定 &lt;&lt; 和 &gt;&gt; 命令移动时的宽度为 4set softtabstop=4 " 使得按退格键时可以一次删掉 4 个空格set tabstop=4 " 设定 tab 长度为 4set nobackup " 覆盖文件时不备份set autochdir " 自动切换当前目录为当前文件所在的目录filetype plugin indent on " 开启插件set backupcopy=yes " 设置备份时的行为为覆盖set ignorecase smartcase " 搜索时忽略大小写，但在有一个或以上大写字母时仍保持对大小写敏感set nowrapscan " 禁止在搜索到文件两端时重新搜索set incsearch " 输入搜索内容时就显示搜索结果set hlsearch " 搜索时高亮显示被找到的文本set noerrorbells " 关闭错误信息响铃set novisualbell " 关闭使用可视响铃代替呼叫set t_vb= " 置空错误铃声的终端代码"set showmatch " 插入括号时，短暂地跳转到匹配的对应括号"set matchtime=2 " 短暂跳转到匹配括号的时间set magic " 设置魔术set hidden " 允许在有未保存的修改时切换缓冲区，此时的修改由 vim 负责保存set guioptions-=T " 隐藏工具栏set guioptions-=m " 隐藏菜单栏set smartindent " 开启新行时使用智能自动缩进set backspace=indent,eol,start" 不设定在插入状态无法用退格键和 Delete 键删除回车符set cmdheight=1 " 设定命令行的行数为 1set laststatus=2 " 显示状态栏 (默认值为 1, 无法显示状态栏)set statusline=\ %&lt;%F[%1*%M%*%n%R%H]%=\ %y\ %0(%{&amp;fileformat}\ %{&amp;encoding}\ %c:%l/%L%)\ " 设置在状态行显示的信息set foldenable " 开始折叠set foldmethod=syntax " 设置语法折叠set foldcolumn=0 " 设置折叠区域的宽度setlocal foldlevel=1 " 设置折叠层数为" set foldclose=all " 设置为自动关闭折叠 " nnoremap &lt;space&gt; @=((foldclosed(line('.')) &lt; 0) ? 'zc' : 'zo')&lt;CR&gt;" 用空格键来开关折叠" return OS type, eg: windows, or linux, mac, et.st..function! MySys()if has("win16") || has("win32") || has("win64") || has("win95")return "windows"elseif has("unix")return "linux"endifendfunction" 用户目录变量$VIMFILESif MySys() == "windows"let $VIMFILES = $VIM.'/vimfiles'elseif MySys() == "linux"let $VIMFILES = $HOME.'/.vim'endif" 设定doc文档目录let helptags=$VIMFILES.'/doc'" 设置字体 以及中文支持if has("win32")set guifont=Inconsolata:h12:cANSIendif" 配置多语言环境if has("multi_byte")" UTF-8 编码set encoding=utf-8set termencoding=utf-8set formatoptions+=mMset fencs=utf-8,gbkif v:lang =~? '^\(zh\)\|\(ja\)\|\(ko\)'set ambiwidth=doubleendifif has("win32")source $VIMRUNTIME/delmenu.vimsource $VIMRUNTIME/menu.vimlanguage messages zh_CN.utf-8endifelseechoerr "S</code></pre><h3 id="服务器配置ssh-server"><a href="#服务器配置ssh-server" class="headerlink" title="服务器配置ssh-server"></a>服务器配置ssh-server</h3><p>参考<a href="https://blog.csdn.net/qq_51385685/article/details/130179997">备忘录—linux 搭建ssh server</a></p><p>Ubuntu缺省没有安装<code>ssh-server</code>，使用以下命令安装：</p><pre class=" language-shell"><code class="language-shell">sudo apt-get install openssh-server</code></pre><p>然后确认<code>sshserver</code>是否启动了：（或用<code>netstat -tlp</code>命令）</p><pre class=" language-shell"><code class="language-shell">sudo ps -e|grep ssh</code></pre><p>如果只有ssh-agent那ssh-server还没有启动，需要sudo service ssh start，如果看到sshd那说明ssh-server已经启动了。<br>如果没有则可以这样启动：</p><pre class=" language-shell"><code class="language-shell">sudo service ssh start</code></pre><p>设置开机自启动</p><pre class=" language-shell"><code class="language-shell">sudo systemctl enable ssh</code></pre><h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><p>1.you are in emergency mode</p><p><a href="https://blog.csdn.net/weixin_40689871/article/details/108451501">https://blog.csdn.net/weixin_40689871/article/details/108451501</a></p><h3 id="GPU监测"><a href="#GPU监测" class="headerlink" title="GPU监测"></a>GPU监测</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> sys<span class="token keyword">import</span> timecmd <span class="token operator">=</span> <span class="token string">'CUDA_VISIBLE_DEVICES=0 nohup bash ../run.sh --stage 6'</span>  <span class="token comment" spellcheck="true">#当GPU空闲时需要跑的脚本</span><span class="token keyword">def</span> <span class="token function">gpu_info</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    gpu_status <span class="token operator">=</span> os<span class="token punctuation">.</span>popen<span class="token punctuation">(</span><span class="token string">'nvidia-smi | grep %'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'|'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#根据nvidia-smi命令的返回值按照'|'为分隔符建立一个列表</span>    <span class="token triple-quoted-string string">'''    结果如：    ['', ' N/A   64C    P0    68W /  70W ', '   9959MiB / 15079MiB ', '     79%      Default ',     '\n', ' N/A   73C    P0   108W /  70W ', '  11055MiB / 15079MiB ', '     63%      Default ',     '\n', ' N/A   60C    P0    55W /  70W ', '   3243MiB / 15079MiB ', '     63%      Default ', '\n']    '''</span>    gpu_memory <span class="token operator">=</span> int<span class="token punctuation">(</span>gpu_status<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'M'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true">#获取当前0号GPU功率值：提取标签为2的元素，按照'/'为分隔符后提取标签为0的元素值再按照'M'为分隔符提取标签为0的元素值，返回值为int形式 </span>    gpu_power <span class="token operator">=</span> int<span class="token punctuation">(</span>gpu_status<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'   '</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'W'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#获取0号GPU当前显存使用量</span>    gpu_util <span class="token operator">=</span> int<span class="token punctuation">(</span>gpu_status<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'   '</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'%'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#获取0号GPU显存核心利用率</span>    <span class="token keyword">return</span> gpu_power<span class="token punctuation">,</span> gpu_memory<span class="token punctuation">,</span> gpu_util<span class="token keyword">def</span> <span class="token function">narrow_setup</span><span class="token punctuation">(</span>secs<span class="token operator">=</span><span class="token number">600</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true">#间隔十分钟检测一次</span>    gpu_power<span class="token punctuation">,</span> gpu_memory<span class="token punctuation">,</span> gpu_util <span class="token operator">=</span> gpu_info<span class="token punctuation">(</span><span class="token punctuation">)</span>    i <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">while</span> <span class="token operator">not</span><span class="token punctuation">(</span>gpu_memory <span class="token operator">&lt;</span> <span class="token number">1000</span> <span class="token operator">and</span> gpu_power <span class="token operator">&lt;</span> <span class="token number">20</span> <span class="token operator">and</span> gpu_util <span class="token operator">&lt;</span> <span class="token number">20</span><span class="token punctuation">)</span> <span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 当功率，使用量，利用率都小于特定值才去退出循环</span>        gpu_power<span class="token punctuation">,</span> gpu_memory<span class="token punctuation">,</span> gpu_util <span class="token operator">=</span> gpu_info<span class="token punctuation">(</span><span class="token punctuation">)</span>        i <span class="token operator">=</span> i <span class="token operator">%</span> <span class="token number">5</span>        symbol <span class="token operator">=</span> <span class="token string">'monitoring: '</span> <span class="token operator">+</span> <span class="token string">'&gt;'</span> <span class="token operator">*</span> i <span class="token operator">+</span> <span class="token string">' '</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">10</span> <span class="token operator">-</span> i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'|'</span>        gpu_power_str <span class="token operator">=</span> <span class="token string">'gpu power:%d W |'</span> <span class="token operator">%</span> gpu_power        gpu_memory_str <span class="token operator">=</span> <span class="token string">'gpu memory:%d MiB |'</span> <span class="token operator">%</span> gpu_memory        gpu_util_str <span class="token operator">=</span> <span class="token string">'gpu util:%d %% |'</span> <span class="token operator">%</span> gpu_util        sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'\r'</span> <span class="token operator">+</span> gpu_memory_str <span class="token operator">+</span> <span class="token string">' '</span> <span class="token operator">+</span> gpu_power_str <span class="token operator">+</span> <span class="token string">' '</span> <span class="token operator">+</span> gpu_util_str <span class="token operator">+</span> <span class="token string">' '</span> <span class="token operator">+</span> symbol<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#sys.stdout.write(obj+'\n')等价于print(obj)</span>        sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>flush<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#刷新输出</span>        time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span>secs<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#推迟调用线程的运行，通过参数指秒数，表示进程挂起的时间。</span>        i <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\n'</span> <span class="token operator">+</span> cmd<span class="token punctuation">)</span>    os<span class="token punctuation">.</span>system<span class="token punctuation">(</span>cmd<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#执行脚本</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    narrow_setup<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="shell脚本"><a href="#shell脚本" class="headerlink" title="shell脚本"></a>shell脚本</h3><p><a href="https://www.youtube.com/watch?v=rHoqfkypwgw&amp;list=PLmOn9nNkQxJEEfgfPo0IMwu8ac5DIWYd3&amp;index=66">学习视频</a></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matlab</title>
      <link href="matlab/"/>
      <url>matlab/</url>
      
        <content type="html"><![CDATA[<p>教程：<a href="https://www.w3cschool.cn/matlab/">https://www.w3cschool.cn/matlab/</a></p><p>MATLAB Api：<a href="https://www.mathworks.com/help/">https://www.mathworks.com/help/</a></p><h4 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h4><p>双百分号%%在matlab代码中的作用是将代码分块，上下两个%%之间的部分作为一块，在运行代码的时候可以分块运行，查看每一块代码的运行情况。常用于调试程序。</p><p>单百分号%是注释    多行注释是%{   }%  注释快捷键CTRL+R 取消CTRL+T</p><p><strong>matlab数组下标从1开始</strong></p><pre class=" language-matlab"><code class="language-matlab">clear<span class="token comment" spellcheck="true">%清空变量</span>clc<span class="token comment" spellcheck="true">%清空命令行窗口</span><span class="token function">class</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">%显示数据类型</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">%查看各矩阵维度</span><span class="token function">length</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">%返回最大数组维度的长度</span>X <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span>L <span class="token operator">=</span> <span class="token function">length</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token comment" spellcheck="true">%输出9</span><span class="token comment" spellcheck="true">%异常检测</span><span class="token keyword">try</span>    <span class="token punctuation">[</span>C<span class="token punctuation">,</span>scores<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">semanticseg</span><span class="token punctuation">(</span>I<span class="token punctuation">,</span>net<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">catch</span>    <span class="token keyword">continue</span><span class="token punctuation">;</span><span class="token keyword">end</span><span class="token comment" spellcheck="true">%循环</span><span class="token keyword">for</span> k <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span><span class="token function">length</span><span class="token punctuation">(</span>imagepath<span class="token punctuation">)</span>    imagepath<span class="token punctuation">{</span>k<span class="token punctuation">}</span><span class="token operator">=</span><span class="token punctuation">[</span>imagepath<span class="token punctuation">{</span>k<span class="token punctuation">}</span><span class="token punctuation">,</span><span class="token string">'.jpg'</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">end</span></code></pre><p>single 单精度数值数据  double 双精度数值数据 </p><p><strong>Cell Array 元胞数组</strong></p><pre class=" language-matlab"><code class="language-matlab">C <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'one'</span><span class="token punctuation">,</span> <span class="token string">'two'</span><span class="token punctuation">,</span> <span class="token string">'three'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%创建元胞数组</span>     <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">}</span><span class="token punctuation">;</span>emptyCell<span class="token operator">=</span><span class="token function">cell</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">%cell函数创建元胞数组</span>upperLeft<span class="token operator">=</span><span class="token function">C</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">:</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token operator">:</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">%圆括号索引</span>imagepath<span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">}</span><span class="token operator">=</span><span class="token string">'45'</span><span class="token comment" spellcheck="true">%这个只能大括号，圆括号就是类型赋值有问题</span></code></pre><p><strong>categorical 字符数组</strong></p><p>从字符矢量创建分类数组</p><pre class=" language-matlab"><code class="language-matlab">A <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'r'</span> <span class="token string">'b'</span> <span class="token string">'g'</span><span class="token punctuation">;</span> <span class="token string">'g'</span> <span class="token string">'r'</span> <span class="token string">'b'</span><span class="token punctuation">;</span> <span class="token string">'b'</span> <span class="token string">'r'</span> <span class="token string">'g'</span><span class="token punctuation">}</span>B <span class="token operator">=</span> <span class="token function">categorical</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token comment" spellcheck="true">%创建分类数组</span><span class="token function">categories</span><span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token comment" spellcheck="true">%显示B的类别</span><span class="token function">class</span><span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token comment" spellcheck="true">%显示B的数据类型   就是‘categorical’</span></code></pre><h4 id="文件和文件夹"><a href="#文件和文件夹" class="headerlink" title="文件和文件夹"></a>文件和文件夹</h4><pre class=" language-matlab"><code class="language-matlab"><span class="token function">fullfile</span><span class="token punctuation">(</span>filepart1<span class="token punctuation">,</span><span class="token punctuation">...</span><span class="token punctuation">,</span>filepartN<span class="token punctuation">)</span>#从各个部分构建完整文件名 有点类似os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join #还会根据平台改变文件分隔符，创建多个文件路径的话会返回元胞数组f <span class="token operator">=</span> <span class="token function">fullfile</span><span class="token punctuation">(</span><span class="token string">'c:\'</span><span class="token punctuation">,</span><span class="token string">'myfiles'</span><span class="token punctuation">,</span><span class="token string">'matlab'</span><span class="token punctuation">,</span><span class="token punctuation">{</span><span class="token string">'myfile1.m'</span><span class="token punctuation">;</span><span class="token string">'myfile2.m'</span><span class="token punctuation">}</span><span class="token punctuation">)</span>f <span class="token operator">=</span>  <span class="token number">2</span>×<span class="token number">1</span> cell array    <span class="token string">'c:\myfiles\matlab\myfile1.m'</span>    <span class="token string">'c:\myfiles\matlab\myfile2.m'</span>textread<span class="token function">dir</span><span class="token punctuation">(</span><span class="token punctuation">)</span>#获得指定文件夹下的所有子文件夹和文件，并存放在一个文件结构的数组<span class="token punctuation">,</span>各结构体如下#name<span class="token operator">:</span>文件名 date<span class="token operator">:</span>修改日期 bytes<span class="token operator">:</span>文件大小 isdir<span class="token operator">:</span>目录是<span class="token number">1</span>，不是为<span class="token number">0</span> datenum<span class="token operator">:</span>matlab中特定的修改日期 eg<span class="token operator">:</span>imgpath  <span class="token operator">=</span> <span class="token function">dir</span><span class="token punctuation">(</span><span class="token function">fullfile</span><span class="token punctuation">(</span>imgdir<span class="token punctuation">,</span><span class="token string">'*.png'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">% 遍历所有png格式文件</span>I<span class="token operator">=</span><span class="token function">imread</span><span class="token punctuation">(</span><span class="token function">fullfile</span><span class="token punctuation">(</span>imgdir<span class="token punctuation">,</span><span class="token function">imgpath</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%读取第k个</span></code></pre><h4 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h4><p><code>imread()</code>读取图像</p><p>figure创建一个画板 <code>imshow()</code>展示图像</p><p><strong>imageDatastore</strong> </p><p>如果一个图像文件集合中的每个图像可以单独放入内存，但整个集合不一定能放入内存，则可以使用 <code>ImageDatastore</code> 对象来管理。您可以使用 <code>imageDatastore</code> 函数创建 <code>ImageDatastore</code> 对象，指定其属性，然后使用对象函数导入和处理数据。</p><p><code>imds = imageDatastore(location)</code>根据 <code>location</code> 指定的图像数据集合创建一个数据存储 </p><h4 id="深度学习入门之旅"><a href="#深度学习入门之旅" class="headerlink" title="深度学习入门之旅"></a>深度学习入门之旅</h4><p><a href="https://www.pianshen.com/article/71481644395/">https://www.pianshen.com/article/71481644395/</a></p><h4 id="SeriesNetwork"><a href="#SeriesNetwork" class="headerlink" title="SeriesNetwork"></a>SeriesNetwork</h4><p>串联网络是一种用于深度学习的神经网络，层次化排列。它有一个单一的输入层和一个单一的输出层</p><h4 id="DAGNetwork"><a href="#DAGNetwork" class="headerlink" title="DAGNetwork"></a>DAGNetwork</h4><p><a href="https://blog.csdn.net/weixin_43687366/article/details/102557729">DAG入门之旅</a></p><p>DAG网络是用于深度学习的神经网络，其中的层为有向无环图。各层有来自多个层的输入和到多个层的输出。DAGNetwork对象具有单个输入层和单个输出层。</p><p>lgraph.Connections 展示层的连接</p><p>lgraph.Layers 展示层</p><p>figure  plot(lgraph)把网络图形画出来</p>]]></content>
      
      
      <categories>
          
          <category> Matlab </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python学习</title>
      <link href="python-xue-xi/"/>
      <url>python-xue-xi/</url>
      
        <content type="html"><![CDATA[<p>pythonAPI:<a href="https://docs.python.org/zh-cn/3/library/index.html">https://docs.python.org/zh-cn/3/library/index.html</a></p><p>教程cookbook:<a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/preface.html">https://python3-cookbook.readthedocs.io/zh_CN/latest/preface.html</a></p><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="浮点数精度"><a href="#浮点数精度" class="headerlink" title="浮点数精度"></a>浮点数精度</h3><p><a href="https://blog.csdn.net/qq_51432387/article/details/122624679">https://blog.csdn.net/qq_51432387/article/details/122624679</a></p><p><a href="https://blog.csdn.net/qq_32727095/article/details/118106061">https://blog.csdn.net/qq_32727095/article/details/118106061</a></p><p>简而言之 最好别用浮点数来比较大小</p><p>Python 3.X对于浮点数默认的是提供<strong>17位数字的精度</strong>。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#使用模块前，需要使用 import 引入</span><span class="token keyword">import</span> decimala <span class="token operator">=</span> decimal<span class="token punctuation">.</span>Decimal<span class="token punctuation">(</span><span class="token string">"10.0"</span><span class="token punctuation">)</span>b <span class="token operator">=</span> decimal<span class="token punctuation">.</span>Decimal<span class="token punctuation">(</span><span class="token string">"3"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token number">10.0</span><span class="token operator">/</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token operator">/</span>b<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#输出：3.3333333333333335</span><span class="token comment" spellcheck="true">#输出：3.333333333333333333333333333</span><span class="token comment" spellcheck="true"># python保留小数点后几位</span><span class="token comment" spellcheck="true"># 1.%f方法 不过最终是字符串</span>ans<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"$%.2f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>quantity <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">100</span> <span class="token operator">-</span> discount<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>f <span class="token operator">=</span> <span class="token number">1.23456</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%.4f'</span> <span class="token operator">%</span> f<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 只不过是字符串,需要再用float转换</span><span class="token comment" spellcheck="true"># 2.format函数  会四舍五入</span><span class="token keyword">print</span><span class="token punctuation">(</span>format<span class="token punctuation">(</span><span class="token number">1.23456</span><span class="token punctuation">,</span> <span class="token string">'.2f'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>format<span class="token punctuation">(</span><span class="token number">1.23456</span><span class="token punctuation">,</span> <span class="token string">'.3f'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>format<span class="token punctuation">(</span><span class="token number">1.23456</span><span class="token punctuation">,</span> <span class="token string">'.4f'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#1.23</span><span class="token comment" spellcheck="true">#1.235</span><span class="token comment" spellcheck="true">#1.2346</span><span class="token comment" spellcheck="true"># 3.不进行四舍五入 直接截短</span><span class="token comment" spellcheck="true">#保留三位小数截断 python3</span><span class="token keyword">print</span><span class="token punctuation">(</span>int<span class="token punctuation">(</span><span class="token number">1.23456</span> <span class="token operator">*</span> <span class="token number">1000</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1000</span> <span class="token punctuation">)</span></code></pre><h3 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h3><p><strong>print 默认输出</strong>是换行的，如果要实现不换行需要在变量末尾加上 **end=””**：</p><p>True和False可以直接当成0和1来用</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token boolean">True</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token boolean">True</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token boolean">False</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token boolean">True</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token boolean">True</span> <span class="token operator">+</span> <span class="token boolean">False</span> <span class="token operator">+</span> <span class="token number">20</span><span class="token number">21</span></code></pre><p>//:整数除法</p><p>\：可以使用这个换行，运算符后边不会错</p><h4 id="格式化输出，文字和数字等混合输出时使用！"><a href="#格式化输出，文字和数字等混合输出时使用！" class="headerlink" title="格式化输出，文字和数字等混合输出时使用！"></a>格式化输出，文字和数字等混合输出时使用！</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 第一种:使用format</span><span class="token string">"Skipping {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>img_path<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 不设置指定位置，按默认顺序</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token string">"{} {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"hello"</span><span class="token punctuation">,</span> <span class="token string">"world"</span><span class="token punctuation">)</span>    <span class="token string">'hello world'</span><span class="token comment" spellcheck="true"># 设置指定位置</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token string">"{1} {0} {1}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"hello"</span><span class="token punctuation">,</span> <span class="token string">"world"</span><span class="token punctuation">)</span>  <span class="token string">'world hello world'</span><span class="token comment" spellcheck="true"># 设置参数</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"网站名：{name}, 地址 {url}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"菜鸟教程"</span><span class="token punctuation">,</span> url<span class="token operator">=</span><span class="token string">"www.runoob.com"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 通过字典设置参数</span>site <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"name"</span><span class="token punctuation">:</span> <span class="token string">"菜鸟教程"</span><span class="token punctuation">,</span> <span class="token string">"url"</span><span class="token punctuation">:</span> <span class="token string">"www.runoob.com"</span><span class="token punctuation">}</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"网站名：{name}, 地址 {url}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token operator">**</span>site<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 通过列表索引设置参数</span>my_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'菜鸟教程'</span><span class="token punctuation">,</span> <span class="token string">'www.runoob.com'</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"网站名：{0[0]}, 地址 {0[1]}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>my_list<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># "0" 是必须的</span></code></pre><p><strong>数字格式化：</strong></p><img src="/python-xue-xi/image-20210921175012478.png" alt="image-20210921175012478"><p><code>^, &lt;, &gt;</code>分别是居中、左对齐、右对齐，后面带宽度</p><p><code>:</code>号后面带填充的字符，只能是一个字符，不指定则默认是用空格填充。</p><p><code>+</code> 表示在正数前显示<code> +</code>，负数前显示 <code>-</code></p><p><code> </code>（空格）表示在正数前加空格</p><p><code>b、d、o、x </code>分别是二进制、十进制、八进制、十六进制。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#第二种</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[%d, %5d] loss: %.3f'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> running_loss <span class="token operator">/</span> <span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#%5d %5s输出站5个字符宽度，不足补足   %.3f表示保留三位小数</span><span class="token comment" spellcheck="true">#第三种</span>f<span class="token string">'asdf {变量名}'</span></code></pre><h4 id="终端输出带颜色的文本"><a href="#终端输出带颜色的文本" class="headerlink" title="终端输出带颜色的文本"></a>终端输出带颜色的文本</h4><p>这在某种程度上取决于您所使用的平台。 最常见的方法是打印ANSI转义序列。 举一个简单的例子，这是来自的一些Python代码：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">bcolors</span><span class="token punctuation">:</span>    HEADER <span class="token operator">=</span> <span class="token string">'\033[95m'</span>    OKBLUE <span class="token operator">=</span> <span class="token string">'\033[94m'</span>    OKCYAN <span class="token operator">=</span> <span class="token string">'\033[96m'</span>    OKGREEN <span class="token operator">=</span> <span class="token string">'\033[92m'</span>    WARNING <span class="token operator">=</span> <span class="token string">'\033[93m'</span>    FAIL <span class="token operator">=</span> <span class="token string">'\033[91m'</span>    ENDC <span class="token operator">=</span> <span class="token string">'\033[0m'</span>    BOLD <span class="token operator">=</span> <span class="token string">'\033[1m'</span>    UNDERLINE <span class="token operator">=</span> <span class="token string">'\033[4m'</span><span class="token comment" spellcheck="true"># 为了使用这样的代码,你可以这么做:</span><span class="token keyword">print</span><span class="token punctuation">(</span>bcolors<span class="token punctuation">.</span>WARNING <span class="token operator">+</span> <span class="token string">"Warning: No active frommets remain. Continue?"</span> <span class="token operator">+</span> bcolors<span class="token punctuation">.</span>ENDC<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 或者</span><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"{bcolors.WARNING}Warning: No active frommets remain. Continue?{bcolors.ENDC}"</span><span class="token punctuation">)</span></code></pre><h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Python3.x 中 input() 函数接受一个标准输入数据，返回为 string 类型。</span>input<span class="token punctuation">(</span><span class="token punctuation">[</span>prompt<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>a <span class="token operator">=</span> input<span class="token punctuation">(</span><span class="token string">"input:"</span><span class="token punctuation">)</span>input<span class="token punctuation">:</span><span class="token number">123</span>                  <span class="token comment" spellcheck="true"># 输入整数</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> type<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'str'</span><span class="token operator">&gt;</span>              <span class="token comment" spellcheck="true"># 字符串</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a <span class="token operator">=</span> input<span class="token punctuation">(</span><span class="token string">"input:"</span><span class="token punctuation">)</span>    input<span class="token punctuation">:</span>runoob              <span class="token comment" spellcheck="true"># 正确，字符串表达式</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> type<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'str'</span><span class="token operator">&gt;</span>             <span class="token comment" spellcheck="true"># 字符串</span></code></pre><h3 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h3><h4 id="文件读取输出"><a href="#文件读取输出" class="headerlink" title="文件读取输出"></a>文件读取输出</h4><p><a href="https://www.cnblogs.com/ymjyqsx/p/6554817.html">python文件读写</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"locs.txt"</span><span class="token punctuation">,</span><span class="token string">"a"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true">#f.write(fname+"\n")</span>    <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true">#每个line都是str</span>        linelist<span class="token operator">=</span>line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#字符串切割</span><span class="token comment" spellcheck="true">#with open as f: 为了保证无论是否出错都能正确地关闭文件</span><span class="token comment" spellcheck="true">#文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的</span><span class="token comment" spellcheck="true">#由于文件读写时都有可能产生IOError，一旦出错，后面的f.close()就不会调用。</span><span class="token comment" spellcheck="true">#所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用try ... finally来实现</span><span class="token keyword">try</span><span class="token punctuation">:</span>    f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'/path/to/file'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">finally</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> f<span class="token punctuation">:</span>        f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#但是每次都这么写实在太繁琐，所以，Python引入了with语句来自动帮我们调用close()方法</span>files <span class="token operator">=</span> file<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#splitlines() 按照行界符('\r', '\r\n', \n'等)分隔，返回一个包含各行作为元素的列表，默认不包含行界符。</span>file<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#一次读取文件的全部内容,把内容读到内存,用一个str对象表示,file.read(size)表示最多读size个字节</span>file<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#每次读取一行；返回的是一个字符串对象，保持当前行的内存</span>file<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#读取所有行然后把它们作为一个字符串列表返回。</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'mytest.txt'</span><span class="token punctuation">,</span><span class="token string">'w'</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#w是覆盖，a是追加</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">+</span><span class="token string">'\n'</span><span class="token punctuation">)</span>   </code></pre><p>一个stackoverflow问题</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> line <span class="token keyword">in</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">)</span><span class="token punctuation">:</span>    do_something<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#为什么经常看到这种写法？为什么不是下面这种</span><span class="token keyword">with</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        do_something<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#1:with更好</span><span class="token comment" spellcheck="true">#2:for line in f足够,for line in f.readlines()反而冗余,我推测是f所属的类实现了相应__iter__()方法</span><span class="token comment" spellcheck="true">#3:在简短的脚本中，人们经常省略这一步，因为在垃圾回收过程中，当文件对象被回收时，Python会自动关闭文件。然而，尽快关闭文件是一种良好的编程实践，在大型程序中尤其如此。</span></code></pre><h4 id="绝对路径和相对路径"><a href="#绝对路径和相对路径" class="headerlink" title="绝对路径和相对路径"></a>绝对路径和相对路径</h4><p><a href="https://blog.csdn.net/databatman/article/details/49453953">绝对路径和相对路径</a></p><ul><li><h3 id="字符串处理"><a href="#字符串处理" class="headerlink" title="字符串处理"></a>字符串处理</h3></li></ul><p><a href="https://www.cnblogs.com/walo/p/10608436.html">python字符串前加u,r,b,f</a></p><ul><li>u:后面字符串以Unicode格式进行编码,一般用在中文字符串前面,防止因为源码储存格式问题,导致再次使用时出现乱码。</li><li>r:raw,去掉反斜杠的转移机制。</li><li>b:后面字符串是bytes 类型。</li><li>f:以f开头表示在字符串内支持大括号内的python 表达式。</li></ul><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><pre class=" language-python"><code class="language-python">str<span class="token punctuation">.</span>split<span class="token punctuation">(</span>str<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">,</span> num<span class="token operator">=</span>string<span class="token punctuation">.</span>count<span class="token punctuation">(</span>str<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回分割后的字符串列表。</span><span class="token comment" spellcheck="true">#str,分割符，默认为所有的空字符，包括空格、换行(\n)、制表符(\t)等。</span><span class="token comment" spellcheck="true">#num,分割次数。默认为 -1, 即分割所有。</span><span class="token comment" spellcheck="true">#split(",")按逗号分割，最后一条是带换行符的！</span><span class="token comment" spellcheck="true"># 统计字符串里某个字符或子字符串出现的次数。可选参数为在字符串搜索的开始与结束位置。</span>str<span class="token punctuation">.</span>count<span class="token punctuation">(</span>sub<span class="token punctuation">,</span> start<span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>end<span class="token operator">=</span>len<span class="token punctuation">(</span>string<span class="token punctuation">)</span><span class="token punctuation">)</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 处理的时候，如果不带参数，默认是清除两边的空白符，例如：\n,\r,\t,' '。</span>str <span class="token operator">=</span> <span class="token string">'123@163.com'</span><span class="token keyword">print</span><span class="token punctuation">(</span>str<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'132'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#@163.com</span>str <span class="token operator">=</span> <span class="token string">'123@163.com'</span><span class="token keyword">print</span><span class="token punctuation">(</span>str<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'23'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#123@163.com</span>S<span class="token punctuation">.</span>rstrip<span class="token punctuation">(</span><span class="token punctuation">[</span>chars<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#用于删除字符串尾部指定的字符，默认字符为所有空字符，包括空格、换行(\n)、制表符(\t)等</span>startwith<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#是否以指定字符串开头</span>str<span class="token punctuation">.</span>endswith<span class="token punctuation">(</span>suffix<span class="token punctuation">[</span><span class="token punctuation">,</span> start<span class="token punctuation">[</span><span class="token punctuation">,</span> end<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 是否以指定字符串结尾</span>str<span class="token punctuation">.</span>endswith<span class="token punctuation">(</span>suffix<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>str <span class="token operator">=</span> <span class="token string">"hello,i love python"</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"1:"</span><span class="token punctuation">,</span>str<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"h"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"2:"</span><span class="token punctuation">,</span>str<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"l"</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#beg=2,end=10</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"3:"</span><span class="token punctuation">,</span>str<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#空字符</span><span class="token number">1</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token number">2</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token number">3</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token string">'sep'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>元组、列表、字典、字符串<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>str<span class="token comment" spellcheck="true">#返回一个以分隔符sep连接各个元素后生成的字符串,字典只对键进行连接</span>replace<span class="token comment" spellcheck="true">#字符串运算遵循数学法则,单引号和双引号可以混合使用</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token number">3</span><span class="token operator">*</span><span class="token string">"python"</span><span class="token operator">+</span><span class="token string">'1'</span><span class="token comment" spellcheck="true">#pythonpythonpython1</span><span class="token comment" spellcheck="true">#多个带引号的字符串，解释器会自动拼接</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token string">'b'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#ab</span><span class="token comment" spellcheck="true"># 判断是否为数字</span><span class="token keyword">print</span><span class="token punctuation">(</span>str_1<span class="token punctuation">.</span>isdigit<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 判断是否为字母</span><span class="token keyword">print</span><span class="token punctuation">(</span>str_1<span class="token punctuation">.</span>isalpha<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 判断是否为字母数组混合</span><span class="token keyword">print</span><span class="token punctuation">(</span>str_1<span class="token punctuation">.</span>isalnum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 把所有字符中的小写字母转换成大写字母</span><span class="token keyword">print</span><span class="token punctuation">(</span>str<span class="token punctuation">.</span>upper<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 把所有字符中的大写字母转换成小写字母</span><span class="token keyword">print</span><span class="token punctuation">(</span>str<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 把第一个字母转化为大写字母，其余小写</span><span class="token keyword">print</span><span class="token punctuation">(</span>str<span class="token punctuation">.</span>capitalize<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 字符串中查找指定字符位置</span>str<span class="token punctuation">.</span>find<span class="token punctuation">(</span>str<span class="token punctuation">,</span> beg<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> end<span class="token operator">=</span>len<span class="token punctuation">(</span>string<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>info <span class="token operator">=</span> <span class="token string">'abca'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">print</span> info<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 从下标0开始，查找在字符串里第一个出现的子串，返回结果：0</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">print</span> info<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 从下标1开始，查找在字符串里第一个出现的子串：返回结果3</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">print</span> info<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'3'</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 查找不到返回-1</span><span class="token comment" spellcheck="true"># 从后往前查找</span>str<span class="token punctuation">.</span>rfind<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 腾讯笔试给我做麻了，字符串in的用法</span><span class="token string">"A"</span> <span class="token keyword">in</span> <span class="token string">"AC"</span> <span class="token comment" spellcheck="true"># True</span><span class="token string">"C"</span> <span class="token keyword">in</span> <span class="token string">"AC"</span> <span class="token comment" spellcheck="true"># True</span><span class="token string">"AC"</span> <span class="token keyword">in</span> <span class="token string">"ACD"</span> <span class="token comment" spellcheck="true"># True</span><span class="token string">"CD"</span> <span class="token keyword">in</span> <span class="token string">"ACD"</span> <span class="token comment" spellcheck="true"># True</span><span class="token string">"AD"</span> <span class="token keyword">in</span> <span class="token string">"ACD"</span> <span class="token comment" spellcheck="true">#False</span></code></pre><h3 id="变量赋值规则"><a href="#变量赋值规则" class="headerlink" title="变量赋值规则"></a>变量赋值规则</h3><p>在python中使用<code>id(obj)</code>函数查看变量的地址</p><ol><li><code>a=10</code>，并不是为标记a建立一个值，而是建立了一个值以后，再用标记a去指向它</li><li><code>a=10 b=a a=20</code>,b=a不是b指向了标记a，而是b指向了标记a指向的值，所以a指向20之后，b还是10</li><li><code>g= 10 a= g a=a+4</code>，a会指向一个新的内存</li><li><code>n=['a','b','c']  n[0]=['x','y'] </code>,改变的是所指向的对象的本身，内部没有改变</li></ol><p><code>nums[nums[i]], nums[i] = nums[i], nums[nums[i]]</code></p><p>实际的执行是<code>t=(nums[i], nums[nums[i]]),nums[nums[i]]=t[0],nums[i]=t[1]</code>,<strong>顺序上是有先后的</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># 如果在循环里将i,j修改了,就会在新的i，j的值上加一继续循环</span></code></pre><h3 id="基本数据结构"><a href="#基本数据结构" class="headerlink" title="基本数据结构"></a>基本数据结构</h3><img src="/python-xue-xi/image-20220510151928526.png" alt="image-20220510151928526" style="zoom:67%;"><img src="/python-xue-xi/image-20220510151942129.png" alt="image-20220510151942129" style="zoom:67%;"><h4 id="列表list-序列"><a href="#列表list-序列" class="headerlink" title="列表list/序列"></a>列表list/序列</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 返回一个新的列表，b和a不共享</span><span class="token comment" spellcheck="true"># a=list("a,b,c,d,e,!")=&gt;a=['a','b','c','d','e']</span>list<span class="token punctuation">(</span><span class="token punctuation">)</span> eg<span class="token punctuation">:</span>b<span class="token operator">=</span>list<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 向列表添加元素</span>list<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 从列表中找出某个值第一个匹配项的索引位置</span>list<span class="token punctuation">.</span>index<span class="token punctuation">(</span>obj<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># extend() 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）</span>list<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将对象插入列表</span><span class="token comment" spellcheck="true"># 这个extend很多时候还容易出错,完全用处不太大</span><span class="token comment" spellcheck="true"># 两个列表直接 list1+list2就行</span><span class="token comment" spellcheck="true"># 只不过extend是原地修改,list1+list2是返回一个新的列表</span>list<span class="token punctuation">.</span>insert<span class="token punctuation">(</span>index<span class="token punctuation">,</span> obj<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 列表乘法</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token number">5</span><span class="token comment" spellcheck="true">#是地址复制五遍 常数可以 [0,0,0,0,0]</span><span class="token punctuation">[</span>a<span class="token punctuation">]</span><span class="token operator">*</span><span class="token number">5</span><span class="token comment" spellcheck="true">#这5个元素地址相同,改一个全都改了</span></code></pre><h4 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a><strong>索引与切片</strong></h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 左闭右开</span>a<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 从第2个开始到后面所有的元素</span>a<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 从a[m]开始，每次走|n|步，当n为负时逆序取数，当n为正的时候，m为空则默认m=0，n为负时，m为空则默认为-1</span>a<span class="token punctuation">[</span>m<span class="token punctuation">:</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 切片中的None用来升维</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># torch.Size([6, 6])</span>b<span class="token operator">=</span>a<span class="token punctuation">[</span>None<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># torch.Size([1, 6, 6]) </span>b <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">:</span>j<span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># 表示复制a[i]到a[j-1]，以生成新的list对象</span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># [1,2]</span><span class="token comment" spellcheck="true">#当越界时</span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#[7,8,9],自动截取到末尾</span><span class="token comment" spellcheck="true"># 当i缺省时，默认为0，即 a[:3]相当于 a[0:3]</span><span class="token comment" spellcheck="true"># 当j缺省时，默认为len(alist), 即a[1:]相当于a[1:10]</span><span class="token comment" spellcheck="true"># 当i,j都缺省时，a[:]就相当于完整复制一份a</span>b <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">:</span>j<span class="token punctuation">:</span>s<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 表示：i,j与上面的一样，但s表示步进，缺省为1.</span><span class="token comment" spellcheck="true"># 所以a[i:j:1]相当于a[i:j]</span><span class="token comment" spellcheck="true"># 当s&lt;0时，i缺省时，默认为-1. j缺省时，默认为-len(a)-1</span><span class="token comment" spellcheck="true"># 所以a[::-1]相当于 a[-1:-len(a)-1:-1]，也就是从最后一个元素到第一个元素复制一遍，即倒序。</span></code></pre><p><a href="https://blog.csdn.net/zhulove86/article/details/53941555?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control">列表推导式</a></p><img src="/python-xue-xi/image-20210120185150919.png" alt="语法" style="zoom: 50%;"><p><strong>集合推导式</strong>和<strong>字典推导式</strong>与<strong>列表推导式</strong>类似,只不过把<strong>中括号[]**换成了</strong>大括号{}**</p><p><strong>生成器推导式</strong>是将<strong>中括号[]**换为</strong>圆括号()**</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 实现切片</span><span class="token keyword">class</span> <span class="token class-name">slice</span><span class="token punctuation">(</span>stop<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">slice</span><span class="token punctuation">(</span>start<span class="token punctuation">,</span> stop<span class="token punctuation">[</span><span class="token punctuation">,</span> step<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># start:开始位置 stop:结束为止 step:间隔</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>myslice <span class="token operator">=</span> slice<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 设置截取5个元素的切片</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> mysliceslice<span class="token punctuation">(</span>None<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> arr <span class="token operator">=</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> arr<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> arr<span class="token punctuation">[</span>myslice<span class="token punctuation">]</span>         <span class="token comment" spellcheck="true"># 截取 5 个元素</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span></code></pre><h4 id="字典dict-集合set"><a href="#字典dict-集合set" class="headerlink" title="字典dict/集合set"></a>字典dict/集合set</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 给字典添加新元素</span>a<span class="token punctuation">[</span><span class="token string">"new_key"</span><span class="token punctuation">]</span> <span class="token operator">=</span> new_valuea<span class="token operator">=</span>b<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">"value"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#删除字典中给定键key所对应的值，返回值为被删除的值。</span>a<span class="token operator">=</span>b<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#或b.values()或b.items()</span><span class="token comment" spellcheck="true">#返回值都是view objects,它们提供了关于字典条目的动态视图，这意味着当字典发生变化时，视图会反映这些变化。</span><span class="token comment" spellcheck="true">#支持len(dictview)、iter(dictview)、x in dictview</span><span class="token comment" spellcheck="true">#若想支持map-style,可以a=list(b.keys())转化成列表</span>car <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"brand"</span><span class="token punctuation">:</span> <span class="token string">"Porsche"</span><span class="token punctuation">,</span> <span class="token string">"model"</span><span class="token punctuation">:</span> <span class="token string">"911"</span><span class="token punctuation">,</span> <span class="token string">"year"</span><span class="token punctuation">:</span> <span class="token number">1963</span><span class="token punctuation">}</span>car<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">"year"</span><span class="token punctuation">:</span> <span class="token string">"White"</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>car<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'brand'</span><span class="token punctuation">:</span> <span class="token string">'Porsche'</span><span class="token punctuation">,</span> <span class="token string">'model'</span><span class="token punctuation">:</span> <span class="token string">'911'</span><span class="token punctuation">,</span> <span class="token string">'year'</span><span class="token punctuation">:</span> <span class="token string">'White'</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true"># 向字典插入项目,如项目已存在，则更新该项目</span>dic<span class="token punctuation">.</span>get<span class="token punctuation">(</span>key<span class="token punctuation">,</span>default<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回指定键的值，如果键不在字典中返回默认值None或者设置的默认值。</span>discard<span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 如果在set集合中存在元素x, 则删除</span>remove<span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 从set集合中删除元素,如果不存在则引发 KeyError</span>pop<span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 删除并且返回set集合中的一个不确定的元素,如果为空则引发KeyError </span><span class="token keyword">for</span> i <span class="token keyword">in</span> dic<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#这样就是只遍历key值,没有value</span><span class="token comment" spellcheck="true"># 集合的交集 并集 差集</span>set1 <span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'卢俊义'</span><span class="token punctuation">,</span><span class="token string">'花荣'</span><span class="token punctuation">,</span> <span class="token string">'吴用'</span><span class="token punctuation">}</span>set2 <span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'公孙胜'</span><span class="token punctuation">,</span><span class="token string">'秦明'</span><span class="token punctuation">,</span><span class="token string">'卢俊义'</span><span class="token punctuation">}</span>res <span class="token operator">=</span> set1 <span class="token operator">&amp;</span> set2<span class="token comment" spellcheck="true">#{'卢俊义'} 交</span>res <span class="token operator">=</span> set1 <span class="token operator">-</span> set2 <span class="token comment" spellcheck="true"># 差</span>res <span class="token operator">=</span> set1 <span class="token operator">|</span> set2 <span class="token comment" spellcheck="true"># 并</span>set1<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token string">"宋江"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 增</span>res <span class="token operator">=</span> set1<span class="token punctuation">.</span>remove<span class="token punctuation">(</span><span class="token string">"吴用"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 删</span><span class="token comment" spellcheck="true">#列表转set</span>a<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span>b<span class="token operator">=</span>set<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#{1,2}</span></code></pre><h3 id="函数-1"><a href="#函数-1" class="headerlink" title="函数"></a>函数</h3><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># zip()函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> c <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> zipped <span class="token operator">=</span> zip<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 打包为元组的列表</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> zip<span class="token punctuation">(</span>a<span class="token punctuation">,</span>c<span class="token punctuation">)</span>              <span class="token comment" spellcheck="true"># 元素个数与最短的列表一致</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 拆包</span>zip<span class="token punctuation">(</span><span class="token operator">*</span>zipped<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 返回一个整数 int 或者长整数 long int 的二进制表示。</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>bin<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token string">'0b1010'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>bin<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token string">'1010'</span><span class="token comment" spellcheck="true"># Python zfill() 方法返回指定长度的字符串，原字符串右对齐，前面填充0。</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>bin<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>zfill<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token string">'0'</span><span class="token comment" spellcheck="true"># ord() 函数是 chr() 函数（对于8位的ASCII字符串）或 unichr() 函数（对于Unicode对象）的配对函数，它以一个字符（长度为1的字符串）作为参数</span><span class="token comment" spellcheck="true"># 返回对应的 ASCII 数值，或者 Unicode 数值，如果所给的 Unicode 字符超出了你的 Python 定义范围，则会引发一个 TypeError 的异常。</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>ord<span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 97</span><span class="token comment" spellcheck="true"># eval()用来执行一个字符串表达式，并返回表达式的值。</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>x <span class="token operator">=</span> <span class="token number">7</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> eval<span class="token punctuation">(</span> <span class="token string">'3 * x'</span> <span class="token punctuation">)</span><span class="token number">21</span><span class="token comment" spellcheck="true"># python分号 如果要在⼀⾏中书写多条句，就必须使⽤分号分隔每个语句，否则Python⽆法识别语句之间的间隔：</span><span class="token comment" spellcheck="true"># 使⽤分号分隔语句 </span>x<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span> y<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">;</span> z<span class="token operator">=</span><span class="token number">1</span><span class="token comment" spellcheck="true"># any(x)判断x对象是否为空对象，如果都为空、0、false，则返回false，如果不都为空、0、false，则返回true。</span>any<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> any<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token string">'0'</span><span class="token punctuation">,</span><span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token boolean">True</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> any<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token boolean">False</span><span class="token comment" spellcheck="true"># all(x)如果all(x)参数x对象的所有元素不为0、’’、False或者x为空对象，则返回True，否则返回False。</span>all<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> all<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token string">'d'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#元组tuple，元素都不为空或0</span><span class="token boolean">True</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> all<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'d'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#元组tuple，存在一个为空的元素</span><span class="token boolean">False</span><span class="token comment" spellcheck="true"># 两列表元素对应相乘</span>map<span class="token punctuation">(</span><span class="token keyword">lambda</span> <span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">:</span>a<span class="token operator">*</span>b<span class="token punctuation">,</span>zip<span class="token punctuation">(</span>List1<span class="token punctuation">,</span>List2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用python不用numpy自己实现逆矩阵</span><span class="token comment" spellcheck="true"># https://integratedmlai.com/matrixinverse/</span>AM <span class="token operator">=</span> In <span class="token operator">=</span>len<span class="token punctuation">(</span>AM<span class="token punctuation">)</span>IM <span class="token operator">=</span> getI<span class="token punctuation">(</span>n<span class="token punctuation">)</span>indices <span class="token operator">=</span> list<span class="token punctuation">(</span>range<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> fd <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>    fdScaler <span class="token operator">=</span> F<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>AM<span class="token punctuation">[</span>fd<span class="token punctuation">]</span><span class="token punctuation">[</span>fd<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>        AM<span class="token punctuation">[</span>fd<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">*=</span> fdScaler        IM<span class="token punctuation">[</span>fd<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">*=</span> fdScaler    <span class="token keyword">for</span> i <span class="token keyword">in</span> indices<span class="token punctuation">[</span><span class="token punctuation">:</span>fd<span class="token punctuation">]</span> <span class="token operator">+</span> indices<span class="token punctuation">[</span>fd<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        crScaler <span class="token operator">=</span> AM<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>fd<span class="token punctuation">]</span>        <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>            AM<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> AM<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">-</span> crScaler <span class="token operator">*</span> AM<span class="token punctuation">[</span>fd<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span>            IM<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> IM<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">-</span> crScaler <span class="token operator">*</span> IM<span class="token punctuation">[</span>fd<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token keyword">return</span> IM<span class="token comment" spellcheck="true"># map竟然是内置函数</span><span class="token comment" spellcheck="true"># 返回一个将 function 应用于 iterable 中每一项并输出其结果的迭代器。</span><span class="token comment" spellcheck="true"># 如果传入了额外的 iterable 参数，function 必须接受相同个数的实参并被应用于从所有可迭代对象中并行获取的项。</span><span class="token comment" spellcheck="true"># 当有多个可迭代对象时，最短的可迭代对象耗尽则整个迭代就将结束。</span>map<span class="token punctuation">(</span>function<span class="token punctuation">,</span> iterable<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#filter()函数用于过滤序列，过滤掉不符合条件的元素，返回符合条件的元素组成新列表。</span>filter<span class="token punctuation">(</span>function<span class="token punctuation">,</span>iterable<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#function:判断函数 iterable:可迭代对象</span><span class="token comment" spellcheck="true"># 在十万级别数据上，filter的速度大约是for循环的1000倍。</span></code></pre><h4 id="enumerate"><a href="#enumerate" class="headerlink" title="enumerate()"></a>enumerate()</h4><p>enumerate() 函数用于将**一个可遍历的数据对象(如列表、元组或字符串)**组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> seasons <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Spring'</span><span class="token punctuation">,</span> <span class="token string">'Summer'</span><span class="token punctuation">,</span> <span class="token string">'Fall'</span><span class="token punctuation">,</span> <span class="token string">'Winter'</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> list<span class="token punctuation">(</span>enumerate<span class="token punctuation">(</span>seasons<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'Spring'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'Summer'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'Fall'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'Winter'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> list<span class="token punctuation">(</span>enumerate<span class="token punctuation">(</span>seasons<span class="token punctuation">,</span> start<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># 小标从 1 开始</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'Spring'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'Summer'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'Fall'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'Winter'</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><h4 id="排序函数"><a href="#排序函数" class="headerlink" title="排序函数"></a>排序函数</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#应用在list上的方法,对已经存在的列表进行操作，是inplace的无返回值</span>list<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">,</span> key<span class="token operator">=</span>None<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 默认从小到大</span><span class="token comment" spellcheck="true"># 如果reverse设置为True就从大到小</span><span class="token comment" spellcheck="true"># 可以使用 functools.cmp_to_key() 将 2.x 风格的 cmp 函数转换为 key 函数。好像python3不支持比较函数</span><span class="token comment" spellcheck="true"># functools.cmp_to_key() &lt;则返回负数,如-1 &gt;则返回整数,如1 =则返回0 </span>sorted<span class="token punctuation">(</span>iterable<span class="token punctuation">,</span> cmp<span class="token operator">=</span>None<span class="token punctuation">,</span> key<span class="token operator">=</span>None<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#内置函数，可以对所有可迭代的对象进行排序操作，返回副本，原始输入不变。</span><span class="token comment" spellcheck="true"># 集合是对可迭代对象的一个统称，他们可以是列表、字典、set、甚至是字符串），它的功能非常强大</span><span class="token comment" spellcheck="true">#iterable:可迭代对象   </span><span class="token comment" spellcheck="true">#cmp:比较函数，具有两个参数，参数的值都是从可迭代对象中取出，此函数必须遵守的规则为，大于则返回1，小于则返回-1，等于则返回0。</span><span class="token comment" spellcheck="true">#key:指定可迭代对象中的一个元素来进行排序。</span><span class="token comment" spellcheck="true">#reverse:排序规则，True为降序，False为升序</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> L<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> sorted<span class="token punctuation">(</span>L<span class="token punctuation">,</span> cmp<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span>y<span class="token punctuation">:</span>cmp<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>y<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 利用cmp函数</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> sorted<span class="token punctuation">(</span>L<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>               <span class="token comment" spellcheck="true"># 利用key</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># sorted 也可以根据多个字段来排序</span><span class="token keyword">class</span> <span class="token class-name">tuple_list</span><span class="token punctuation">:</span> <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> one<span class="token punctuation">,</span> two<span class="token punctuation">,</span> three<span class="token punctuation">)</span><span class="token punctuation">:</span>  self<span class="token punctuation">.</span>one <span class="token operator">=</span> one  self<span class="token punctuation">.</span>two <span class="token operator">=</span> two  self<span class="token punctuation">.</span>three <span class="token operator">=</span> three <span class="token keyword">def</span> <span class="token function">__repr__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">return</span> repr<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>one<span class="token punctuation">,</span> self<span class="token punctuation">.</span>two<span class="token punctuation">,</span> self<span class="token punctuation">.</span>three<span class="token punctuation">)</span><span class="token punctuation">)</span>tuple_list_ <span class="token operator">=</span> <span class="token punctuation">[</span>tuple_list<span class="token punctuation">(</span><span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tuple_list<span class="token punctuation">(</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tuple_list<span class="token punctuation">(</span><span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 首先根据one的位置来排序，然后根据two的位置来排序</span>sorted<span class="token punctuation">(</span>tuple_list_<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>one<span class="token punctuation">,</span> x<span class="token punctuation">.</span>two<span class="token punctuation">)</span><span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">112</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># sorted对字典进行排序</span>dic <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'chen'</span><span class="token punctuation">:</span> <span class="token number">24</span><span class="token punctuation">,</span> <span class="token string">'alex'</span><span class="token punctuation">:</span> <span class="token number">34</span><span class="token punctuation">,</span> <span class="token string">'egon'</span><span class="token punctuation">:</span> <span class="token number">37</span><span class="token punctuation">,</span> <span class="token string">'evaJ'</span><span class="token punctuation">:</span><span class="token string">'18'</span><span class="token punctuation">}</span>s_dic <span class="token operator">=</span> sorted<span class="token punctuation">(</span>dic<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 只对key排序，结果也只有key</span><span class="token keyword">print</span><span class="token punctuation">(</span>s_dic<span class="token punctuation">)</span>s_dic1 <span class="token operator">=</span> sorted<span class="token punctuation">(</span>dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 结果包含key，和value</span><span class="token keyword">print</span><span class="token punctuation">(</span>s_dic1<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出结果</span><span class="token comment" spellcheck="true"># ['alex', 'chen', 'egon', 'evaJ']</span><span class="token comment" spellcheck="true"># [('alex', 34), ('chen', 24), ('egon', 37), ('evaJ', '18')]</span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span>b <span class="token operator">=</span> sorted<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这种有两个元素的,如果没有指定key,默认是按照第一个元素当做key</span></code></pre><p>自定义排序（招商银行某道题）</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">compare1</span><span class="token punctuation">(</span>elem<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>elem<span class="token punctuation">,</span> int<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> elem    <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>elem<span class="token punctuation">,</span> str<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token keyword">def</span> <span class="token function">compare2</span><span class="token punctuation">(</span>elem<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> elem <span class="token operator">==</span> <span class="token string">'?'</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> chr<span class="token punctuation">(</span><span class="token number">127</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> elemstudent <span class="token operator">=</span> sorted<span class="token punctuation">(</span>student<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> w<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token operator">-</span>compare1<span class="token punctuation">(</span>w<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> compare2<span class="token punctuation">(</span>w<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h4><p>用lambda函数首先减少了代码的冗余，其次，用lambda函数，不用费神地去命名一个函数的名字，可以快速的实现某项功能，最后，lambda函数使代码的可读性更强，程序看起来更加简洁。</p><p><code>lambda 参数列表:表达式</code></p><p>表达式中出现的参数需要在argument_list中有定义，并且表达式只能是单行的</p><pre class=" language-python"><code class="language-python">c<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>z<span class="token punctuation">:</span>x<span class="token operator">*</span>y<span class="token operator">*</span>z c<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#24</span></code></pre><h4 id="函数属性"><a href="#函数属性" class="headerlink" title="函数属性"></a>函数属性</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_fun_name</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> func<span class="token punctuation">.</span>__name__<span class="token comment" spellcheck="true">#返回的就是函数的名字</span><span class="token keyword">def</span> <span class="token function">say_hello</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span><span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 看起来没有用处呀</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>get_fun_name<span class="token punctuation">(</span>say_hello<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 那么看下面</span>    haha <span class="token operator">=</span> say_hello    hahaha <span class="token operator">=</span> haha    <span class="token comment" spellcheck="true"># 请问 hahaha 到底是啥？</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>get_fun_name<span class="token punctuation">(</span>hahaha<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#函数名字是最开始的那个实例，其他都是函数的引用。</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    main<span class="token punctuation">(</span><span class="token punctuation">)</span>say_hellosay_hello</code></pre><h4 id="and和or"><a href="#and和or" class="headerlink" title="and和or"></a>and和or</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># and的返回结果问题:</span><span class="token comment" spellcheck="true"># 从左到右计算表达式,若所有的都为真,则返回最后一个值,若存在假,返回第一个假值.</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token operator">and</span> <span class="token boolean">False</span> <span class="token operator">and</span> <span class="token number">1</span><span class="token number">0</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token operator">and</span> <span class="token boolean">False</span> <span class="token operator">and</span> <span class="token number">5</span><span class="token boolean">False</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token operator">and</span> <span class="token number">3</span> <span class="token operator">and</span> <span class="token boolean">False</span> <span class="token operator">and</span> <span class="token number">5</span><span class="token boolean">False</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token operator">and</span> <span class="token number">2</span> <span class="token operator">and</span> <span class="token number">0</span> <span class="token operator">and</span> <span class="token boolean">False</span> <span class="token operator">and</span> <span class="token number">5</span><span class="token number">0</span><span class="token comment" spellcheck="true"># or的返回结果问题:</span><span class="token comment" spellcheck="true"># 从左到右计算表达式,只要遇到真值就返回那个真值,如果表达式结束依旧没有遇到真值,就返回最后一个假值.</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token operator">or</span> <span class="token number">2</span> <span class="token operator">or</span> <span class="token boolean">False</span><span class="token number">1</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token boolean">False</span> <span class="token operator">or</span> <span class="token number">2</span> <span class="token operator">or</span> <span class="token boolean">False</span><span class="token number">2</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token boolean">False</span> <span class="token operator">or</span> <span class="token number">1</span> <span class="token operator">and</span> <span class="token number">2</span><span class="token number">2</span></code></pre><h2 id="稍微不那么基础的"><a href="#稍微不那么基础的" class="headerlink" title="稍微不那么基础的"></a>稍微不那么基础的</h2><h3 id="类"><a href="#类" class="headerlink" title="类"></a>类</h3><h4 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h4><p><strong><code>dir()</code>和<code>__dict__</code></strong></p><p><code> eg:dir(nn.Module)</code> dir函数可以返回参数的属性、方法的列表</p><p><strong><code>__init__()</code>和<code>__new__()</code></strong></p><p>造方法包括创建对象和初始化对象，在python当中，分为两步执行：先执行<code>__new__</code>方法，然后执行<code>__init__</code>方法；</p><p><code>__init__</code>是当实例对象创建完成后被调用的，然后设置对象属性的一些初始值。</p><p><code>__new__</code>是在实例创建之前被调用的，因为它的任务就是创建实例然后返回该实例，是个静态方法。</p><p>也就是，<code>__new__</code>在<code>__init__</code>之前被调用，<code>__new__</code>的返回值（实例）将传递给<code>__init__</code>方法的第一个参数，然后<code>__init__</code>给这个实例设置一些参数。<br><code>__new__</code>至少要有一个参数cls，代表要实例化的类，此参数在实例化时由Python解释器自动提供</p><p><code>__new__</code>必须要有返回值，返回实例化出来的实例，这点在自己实现<code>__new__</code>时要特别注意，可以return父类<code>__new__</code>出来的实例，或者直接是object的<code>__new__</code>出来的实例</p><p><code>__init__</code>有一个参数self，就是这个<code>__new__</code>返回的实例，<code>__init__</code>在<code>__new__</code>的基础上可以完成一些其它初始化的动作，<code>__init__</code>不需要返回值</p><p>我们可以将类比作制造商，<code>__new__</code>方法就是前期的原材料购买环节，<code>__init__</code>方法就是在有原材料的基础上，加工，初始化商品环节</p><p><strong><code>__call__()</code></strong></p><p>为了将类的实例对象变为可调用对象,相当于重载<code>()</code>运算符</p><p>Python中的函数是一级对象。这意味着<strong>Python中的函数的引用可以作为输入传递到其他的函数/方法中，并在其中被执行</strong>。<br>而Python中<strong>类的实例（对象）可以被当做函数对待</strong>。也就是说，我们可以将它们作为输入传递到其他的函数/方法中并调用他们，正如我们调用一个正常的函数那样。而类中<code>__call__()</code>函数的意义正在于此。为了将一个类实例当做函数调用，我们需要在类中实现<code>__call__()</code>方法。也就是我们要在类中实现如下方法：<code>def __call__(self, *args)</code>。这个方法接受一定数量的变量作为输入。<br>假设x是X类的一个实例。那么调用<code>x.__call__(1,2)</code>等同于调用<code>x(1,2)</code>。这个实例本身在这里相当于一个函数。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">X</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> range<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>a <span class="token operator">=</span> a        self<span class="token punctuation">.</span>b <span class="token operator">=</span> b        self<span class="token punctuation">.</span>range <span class="token operator">=</span> range    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>a <span class="token operator">=</span> a        self<span class="token punctuation">.</span>b <span class="token operator">=</span> b        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'__call__ with （{}, {}）'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">.</span>a<span class="token punctuation">,</span> self<span class="token punctuation">.</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__del__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> range<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">del</span> self<span class="token punctuation">.</span>a        <span class="token keyword">del</span> self<span class="token punctuation">.</span>b        <span class="token keyword">del</span> self<span class="token punctuation">.</span>range<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> xInstance <span class="token operator">=</span> X<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> xInstance<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>__call__ <span class="token keyword">with</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></code></pre><p><strong><code>__len__()</code></strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ids<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对于一个list,可以直接使用len([1,2,3])</span><span class="token comment" spellcheck="true">#而对于一个对象，里面有很多东西，没法直接使用len,所以要写内置函数__len__(),以后就可以len(对象)</span></code></pre><p><strong><code>__getitem__()</code></strong></p><p>如果在类中定义了<code>__getitem__()</code>方法，那么他的实例对象（假设为P）就可以这样P[key]取值。当实例对象做P[key]运算时，就会调用类中的<code>__getitem__()</code>方法。</p><p>此外，在用 <code>for..in..</code> 迭代对象时，如果对象没有实现 <code>__iter__</code> <code>__next__</code> 迭代器协议，Python的解释器就会去寻找<code>__getitem__</code> 来迭代对象，如果连<code>__getitem__</code> 都没有定义，这解释器就会报对象不是迭代器的错</p><p><strong><code>__qualname__</code></strong></p><p><code>__qualname</code>相比<code>__name__</code>给予更多的信息,因此在debugging的时候可以给予更多的帮助,例如:</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">pass</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">pass</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>        <span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">pass</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># __name__ is not showing the path, so these functions look equal</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> f<span class="token punctuation">.</span>__name__<span class="token string">'f'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A<span class="token punctuation">.</span>f<span class="token punctuation">.</span>__name__<span class="token string">'f'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A<span class="token punctuation">.</span>A<span class="token punctuation">.</span>f<span class="token punctuation">.</span>__name__<span class="token string">'f'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># And these classes looks equal</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A<span class="token punctuation">.</span>__name__<span class="token string">'A'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A<span class="token punctuation">.</span>A<span class="token punctuation">.</span>__name__<span class="token string">'A'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># __qualname__ shows the path, so these functions are distinguishable</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> f<span class="token punctuation">.</span>__qualname__<span class="token string">'f'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A<span class="token punctuation">.</span>f<span class="token punctuation">.</span>__qualname__<span class="token string">'A.f'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A<span class="token punctuation">.</span>A<span class="token punctuation">.</span>f<span class="token punctuation">.</span>__qualname__<span class="token string">'A.A.f'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># And these classes are distinguishable</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A<span class="token punctuation">.</span>__qualname__<span class="token string">'A'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A<span class="token punctuation">.</span>A<span class="token punctuation">.</span>__qualname__<span class="token string">'A.A'</span></code></pre><p><strong><code>__slots__</code></strong></p><p>正常情况下，当我们定义了一个class，创建了一个class的实例后，我们可以给该实例绑定任何属性和方法。但是，如果我们想要限制实例的属性怎么办？为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的<code>__slots__</code>变量，来限制该class实例能添加的属性：</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">class</span> <span class="token class-name">Student</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     __slots__ <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">,</span> <span class="token string">'age'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> s <span class="token operator">=</span> Student<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> s<span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">'digg'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> s<span class="token punctuation">.</span>age <span class="token operator">=</span> <span class="token string">'19'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> s<span class="token punctuation">.</span>score <span class="token operator">=</span> <span class="token number">99</span>Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span><span class="token punctuation">:</span>  File <span class="token string">"&lt;stdin&gt;"</span><span class="token punctuation">,</span> line <span class="token number">1</span><span class="token punctuation">,</span> <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">&gt;</span>AttributeError<span class="token punctuation">:</span> <span class="token string">'Student'</span> object has no attribute <span class="token string">'score'</span></code></pre><p>使用<code>__slots__</code>要注意，<code>__slots__</code>定义的属性仅对当前类实例起作用，对继承的子类是不起作用的。</p><p><code>__slots__ </code>存在的真正原因是用于优化，否则我们是以<code>__dict__</code>来存储实例属性，如果我们涉及到很多需要处理的数据，使用元组来存储当然会节省时间和内存。如果我们还是想要有可以随意添加实例属性，那么把<code>__dict__</code>放入<code>__slots__</code>中既可，实例会在元组中保存各个实例的属性，此外还支持动态创建属性，这些属性存储在常规的<code>__dict__</code>中。优化完全就不见了。o(╯□╰)o</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">class</span> <span class="token class-name">Student</span><span class="token punctuation">:</span>    __slots__ <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">,</span> <span class="token string">'age'</span><span class="token punctuation">,</span> <span class="token string">'__dict__'</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> s<span class="token punctuation">.</span>score <span class="token operator">=</span> <span class="token number">99</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> s<span class="token punctuation">.</span>score<span class="token number">99</span></code></pre><p>对于具有已知属性的小类，它可能是一个瓶颈。浪费了<code>dict</code>很多内存。Python 不能只在创建对象时分配静态内存量来存储所有属性。因此，如果您创建大量对象（我说的是成千上万），它会占用大量 RAM。仍然有一种方法可以规避这个问题。它涉及使用<code>__slots__</code>来告诉 Python 不要使用 dict，并且只为一组固定的属性分配空间。</p><p><code>@staticmethod</code>静态方法和<code>@classmethod</code>类方法</p><p><strong>一般来说，要使用某个类的方法，需要先实例化一个对象再调用方法。</strong><br><strong>而使用@staticmethod或@classmethod，就可以不需要实例化，直接类名.方法名()来调用。</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">foo</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span> <span class="token string">"executing foo(%s)"</span><span class="token operator">%</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">foo</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#self是对实例的绑定,a.foo(x)其实是foo(a,x),对象实例a隐式地作为第一个参数传递</span>        <span class="token keyword">print</span> <span class="token string">"executing foo(%s, %s)"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span>    @classmethod    <span class="token keyword">def</span> <span class="token function">class_foo</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#cls是对类的绑定,和self类似</span>        <span class="token keyword">print</span> <span class="token string">"executing class_foo(%s, %s)"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>cls<span class="token punctuation">,</span> x<span class="token punctuation">)</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">static_foo</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#self（对象实例）和 cls（类）都不会隐式传递为第一个参数。它们的行为类似于普通函数</span>        <span class="token keyword">print</span> <span class="token string">"executing static_foo(%s)"</span> <span class="token operator">%</span> x    a <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><strong>私有属性和方法</strong></p><p>仅<strong>开头带双下划线__的命名</strong>，用于对象的数据封装，以此命名的属性或者方法为类的私有属性或者私有方法，eg:</p><p><code>def __spam(self):</code></p><p>这就起到了隐藏数据的作用，但是这种实现机制并不是很严格，机制是通过自动”变形”实现的，类中所有以双下划线开头的名称__name都会自动变为<code>_类名__name</code>的新名称。另外这种机制可以阻止继承类重新定义或者更改方法的实现。</p><p>在<strong>类中也可以用单下划线开头来命名属性或者方法</strong>，这只是表示类的定义者<strong>希望这些属性或者方法是”私有的”**，但</strong>实际上并不会起任何作用。**</p><h4 id="装饰器和-符号"><a href="#装饰器和-符号" class="headerlink" title="装饰器和@符号"></a>装饰器和@符号</h4><p>python中的函数可以像普通变量一样当做参数传递给另外一个函数</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">foo</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">bar</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    func<span class="token punctuation">(</span><span class="token punctuation">)</span>bar<span class="token punctuation">(</span>foo<span class="token punctuation">)</span></code></pre><p>具体看这一篇：<a href="https://gohom.win/2015/10/25/pyDecorator/">https://gohom.win/2015/10/25/pyDecorator/</a></p><p>这一篇讲的更清楚<a href="https://foofish.net/python-decorator.html">https://foofish.net/python-decorator.html</a></p><p>简单来说就是复合函数，概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">log</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">wrapper</span><span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kw<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span> <span class="token string">'call %s():'</span> <span class="token operator">%</span> func<span class="token punctuation">.</span>__name__        <span class="token keyword">return</span> func<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kw<span class="token punctuation">)</span>    <span class="token keyword">return</span> wrapper@log<span class="token keyword">def</span> <span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span> <span class="token string">'2015-10-26'</span>    <span class="token keyword">return</span> <span class="token string">"done"</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#添加了装饰器之后，现在运行now(),等于运行log(now)把now传递进去了</span></code></pre><p>内置的@property和@*.setter，其实认真看完实例就明白了，就相当于python的get和set方法，关于这两个内置装饰器可以看这个：<a href="https://www.liaoxuefeng.com/wiki/897692888725344/923030547069856">@property和@xxx.setter</a></p><p>看这段代码：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">class</span> <span class="token class-name">D</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataset_dir<span class="token punctuation">:</span> str<span class="token punctuation">,</span> batch_size<span class="token punctuation">:</span> int<span class="token punctuation">,</span>train_dataset<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>dataset_dir <span class="token operator">=</span> dataset_dir        self<span class="token punctuation">.</span>batch_size <span class="token operator">=</span> batch_size        self<span class="token punctuation">.</span>val_dataset <span class="token operator">=</span> None        self<span class="token punctuation">.</span>train_dataset <span class="token operator">=</span> train_dataset<span class="token comment" spellcheck="true">#这里调用的set方法</span>        self<span class="token punctuation">.</span>test_dataset <span class="token operator">=</span> None    <span class="token comment" spellcheck="true">#train_dataset get set 方法</span>    @property    <span class="token keyword">def</span> <span class="token function">train_dataset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token keyword">print</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>__train_dataset<span class="token punctuation">)</span>    @train_dataset<span class="token punctuation">.</span>setter    <span class="token keyword">def</span> <span class="token function">train_dataset</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>__train_dataset <span class="token operator">=</span> dataset <span class="token comment" spellcheck="true">#这里新定义了一个属性</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"!!!!"</span><span class="token punctuation">)</span>d<span class="token operator">=</span>D<span class="token punctuation">(</span><span class="token string">"asdf"</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>d<span class="token punctuation">.</span>train_dataset<span class="token comment" spellcheck="true">#！！！！</span><span class="token comment" spellcheck="true">#2</span></code></pre><h4 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h4><p><a href="https://blog.csdn.net/yilulvxing/article/details/85374142">类的继承,调用父类的属性和方法</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">UNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#继承了nn.Module</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_channels<span class="token punctuation">,</span> n_classes<span class="token punctuation">,</span> bilinear<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#子类的__init__函数</span>        super<span class="token punctuation">(</span>UNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#把父类全部继承，你有想改的，你再自己改！你重写了父类的方法，还想用父类的方法，怎么办？利用super()函数 eg：super().fun()</span><span class="token comment" spellcheck="true">#如果自己也定义了__ __init____ 方法,那么父类的属性是不能直接调用的</span><span class="token comment" spellcheck="true">#可以在 子类的 __init__中调用一下父类的 __init__ 方法,这样就可以调用</span></code></pre><p><a href="https://blog.csdn.net/Windgs_YF/article/details/89026857">python中父类调用子类的属性和方法</a></p><p>python中父类可以调用子类的属性和方法,我怀疑是子类<code>super.__init__()</code>时把子类的self传过去了,父类本身找不到,就找子类的。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Animal</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">a1</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"调用a1"</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>eat<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这一步会调用子类的方法</span>    <span class="token keyword">def</span> <span class="token function">eat</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'kkkkkk'</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Person</span><span class="token punctuation">(</span>Animal<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">p1</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"调用p1"</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>a1<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">eat</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'hhhhhhhh'</span><span class="token punctuation">)</span>p <span class="token operator">=</span> Person<span class="token punctuation">(</span><span class="token punctuation">)</span>p<span class="token punctuation">.</span>p1<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#父类也实现了,子类也实现了,会优先使用子类重写的方法</span><span class="token comment" spellcheck="true">#调用p1</span><span class="token comment" spellcheck="true">#调用a1</span><span class="token comment" spellcheck="true">#hhhhhhhh</span></code></pre><h4 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h4><p><a href="https://www.cnblogs.com/szy13037-5/articles/9562639.html">super详解</a></p><p>python官网文档对于<a href="https://docs.python.org/2/library/functions.html#super">super</a>的介绍来看,其作用为返回一个代理对象作为代表调用父类或亲类方法。（Return a proxy object that delegates method calls to a parent or sibling class of type. This is useful for accessing inherited methods that have been overridden in a class. )</p><p>super()的主要用法有两种： 在<strong>单类继承</strong>中，其意义就是不需要父类的名称来调用父类的函数，因此当子类改为继承其他父类的时候，不需要对子类内部的父类调用函数做任何修改就能调用新父类的方法。 比如：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/env python2.7</span><span class="token keyword">class</span> <span class="token class-name">base1</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># class base2(object):</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment" spellcheck="true">#    def __init__(self): </span>        <span class="token keyword">print</span> <span class="token string">"base1 class"</span>         <span class="token comment" spellcheck="true">#        print "base2 class"</span>                                    <span class="token comment" spellcheck="true"># 若继承父类需要换成base2</span><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span>base1<span class="token punctuation">)</span><span class="token punctuation">:</span>                     <span class="token comment" spellcheck="true"># class A(base2):</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment" spellcheck="true">#     def __init__(self):</span>        base1<span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#         base2.__init__(self)    </span><span class="token keyword">class</span> <span class="token class-name">B</span><span class="token punctuation">(</span>base1<span class="token punctuation">)</span><span class="token punctuation">:</span>                     <span class="token comment" spellcheck="true"># class B(base2):</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment" spellcheck="true">#     def __init__(self):</span>        super<span class="token punctuation">(</span>B<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true">#         super(B, self).__init__()</span>super<span class="token punctuation">(</span>Class<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#在python2 super把子类的类名和self示例传给父类</span>super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#python3变成了隐式的了</span><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>n <span class="token operator">=</span> <span class="token number">2</span>    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#执行时执行的是super传递过来的self，是子类的实例而不是父类的实例</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'self is {0} @A.add'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n <span class="token operator">+=</span> m<span class="token keyword">class</span> <span class="token class-name">B</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>n <span class="token operator">=</span> <span class="token number">3</span>    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'self is {0} @B.add'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">)</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>m<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n <span class="token operator">+=</span> <span class="token number">3</span>b <span class="token operator">=</span> B<span class="token punctuation">(</span><span class="token punctuation">)</span>b<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#输出</span><span class="token comment" spellcheck="true">#self is &lt;__main__.B object at 0x106c49b38&gt; @B.add</span><span class="token comment" spellcheck="true">#self is &lt;__main__.B object at 0x106c49b38&gt; @A.add</span><span class="token comment" spellcheck="true">#8</span></code></pre><p>关于<code>cls</code>也是类似的</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__new__</span><span class="token punctuation">(</span>cls<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>A<span class="token punctuation">,</span> cls<span class="token punctuation">)</span><span class="token punctuation">.</span>__new__<span class="token punctuation">(</span>cls<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">B</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__new__</span><span class="token punctuation">(</span>cls<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>B<span class="token punctuation">,</span> cls<span class="token punctuation">)</span><span class="token punctuation">.</span>__new__<span class="token punctuation">(</span>cls<span class="token punctuation">)</span></code></pre><p><code>B()</code> would trigger <code>B.__new__(B)</code> which would trigger <code>A.__new__(B)</code>, which would trigger <code>object.__new__(B)</code>, so you’d end up with an instance of <code>B</code></p><p><strong>global</strong></p><p><strong>如果局部要对全局变量修改，应在局部声明该全局变量。</strong> </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 一旦在函数内给变量赋值，就定义了一个局部变量。但如果确实需要在函数内去修改全局变量的值，而不是定义局部变量，</span><span class="token comment" spellcheck="true"># 使用global关键词声明这个变量就是外面的全局变量；</span><span class="token comment" spellcheck="true"># global关键字</span>name <span class="token operator">=</span> <span class="token string">'张三'</span><span class="token keyword">def</span> <span class="token function">change_name</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">global</span> name    name <span class="token operator">=</span> <span class="token string">'lisi'</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'函数内改成'</span><span class="token punctuation">,</span>name<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'全局变量'</span><span class="token punctuation">,</span>name<span class="token punctuation">)</span>change_name<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'全局变量'</span><span class="token punctuation">,</span>name<span class="token punctuation">)</span></code></pre><p><strong>nonlocal</strong></p><p><strong>nonlocal声明的变量不是局部变量,也不是全局变量,而是外部嵌套函数内的变量。</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">nonlocal_test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    count <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">def</span> <span class="token function">test2</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        nonlocal count        count <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">return</span> count    <span class="token keyword">return</span> test2val <span class="token operator">=</span> nonlocal_test<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>val<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 1</span><span class="token keyword">print</span><span class="token punctuation">(</span>val<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#</span><span class="token keyword">print</span><span class="token punctuation">(</span>val<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p><strong>not</strong>和<strong>is</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># None, False, 空字符串"", 0, 空列表[], 空字典{}, 空元组()都相当于False,但是不包括自定义类</span><span class="token comment" spellcheck="true"># not None == not False == not '' == not 0 == not [] == not {} == not ()  </span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> y <span class="token operator">=</span> None  <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>   <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> x <span class="token keyword">is</span> None  <span class="token boolean">False</span>  <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> y <span class="token keyword">is</span> None  <span class="token boolean">True</span>  <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>   <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>   <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token operator">not</span> x  <span class="token boolean">True</span>  <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token operator">not</span> y  <span class="token boolean">True</span> <span class="token comment" spellcheck="true"># 结论： `if x is not None`是最好的写法，清晰，不会出现错误，以后坚持使用这种写法。</span><span class="token comment" spellcheck="true"># 如果比较相同的对象实例，is总是返回True 而 == 最终取决于 "eq()"</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">class</span> <span class="token class-name">foo</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__eq__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token boolean">True</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> f <span class="token operator">=</span> foo<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> f <span class="token operator">==</span> None<span class="token boolean">True</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> f <span class="token keyword">is</span> None<span class="token boolean">False</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> list1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> list2 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> list1<span class="token operator">==</span>list2<span class="token boolean">True</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> list1 <span class="token keyword">is</span> list2<span class="token boolean">False</span><span class="token comment" spellcheck="true"># (ob1 is ob2) 等价于 (id(ob1) == id(ob2))</span></code></pre><h4 id="函数-2"><a href="#函数-2" class="headerlink" title="函数"></a>函数</h4><pre class=" language-python"><code class="language-python">hasattr<span class="token punctuation">(</span>object<span class="token punctuation">,</span>name<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#object:对象 name:字符串,属性名</span><span class="token comment" spellcheck="true">#如果对象有该属性返回true,否则返回false</span>getattr<span class="token punctuation">(</span>object<span class="token punctuation">,</span> name<span class="token punctuation">[</span><span class="token punctuation">,</span> default<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#用于返回一个对象属性值</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>        bar <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> getattr<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token string">'bar'</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 获取属性 bar 值</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token number">1</span>setattr<span class="token punctuation">(</span>object<span class="token punctuation">,</span> name<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 对应函数 getattr()，用于设置属性值，该属性不一定是存在的。</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     bar <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> getattr<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token string">'bar'</span><span class="token punctuation">)</span>          <span class="token comment" spellcheck="true"># 获取属性 bar 值</span><span class="token number">1</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> setattr<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token string">'bar'</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># 设置属性 bar 值</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a<span class="token punctuation">.</span>barvars<span class="token punctuation">(</span><span class="token punctuation">[</span>object<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#带参数:返回对象object的属性和属性值的字典对象,或者说返回对象的__dict__属性</span><span class="token comment" spellcheck="true">#不带参数:作用同locals(),即以字典对象返回当前位置的全部局部变量</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">print</span><span class="token punctuation">(</span>vars<span class="token punctuation">(</span>Runoob<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'a'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'__module__'</span><span class="token punctuation">:</span> <span class="token string">'__main__'</span><span class="token punctuation">,</span> <span class="token string">'__doc__'</span><span class="token punctuation">:</span> None<span class="token punctuation">}</span></code></pre><h3 id="参数和返回值"><a href="#参数和返回值" class="headerlink" title="参数和返回值"></a>参数和返回值</h3><h4 id="return-self"><a href="#return-self" class="headerlink" title="return self"></a>return self</h4><p>其实就是返回自身实例，用于链式调用  <a href="https://blog.csdn.net/jclian91/article/details/81238782">看这篇教程</a></p><h4 id="多个返回值或返回字典"><a href="#多个返回值或返回字典" class="headerlink" title="多个返回值或返回字典"></a>多个返回值或返回字典</h4><p>多个返回值，返回的是一个tuple</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># __slots__是一个元组</span>__slots__ <span class="token operator">=</span> <span class="token string">"func"</span><span class="token punctuation">,</span> <span class="token string">"args"</span><span class="token punctuation">,</span> <span class="token string">"keywords"</span><span class="token punctuation">,</span> <span class="token string">"__dict__"</span><span class="token punctuation">,</span> <span class="token string">"__weakref__"</span><span class="token comment" spellcheck="true"># 返回的是一个字典</span><span class="token keyword">return</span> <span class="token punctuation">{</span>    <span class="token string">'image'</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">'mask'</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>mask<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre><h4 id="可变长度参数与可迭代对象参数"><a href="#可变长度参数与可迭代对象参数" class="headerlink" title="可变长度参数与可迭代对象参数"></a>可变长度参数与可迭代对象参数</h4><p><code>*args</code>将参数打包成tuple给函数体调用</p><p><code>**kwargs</code>打包关键字参数成dict给函数体调用</p><p>注意点：参数<code>arg</code>、<code>*args</code>、<code>**kwargs</code>三个参数的位置必须是一定的。必须是<code>(arg,*args,**kwargs)</code>这个顺序，否则程序会报错。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">function</span><span class="token punctuation">(</span>arg<span class="token punctuation">,</span><span class="token operator">*</span>args<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>arg<span class="token punctuation">,</span>args<span class="token punctuation">,</span>kwargs<span class="token punctuation">)</span>function<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span>a<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> b<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> c<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#注意传参方式！</span><span class="token comment" spellcheck="true">#6 (7,8,9) {'c':3,'a':1,'b':2}</span>data_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'transform'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'base_size'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>               <span class="token string">'crop_size'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'logger'</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span>               <span class="token string">'scale'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">}</span><span class="token keyword">def</span> <span class="token function">get_segmentation_dataset</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>get_segmentation_dataset<span class="token punctuation">(</span><span class="token string">"1"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> a<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">**</span>data_kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#**data_kwargs先解包,然后再和split=1, a=2 一起传入为字典</span><span class="token comment" spellcheck="true">#注意，元组整体传参前面必须加*，字典前面必须加**</span><span class="token comment" spellcheck="true">#列表前面加星号作用是将列表中所有元素解开成独立的参数，传入函数，参数数量等于len(data)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">1</span> <span class="token number">2</span> <span class="token number">3</span></code></pre><h4 id="类型注解"><a href="#类型注解" class="headerlink" title="类型注解"></a>类型注解</h4><p><code>def add_to_graph(self, dataset) -&gt; tf.data.Dataset:</code> 箭头的作用是给函数添加注解，用来说明返回值的数据类型</p><p><code>def get_filters_count(level: int, initial_filters: int) -&gt; int:</code>冒号的作用是说参数的数据类型</p><p><a href="https://blog.csdn.net/jeffery0207/article/details/93734942">Typing模块介绍</a></p><p>在实际使用中， <code>Any, Union, Tuple, List, Sequence, Mapping, Callable, TypeVar,Optional, Generic</code>等的使用频率比较高，其中<code>Union、Optional、Sequence、Mapping</code>非常有用，注意掌握。</p><p>Union</p><p>即并集，所以<code>Union[X, Y]</code> 意思是要么X类型、要么Y类型</p><p>Optional</p><p><code>Optional[X]</code>与<code>Union[X, None]</code>，即它默认允许None类型</p><p>Sequence</p><p>即序列，需要注意的是，<code>List</code>一般用来标注返回值；<code>Sequence、Iterable</code>用来标注参数类型</p><p>Mapping</p><p>即字典，需要注意的是，<code>Dict</code>一般用来标注返回值；<code>Mapping</code>用来标注参数类型</p><p>Any<br>Any与任何类型兼容</p><p>类型检查函数</p><p><code>isinstance(object,classinfo)</code></p><p>判断一个变量object是否是classinfo类型的。与type区别:type不会考虑子类是一种父类类型，不考虑继承关系;instance认为是，考虑继承关系。</p><h3 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h3><p><a href="https://www.cnblogs.com/zhaopanpan/p/8577045.html">好的博客教程</a></p><p>python解释器检测到错误，触发异常（也允许程序员自己触发异常）。程序员编写特定的代码，专门用来捕捉这个异常（这段代码与程序逻辑无关，与异常处理有关）。如果捕捉成功则进入另外一个处理分支，执行你为其定制的逻辑，使程序不会崩溃，这就是异常处理。</p><p><strong>程序运行中的异常可以分为两类：语法错误和逻辑错误。首先，我们必须知道，语法错误跟异常处理无关，所以我们在处理异常之前，必须避免语法上的错误。</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#AttributeError 试图访问一个对象没有的树形，比如foo.x，但是foo没有属性x</span><span class="token comment" spellcheck="true">#IOError 输入/输出异常；基本上是无法打开文件</span><span class="token comment" spellcheck="true">#ImportError 无法引入模块或包；基本上是路径问题或名称错误</span><span class="token comment" spellcheck="true">#IndentationError 语法错误（的子类） ；代码没有正确对齐</span><span class="token comment" spellcheck="true">#IndexError 下标索引超出序列边界，比如当x只有三个元素，却试图访问x[5]</span><span class="token comment" spellcheck="true">#KeyError 试图访问字典里不存在的键</span><span class="token comment" spellcheck="true">#KeyboardInterrupt Ctrl+C被按下</span><span class="token comment" spellcheck="true">#NameError 使用一个还未被赋予对象的变量</span><span class="token comment" spellcheck="true">#SyntaxError Python代码非法，代码不能编译(个人认为这是语法错误，写错了）</span><span class="token comment" spellcheck="true">#TypeError 传入对象类型与要求的不符合</span><span class="token comment" spellcheck="true">#UnboundLocalError 试图访问一个还未被设置的局部变量，基本上是由于另有一个同名的全局变量，导致你以为正在访问它</span><span class="token comment" spellcheck="true">#ValueError 传入一个调用者不期望的值，即使值的类型是正确的</span><span class="token comment" spellcheck="true">#SystemExit 解释器请求退出</span><span class="token comment" spellcheck="true">#Exception 万能异常，捕获任意异常</span></code></pre><h4 id="try-except"><a href="#try-except" class="headerlink" title="try except"></a>try except</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">try</span><span class="token punctuation">:</span>     被检测的代码块<span class="token keyword">except</span> 异常类型：     <span class="token keyword">try</span>中一旦检测到异常，就执行这个位置的逻辑eg：    <span class="token keyword">try</span><span class="token punctuation">:</span>        train_net<span class="token punctuation">(</span>net<span class="token operator">=</span>net<span class="token punctuation">,</span>                  epochs<span class="token operator">=</span>args<span class="token punctuation">.</span>epochs<span class="token punctuation">,</span>                  batch_size<span class="token operator">=</span>args<span class="token punctuation">.</span>batchsize<span class="token punctuation">,</span>                  lr<span class="token operator">=</span>args<span class="token punctuation">.</span>lr<span class="token punctuation">,</span>                  device<span class="token operator">=</span>device<span class="token punctuation">,</span>                  img_scale<span class="token operator">=</span>args<span class="token punctuation">.</span>scale<span class="token punctuation">,</span>                  val_percent<span class="token operator">=</span>args<span class="token punctuation">.</span>val <span class="token operator">/</span> <span class="token number">100</span><span class="token punctuation">)</span>    <span class="token keyword">except</span> KeyboardInterrupt<span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'INTERRUPTED.pth'</span><span class="token punctuation">)</span>        logging<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'Saved interrupt'</span><span class="token punctuation">)</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            sys<span class="token punctuation">.</span>exit<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">except</span> SystemExit<span class="token punctuation">:</span>            os<span class="token punctuation">.</span>_exit<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></code></pre><p><code>try finally</code></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">dealwith_file</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">try</span><span class="token punctuation">:</span>        f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'file'</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">:</span>            int<span class="token punctuation">(</span>line<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">True</span>    <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">False</span>    <span class="token keyword">finally</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''不管try语句中的代码是否报错,都会执行finally分支中的代码'''</span>        <span class="token triple-quoted-string string">'''去完成一些连接操作的收尾工作'''</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'finally 被执行了'</span><span class="token punctuation">)</span>        f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>ret <span class="token operator">=</span> dealwith_file<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>ret<span class="token punctuation">)</span></code></pre><h4 id="assert"><a href="#assert" class="headerlink" title="assert"></a>assert</h4><p>Python assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。</p><p><code>assert newW &gt; 0 and newH &gt; 0, 'Scale is too small'</code></p><h4 id="raise"><a href="#raise" class="headerlink" title="raise"></a>raise</h4><p><a href="https://blog.csdn.net/sinat_38682860/article/details/98469803">NotImplemented详解</a></p><p><code>raise NotImplemented</code></p><p>NotImplemented是Python在内置命名空间中的六个常数之一。其他有False、True、None、Ellipsis 和 <strong>debug</strong>。和 Ellipsis很像，NotImplemented]能被重新赋值（覆盖）。对它赋值，甚至改变属性名称， 不会产生 SyntaxError。所以它不是一个真正的“真”常数。当然，我们应该永远不改变它。</p><p>NotImplemented 是个特殊值，它能被二元特殊方法返回（比如<code>__eq__()、__lt__() 、__add__() 、__rsub__() </code>等），表明某个类型没有像其他类型那样实现这些操作。同样，它或许会被原地处理（in place）的二元特殊方法返回（比如<code>__imul__()、__iand__()</code>等）。还有，它的实际值为True：</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> bool<span class="token punctuation">(</span>NotImplemented<span class="token punctuation">)</span><span class="token boolean">True</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b1 <span class="token operator">==</span> a1Could <span class="token operator">not</span> compare B against the other <span class="token keyword">class</span><span class="token class-name">Comparing</span> an A <span class="token keyword">with</span> a B<span class="token boolean">True</span></code></pre><p>这就是返回了NotImplemented的所做的。NotImplemented告诉运行时，应该让其他对象来完成某个操作。在表达b1 == a1中，<code>b1.__eq__(a1)</code>返回了NotImplemented，这说明Python试着用<code>a1.__eq__(b1)</code>。由于a1足够可以返回True，因此这个表达可以成功。如果A中的<code>__eq__()</code>也返回NotImplemented，那么运行时会退化到使用内置的比较行为，即比较对象的标识符（在CPython中，是对象在内存中的地址）。</p><p>注意：如果在调用<code>b1.__eq__(a1)</code>时抛出NotImpementedError，而不进行处理，就会中断代码的执行。而NotImplemented无法抛出，仅仅是用来进一步测试是否有其他方法可供调用。</p><h2 id="各种包"><a href="#各种包" class="headerlink" title="各种包"></a>各种包</h2><h3 id="模块与包"><a href="#模块与包" class="headerlink" title="模块与包"></a>模块与包</h3><p><a href="https://www.jianshu.com/p/95afe2c3d526">模块与包的区别</a></p><p><strong>模块:</strong> 一般来说, 单个py文件就叫模块(module), 调用这个模块直接使用<code>import 模块名</code>即可, 也可以使用<code>from 模块名 import 函数名/变量名/类名</code>和<code>from 模块名 import *</code></p><p><strong>包:</strong> 当我们写了几个相近的py文件后, 想要集成起来给别人使用. 这时候就需要用到我们的<strong>包</strong>了。包就是为了多个py文件打包起来访问的东西. 只要在包里面放一个<code>__init__.py</code>文件, 在<code>__init__.py</code>文件<code>import包里面的模块(py文件)</code>, 就可以实现<code>import 包名</code>,实现对多个模块的调用。</p><pre class=" language-python"><code class="language-python">packet<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>__init__<span class="token punctuation">.</span>py<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>a<span class="token punctuation">.</span>py<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>b<span class="token punctuation">.</span>py<span class="token keyword">import</span> packet<span class="token punctuation">.</span>a <span class="token comment" spellcheck="true">#用.</span><span class="token keyword">import</span> packet <span class="token comment" spellcheck="true">#导入的是__inti__.py里面导入的</span></code></pre><p>事实上，当我们向文件导入某个模块时，<strong>导入的是该模块中那些名称不以下划线（单下划线“_”或者双下划线“__”）开头的变量、函数和类</strong>。因此，如果我们不想模块文件中的某个成员被引入到其它文件中使用，可以在其名称前添加下划线。</p><p><code>__all__</code></p><p>除此之外，还可以借助模块提供的<code> __all__</code> 变量，该变量的值是一个列表，存储的是当前模块中一些成员（变量、函数或者类）的名称。通过在模块文件中设置 <code>__all__</code> 变量，当其它文件以<code>from 模块名 import *</code>的形式(仅此种形式)导入该模块时，该文件中只能使用 <code>__all__</code> 列表中指定的成员。</p><p><a href="https://zhuanlan.zhihu.com/p/64893308"><strong>导入不同层级目录的模块</strong></a></p><p>假设有如下目录结构：</p><pre class=" language-python"><code class="language-python"><span class="token operator">-</span><span class="token operator">-</span> dir0　　<span class="token operator">|</span> file1<span class="token punctuation">.</span>py　　<span class="token operator">|</span> file2<span class="token punctuation">.</span>py　　<span class="token operator">|</span> dir3　　　<span class="token operator">|</span> file3<span class="token punctuation">.</span>py　　<span class="token operator">|</span> dir4　　　<span class="token operator">|</span> file4<span class="token punctuation">.</span>py</code></pre><p><strong>1.导入同级模块</strong></p><p>如在<code>file1.py</code>中想导入<code>file2.py</code>,注意无需加后缀<code>".py"</code>：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> file2<span class="token comment" spellcheck="true"># 使用file2中函数时需加上前缀"file2."，即：</span><span class="token comment" spellcheck="true"># file2.fuction_name()</span></code></pre><p><strong>2.导入下级模块</strong></p><p>方式一:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 需要一直带着dir3,书写比较麻烦,因此建议起一个别名,例如df3</span><span class="token keyword">import</span> dir3<span class="token punctuation">.</span>file3<span class="token comment" spellcheck="true"># import dir3.file3 as df3</span></code></pre><p>方式二:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 在dir3下新建一个__init__.py</span><span class="token operator">-</span><span class="token operator">-</span> dir0　　<span class="token operator">|</span> file1<span class="token punctuation">.</span>py　　<span class="token operator">|</span> file2<span class="token punctuation">.</span>py　　<span class="token operator">|</span> dir3　　　<span class="token operator">|</span> __init__<span class="token punctuation">.</span>py　　　<span class="token operator">|</span> file3<span class="token punctuation">.</span>py　　<span class="token operator">|</span> dir4　　　<span class="token operator">|</span> file4<span class="token punctuation">.</span>py<span class="token keyword">import</span> dir3 <span class="token comment" spellcheck="true"># 导入的是__init__.py中导入的</span></code></pre><p><strong>3.导入上级模块</strong></p><p>要导入上级目录下模块，可以使用<code>sys.path</code>,如在<code>file4.py</code>中想引入<code>import</code>上级目录下的<code>file1.py</code>：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> sys sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"/home/dch"</span><span class="token punctuation">)</span> <span class="token keyword">import</span> file1<span class="token comment" spellcheck="true"># sys.path特指模块的查询路径的列表，初始化是从环境变量PYTHONPATH。</span><span class="token comment" spellcheck="true">#不知道怎么加可以先print(sys.path)</span><span class="token comment" spellcheck="true"># 运行结果如下所示，该python脚本在调用模块的时候可以按照下面的路径依次查找python模块，最后一行是我们加入的第三方的python程序模块的路径。</span><span class="token punctuation">[</span><span class="token string">'C:\\Users\\syd\\Desktop\\program'</span><span class="token punctuation">,</span> <span class="token string">'D:\\Installation_program\\anaconda\\python36.zip'</span><span class="token punctuation">,</span>  <span class="token string">'D:\\Installation_program\\anaconda\\DLLs'</span><span class="token punctuation">,</span>  <span class="token string">'D:\\Installation_program\\anaconda\\lib'</span><span class="token punctuation">,</span>  <span class="token string">'D:\\Installation_program\\anaconda'</span><span class="token punctuation">,</span>  <span class="token string">'D:\\Installation_program\\anaconda\\lib\\site-packages'</span><span class="token punctuation">,</span>  <span class="token string">'D:\\Installation_program\\anaconda\\lib\\site-packages\\win32'</span><span class="token punctuation">,</span>  <span class="token string">'D:\\Installation_program\\anaconda\\lib\\site-packages\\win32\\lib'</span><span class="token punctuation">,</span>  <span class="token string">'D:\\Installation_program\\anaconda\\lib\\site-packages\\Pythonwin'</span><span class="token punctuation">,</span>  <span class="token string">'/home/syd/GAN'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#或者</span>export PYTHONPATH<span class="token operator">=</span><span class="token operator">/</span>home<span class="token operator">/</span>pi<span class="token operator">/</span>my_lib</code></pre><h3 id="math"><a href="#math" class="headerlink" title="math"></a>math</h3><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 返回x的阶乘值。如果x不是整数或者为负数，抛出ValueError。</span>math<span class="token punctuation">.</span>factorial<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回e的x次方</span>math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><p><img src="/python-xue-xi/image-20220526133905941.png" alt="image-20220526133905941"></p><h3 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a>tqdm</h3><p><a href="https://www.cnblogs.com/wanghui-garcia/p/11514579.html">tqdm详细教程</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm<span class="token keyword">for</span> fname <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>input_dir<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#fname就是os.listdir(input_dir)的每一项</span><span class="token comment" spellcheck="true">#tqdm(range(i))可以使用trange(i)替换</span></code></pre><p>另一种调用方式：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> tqdm<span class="token punctuation">(</span>total<span class="token operator">=</span>n_train<span class="token punctuation">,</span> desc<span class="token operator">=</span>f<span class="token string">'Epoch {epoch + 1}/{epochs}'</span><span class="token punctuation">,</span> unit<span class="token operator">=</span><span class="token string">'img'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#使用with的好处是不用手动关闭pbar</span></code></pre><p>文档</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">tqdm</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#装饰一个迭代器对象，返回一个表现得就像原来可迭代的迭代器；但是在每次值被请求时就打印一个动态的更新进度条</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> iterable<span class="token operator">=</span>None<span class="token punctuation">,</span> desc<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#进度条的前缀</span>               total<span class="token operator">=</span>None<span class="token punctuation">,</span>               leave<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#默认为True，即在迭代的最后保持进度条的所有踪迹，简单来说就是会把进度条的最终形态保留下来。</span>               <span class="token comment" spellcheck="true">#否则最后进度条消失</span>               file<span class="token operator">=</span>None<span class="token punctuation">,</span> ncols<span class="token operator">=</span>None<span class="token punctuation">,</span> mininterval<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>               maxinterval<span class="token operator">=</span><span class="token number">10.0</span><span class="token punctuation">,</span> miniters<span class="token operator">=</span>None<span class="token punctuation">,</span> ascii<span class="token operator">=</span>None<span class="token punctuation">,</span> disable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>               unit<span class="token operator">=</span><span class="token string">'it'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#用来定义每个迭代单元的字符串。默认为"it"，表示每个迭代；在下载或解压时，设为"B"，代表每个“块”。</span><span class="token comment" spellcheck="true">#显示速度时,是1s/it还是1s/B,在时间后面</span>               unit_scale<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> dynamic_ncols<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>               smoothing<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> bar_format<span class="token operator">=</span>None<span class="token punctuation">,</span> initial<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> position<span class="token operator">=</span>None<span class="token punctuation">,</span>               postfix<span class="token operator">=</span>None<span class="token punctuation">,</span> unit_divisor<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>pbar<span class="token punctuation">.</span>set_description<span class="token punctuation">(</span><span class="token string">'Epoch %d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#与前面tqdm构造时的desc相同</span><span class="token keyword">def</span> <span class="token function">set_postfix</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ordered_dict<span class="token operator">=</span>None<span class="token punctuation">,</span> refresh<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#设置/修改后缀,在速度后面</span><span class="token comment" spellcheck="true">#ordered_dict是传入有序字典,后边的**kwargs则是对ordered_dict的覆盖和补充。输出为a=1这种形式。</span><span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#手动更新进度条，对流streams有用，比如读文件</span><span class="token comment" spellcheck="true">#n:int, optional 添加到迭代内部计数器的增长数[default:1]</span></code></pre><h3 id="os"><a href="#os" class="headerlink" title="os"></a>os</h3><p>os模块提供了一个统一的操作系统接口函数，os模块能在不同操作系统平台如nt，posix中的特定函数间自动切换，从而实现跨平台操作。</p><p><a href="https://www.jianshu.com/p/86f88b3d7efd">简书教程</a></p><pre class=" language-python"><code class="language-python">a<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">"datasets"</span><span class="token punctuation">,</span> <span class="token string">"lifesat"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#连接两个路径</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#datasets\lifesat\</span>os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#递归的创建文件目录</span><span class="token comment" spellcheck="true">#exist_ok:是否在目录存在时触发异常。</span><span class="token comment" spellcheck="true">#如果exist_ok为False（默认值），则在目标目录已存在的情况下触发FileExistsError异常；</span><span class="token comment" spellcheck="true">#如果exist_ok为True，则在目标目录已存在的情况下不会触发FileExistsError异常。</span>os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#获取指定目录下的所有文件和目录 包括隐藏文件</span><span class="token comment" spellcheck="true">#eg:a/b.txt,listdir(a),输出b.txt,没有/之前的</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#分离文件名与扩展名 eg：</span>path_01<span class="token operator">=</span><span class="token string">'E:\STH\Foobar2000\install.log'</span>path_02<span class="token operator">=</span><span class="token string">'E:\STH\Foobar2000'</span>res_01<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span>path_01<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回的是tuple</span>res_02<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span>path_02<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>root_01<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>root_02<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#('E:\\STH\\Foobar2000\\install', '.log')</span><span class="token comment" spellcheck="true">#('E:\\STH\\Foobar2000', '')</span><span class="token comment" spellcheck="true">#返回文件名和路径名</span>a <span class="token operator">=</span> <span class="token string">"D:\\class_datas\\master\\semanticseg\\Pytorch-UNet\\train.py"</span><span class="token keyword">print</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>basename<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#train.py</span><span class="token comment" spellcheck="true">#D:\class_datas\master\semanticseg\Pytorch-UNet</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#environ是一个字符串所对应环境的映像对象。举个例子来说，environ['HOME']就代表了当前这个用户的主目录。</span><span class="token comment" spellcheck="true">#CUDA——VISIBLE</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>expanduser<span class="token punctuation">(</span>root<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#主要的功能在于把路径中的~转化为user目录，一般使用在Linux系统，代码中设置了某些路径的环境变量的时候。</span><span class="token comment" spellcheck="true">#改变当前脚本工作目录到path</span>os<span class="token punctuation">.</span>chdir<span class="token punctuation">(</span>path<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#用于检验文件是否存在。</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>file<span class="token operator">/</span>dir<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 删除文件</span>os<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>file<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取当前工作目录,再出现文件路径问题的时候用这个可太好用了</span>os<span class="token punctuation">.</span>getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 修改文件名</span>os<span class="token punctuation">.</span>rename<span class="token punctuation">(</span>filename<span class="token punctuation">,</span>newname<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 扫描某个指定目录下所包含的子目录和文件 https://zhuanlan.zhihu.com/p/149824829</span><span class="token keyword">for</span> curDir<span class="token punctuation">,</span> dirs<span class="token punctuation">,</span> files <span class="token keyword">in</span> os<span class="token punctuation">.</span>walk<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># 递归地输出当前目录,当前目录下的文件夹名字,当前目录下的文件名字</span><span class="token comment" spellcheck="true"># 禁止hash随机化,使得实验可复现</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYTHONHASHSEED'</span><span class="token punctuation">]</span> <span class="token operator">=</span> str<span class="token punctuation">(</span>seed<span class="token punctuation">)</span></code></pre><h3 id="pathlib"><a href="#pathlib" class="headerlink" title="pathlib"></a>pathlib</h3><p><a href="https://blog.csdn.net/itanders/article/details/88754606">pathlib库使用详解</a></p><p><strong>面向对象的文件系统路径</strong>,对于底层的路径字符串操作，你也可以使用 <code>os.path</code> 模块。但是<code>pathlib</code>可能更好用。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path <span class="token comment" spellcheck="true">#老版本是pathlib,新版本是pathlib2,完美兼容老版本</span><span class="token comment" spellcheck="true"># 文件名操作</span><span class="token keyword">from</span> pathlib2 <span class="token keyword">import</span> Path<span class="token comment" spellcheck="true"># 返回目录中最后一个部分的扩展名</span>example_path <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">'/Users/Anders/Documents/abc.gif'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>example_path<span class="token punctuation">.</span>suffix<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出如下：</span><span class="token comment" spellcheck="true"># .gif</span><span class="token comment" spellcheck="true"># 返回目录中多个扩展名列表</span>example_paths <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">'/Users/Anders/Documents/abc.tar.gz'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>example_paths<span class="token punctuation">.</span>suffixes<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出如下：</span><span class="token comment" spellcheck="true"># ['.tar', '.gz']</span><span class="token comment" spellcheck="true"># 返回目录中最后一个部分的文件名（但是不包含后缀）</span>example_path <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">'/Users/Anders/Documents/abc.gif'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>example_path<span class="token punctuation">.</span>stem<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出如下：</span><span class="token comment" spellcheck="true"># abc</span><span class="token comment" spellcheck="true"># 返回目录中最后一个部分的文件名</span>example_path <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">'/Users/Anders/Documents/abc.gif'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>example_path<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出如下：</span><span class="token comment" spellcheck="true"># abc.gif</span><span class="token comment" spellcheck="true"># 替换目录最后一个部分的文件名并返回一个新的路径</span>new_path1 <span class="token operator">=</span> example_path<span class="token punctuation">.</span>with_name<span class="token punctuation">(</span><span class="token string">'def.gif'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>new_path1<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出如下：</span><span class="token comment" spellcheck="true"># /Users/Anders/Documents/def.gif</span><span class="token comment" spellcheck="true"># 替换目录最后一个部分的文件名并返回一个新的路径</span>new_path2 <span class="token operator">=</span> example_path<span class="token punctuation">.</span>with_suffix<span class="token punctuation">(</span><span class="token string">'.txt'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>new_path2<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出如下：</span><span class="token comment" spellcheck="true"># /Users/Anders/Documents/abc.txt</span></code></pre><h3 id="shutil"><a href="#shutil" class="headerlink" title="shutil"></a>shutil</h3><p><a href="https://liujiangblog.com/course/python/61">https://liujiangblog.com/course/python/61</a></p><p><strong>shutil模块是对os模块的补充，主要针对文件的拷贝、删除、移动、压缩和解压操作。</strong></p><h3 id="sys"><a href="#sys" class="headerlink" title="sys"></a>sys</h3><p><a href="https://www.liujiangblog.com/course/python/54">教程</a></p><p><strong>sys模块主要是针对与Python解释器相关的变量和方法，不是主机操作系统。</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 退出Python程序，exit(0)表示正常退出。当参数非0时，会引发一个SystemExit异常，可以在程序中捕获该异常</span>sys<span class="token punctuation">.</span>exit<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 详解如下</span><span class="token comment" spellcheck="true"># https://www.cnblogs.com/hls-code/p/15337302.html</span>sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="argparse"><a href="#argparse" class="headerlink" title="argparse"></a>argparse</h3><p>python自带的命令行参数解析包，可以用来方便地读取命令行参数，当你的代码需要频繁地修改参数的时候，使用这个工具可以将参数和代码分离开来，让你的代码更简洁，适用范围更广。它解析sys.argv中的参数。</p><p><a href="https://geek-docs.com/python/python-tutorial/python-argparse.html">教程</a></p><p><a href="https://vra.github.io/2017/12/02/argparse-usage/">一篇比较好的个人博客 add_argument里的各个参数在这看</a></p><h4 id="可选参数、必需参数和位置参数"><a href="#可选参数、必需参数和位置参数" class="headerlink" title="可选参数、必需参数和位置参数"></a>可选参数、必需参数和位置参数</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#位置参数</span><span class="token keyword">import</span> argparseparser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'age'</span><span class="token punctuation">)</span>args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'{args.name} is {args.age} years old'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#$ positional_arg.py Peter 23</span><span class="token comment" spellcheck="true">#Peter is 23 years old</span><span class="token comment" spellcheck="true">#可选参数</span><span class="token keyword">import</span> argparse<span class="token comment" spellcheck="true"># help flag provides flag help</span><span class="token comment" spellcheck="true"># store_true actions stores argument as True</span>parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-o'</span><span class="token punctuation">,</span> <span class="token string">'--output'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">"shows output"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#具有两个选项的参数,short -o和long --output,如果设置为store_true，则将参数存储为True。</span><span class="token comment" spellcheck="true">#参数一旦存在，则action将其设置为true，如果加上default时，未设置就是default起作用，否则就是action的相反。</span>args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> args<span class="token punctuation">.</span>output<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#存在该参数，则显示一些输出。</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"This is some output"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#可选参数推断规则 --foo-bar -&gt; foo_bar -x -&gt; x</span><span class="token comment" spellcheck="true">#可将可选参数变为必需参数</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--name'</span><span class="token punctuation">,</span> required<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置required为True</span></code></pre><h4 id="add-argument的各种参数"><a href="#add-argument的各种参数" class="headerlink" title="add_argument的各种参数"></a>add_argument的各种参数</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#metavar参数可以让命令的帮助信息更好看一些！</span><span class="token comment" spellcheck="true">#除此之外，对于有nargs参数的命令行参数，可以用metavar来设置每一个具体的参数的名称：</span>parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>prog<span class="token operator">=</span><span class="token string">'PROG'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#prog代替了test.py</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-x'</span><span class="token punctuation">,</span> nargs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--foo'</span><span class="token punctuation">,</span> nargs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'bar'</span><span class="token punctuation">,</span> <span class="token string">'baz'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>print_help<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这个可以输出帮助信息</span><span class="token comment" spellcheck="true">#输出如下</span>usage<span class="token punctuation">:</span> PROG <span class="token punctuation">[</span><span class="token operator">-</span>h<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span>x X X<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>foo bar baz<span class="token punctuation">]</span>optional arguments<span class="token punctuation">:</span> <span class="token operator">-</span>h<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>help     show this help message <span class="token operator">and</span> exit <span class="token operator">-</span>x X X <span class="token operator">-</span><span class="token operator">-</span>foo bar baz<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>python train<span class="token punctuation">.</span>py <span class="token operator">-</span>h<span class="token comment" spellcheck="true">#可以显示提示信息,和上面的parser.print_help()一样</span>parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">'Train the UNet on images and target masks'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#usage下面加了一行描述信息</span>                      formatter_class<span class="token operator">=</span>argparse<span class="token punctuation">.</span>ArgumentDefaultsHelpFormatter<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#提示信息格式</span><span class="token comment" spellcheck="true">#创建一个解析器</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-l'</span><span class="token punctuation">,</span> <span class="token string">'--learning-rate'</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'LR'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>float<span class="token punctuation">,</span> nargs<span class="token operator">=</span><span class="token string">'?'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span>                        help<span class="token operator">=</span><span class="token string">'Learning rate'</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">'lr'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#添加一个新参数</span><span class="token comment" spellcheck="true">#argparse默认的变量名是--或-后面的字符串</span><span class="token comment" spellcheck="true">#但是你也可以通过dest=xxx来设置参数的变量名，然后在代码中用args.xxx来获取参数的值。</span><span class="token comment" spellcheck="true">#nargs=x</span><span class="token comment" spellcheck="true">#x的值  含义</span><span class="token comment" spellcheck="true">#N   参数的绝对个数（例如：3）</span><span class="token comment" spellcheck="true">#'?'   0或1个参数</span><span class="token comment" spellcheck="true">#'*'   0或所有参数</span><span class="token comment" spellcheck="true">#'+'   所有，并且至少一个参数</span><span class="token comment" spellcheck="true"># argparse.REMAINDER 所有剩余的参数，均转化为一个列表赋值给此项，通常用此方法来将剩余的参数传入另一个parser进行解析</span>arg<span class="token operator">=</span>parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果dataset之前没定义,那么就是给arg添加了一个新属性dataset</span>arg<span class="token punctuation">.</span>dataset<span class="token operator">=</span><span class="token comment" spellcheck="true">#某个值</span><span class="token comment" spellcheck="true"># 对于目前的parser未知的argument,不报错,而是将多出来的部分保存起来，留到后面使用。之后给parser addargument之后,可以在parser.parse_known_args()</span><span class="token comment" spellcheck="true"># 或者parser.parse_args()</span>opt<span class="token punctuation">,</span> unparsed <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_known_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 设置一些参数的默认值,且会覆盖parser.addargument()中的default设置</span>parser<span class="token punctuation">.</span>set_defaults<span class="token punctuation">(</span>foo<span class="token operator">=</span><span class="token string">'test'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># get_defaults()方法可以获取add_argument()和set_defaults()中设置的默认值</span>parser<span class="token punctuation">.</span>get_default<span class="token punctuation">(</span><span class="token string">'air'</span><span class="token punctuation">)</span></code></pre><h4 id="修改args来不用命令行使用"><a href="#修改args来不用命令行使用" class="headerlink" title="修改args来不用命令行使用"></a>修改args来不用命令行使用</h4><p><a href="https://stackoom.com/question/3RVtX/%E8%B0%83%E8%AF%95%E6%97%B6%E6%A8%A1%E6%8B%9Fargparse%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%BE%93%E5%85%A5">修改args来不用命令行使用</a></p><p><img src="/python-xue-xi/image-20201116220928895.png" alt="首选方式"></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># jupyter也可以使用</span><span class="token keyword">import</span> syssys<span class="token punctuation">.</span>argv<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'damaenet_res101_40e_DarkZ.ipynb'</span><span class="token punctuation">,</span><span class="token string">'--num-steps'</span><span class="token punctuation">,</span><span class="token string">'48320'</span><span class="token punctuation">,</span><span class="token string">'--help-info'</span><span class="token punctuation">,</span><span class="token string">'wokecheng+mae'</span><span class="token punctuation">]</span></code></pre><h3 id="logging"><a href="#logging" class="headerlink" title="logging"></a>logging</h3><p>logging模块定义的函数和类为应用程序和库的开发实现了一个灵活的事件日志系统。</p><p>logging模块提供了两种记录日志的方式：</p><ul><li>第一种方式是使用logging提供的模块级别的函数</li><li>第二种方式是使用Logging日志系统的四大组件</li></ul><p>其实，logging所提供的模块级别的日志记录函数也是对logging日志系统相关类的封装而已。</p><p><a href="https://blog.csdn.net/pansaky/article/details/82685663">logging用法详解：组件的办法</a></p><p><a href="https://www.cnblogs.com/yyds/p/6901864.html">日志logging模块教程</a></p><p>logging模块与log4j的机制是一样的，只是具体的实现细节不同。模块提供logger，handler，filter，formatter。</p><ul><li><p>logger：提供日志接口，供应用代码使用。logger最常用的操作有两类：配置和发送日志消息。可以通过logging.getLogger(name)获取logger对象，如果不指定name则返回root对象，多次使用相同的name调用getLogger方法返回同一个logger对象。</p></li><li><p>handler：将日志记录（log record）发送到合适的目的地（destination），比如文件，socket等。一个logger对象可以通过addHandler方法添加0到多个handler，每个handler又可以定义不同日志级别，以实现日志分级过滤显示。用于将日志记录发送到指定的目的位置</p></li><li><p>filter：提供一种优雅的方式决定一个日志记录是否发送到handler。提供更细粒度的日志过滤功能，用于决定哪些日志记录将会被输出（其它的日志记录将会被忽略）</p></li><li><p>formatter：指定日志记录输出的具体格式。formatter的构造方法需要两个参数：消息的格式字符串和日期字符串，这两个参数都是可选的。</p><p>logging模块提供的模块级别的那些函数实际上也是通过这几个组件的相关实现类来记录日志的，只是在创建这些类的实例时设置了一些默认值。</p></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#四大组件的方法</span>logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#日志对象，logging模块中最基础的对象</span>logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置日志级别</span>BASIC_FORMAT <span class="token operator">=</span> <span class="token string">"%(asctime)s: %(message)s"</span>DATE_FORMAT <span class="token operator">=</span> <span class="token string">'%Y-%m-%d %H:%M:%S'</span>formatter <span class="token operator">=</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span>BASIC_FORMAT<span class="token punctuation">,</span> DATE_FORMAT<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#日志记录输出的具体格式</span>chlr <span class="token operator">=</span> logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#就好比windows的console，打印在CMD</span>chlr<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置chlr handler的格式</span>chlr<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span><span class="token string">'INFO'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置chlr handler的级别,INFO级别及以上输出</span>fhlr <span class="token operator">=</span> logging<span class="token punctuation">.</span>FileHandler<span class="token punctuation">(</span>osp<span class="token punctuation">.</span>join<span class="token punctuation">(</span>final_log_path<span class="token punctuation">,</span> log_file<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#文件日志，打印在文件里</span>fhlr<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置格式</span><span class="token comment" spellcheck="true">#文件没有setLevel,我怀疑就是使用logger的Level</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>chlr<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>fhlr<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将相应的handler添加到logger上</span><span class="token keyword">return</span> logger<span class="token comment" spellcheck="true">#之后使用就是eg:logger.debug(message)</span><span class="token comment" spellcheck="true">#模块级别</span>logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">,</span> format<span class="token operator">=</span><span class="token string">'%(levelname)s:%(name)s:%(message)s'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#basicConfig:对root logger进行一次性配置</span><span class="token comment" spellcheck="true">#level:指定日志器的日志级别,&gt;=INFO级别的日志才会输出 </span><span class="token comment" spellcheck="true">#format:指定日志格式字符串，即指定日志输出时所包含的字段信息以及它们的顺序。</span><span class="token comment" spellcheck="true"># eg WARNING:root:This is a warning log.</span><span class="token comment" spellcheck="true">#如果不设置basicConfig默认级别为Warning</span><span class="token comment" spellcheck="true">#filename:指定使用指定的文件名而不是StreamHandler创建FileHandler。</span><span class="token comment" spellcheck="true">#filemode:指定打开文件的模式，如果指定了filename（如果文件模式未指定，则默认为'a'）。</span><span class="token comment" spellcheck="true">#datefmt:使用指定的日期/时间格式。</span><span class="token comment" spellcheck="true">#handlers:如果指定，这应该是一个已经创建的处理程序的迭代器添加到根记录器。任何尚未设置格式化程序的处理程序都将被分配在此函数中创建的默认格式化程序。</span>logging<span class="token punctuation">.</span>info<span class="token punctuation">(</span>f<span class="token string">'Using device {device}'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#创建一条严重级别为INFO的日志记录,也可这么写logging.log(logging.INFO, "This is a info log.")</span></code></pre><h4 id="格式字符串字段"><a href="#格式字符串字段" class="headerlink" title="格式字符串字段"></a>格式字符串字段</h4><pre class=" language-python"><code class="language-python"><span class="token operator">%</span><span class="token punctuation">(</span>levelname<span class="token punctuation">)</span>s<span class="token comment" spellcheck="true">#该日志记录的文字形式的日志级别（'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'）</span><span class="token operator">%</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span>s<span class="token comment" spellcheck="true">#所使用的日志器名称，默认是'root'，因为默认使用的是 rootLogger</span><span class="token operator">%</span><span class="token punctuation">(</span>message<span class="token punctuation">)</span>s<span class="token comment" spellcheck="true">#日志记录的文本内容，通过 msg % args计算得到的</span><span class="token operator">%</span><span class="token punctuation">(</span>asctime<span class="token punctuation">)</span>s<span class="token comment" spellcheck="true">#字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒</span></code></pre><h3 id="glob"><a href="#glob" class="headerlink" title="glob"></a>glob</h3><p><a href="https://blog.csdn.net/gufenchen/article/details/90723418">教程</a></p><p>glob是python自带的一个操作文件的相关模块，由于模块功能比较少，所以很容易掌握。用它可以查找符合特定规则的文件路径名。使用该模块查找文件，只需要用到： <code>*</code>  <code>?</code>  <code>[]</code>三个匹配符;</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#”*”匹配0个或多个字符；</span><span class="token comment" spellcheck="true">#”?”匹配单个字符；</span><span class="token comment" spellcheck="true">#”[]”匹配指定范围内的字符，如：[0-9]匹配数字。</span>glob<span class="token punctuation">.</span>glob<span class="token comment" spellcheck="true">#返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径。</span><span class="token comment" spellcheck="true">#python的glob模块可以对文件夹下所有文件进行遍历，并保存为一个list列表</span>mask_file <span class="token operator">=</span> glob<span class="token punctuation">.</span>glob<span class="token punctuation">(</span>self<span class="token punctuation">.</span>masks_dir <span class="token operator">+</span> idx <span class="token operator">+</span> self<span class="token punctuation">.</span>mask_suffix <span class="token operator">+</span> <span class="token string">'.*'</span><span class="token punctuation">)</span></code></pre><h3 id="time"><a href="#time" class="headerlink" title="time"></a>time</h3><p><a href="https://www.cnblogs.com/pal-duan/p/10568829.html">time模块常用方法</a></p><pre class=" language-python"><code class="language-python">time<span class="token punctuation">.</span>strftime<span class="token punctuation">(</span>format<span class="token punctuation">[</span><span class="token punctuation">,</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将struct_time类型的时间转换为format参数指定格式的字符串。</span><span class="token comment" spellcheck="true">#format:指定转换时间的字符串格式。</span><span class="token comment" spellcheck="true">#t:struct_time类型的时间，如果不填默认为当前时间（即time.localtime()返回的时间）</span><span class="token comment" spellcheck="true"># 计时 分析网络运行的操作时可太好用了</span>start<span class="token operator">=</span>time<span class="token punctuation">.</span>perf_counter<span class="token punctuation">(</span><span class="token punctuation">)</span>end <span class="token operator">=</span> time<span class="token punctuation">.</span>perf_counter<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>end<span class="token operator">-</span>start<span class="token punctuation">)</span></code></pre><h3 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h3><h4 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h4><pre class=" language-python"><code class="language-python">Image<span class="token punctuation">.</span>crop<span class="token punctuation">(</span>left<span class="token punctuation">,</span> up<span class="token punctuation">,</span> right<span class="token punctuation">,</span> below<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#切四刀获得一个新的小块</span><span class="token comment" spellcheck="true">#left:与左边界的距离</span><span class="token comment" spellcheck="true">#up:与上边界的距离</span><span class="token comment" spellcheck="true">#right:还是与左边界的距离</span><span class="token comment" spellcheck="true">#below:还是与上边界的距离</span>img <span class="token operator">=</span> img<span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span>ow<span class="token punctuation">,</span> oh<span class="token punctuation">)</span><span class="token punctuation">,</span> Image<span class="token punctuation">.</span>BILINEAR<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Image.NEAREST:最近邻插值</span><span class="token comment" spellcheck="true">#Image.BILINEAR:双线性插值</span><span class="token comment" spellcheck="true">#Image.BICUBIC:双三次插值</span><span class="token comment" spellcheck="true">#Image.ANTIALIAS:面积插值</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>rotate<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#45°旋转</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>FLIP_LEFT_RIGHT<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#水平翻转</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>FLIP_TOP_BOTTOM<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#垂直翻转</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>ROTATE_90<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 90</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>ROTATE_180<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#180°顺时针翻转</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>ROTATE_270<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#270°顺时针翻转</span></code></pre><h4 id="ImageOps"><a href="#ImageOps" class="headerlink" title="ImageOps"></a>ImageOps</h4><p><a href="https://blog.csdn.net/icamera0/article/details/50785776">ImageOps模块介绍</a></p><p>PIL的一些图像处理操作</p><pre class=" language-python"><code class="language-python">ImageOps<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>image<span class="token punctuation">,</span> border<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> fill<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>⇒ image<span class="token comment" spellcheck="true">#eg:ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)</span><span class="token comment" spellcheck="true">#按照变量border的四元组，在图像的左、上、右、下四个边，使用给定的颜色填充相应的行和列。</span></code></pre><h3 id="random"><a href="#random" class="headerlink" title="random"></a>random</h3><p><a href="https://blog.csdn.net/qq_42849332/article/details/81516356?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.control">random新手必看</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> random<span class="token comment" spellcheck="true">#随机生成[0,1.0)的浮点数</span><span class="token keyword">print</span><span class="token punctuation">(</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#用于生成一个指定范围内的整数。其中参数a是下限，参数b是上限，生成的随机数n: a &lt;= n &lt;= b(闭区间)</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 从sequence中随机选出一个元素,参数sequence表示一个有序类型</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>sequence<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用于将一个列表中的元素随机打乱</span>L<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>L<span class="token punctuation">)</span></code></pre><h3 id="requests"><a href="#requests" class="headerlink" title="requests"></a>requests</h3><p><a href="https://geek-docs.com/python/python-tutorial/python-requests.html">Python Requests教程</a></p><p><code>get()</code></p><p><code>get()</code>方法发出 GET 请求,它获取由给定 URL 标识的文档。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> requests <span class="token keyword">as</span> reqa <span class="token operator">=</span> req<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"http://dchlovestudy.xyz/"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>text<span class="token punctuation">)</span></code></pre><p>输出如下:</p><p><img src="/python-xue-xi/image-20210615230849119.png" alt="dchlovestudy.xyz输出的内容"></p><p>返回的对象<code>a</code>类型为<code>&lt;class 'requests.models.Response'&gt;</code>,该对象的属性<code>status_code</code>返回相应的HTTP代码,例如200或404。200 是成功 HTTP 请求的标准响应，而 404 则表明找不到所请求的资源。</p><p><code>self.sess = requests.Session()</code></p><p>会话对象让你能够跨请求保持某些参数。</p><h3 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h3><p><a href="https://www.runoob.com/python3/python-urllib.html">菜鸟教程urllib</a></p><p>Python urllib 库用于操作网页 URL，并对网页的内容进行抓取处理。</p><p>urllib 包 包含以下几个模块：</p><ul><li><strong>urllib.request</strong> - 打开和读取 URL。</li><li><strong>urllib.error</strong> - 包含 urllib.request 抛出的异常。</li><li><strong>urllib.parse</strong> - 解析 URL。</li><li><strong>urllib.robotparser</strong> - 解析 robots.txt 文件。</li></ul><p><code>urllib.parse</code></p><pre class=" language-python"><code class="language-python">urllib<span class="token punctuation">.</span>parse<span class="token punctuation">.</span>urlparse<span class="token punctuation">(</span>urlstring<span class="token punctuation">,</span> scheme<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> allow_fragments<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># urlstring为字符串的url地址</span><span class="token comment" spellcheck="true"># scheme为协议类型</span><span class="token comment" spellcheck="true"># allow_fragments参数为false，则无法识别片段标识符。相反，它们被解析为路径、参数或查询组件的一部分，并且fragment在返回值中设置为空字符串。</span><span class="token keyword">from</span> urllib<span class="token punctuation">.</span>parse <span class="token keyword">import</span> urlparseo <span class="token operator">=</span> urlparse<span class="token punctuation">(</span><span class="token string">"https://www.runoob.com/?s=python+%E6%95%99%E7%A8%8B"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>o<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># ParseResult(scheme='https', netloc='www.runoob.com', path='/', params='', query='s=python+%E6%95%99%E7%A8%8B', fragment='')</span><span class="token comment" spellcheck="true"># 从结果可以看出，内容是一个元组，包含 6 个字符串：协议，位置，路径，参数，查询，判断。</span>o <span class="token operator">=</span> urlparse<span class="token punctuation">(</span><span class="token string">"https://www.runoob.com/?s=python+%E6%95%99%E7%A8%8B"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>o<span class="token punctuation">.</span>scheme<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># https</span></code></pre><img src="/python-xue-xi/new1\jianguoyun\_posts\python学习\image-20210921182806439.png" alt="属性"><h3 id="collections"><a href="#collections" class="headerlink" title="collections"></a>collections</h3><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017681679479008">教程</a></p><p><strong><code>defaultdict</code></strong></p><p>使用<code>dict</code>时，如果引用的Key不存在，就会抛出<code>KeyError</code>。如果希望key不存在时，返回一个默认值，就可以用<code>defaultdict</code>：</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> dd <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token keyword">lambda</span><span class="token punctuation">:</span> <span class="token string">'N/A'</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> dd<span class="token punctuation">[</span><span class="token string">'key1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'abc'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> dd<span class="token punctuation">[</span><span class="token string">'key1'</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># key1存在</span><span class="token string">'abc'</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> dd<span class="token punctuation">[</span><span class="token string">'key2'</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># key2不存在，返回默认值</span><span class="token string">'N/A'</span>dict1 <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>int<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 0</span>dict2 <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>set<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># set()</span>dict3 <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>str<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># </span>dict4 <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>list<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># []</span></code></pre><p>注意默认值是调用函数返回的，而函数在创建<code>defaultdict</code>对象时传入。除了在Key不存在时返回默认值，<code>defaultdict</code>的其他行为跟<code>dict</code>是完全一样的。</p><p><code>defaultdict</code>传入的函数是<strong>工厂函数:</strong></p><p><em>工厂函数看上去有点像函数，实质上他们是类，当你调用他们时，实际上是生成了该类型的一个实例，就像工厂生产货物一样。</em></p><p><strong><code>OrderedDict</code></strong></p><p>使用<code>dict</code>时，Key是无序的。在对<code>dict</code>做迭代时，我们无法确定Key的顺序。</p><p>如果要保持Key的顺序，可以用<code>OrderedDict</code>：</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> d <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> d <span class="token comment" spellcheck="true"># dict的Key是无序的</span><span class="token punctuation">{</span><span class="token string">'a'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">}</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> od <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> od <span class="token comment" spellcheck="true"># OrderedDict的Key是有序的</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><a href="https://www.cnblogs.com/gide/p/6370082.html">ordered_dict</a></p><p><strong>模块collections中的子类，使字典的迭代顺序就是插入的顺序</strong></p><p>注:<strong>从python3.6开始,普通的字典也是有序的了，即按照插入的顺序迭代。字典也是有顺序的,顺序就是插入的顺序。</strong></p><p><strong><code>namedtuple</code></strong></p><p><code>namedtuple</code>是一个函数，它用来创建一个自定义的<code>tuple</code>对象，并且规定了<code>tuple</code>元素的个数，并可以用属性而不是索引来引用<code>tuple</code>的某个元素。</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">from</span> collections <span class="token keyword">import</span> namedtuple<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> Point <span class="token operator">=</span> namedtuple<span class="token punctuation">(</span><span class="token string">'Point'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">,</span> <span class="token string">'y'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> p <span class="token operator">=</span> Point<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> p<span class="token punctuation">.</span>x<span class="token number">1</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> p<span class="token punctuation">.</span>y<span class="token number">2</span></code></pre><p><strong><code>deque</code></strong></p><p>双端队列(double-ended queue)的缩写,由于两端都能编辑,deque既可以用来实现栈(stack)也可以用来实现队列(queue)。</p><img src="/python-xue-xi/image-20211125152006312.png" alt="image-20211125152006312" style="zoom:67%;"><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">collections</span><span class="token punctuation">.</span>deque<span class="token punctuation">(</span><span class="token punctuation">[</span>iterable<span class="token punctuation">[</span><span class="token punctuation">,</span> maxlen<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>q <span class="token operator">=</span> collections<span class="token punctuation">.</span>deque<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token number">5</span> <span class="token keyword">in</span> q<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># False</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token keyword">in</span> q<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># True</span></code></pre><p><code>Counter</code></p><p><strong>Counter（计数器）是对字典的补充，用于追踪值的出现次数。</strong></p><p><strong>Counter是一个继承了字典的类<code>(Counter(dict))</code>。(就是我想实现的那个!!!!<span class="github-emoji"><span>😂</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>)</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> collectionsobj <span class="token operator">=</span> collections<span class="token punctuation">.</span>Counter<span class="token punctuation">(</span><span class="token string">"asfsfsa"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用来方便的进行计数</span>obj<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>Counter<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'a'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'f'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'s'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">}</span><span class="token punctuation">)</span></code></pre><p>重写了加和减</p><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">__sub__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">''' Subtract count, but keep only results with positive counts.        &gt;&gt;&gt; Counter('abbbc') - Counter('bccd')        Counter({'b': 2, 'a': 1})        '''</span>        <span class="token keyword">if</span> <span class="token operator">not</span> isinstance<span class="token punctuation">(</span>other<span class="token punctuation">,</span> Counter<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> NotImplemented        result <span class="token operator">=</span> Counter<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> elem<span class="token punctuation">,</span> count <span class="token keyword">in</span> self<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            newcount <span class="token operator">=</span> count <span class="token operator">-</span> other<span class="token punctuation">[</span>elem<span class="token punctuation">]</span>            <span class="token keyword">if</span> newcount <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>                result<span class="token punctuation">[</span>elem<span class="token punctuation">]</span> <span class="token operator">=</span> newcount        <span class="token keyword">for</span> elem<span class="token punctuation">,</span> count <span class="token keyword">in</span> other<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> elem <span class="token operator">not</span> <span class="token keyword">in</span> self <span class="token operator">and</span> count <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">:</span>                result<span class="token punctuation">[</span>elem<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">-</span> count        <span class="token keyword">return</span> result</code></pre><h3 id="functools"><a href="#functools" class="headerlink" title="functools"></a>functools</h3><p><a href="https://docs.python.org/3/library/functools.html">https://docs.python.org/3/library/functools.html</a></p><p>functools 模块用于高阶函数：接受函数为参数或返回其他函数的函数。 通常，出于此模块的目的，任何可调用对象都可以视为函数。</p><p><code>functools.partial</code></p><p><code>functools.partial</code>的作用和<code>tensorflow</code>的<code>slim</code>很像，主要作用就是简化函数，更少更灵活的函数参数调用。<code>functools.partial</code>可以让我们通过包装的方法，减少你的函数参数。</p><p>functools.partial用于部分应用一个函数，它基于一个函数创建一个可调用对象，把原函数的某些参数固定，调用时只需要传递未固定的参数即可。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> functools<span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"当前结果值"</span><span class="token punctuation">,</span> a<span class="token operator">+</span>b<span class="token punctuation">)</span>add <span class="token operator">=</span> functools<span class="token punctuation">.</span>partial<span class="token punctuation">(</span>add<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>add<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出</span><span class="token comment" spellcheck="true"># 当前结果值3</span><span class="token comment" spellcheck="true"># 核心源码</span><span class="token keyword">class</span> <span class="token class-name">partial</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""New function with partial application of the given arguments    and keywords.    """</span>    __slots__ <span class="token operator">=</span> <span class="token string">"func"</span><span class="token punctuation">,</span> <span class="token string">"args"</span><span class="token punctuation">,</span> <span class="token string">"keywords"</span><span class="token punctuation">,</span> <span class="token string">"__dict__"</span><span class="token punctuation">,</span> <span class="token string">"__weakref__"</span>    <span class="token keyword">def</span> <span class="token function">__new__</span><span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>keywords<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token operator">not</span> args<span class="token punctuation">:</span>            <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">"descriptor '__new__' of partial needs an argument"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>args<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">"type 'partial' takes at least one argument"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 类实例,原函数,被固定的参数</span>        cls<span class="token punctuation">,</span> func<span class="token punctuation">,</span> <span class="token operator">*</span>args <span class="token operator">=</span> args        <span class="token keyword">if</span> <span class="token operator">not</span> callable<span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">"the first argument must be callable"</span><span class="token punctuation">)</span>        args <span class="token operator">=</span> tuple<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        <span class="token keyword">if</span> hasattr<span class="token punctuation">(</span>func<span class="token punctuation">,</span> <span class="token string">"func"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            args <span class="token operator">=</span> func<span class="token punctuation">.</span>args <span class="token operator">+</span> args            tmpkw <span class="token operator">=</span> func<span class="token punctuation">.</span>keywords<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>            tmpkw<span class="token punctuation">.</span>update<span class="token punctuation">(</span>keywords<span class="token punctuation">)</span>            keywords <span class="token operator">=</span> tmpkw            <span class="token keyword">del</span> tmpkw            func <span class="token operator">=</span> func<span class="token punctuation">.</span>func        <span class="token comment" spellcheck="true"># 实例化partial对象，将传入的函数和参数设置为当前对象的属性</span>        self <span class="token operator">=</span> super<span class="token punctuation">(</span>partial<span class="token punctuation">,</span> cls<span class="token punctuation">)</span><span class="token punctuation">.</span>__new__<span class="token punctuation">(</span>cls<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>func <span class="token operator">=</span> func        self<span class="token punctuation">.</span>args <span class="token operator">=</span> args        self<span class="token punctuation">.</span>keywords <span class="token operator">=</span> keywords        <span class="token keyword">return</span> self    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>keywords<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token operator">not</span> args<span class="token punctuation">:</span>            <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">"descriptor '__call__' of partial needs an argument"</span><span class="token punctuation">)</span>        self<span class="token punctuation">,</span> <span class="token operator">*</span>args <span class="token operator">=</span> args        newkeywords <span class="token operator">=</span> self<span class="token punctuation">.</span>keywords<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        newkeywords<span class="token punctuation">.</span>update<span class="token punctuation">(</span>keywords<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>func<span class="token punctuation">(</span><span class="token operator">*</span>self<span class="token punctuation">.</span>args<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>newkeywords<span class="token punctuation">)</span></code></pre><p><code>reduce</code></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># reduce() 函数会对参数序列中元素进行累积。</span><span class="token comment" spellcheck="true"># 函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，</span><span class="token comment" spellcheck="true"># 得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。</span><span class="token keyword">from</span> functools <span class="token keyword">import</span> reduce<span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 两数相加</span>    <span class="token keyword">return</span> x <span class="token operator">+</span> ysum1 <span class="token operator">=</span> reduce<span class="token punctuation">(</span>add<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 计算列表和：1+2+3+4+5</span>sum2 <span class="token operator">=</span> reduce<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> x<span class="token operator">+</span>y<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 使用 lambda 匿名函数</span><span class="token keyword">print</span><span class="token punctuation">(</span>sum1<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>sum2<span class="token punctuation">)</span></code></pre><p><code>lru_cache()</code></p><p><a href="https://blog.csdn.net/momoda118/article/details/120726050">教程2</a></p><h3 id="itertools"><a href="#itertools" class="headerlink" title="itertools"></a>itertools</h3><p>为高效循环创建迭代器的函数</p><p>该模块实现了许多受 APL、Haskell 和 SML 构造启发的迭代器构建块。 每个都以适合 Python 的形式重铸。该模块标准化了一组核心的快速、高效的内存工具，这些工具本身或组合使用。 它们一起形成了一个“迭代器代数”，使得在纯 Python 中简洁有效地构建专用工具成为可能。</p><p>这些工具及其内置的对应物也可以很好地与operator模块中的高速功能配合使用。 例如，乘法运算符可以映射到两个向量以形成有效的点积：sum(map(operator.mul, vector1, vector2))。</p><p><a href="https://docs.python.org/3/library/itertools.html">https://docs.python.org/3/library/itertools.html</a></p><p>直接看文档,文档写的太清楚了</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># accumulate用来求前缀和真是太方便了</span>accumulate<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># --&gt; 1 3 6 10 15</span>accumulate<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> initial<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># --&gt; 100 101 103 106 110 115</span>accumulate<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> operator<span class="token punctuation">.</span>mul<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># --&gt; 1 2 6 24 120</span></code></pre><h3 id="json"><a href="#json" class="headerlink" title="json"></a>json</h3><p>JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，易于人阅读和编写。</p><p><code>json.loads</code> 用于解码 JSON 数据。该函数返回 Python 字段的数据类型。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/python</span><span class="token keyword">import</span> jsonjsonData <span class="token operator">=</span> <span class="token string">'{"a":1,"b":2,"c":3,"d":4,"e":5}'</span><span class="token punctuation">;</span>text <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>jsonData<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#{u'a': 1, u'c': 3, u'b': 2, u'e': 5, u'd': 4}</span></code></pre><p><code>json.dump</code>将python中的对象转化成json储存到文件中</p><pre class=" language-python"><code class="language-python">json<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>obj<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 转化成json的对象。</span>          fp<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 文件指针</span>          <span class="token operator">*</span><span class="token punctuation">,</span> skipkeys<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>          ensure_ascii<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 输出保证所有传入的非ASCII字符都被转义;为false，则这些字符将按原样输出。</span>          check_circular<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> allow_nan<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cls<span class="token operator">=</span>None<span class="token punctuation">,</span>          indent<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 如果缩进是非负整数或字符串，那么 JSON 数组元素和对象成员将使用该缩进级别进行漂亮打印</span>          <span class="token comment" spellcheck="true"># 用正整数缩进每级缩进多少个空格。</span>          separators<span class="token operator">=</span>None<span class="token punctuation">,</span> default<span class="token operator">=</span>None<span class="token punctuation">,</span> sort_keys<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kw<span class="token punctuation">)</span></code></pre><h3 id="timm"><a href="#timm" class="headerlink" title="timm"></a>timm</h3><p>PyTorch Image Models (timm)是一个图像模型(models)、层(layers)、实用程序(utilities)、优化器(optimizers)、调度器(schedulers)、数据加载/增强(data-loaders / augmentations)和参考训练/验证脚本(reference training / validation scripts)的集合,目的是将各种SOTA模型组合在一起,从而能够重现ImageNet的训练结果。<br><code>to_2tuple</code>:将传进来的x变成一个(x,x)的tuple</p><p><code>DropPath</code>:将深度学习模型中的多分支结构随机”删除“</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 假设在前向传播中有如下代码</span>x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>那么在drop_path分支中，每个batch有drop_prob的概率样本在self.mlp(self.norm2(x))不会”执行“，会以0直接传递。</p><h3 id="copy"><a href="#copy" class="headerlink" title="copy"></a>copy</h3><p>提到copy包,就不得不提浅拷贝和深拷贝了~</p><p><a href="https://www.cnblogs.com/wilber2013/p/4645353.html">图解python浅拷贝和深拷贝</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> copywill <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"Will"</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">"Python"</span><span class="token punctuation">,</span> <span class="token string">"C#"</span><span class="token punctuation">,</span> <span class="token string">"JavaScript"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 浅拷贝,对will的修改也会影响到wilber</span>wilber <span class="token operator">=</span> copy<span class="token punctuation">.</span>copy<span class="token punctuation">(</span>will<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 另一个例子</span>b <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span>a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>a<span class="token punctuation">.</span>append<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># a:[[1,2,3]] 现在修改b,a也会改变,所以也是类似浅拷贝,只是把b的引用添加到a里了</span></code></pre><p>总结一下，当我们使用下面的操作的时候，会产生浅拷贝的效果：</p><ul><li>使用切片[:]操作</li><li>使用工厂函数（如list/dir/set）</li><li>使用copy模块中的copy()函数</li></ul><p>如果要深拷贝,可以使用copy.deepcopy()进行深拷贝</p><h3 id="heapq"><a href="#heapq" class="headerlink" title="heapq"></a>heapq</h3><p><a href="https://docs.python.org/zh-cn/3/library/heapq.html">heapq</a></p><p>q是队列的意思,堆本来就是一种优先队列,heapq里的堆是小根堆。</p><p>Python中heapq 模块是小根堆。实现大根堆方法： <strong>小根堆的插入和弹出操作均将元素取反即可。</strong></p><p>要创建一个堆,可以使用list来初始化为<code>[]</code>,或者你可以通过一个函数<code>heapify()</code>,来把一个list转换成堆。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 将item的值加入heap中,保持堆的不变性。</span>heapq<span class="token punctuation">.</span>heappush<span class="token punctuation">(</span>heap<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 弹出并返回heap的最小的元素,保持堆的不变性.如果堆为空,抛出IndexError 。</span><span class="token comment" spellcheck="true"># 使用 heap[0],可以只访问最小的元素而不弹出它。</span>heapq<span class="token punctuation">.</span>heappop<span class="token punctuation">(</span>heap<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将item放入堆中,然后弹出并返回heap的最小元素。该组合操作比先调用heappush()再调用heappop()运行起来更有效率。</span>heapqpushpop<span class="token punctuation">(</span>heap<span class="token punctuation">,</span>item<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将list x转换成堆,原地,线性时间内。</span>heapq<span class="token punctuation">.</span>heapify<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 从 iterable 所定义的数据集中返回前 n 个最大元素组成的列表。</span><span class="token comment" spellcheck="true"># 如果提供了 key 则其应指定一个单参数的函数，用于从 iterable 的每个元素中提取比较键 (例如 key=str.lower)。</span>heapq<span class="token punctuation">.</span>nlargest<span class="token punctuation">(</span>n<span class="token punctuation">,</span> iterable<span class="token punctuation">,</span> key<span class="token operator">=</span>None<span class="token punctuation">)</span>heapq<span class="token punctuation">.</span>nsmallest<span class="token punctuation">(</span>n<span class="token punctuation">,</span> iterable<span class="token punctuation">,</span> key<span class="token operator">=</span>None<span class="token punctuation">)</span>heapq<span class="token punctuation">.</span>heapreplace<span class="token punctuation">(</span>heap<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#弹出并返回 heap 中最小的一项，同时推入新的 item。 堆的大小不变。 如果堆为空则引发 IndexError。</span><span class="token comment" spellcheck="true">#这个单步骤操作比 heappop() 加 heappush() 更高效，并且在使用固定大小的堆时更为适宜。 pop/push 组合总是会从堆中返回一个元素并将其替换为 item。</span><span class="token comment" spellcheck="true"># 返回的值可能会比添加的 item 更大。 如果不希望如此，可考虑改用 heappushpop()。 它的 push/pop 组合会返回两个值中较小的一个，将较大的值留在堆中。</span></code></pre><p>heapq模块可以接受元组对象，默认元组的第一个元素作为<code>priority</code>，即按照元组的第一个元素构成小根堆。</p><h3 id="bisect"><a href="#bisect" class="headerlink" title="bisect"></a>bisect</h3><p><a href="https://blog.csdn.net/qq_39478403/article/details/105373620">教程</a></p><p><a href="https://docs.python.org/3/library/bisect.html">API</a></p><p>[bisection n.二等分]该模块支持以排序顺序维护列表，而无需在每次插入后对列表进行排序。 对于具有昂贵比较操作的长列表项目，这可能是对更常见方法的改进。 该模块被称为 bisect，因为它使用基本的二分算法来完成它的工作。 源代码作为算法的工作示例可能是最有用的（边界条件已经正确！）。</p><p>想要使用二分搜索/二分查找但又懒得写肿么办？！当然是使用 bisect 模块 啦 ~~ 顾名思义，它是实现了 二分 (bisection) 算法的模块，能够保持序列 sequence 顺序不变 的情况下对其进行二分查找和插入，适合用于降低对冗长序列查找的时间成本。当然，通过 “以空间换时间” 的方式也是可行的，例如用于构造 hashmap 的 Counter 类 (关于 Counter 模块详见 链接)。然而，本文的焦点是使用 bisect 模块 “凭查找方式降时间”。</p><p><img src="/python-xue-xi/image-20220430190159822.png" alt="image-20220430190159822"></p><h3 id="importlib"><a href="#importlib" class="headerlink" title="importlib"></a>importlib</h3><p>Python 的模块载入机制。模块包含实现 Python 导入机制的函数，用于在包和模块中加载代码。 它是动态导入模块的一个访问点，在编写代码时需要导入的模块名称未知的情况下很有用（例如，对于应用程序的插件或扩展）。</p><pre class=" language-python"><code class="language-python">importlib<span class="token punctuation">.</span>import_module<span class="token punctuation">(</span>name<span class="token punctuation">,</span> package<span class="token operator">=</span>None<span class="token punctuation">)</span>├── clazz│   ├── __init__<span class="token punctuation">.</span>py│   ├── a<span class="token punctuation">.</span>py│   └── b<span class="token punctuation">.</span>py└── main<span class="token punctuation">.</span>pya<span class="token punctuation">.</span>py<span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">show</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"show A"</span><span class="token punctuation">)</span>b<span class="token punctuation">.</span>py<span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">show</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"show B"</span><span class="token punctuation">)</span>main<span class="token punctuation">.</span>py<span class="token punctuation">:</span><span class="token keyword">import</span> importlib<span class="token comment" spellcheck="true"># 绝对导入</span>a <span class="token operator">=</span> importlib<span class="token punctuation">.</span>import_module<span class="token punctuation">(</span><span class="token string">"clazz.a"</span><span class="token punctuation">)</span>a<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># show A</span><span class="token comment" spellcheck="true"># 相对导入</span>b <span class="token operator">=</span> importlib<span class="token punctuation">.</span>import_module<span class="token punctuation">(</span><span class="token string">".b"</span><span class="token punctuation">,</span> <span class="token string">"clazz"</span><span class="token punctuation">)</span>b<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># show B</span></code></pre><h3 id="xlrd-xlwt"><a href="#xlrd-xlwt" class="headerlink" title="xlrd/xlwt"></a>xlrd/xlwt</h3><p><a href="https://www.cnblogs.com/insane-Mr-Li/p/9092619.html">xlrd教程</a></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 返回了一个Book对象</span>data3 <span class="token operator">=</span> xlrd<span class="token punctuation">.</span>open_workbook<span class="token punctuation">(</span><span class="token string">'../datas/net1//yewu_net1_300.xlsx'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取sheet0</span>table <span class="token operator">=</span> data3<span class="token punctuation">.</span>sheets<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 获取行数和列数</span>row<span class="token operator">=</span>table<span class="token punctuation">.</span>nrowscol<span class="token operator">=</span>table<span class="token punctuation">.</span>ncols<span class="token comment" spellcheck="true"># 获取单元格的值 &lt;class 'xlrd.sheet.Cell'&gt; Cell对象</span>table<span class="token punctuation">.</span>cell<span class="token punctuation">(</span>i<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">.</span>value</code></pre><h3 id="scipy"><a href="#scipy" class="headerlink" title="scipy"></a>scipy</h3><p>SciPy 是一个开源的 Python 算法库和数学工具包。</p><p>Scipy 是基于 Numpy 的科学计算库，用于数学、科学、工程学等领域，很多有一些高阶抽象和物理模型需要使用 Scipy。</p><p>SciPy 包含的模块有最优化、线性代数、积分、插值、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算。</p><p><a href="https://www.runoob.com/scipy/scipy-tutorial.html">菜鸟教程</a></p><p><a href="https://scipy.github.io/devdocs/reference/index.html#scipy-api">API文档</a></p><h3 id="IPython"><a href="#IPython" class="headerlink" title="IPython"></a>IPython</h3><p><a href="https://ipython.readthedocs.io/en/stable/">API</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> Ipython <span class="token keyword">import</span> display <span class="token keyword">as</span> ipdipd<span class="token punctuation">.</span>display<span class="token punctuation">(</span>dataframe<span class="token punctuation">)</span></code></pre><h3 id="fractions"><a href="#fractions" class="headerlink" title="fractions"></a>fractions</h3><p><a href="https://docs.python.org/zh-cn/3/library/fractions.html">https://docs.python.org/zh-cn/3/library/fractions.html</a></p><p>分数库</p><h3 id="pdb"><a href="#pdb" class="headerlink" title="pdb"></a>pdb</h3><p><img src="/python-xue-xi/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20220831111027572.png" alt="image-20220831111027572"></p><h3 id="warning"><a href="#warning" class="headerlink" title="warning"></a>warning</h3><p><a href="https://docs.python.org/zh-cn/3/library/warnings.html">https://docs.python.org/zh-cn/3/library/warnings.html</a></p><p>警告信息的控制-通常以下情况会引发警告：提醒用户注意程序中的某些情况，而这些情况（通常）还不值得触发异常并终止程序。例如，当程序用到了某个过时的模块时，就可能需要发出一条警告。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 通过警告过滤器来控制是否发出警告信息</span><span class="token keyword">import</span> warningswarnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">'ignore'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#忽略警告信息</span></code></pre><h3 id="mpi4py的安装"><a href="#mpi4py的安装" class="headerlink" title="mpi4py的安装"></a>mpi4py的安装</h3><pre class=" language-python"><code class="language-python">sudo apt install libopenmpi<span class="token operator">-</span>devconda install mpi4py <span class="token comment" spellcheck="true">#pip不行</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch学习</title>
      <link href="pytorch-xue-xi/"/>
      <url>pytorch-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><p>handbook：<a href="https://github.com/zergtant/pytorch-handbook">https://github.com/zergtant/pytorch-handbook</a></p><p>pytorch API:<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><p>类似例程：<a href="https://github.com/yunjey/pytorch-tutorial">https://github.com/yunjey/pytorch-tutorial</a></p><p>Docs：<a href="https://github.com/fendouai/PyTorchDocs">https://github.com/fendouai/PyTorchDocs</a></p><h3 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h3><p>torch运用就和np一样</p><p>一个简单的网络最基本的步骤就是<strong>预处理，前向，损失，反向，更新</strong></p><h4 id="torch-tensor"><a href="#torch-tensor" class="headerlink" title="torch.tensor"></a>torch.tensor</h4><p>torch.tensor(3.14)这是标量   torch.tensor([3.14])这是向量，判断是几维张量主要是<strong>看有几个中括号</strong></p><p><img src="/pytorch-xue-xi/image-20201113101903090.png" alt="标量、向量和矩阵"></p><p>不是基本数据类型如int，float，string等，而是<strong>引用数据类型</strong></p><p>是在类中封装好的。所以肯定相应操作比如运算符等人家已经给你重载了，所以不用想的太多</p><p>两个tensor<strong>相加如果是同维度</strong>的话，就直接<strong>对应元素相加</strong></p><h4 id="pytorch通道顺序及索引"><a href="#pytorch通道顺序及索引" class="headerlink" title="pytorch通道顺序及索引"></a>pytorch通道顺序及索引</h4><p><strong>NCHW</strong></p><p><a href="https://www.cnblogs.com/Jason66661010/p/13592020.html">很棒的索引教程</a></p><pre class=" language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#基本索引</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#torch.Size([3,28,28])</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#torch.Size([28,28])</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tensor(0.8082)</span><span class="token comment" spellcheck="true">#连续选取</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#torch.Size([2,3,28,28])</span><span class="token comment" spellcheck="true">#由于是两张图片，所以第一维变为2</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#torch.Size(2,1,28,28)</span><span class="token comment" spellcheck="true"># ...作用</span><span class="token comment" spellcheck="true"># …代替了切片操作中前面所有的:， 即a[:, :, None] 和a[…, None]等价</span><span class="token comment" spellcheck="true"># None作用</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span>–<span class="token operator">&gt;</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> None<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>–<span class="token operator">&gt;</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> b<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># None的作用就相当于在对应维度增加了一个维度</span><span class="token comment" spellcheck="true"># pos[..., (1, 0)] y, x -&gt; x, y</span></code></pre><h4 id="基本函数"><a href="#基本函数" class="headerlink" title="基本函数"></a>基本函数</h4><pre class=" language-python"><code class="language-python">x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回一个新的与原张量数据相同但形状不同的张量,-1是指从其他维度推断！</span>y<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#"_"结尾的函数,会用结果替换原变量</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>   eg<span class="token punctuation">:</span>x<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回这个张量的值作为一个标准的Python数。这只适用于只有一个元素的张量。不可微操作</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span> eg<span class="token punctuation">:</span>a<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回张量作为一个(嵌套的)列表。对于标量，返回一个标准的Python数字，就像item()一样。如果需要，张量会首先自动移动到CPU。</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  a<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true">#返回维度 eg:torch.Size([4, 4])</span>numpy_a<span class="token operator">=</span>a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tensor转numpy</span>torch_a<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>numpy_a<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#numpy转tensor</span><span class="token comment" spellcheck="true">#Tensor和numpy对象共享内存，转换很快，但这也意味着，如果其中一个变了，另一个也会变</span>x<span class="token punctuation">.</span>type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果没有提供dtype返回类型，否则将该对象强制转换为指定的类型,并返回该对象。</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回自然对数的新张量</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#禁止梯度计算的上下文管理器，当您确定不会调用张量.backward()时，禁用梯度计算对于推断是很有用的。</span><span class="token comment" spellcheck="true">#它将减少计算的内存消耗，否则需要require_grad =True。</span>torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#eg:输入shape为(A·1·B·1·C·1·D),输入张量的shape就是(A·B·C·D)</span><span class="token comment" spellcheck="true">#如果指定维度的话，那只对该维度去1。注意：返回的张量与输入张量共享存储空间，因此改变一个张量的内容将改变另一个张量的内容。</span><span class="token comment" spellcheck="true">#另外如果对批次batch为1也去掉的话，可能会引发错误。</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 增加一个1维度</span>troch<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#_, predicted = torch.max(outputs, 1)</span><span class="token comment" spellcheck="true">#outputs是数据Tensor，1表示求第一维度上的最大值</span><span class="token comment" spellcheck="true">#_是不要了  torch.max（）的返回值分两部分，分别是values和indices</span>torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>input<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>Tensor<span class="token punctuation">,</span> LongTensor<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回一个命名元组(values, indices)，其中values是给定维度dim中输入张量的每一行的最大值。indices是找到的每个最大值(argmax)的索引位置。</span><span class="token comment" spellcheck="true"># 返回一个命名元组(values,indices)</span><span class="token comment" spellcheck="true"># 其中values是给定维度dim中输入张量的每一行的最大值</span><span class="token comment" spellcheck="true"># indices是找到的每个最大值(argmax)的索引位置。</span><span class="token comment" spellcheck="true"># 如果keepdim为True，则输出张量与输入张量的大小相同，除了dim维度的大小为1。</span><span class="token comment" spellcheck="true"># 否则，dim被压缩(参见torch.squeeze())，导致输出张量比输入少1维。</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">import</span> torch<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a<span class="token operator">=</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b<span class="token operator">=</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> indices<span class="token operator">=</span>torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b_max <span class="token operator">=</span> torch<span class="token punctuation">.</span>take_along_dim<span class="token punctuation">(</span>b<span class="token punctuation">,</span>indices<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a<span class="token operator">=</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> atensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0163</span><span class="token punctuation">,</span> <span class="token number">0.0711</span><span class="token punctuation">,</span> <span class="token number">0.5564</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.4507</span><span class="token punctuation">,</span> <span class="token number">0.8675</span><span class="token punctuation">,</span> <span class="token number">0.5974</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b<span class="token operator">=</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> btensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.7542</span><span class="token punctuation">,</span> <span class="token number">0.1793</span><span class="token punctuation">,</span> <span class="token number">0.5399</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.2292</span><span class="token punctuation">,</span> <span class="token number">0.5329</span><span class="token punctuation">,</span> <span class="token number">0.2084</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> indices<span class="token operator">=</span>torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>take_along_dim<span class="token punctuation">(</span>b<span class="token punctuation">,</span>indices<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.5399</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.5329</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#rand从(0,1)的均匀分布中随机抽样</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#randn从标准正态分布随机抽样</span><span class="token comment" spellcheck="true">#torch.normal(mean,std) 正态分布随机抽样</span><span class="token comment" spellcheck="true">#torch.linspace()线性间距向量  </span><span class="token comment" spellcheck="true">#torch.ones()初始化为1   torch.zeros()初始化为0  torch.eye()初始化为单位矩阵</span>torch<span class="token punctuation">.</span>complex<span class="token punctuation">(</span>real<span class="token punctuation">,</span> imag<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensor<span class="token comment" spellcheck="true">#real为实部，imag为虚部，real和imag必须位数相同，如果real和imag同为float32那么生成的complex就为complex64。</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> real <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> imag <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> z <span class="token operator">=</span> torch<span class="token punctuation">.</span>complex<span class="token punctuation">(</span>real<span class="token punctuation">,</span> imag<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ztensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">+</span><span class="token number">3.j</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token operator">+</span><span class="token number">4.j</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> z<span class="token punctuation">.</span>dtypetorch<span class="token punctuation">.</span>complex64torch<span class="token punctuation">.</span>__version__<span class="token comment" spellcheck="true">#查看torch版本</span>torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#最小</span>              high<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#最大 </span>              size<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#维度 </span>              <span class="token operator">*</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>None<span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">,</span> dtype<span class="token operator">=</span>None<span class="token punctuation">,</span> layout<span class="token operator">=</span>torch<span class="token punctuation">.</span>strided<span class="token punctuation">,</span> device<span class="token operator">=</span>None<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> → Tensor <span class="token comment" spellcheck="true">#均匀分布取样</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>input<span class="token punctuation">,</span> mat2<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span> Tensor<span class="token comment" spellcheck="true">#矩阵乘</span>torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>input<span class="token punctuation">,</span> mat2<span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensor<span class="token comment" spellcheck="true"># 对一个batch的矩阵进行矩阵乘积,(bxnxm)x(bxmxp)=(bxnxp)</span>torch<span class="token punctuation">.</span>permute<span class="token punctuation">(</span>input<span class="token punctuation">,</span> dims<span class="token punctuation">)</span> → Tensor <span class="token comment" spellcheck="true"># 调整通道顺序</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>other<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>Tensor<span class="token comment" spellcheck="true"># 和其它张量具有相同的维度。就从现有的值复制扩充。</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>tensors<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">*</span><span class="token punctuation">,</span>out<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将一个序列里的张量拼接在一起，按维数拼接</span>torch<span class="token punctuation">.</span>clone<span class="token punctuation">(</span>input<span class="token punctuation">,</span><span class="token operator">*</span><span class="token punctuation">,</span> memory_format<span class="token operator">=</span>torch<span class="token punctuation">.</span>preserve_format<span class="token punctuation">)</span> → Tensor<span class="token comment" spellcheck="true"># return a copy of input</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>input<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensorloss<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#这是一个列表,和tensor不通用</span>b <span class="token operator">=</span> list<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#a维度是(4,4,4)  b维度是(4,)</span>b <span class="token operator">=</span> <span class="token punctuation">[</span>a<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#a维度是(4,4,4)  b维度是(1 ,) #上面的会把batch信息保留</span>torch<span class="token punctuation">.</span>numel<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>int<span class="token comment" spellcheck="true"># 返回input tensor中元素的总数,eg:</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>numel<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 120</span>torhch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>nelement<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>int<span class="token comment" spellcheck="true"># numel()的别名</span>torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> <span class="token punctuation">[</span>split<span class="token punctuation">,</span> split<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> t <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使变平</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>t<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>t<span class="token punctuation">,</span> start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Tensor<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将tensor中和mask为1的位置相对应的替换为value</span>mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>mask<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>a<span class="token punctuation">.</span>data<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>mask<span class="token punctuation">.</span>byte<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token operator">-</span>float<span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1053</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0352</span><span class="token punctuation">,</span>  <span class="token number">1.4759</span><span class="token punctuation">,</span>  <span class="token number">0.8849</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7233</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0529</span><span class="token punctuation">,</span>  <span class="token number">0.6663</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1082</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7243</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0364</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0657</span><span class="token punctuation">,</span>  <span class="token number">0.8359</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.4160</span><span class="token punctuation">,</span>  <span class="token number">1.1594</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.4163</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>input<span class="token punctuation">,</span> shifts<span class="token punctuation">,</span> dims<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensor<span class="token comment" spellcheck="true"># shifts:元素移位的维数,如果该参数是一个元组（例如shifts=(x,y)）</span><span class="token comment" spellcheck="true"># dims必须是一个相同大小的元组（例如dims=(a,b)），相当于在第a维度移x位，在b维度移y位</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>x<span class="token punctuation">,</span> shifts<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dims<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> steps<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>  <span class="token number">3.0000</span><span class="token punctuation">,</span>   <span class="token number">4.7500</span><span class="token punctuation">,</span>   <span class="token number">6.5000</span><span class="token punctuation">,</span>   <span class="token number">8.2500</span><span class="token punctuation">,</span>  <span class="token number">10.0000</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># (end - start)/(steps - 1)</span><span class="token comment" spellcheck="true"># 设置梯度计算为开或关的上下文管理器,可以用作上下文管理器或一个函数,这个上下文管理器是线程本地的,他不会影响其他线程。</span>torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span>mode<span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span>is_train<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>   y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> y<span class="token punctuation">.</span>requires_grad<span class="token boolean">False</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> y<span class="token punctuation">.</span>requires_grad<span class="token boolean">True</span><span class="token comment" spellcheck="true"># 沿给定的dim计算张量输入中非零值的数量。 如果没有指定dim，则计算张量中的所有非零值。 </span>torch<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>a<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>input<span class="token punctuation">,</span> min<span class="token operator">=</span>None<span class="token punctuation">,</span> max<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensor<span class="token comment" spellcheck="true"># 将input中的所有元素限制在[min,max]范围内,操作定义如下</span><span class="token comment" spellcheck="true">#      | min, if x_i &lt; min</span><span class="token comment" spellcheck="true">#y_i = | x_i, if min &lt;= x_i &lt;= max</span><span class="token comment" spellcheck="true">#      | max, if x_i &gt; max</span><span class="token comment" spellcheck="true"># 和np.where用法相同</span>torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>a<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">,</span>a<span class="token punctuation">,</span><span class="token number">5</span><span class="token operator">*</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># https://zhuanlan.zhihu.com/p/352877584</span>torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回排序后的值所对应原a的下标，即torch.sort()返回的indices</span>torch<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#可以理解为填充或修改:https://blog.csdn.net/weixin_45547563/article/details/105311543 </span>scatter_<span class="token punctuation">(</span>input<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> index<span class="token punctuation">,</span> src<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 网格函数</span>torch<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span><span class="token operator">*</span>tensors<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tensors: 两个一维向量，如果是0维，当作1维处理</span><span class="token comment" spellcheck="true"># 返回：两个矩阵</span><span class="token comment" spellcheck="true"># 第一个矩阵行相同，列是第一个向量的各个元素</span><span class="token comment" spellcheck="true"># 第二个矩阵列相同，行是第二个向量的各个元素</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>grid_x<span class="token punctuation">,</span> grid_y <span class="token operator">=</span> torch<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''grid_x:  tensor([[1, 1, 1, 1],        [2, 2, 2, 2],        [3, 3, 3, 3]])grid_y:  tensor([[4, 5, 6, 7],        [4, 5, 6, 7],        [4, 5, 6, 7]])'''</span><span class="token comment" spellcheck="true"># torch.stack 不会在现有的维度上加</span>A <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                  <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>B <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">33</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token punctuation">[</span><span class="token number">44</span><span class="token punctuation">,</span> <span class="token number">55</span><span class="token punctuation">,</span> <span class="token number">66</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                  <span class="token punctuation">[</span><span class="token number">77</span><span class="token punctuation">,</span> <span class="token number">88</span><span class="token punctuation">,</span><span class="token number">99</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>result1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">)</span><span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># torch.Size([2, 3, 3])</span><span class="token comment" spellcheck="true"># torch.chunk(tensor,chunk数,维度)</span>a<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="求导和网络相关知识"><a href="#求导和网络相关知识" class="headerlink" title="求导和网络相关知识"></a>求导和网络相关知识</h4><p>grad属性保存梯度值，grad_fn保存梯度函数</p><p>nn.functional函数的特点是不具有可学习的参数，<code>net.parameters()​</code>返回网络可学习的参数</p><p>forward函数的输入和输出都是Tensor ,在反向传播前，先要将所有参数的梯度清零,如果不清0，计算得到的梯度值会进行累加</p><p><strong>torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。</strong></p><h4 id="经典报错"><a href="#经典报错" class="headerlink" title="经典报错"></a>经典报错</h4><p><strong>一:int和torch</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">count_parameters</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    total_params <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> p <span class="token keyword">in</span> m<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        total_params <span class="token operator">+=</span> torch<span class="token punctuation">.</span>DoubleTensor<span class="token punctuation">(</span><span class="token punctuation">[</span>p<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"m.total_params"</span><span class="token punctuation">,</span> m<span class="token punctuation">.</span>total_params<span class="token punctuation">,</span> <span class="token string">"total_params"</span><span class="token punctuation">,</span> total_params<span class="token punctuation">)</span>    m<span class="token punctuation">.</span>total_params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> total_params</code></pre><p><code>m.total_params[0] += total_params</code>,如果<code>m.parameters</code>为空,那么<code>total_params</code>就是0,类型为int,<code>m.total_params[0]</code>的类型为<code>torch.Size([])</code>,相当于一个空tensor,要是<code>m.total_params=total_params</code>,就相当于把int赋值给tensor,触发TypeError</p><p><strong>二：RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchx <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>w1<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>w2<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>d <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w1<span class="token punctuation">)</span>f <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>d<span class="token punctuation">,</span> w2<span class="token punctuation">)</span>d<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment" spellcheck="true"># 因为这句, 代码报错了 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</span>f<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>因为f的求导需要用到d，但是这个时候d已经被改变了，所以报错</p><p>这种报错会显示在这一行<code>batch_loss.backward()</code>,多用<code>b=...a...</code>之类的形式，少用<code>a=...a...</code>之类的形式</p><p>似乎对于tensor变量的整体运算覆盖原变量，比如<code>x=2*x</code>不会导致以上问题；但是逐元素操作覆盖原位置元素就会引起这个问题，举例也是这个情况，在循环中，每次访问<code>feature[i,:,:]</code>，如果写成<code>feature[i,:,:]=...feature[i,:,:]...</code>，这设计的是python的变量赋值规则了</p><p><strong>三：view size is not compatible with input tensor‘s size and stride</strong></p><p>tensor不是contiguous连续引起的错误,查看<code>targets.is_contiguous()</code>为False</p><p>两种解决办法：1）按照提示使用reshape代替；2）将变量先转为contiguous ，再进行view:</p><pre class=" language-python"><code class="language-python">targets<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>targets<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token operator">*</span>targets<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p><a href="https://cloud.tencent.com/developer/article/1774900">如何判断张量是否连续?</a></p><p>nD 张量底层实现是使用一块连续内存的一维数组，由于 PyTorch 底层实现是 C 语言 (C/C++ 使用行优先的存储方式)，所以 PyTorch 中的 nD 张量也按照行优先的顺序进行存储的。</p><p>下图为一个形状为$(2×3)$的2D张量，为了方便将其命名为$A$。</p><p><img src="/pytorch-xue-xi/image-20210519165512549.png" alt="A"></p><p>张量 $A$ 在内存中实际以一维数组的形式进行存储，并且使用<strong>行优先的顺序进行存储</strong>,其中一维数组的形式存储比较好理解,而行优先指的就是存储顺序按照张量$A$的行依次存储。 张量$A$在内存中的实际存储形式如下所示。</p><p><img src="/pytorch-xue-xi/image-20210519165623798.png" alt="A"></p><p>张量$A$通常称为存储的逻辑结构，而实际存储的一维数组形式称为存储的物理结构。</p><ul><li>如果元素在存储的逻辑结构上相邻，在存储的物理结构中也相邻，则称为连续存储的张量；</li><li>如果元素在存储的逻辑结构上相邻，但是在存储的物理结构中不相邻，则称为不连续存储的张量；</li></ul><p><strong>交换维度的操作能够将连续存储的张量转变成不连续存储的张量。</strong></p><p>nD 张量，对于任意一个维度$i(i=0,…,n−1,i≠n−1)$都满足下面的等式则说明 nD 张量连续，不满足则说明 nD 张量不连续。<br>$$<br>stride[i]=stride[i+1]\times size[i+1]<br>$$<br>$stride[i]$表示逻辑结构中第 $i$个维度上相邻的元素在物理结构中间隔的元素个数.</p><p>$size[i]$表示逻辑结构中第$i$个维度的元素个数。</p><p>对于$A$,$stride[0]=3,stride[1]=1,size[1]=3$,所以是连续的</p><p>假设将$A$转置得到$A^T$,如下图：</p><img src="/pytorch-xue-xi/image-20210519171403494.png" alt="image-20210519171403494" style="zoom:67%;"><p><strong>在 PyTorch 中交换维度的操作并没有改变其实际的存储，换句话说，交换维度后的张量与原始张量共享同一块内存</strong>，因此交换维度后的张量 AT 底层存储和原始张量 A 都是相同的一维数组。</p><p>对于$A^T$,$stride[0]=1,stride[1]=3,size[1]=2$,不连续</p><p><code>view</code>只能用于数据连续存储的张量，而<code>reshape</code>则不需要考虑张量中的数据是否连续存储</p><p>原始张量的视图简单来说就是和原始张量共享数据，因此如果改变使用 view 方法返回的新张量，原始张量也会发生相对应的改变。</p><p>reshape 方法可能返回的是原始张量的视图或者拷贝，当处理连续存储的张量 reshape 返回的是原始张量的视图，而当处理不连续存储的张量 reshape 返回的是原始张量的拷贝</p><p><strong>四:Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same</strong></p><p>整理数据集的时候,使用了np.array,然后默认保存成float64,但是pytorch中默认是float32。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 解决办法,dtype="float32"</span>image_n <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>image_n<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">255.0</span>image_d <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>image_d<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">255.0</span></code></pre><p><strong>五:RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</strong></p><p>比如整体网络是G+S,有一个G_loss,有一个S_loss,必须<code>loss=G_loss+S_loss,loss.backward()</code>,</p><p>而不能<code>G_loss.backward(),S_loss.backward()</code>,因为为了节省空间,<code>G_loss.backward()</code>时已经把G的中间结果删除了</p><blockquote><p>To reduce memory usage, during the <code>.backward()</code> call, all the intermediary results are deleted when they are not needed anymore. Hence if you try to call <code>.backward()</code> again, the intermediary results don’t exist and the backward pass cannot be performed (and you get the error you see).You can call <code>.backward(retain_graph=True)</code> to make a backward pass that will not delete intermediary results, and so you will be able to call <code>.backward()</code> again. All but the last call to backward should have the <code>retain_graph=True</code> option.</p></blockquote><p><strong>六:TypeError: can’t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.</strong></p><p>所有在CPU上的<code>Tensor</code>（除了<code>CharTensor</code>）都支持与NumPy数组相互转换。</p><p>此外上面提到还有一个常用的方法就是直接用<code>torch.tensor()</code>将NumPy数组转换成<code>Tensor</code>，需要注意的是该方法总是会进行数据拷贝，返回的<code>Tensor</code>和原来的数据不再共享内存。</p><p><strong>RuntimeError: Can’t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.</strong></p><p>requires_grad = True的也不能转换成numpy</p><p><strong>七:错误归一化</strong></p><p>対生成的图片,错误的使用了归一化</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 比如输出图片,只要确保图片在0-1之间就好</span>temp1<span class="token operator">=</span>np<span class="token punctuation">.</span>clip<span class="token punctuation">(</span>temp1<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>temp2<span class="token operator">=</span>np<span class="token punctuation">.</span>clip<span class="token punctuation">(</span>temp2<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 如果进行归一化,如下图</span>temp1<span class="token operator">=</span>temp1<span class="token operator">/</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>temp1<span class="token punctuation">)</span><span class="token operator">-</span>np<span class="token punctuation">.</span>min<span class="token punctuation">(</span>temp1<span class="token punctuation">)</span><span class="token punctuation">)</span>temp2<span class="token operator">=</span>temp2<span class="token operator">/</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>temp2<span class="token punctuation">)</span><span class="token operator">-</span>np<span class="token punctuation">.</span>min<span class="token punctuation">(</span>temp2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 相当于对图片进行了放缩</span></code></pre><p>想象一下 本来图片的像素是0.5 1 放缩完之后就成了1 2了 怪不得那么亮~</p><h4 id="经典示例"><a href="#经典示例" class="headerlink" title="经典示例"></a>经典示例</h4><p><strong>1.resnet.conv1</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">if</span> layers <span class="token operator">==</span> <span class="token number">50</span><span class="token punctuation">:</span>    resnet <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet50<span class="token punctuation">(</span>pretrained<span class="token operator">=</span>pretrained<span class="token punctuation">)</span><span class="token keyword">elif</span> layers <span class="token operator">==</span> <span class="token number">101</span><span class="token punctuation">:</span>    resnet <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet101<span class="token punctuation">(</span>pretrained<span class="token operator">=</span>pretrained<span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>    resnet <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet152<span class="token punctuation">(</span>pretrained<span class="token operator">=</span>pretrained<span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>resnet<span class="token punctuation">.</span>conv1<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>bn1<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>relu<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>conv2<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>bn2<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>relu<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>conv3<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>bn3<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>relu<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>maxpool<span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer4 <span class="token operator">=</span> resnet<span class="token punctuation">.</span>layer1<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>layer2<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>layer3<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>layer4</code></pre><p>resnet是你定义的一个类,那么类中的属性如<code>resnet.conv1</code>自然也可以调用啊。</p><h3 id="使用GPU"><a href="#使用GPU" class="headerlink" title="使用GPU"></a>使用GPU</h3><p><a href="https://blog.csdn.net/u014380165/article/details/77340765">CUDA\cuDNN是什么</a></p><p>CUDA:NVIDIA推出的用于自家GPU的<strong>并行计算</strong>框架，也就是说CUDA只能在NVIDIA的GPU上运行，<strong>而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。</strong></p><p><strong>在 CUDA 的架构下，一个程序分为两个部份：host 端和 device 端。Host 端是指在 CPU 上执行的部份，而 device 端则是在显示芯片上执行的部份。Device 端的程序又称为 “kernel”。通常 host 端程序会将数据准备好后，复制到显卡的内存中，再由显示芯片执行 device 端程序，完成后再由 host 端程序将结果从显卡的内存中取回。</strong></p><p>cuDNN:是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。</p><pre class=" language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这一步是设置我们使用的GPU</span><span class="token comment" spellcheck="true"># 确认我们的电脑支持CUDA，然后显示CUDA信息：</span><span class="token keyword">print</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#然后这些方法将递归遍历所有模块并将模块的参数和缓冲区 转换成CUDA张量：</span>net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#记住：inputs, targets 和 images 也要转换。</span>inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></code></pre><h4 id="多GPU"><a href="#多GPU" class="headerlink" title="多GPU"></a>多GPU</h4><p><a href="https://zhuanlan.zhihu.com/p/86441879">教程</a></p><h3 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h3><p><strong>ResNet的调用</strong></p><p><img src="/pytorch-xue-xi/image-20210709100522321.png" alt="resnet预训练模型下载"></p><pre class=" language-python"><code class="language-python"><span class="token keyword">if</span> pretrained<span class="token punctuation">:</span><span class="token comment" spellcheck="true"># model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))</span>    model_path <span class="token operator">=</span> <span class="token string">'./initmodel/resnet152_v2.pth'</span>    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_path<span class="token punctuation">)</span><span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p><code>torch.save</code></p><p>将对象从内存中保存到磁盘文件中。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>obj<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#被保存的对象</span>           f<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>str<span class="token punctuation">,</span> os<span class="token punctuation">.</span>PathLike<span class="token punctuation">,</span> BinaryIO<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#</span>           pickle_module<span class="token operator">=</span><span class="token operator">&lt;</span>module <span class="token string">'pickle'</span> <span class="token keyword">from</span> <span class="token string">'/opt/conda/lib/python3.6/pickle.py'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>           pickle_protocol<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> _use_new_zipfile_serialization<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> → None<span class="token comment" spellcheck="true">#常见的PyTorch约定是使用.pt文件扩展名保存张量。</span><span class="token comment" spellcheck="true"># Save to file</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span></code></pre><p><code>torch.load</code></p><p>加载用<code>torch.save()</code>保存的磁盘文件到内存中。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#文件路径</span>           map_location<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#加载到的位置</span>           pickle_module<span class="token operator">=</span><span class="token operator">&lt;</span>module <span class="token string">'pickle'</span> <span class="token keyword">from</span> <span class="token string">'/opt/conda/lib/python3.6/pickle.py'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>           <span class="token operator">**</span>pickle_load_args<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#eg:torch.load(args.load, map_location=device)</span></code></pre><p>保存和加载,<code>torch.save()</code>将参数由内存或显存保存到硬盘,<code>torch.load()</code>再由硬盘加载到内存或显存,</p><p>然后再调用<code>model.load_state_dict()</code>将参数加载到模型中</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 只保存模型参数</span><span class="token comment" spellcheck="true"># 保存</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'\parameter.pkl'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 加载</span>model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'\parameter.pkl'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 保存完整模型</span><span class="token comment" spellcheck="true"># 保存</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'\model.pkl'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 加载</span>model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'\model.pkl'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 但是这样直接加载有时候会出问题，比如分割,本来的模型是2类输出,但是假设我们现在要新训练一个模型,同时利用之前模型的预训练信息，但是这个时候最后的那个prediction部分模型大小就不匹配了</span></code></pre><img src="/pytorch-xue-xi/image-20211009203504314.png" alt="image-20211009203504314" style="zoom:80%;"><p><a href="https://zhuanlan.zhihu.com/p/133250753">Pytorch断点续接</a></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 模型参数的加载 优化器参数的加载 epoch的恢复</span>checkpoint <span class="token operator">=</span> <span class="token punctuation">{</span>        <span class="token string">"net"</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token string">'optimizer'</span><span class="token punctuation">:</span>optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token string">"epoch"</span><span class="token punctuation">:</span> epoch    <span class="token punctuation">}</span>    <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span><span class="token string">"./models/checkpoint"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span><span class="token string">"./models/checkpoint"</span><span class="token punctuation">)</span>    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> <span class="token string">'./models/checkpoint/ckpt_best_%s.pth'</span> <span class="token operator">%</span><span class="token punctuation">(</span>str<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">if</span> RESUME<span class="token punctuation">:</span>    path_checkpoint <span class="token operator">=</span> <span class="token string">"./models/checkpoint/ckpt_best_1.pth"</span>  <span class="token comment" spellcheck="true"># 断点路径</span>    checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>path_checkpoint<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 加载断点</span>    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'net'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 加载模型可学习参数</span>    optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 加载优化器参数</span>    start_epoch <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 设置开始的epoch</span></code></pre><p>学习率的调节会用到epoch</p><h3 id="torch-Tensor"><a href="#torch-Tensor" class="headerlink" title="torch.Tensor"></a>torch.Tensor</h3><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># tensor类型转换 如long int 转换成torch.float 直接利用type函数</span>mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>mask<span class="token operator">&lt;</span>threshold<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># .cuda non_blocking经常与DataLoader的pin_memory搭配使用</span>src_input <span class="token operator">=</span> src_input<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 1. x = x.cuda(non_blocking=True)</span><span class="token comment" spellcheck="true"># 2. 进行一些和x无关的操作</span><span class="token comment" spellcheck="true"># 3. 执行和x有关的操作</span><span class="token comment" spellcheck="true"># 在non_blocking=true下，1不会阻塞2，1和2并行。这样将数据从CPU移动到GPU的时候，它是异步的。在它传输的时候，CPU还可以干其他的事情（不依赖于数据的事情）</span></code></pre><h3 id="Tensor-Attributes"><a href="#Tensor-Attributes" class="headerlink" title="Tensor Attributes"></a>Tensor Attributes</h3><p>Each <code>torch.Tensor</code> has a <code>torch.dtype</code> <code>torch.device</code>, and <code>torch.layout</code>.</p><p><strong><code>torch.dtype</code></strong></p><p><code>os.environ['CUDA_VISIBLE_DEVICES'] = '0'</code></p><p><code>CUDA_VISIBLE_DEVICES</code> is the mask used by CUDA to determine what devices it exposes to the user program, which is pytorch in this case. There is no way pytorch can know about that reliably.</p><p>Within you application, gpu numbers will always start at 0 and grow up from there.<br>When you use <code>CUDA_VISIBLE_DEVICES</code>, you hide some devices so they won’t be numbered.<br>If you have 4 gpus: 0, 1, 2, 3.<br>And run CUDA_VISIBLE_DEVICES=1,2 python some_code.py. Then the device that you will see within python are device 0, 1. Using device 0 in your code will use device 1 from global numering. Using device 1 in your code will use 2 outside.<br>So in your case if you always set <code>CUDA_VISIBLE_DEVICES</code> to a single device, in your code, the device id will always be 0, that is expected. Unfortunately, there is no way to know what is the global numbering.</p><p>A <code>torch.dtype</code>is an object that represents the data type of a <code>torch.Tensor</code>.Pytorch has different 12 types.</p><p><strong><code>torch.device</code></strong></p><p>A <code>torch.device</code> is an object representing the device on which a <code>torch.Tensor</code> is or will be allocated.</p><p>The <code>torch.device</code> contains a device type (<code>'cpu'</code> or <code>'cuda'</code>) and optional device ordinal for the device type. If the device ordinal is not present, this object will always represent the current device for the device type, even after <code>torch.cuda.set_device()</code> is called; e.g., a <code>torch.Tensor</code> constructed with device <code>'cuda'</code> is equivalent to <code>'cuda:X'</code> where X is the result of <code>torch.cuda.current_device()</code>.</p><p>A <code>torch.Tensor</code>’s device can be accessed via the <code>Tensor.device</code> property.</p><p>A <code>torch.device</code> can be constructed via a string or via a string and device ordinal</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># via a string </span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>device<span class="token punctuation">(</span>type<span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>device<span class="token punctuation">(</span>type<span class="token operator">=</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># current cuda device</span>device<span class="token punctuation">(</span>type<span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># via a string device ordinal</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>device<span class="token punctuation">(</span>type<span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>device<span class="token punctuation">(</span>type<span class="token operator">=</span><span class="token string">'cpu'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># torch.cuda.set_device(device)</span><span class="token comment" spellcheck="true"># Sets the current device. 不鼓励使用此函数。在大多数情况下，最好使用 CUDA_VISIBLE_DEVICES 环境变量。</span></code></pre><p>The <code>torch.device</code> argument in functions can generally be substituted with a string. This allows for fast prototyping of code.</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># Example of a function that takes in a torch.device</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> cuda1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>cuda1<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># You can substitute the torch.device with a string</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span></code></pre><p>For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches <code>Tensor.get_device()</code>, which returns an ordinal for cuda tensors and is not supported for cpu tensors.</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>device<span class="token punctuation">(</span>type<span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p>Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># legacy</span></code></pre><p><strong><code>torch.layout</code></strong></p><p>in beta</p><h3 id="torch-autograd"><a href="#torch-autograd" class="headerlink" title="torch.autograd"></a>torch.autograd</h3><p>torch.autograd提供了实现任意标量值函数的自动微分的类和函数。它只需要对现有代码进行最小的更改——只需要声明张量s，对于这些张量，计算梯度时应带有requires_grad=True关键字。到目前为止，我们只支持浮点张量类型(half、float、double和bfloat16)和复数张量类型(cfloat、cdouble)的自动求导。</p><h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p>Variable API已被弃用:在使用autograd时，不再需要Variable。Autograd自动支持将requires_grad设置为True的张量。下面是一些变化的快速指南:</p><p><code>Variable(tensor)</code>和<code>Variable(tensor, requires_grad)</code>仍然按预期工作，但它们返回的是张量而不是变量。</p><p><code>var.data</code>和<code>tensor.data</code>是一样的。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tensor([[-0.4404]], requires_grad=True)</span><span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tensor([[-0.4404]])</span></code></pre><p>像<code>var.backward()</code>、<code>var.detach()</code>、<code>var.register_hook()</code>这样的方法现在可以在具有相同方法名的张量上工作。</p><p>此外,现在可以使用工厂方法创建requires_grad=True的张量,如<code>torch.randn()</code>、<code>torch.zeros()</code>、<code>torch.ones()</code>和其他类似如下的方法:</p><p><code>autograd_tensor =torch.randn((2, 3, 4),requires_grad=True)</code></p><p>具体来说，在pytorch中的Variable就是一个存放会变化值的地理位置，里面的值会不停发生变化，就像一个装鸡蛋的篮子，鸡蛋数会不断发生变化。那谁是里面的鸡蛋呢，自然就是pytorch中的tensor了。（也就是说，<strong>pytorch都是有tensor计算的，而tensor里面的参数都是Variable的形式</strong>）。如果用Variable计算的话，那返回的也是一个同类型的Variable。</p><p>也就是说现在requires_grad=True的tensor就相当于以前的Variable,也就是进行反向传播的变量。</p><p><code>detach()  data()  detach_()</code></p><p><a href="https://blog.csdn.net/weixin_33913332/article/details/93300411">教程</a></p><p>返回一个新的<code>Variable</code>，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个<code>Variable</code>永远不需要计算其梯度，不具有grad。**即使之后重新将它的requires_grad置为true,它也不会具有梯度grad.**这样我们就会继续使用这个新的<code>Variable</code>进行计算，后面当我们进行反向传播时，到该调用detach()的<code>Variable</code>就会停止，不能再继续向前进行传播</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># detach与data的区别</span><span class="token comment" spellcheck="true"># 相同 1.都和x共享同一块数据 2.都和x的计算历史无关 3.requires_grad = False</span><span class="token comment" spellcheck="true"># 不同</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>out <span class="token operator">=</span> a<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>b <span class="token operator">=</span> out<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>c <span class="token operator">=</span> out<span class="token punctuation">.</span>data<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 修改b、c均会同时改变a</span><span class="token comment" spellcheck="true"># 不修改b,out可以反向传播；修改b,out反向传播会报错</span><span class="token comment" spellcheck="true"># 修改c,out可以反向传播,但会生成错误的梯度值。</span><span class="token comment" spellcheck="true"># 所以out.data()在某些情况下不安全</span></code></pre><p>上面内容实现的原理是:In-place 正确性检查(In-place操作是指函数最后带”_”的操作)</p><p>所有的<code>Variable</code>都会记录用在他们身上的 <code>in-place operations</code>。如果<code>pytorch</code>检测到<code>variable</code>在一个<code>Function</code>中已经被保存用来<code>backward</code>，但是之后它又被<code>in-place operations</code>修改。当这种情况发生时，在<code>backward</code>的时候，<code>pytorch</code>就会报错。这种机制保证了，如果你用了<code>in-place operations</code>，但是在<code>backward</code>过程中没有报错，那么梯度的计算就是正确的。</p><p><code>pred_fake = self.netD(fake_AB.detach()) </code># 固定了G,detach隔断了返现传播流</p><p><code>detach()</code>会截断反向传播</p><h4 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h4><p><a href="https://zhuanlan.zhihu.com/p/27783097">知乎教程</a> <a href="https://blog.csdn.net/tsq292978891/article/details/79364140">CSDN教程</a></p><p><strong>对Function的直观理解</strong></p><ul><li>的</li></ul><p>虽然pytorch可以自动求导，但是有时候一些操作是不可导的，这时候你需要自定义求导方式。也就是所谓的 <code>Extending torch.autograd</code></p><p><strong>Function与Module的差异与应用场景</strong></p><ul><li><strong>Function一般只定义一个操作，因为其无法保存参数，因此适用于激活函数、pooling</strong>等操作;<strong>Module是保存了参数，因此适合于定义一层，如线性层，卷积层，也适用于定义一个网络</strong></li><li>Function需要定义三个方法：<code>__init__</code>, forward, backward(需要自己写求导公式);Module：只需定义<code>__init__</code>和forward，而backward的计算由自动求导机制构成</li><li>可以不严谨的认为，Module是由一系列Function组成，因此其在forward的过程中，Function和Variable组成了计算图，在backward时，只需调用Function的backward就得到结果，因此Module不需要再定义backward。</li><li>Module不仅包括了Function，<strong>还包括了对应的参数，以及其他函数与变量，这是Function所不具备的</strong></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#属性（成员变量）</span><span class="token comment" spellcheck="true">#saved_tensors: 传给forward()的参数，在backward()中会用到。</span><span class="token comment" spellcheck="true">#needs_input_grad:长度为 :attr:num_inputs的bool元组，表示输出是否需要梯度。可以用于优化反向过程的缓存。</span><span class="token comment" spellcheck="true">#num_inputs: 传给函数 :func:forward的参数的数量。</span><span class="token comment" spellcheck="true">#num_outputs: 函数 :func:forward返回的值的数目。</span><span class="token comment" spellcheck="true">#requires_grad: 布尔值，表示函数 :func:backward 是否永远不会被调用。</span><span class="token comment" spellcheck="true">#成员函数</span><span class="token comment" spellcheck="true">#forward()</span><span class="token comment" spellcheck="true">#forward()可以有任意多个输入、任意多个输出，但是输入和输出必须是Variable。(官方给的例子中有只传入tensor作为参数的例子)</span><span class="token comment" spellcheck="true">#backward()</span><span class="token comment" spellcheck="true">#backward()的输入和输出的个数就是forward()函数的输出和输入的个数。其中，backward()输入表示关于forward()输出的梯度(计算图中上一节点的梯度)，#backward()的输出表示关于forward()的输入的梯度。在输入不需要梯度时（通过查看needs_input_grad参数）或者不可导时，可以返回None。</span></code></pre><h4 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h4><p>上下文管理器，为autograd引擎启用异常检测。做了两件事:</p><p>Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.。任何生成” nan “值的向后计算都将引发错误。</p><p>此模式应仅用于调试，因为不同的测试将降低程序执行速度。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>detect_anomaly<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">with</span> autograd<span class="token punctuation">.</span>detect_anomaly<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     inp <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     out <span class="token operator">=</span> run_fn<span class="token punctuation">(</span>inp<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>set_detect_anomaly<span class="token punctuation">(</span>mode<span class="token punctuation">:</span><span class="token boolean">True</span> <span class="token operator">or</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 上下文管理器，设置自动grad引擎的异常检测开关。</span><span class="token comment" spellcheck="true"># Set_detect_anomaly将根据参数模式启用或禁用自grad异常检测。它可以用作上下文管理器或函数。</span></code></pre><p>在模型正常训练阶段不建议打开**<code>autograd.detect_anomaly，</code>**会使训练速度大大减慢，以笔者 这里的测试，打开后，原本4个小时的训练被减慢至7.5个小时；打开后可以辅助找到出现Nan值的位置</p><p><code>assert torch.isnan(src_n_feat).int().sum() ==0</code> 判断张量中是否有值为nan</p><h3 id="torch-cuda"><a href="#torch-cuda" class="headerlink" title="torch.cuda"></a>torch.cuda</h3><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> → int <span class="token comment" spellcheck="true"># 返回可用的GPU数量</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_device_name<span class="token punctuation">(</span><span class="token punctuation">)</span> → str <span class="token comment" spellcheck="true"># 返回GPU的名字,如NVIDIA GeForce RTX 3080 Ti </span></code></pre><h4 id="memory-managetment"><a href="#memory-managetment" class="headerlink" title="memory managetment"></a>memory managetment</h4><p><a href="https://zhuanlan.zhihu.com/p/424512257">pytorch显存机制分析</a></p><p><code>torch.cuda.memory_reserved(device=None)</code></p><h3 id="torch-cuda-amp"><a href="#torch-cuda-amp" class="headerlink" title="torch.cuda.amp"></a>torch.cuda.amp</h3><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><p><a href="https://blog.csdn.net/l7H9JA4/article/details/114324414">Pytorch自动混合精度教程</a></p><p>Automatic mixed precision package自动混合精度包</p><p>torch.cuda.amp提供了方便的混合精度方法，在某些操作中需要使用torch.float32 (float)数据类型而有些操作使用torch.float16(half)。<br>有些操作，比如线性层和卷积，在float16中要快得多。<br>其他操作，比如减少操作，通常需要float32的动态范围。<br>混合精度尝试将每个op匹配到其适当的数据类型。</p><p>一般来说，自动混合精度训练同时使用<code>torch.cuda.amp.autocast</code>和<code>torch.cuda.amp.GradScaler</code>,当然如果需要也可以单独使用。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>amp <span class="token keyword">import</span> autocast<span class="token punctuation">,</span> GradScaler<span class="token comment" spellcheck="true"># 用户使用混合精度训练基本操作：</span><span class="token comment" spellcheck="true"># amp依赖Tensor core架构，所以model参数必须是cuda tensor类型</span>model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># GradScaler对象用来自动做梯度缩放</span>scaler <span class="token operator">=</span> GradScaler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> epochs<span class="token punctuation">:</span>    <span class="token keyword">for</span> input<span class="token punctuation">,</span> target <span class="token keyword">in</span> data<span class="token punctuation">:</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 在autocast enable 区域运行forward</span>        <span class="token keyword">with</span> autocast<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># model做一个FP16的副本，forward</span>            output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>            loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 用scaler，scale loss(FP16)，backward得到scaled的梯度(FP16)</span>        scaler<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># scaler 更新参数，会先自动unscale梯度</span>        <span class="token comment" spellcheck="true"># 如果有nan或inf，自动跳过</span>        scaler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># scaler factor更新</span>        scaler<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h4 id="autocast自定义函数"><a href="#autocast自定义函数" class="headerlink" title="autocast自定义函数"></a>autocast自定义函数</h4><p>对于用户自定义的autograd函数，需要用<code>@torch.cuda.amp.custom_fwd</code>装饰forward函数,<code>@torch.cuda.amp.custom_bwd</code>装饰backward函数:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyMM</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>    @staticmethod    @custom_fwd    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>        ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        <span class="token keyword">return</span> a<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>b<span class="token punctuation">)</span>    @staticmethod    @custom_bwd    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">:</span>        a<span class="token punctuation">,</span> b <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors        <span class="token keyword">return</span> grad<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>b<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>grad<span class="token punctuation">)</span></code></pre><p>调用时再autocat</p><pre class=" language-python"><code class="language-python">mymm <span class="token operator">=</span> MyMM<span class="token punctuation">.</span>apply<span class="token keyword">with</span> autocast<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    output <span class="token operator">=</span> mymm<span class="token punctuation">(</span>input1<span class="token punctuation">,</span> input2<span class="token punctuation">)</span></code></pre><h3 id="torch-backends"><a href="#torch-backends" class="headerlink" title="torch.backends"></a>torch.backends</h3><p>torch.backends控制PyTorch支持的各种后端的行为。这些后端包括:</p><ul><li><code>torch.backends.cuda</code></li><li><code>torch.backends.cudnn</code></li><li><code>torch.backends.mkl</code></li><li><code>torch.backends.mkldnn</code></li><li><code>torch.backends.openmp</code></li></ul><h4 id="torch-cudnn"><a href="#torch-cudnn" class="headerlink" title="torch.cudnn"></a>torch.cudnn</h4><p><a href="https://blog.csdn.net/byron123456sfsfsfa/article/details/96003317">torch.backends.cudnn.benchmark详解</a></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>benchmark<span class="token comment" spellcheck="true">#一个bool值，如果为真，将导致cuDNN对多个卷积算法进行基准测试并选择最快的。</span><span class="token comment" spellcheck="true">#耗费一些预处理时间，选择最好的卷积算法，大大减少之后的训练时间，网络结构不能变，输入输出不能变等</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic<span class="token comment" spellcheck="true">#如果该bool为真，则导致cuDNN只使用确定性卷积算法。参见torch.is_deterministic()和torch.set_deterministic()。</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>enabled<span class="token comment" spellcheck="true">#一个控制是否启用cuDNN的bool值,默认为True，启用cudnn</span></code></pre><h3 id="torch-distributed"><a href="#torch-distributed" class="headerlink" title="torch.distributed"></a>torch.distributed</h3><p><a href="https://zhuanlan.zhihu.com/p/178402798">DDP系列第一篇：入门教程</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist</code></pre><h4 id="DDP与DP模式的不同"><a href="#DDP与DP模式的不同" class="headerlink" title="DDP与DP模式的不同"></a>DDP与DP模式的不同</h4><p>DP模式是很早就出现的、单机多卡的、参数服务器架构的多卡训练模式，在PyTorch，即是:</p><p><code>model=torch.nn.DataParaller(model)</code></p><p>在DP模式中，<strong>总共只有一个进程</strong>(受到GIL很强限制)。<strong>master节点相当于参数服务器</strong>，<strong>其会向其他卡广播其参数</strong>；在<strong>梯度反向传播</strong>后，<strong>各卡将梯度集中到master节点，master节点对搜集来的参数进行平均后更新参数</strong>，<strong>再将参数统一发送到其他卡上</strong>。这种参数更新方式，会导致<strong>master节点的计算任务、通讯量很重</strong>，从而导致网络阻塞，<strong>降低训练速度</strong>。</p><p>但是<strong>DP也有优点</strong>,<strong>优点就是代码实现简单。要速度还是要方便，看官可以自行选用哟</strong>。</p><h4 id="DDP"><a href="#DDP" class="headerlink" title="DDP"></a>DDP</h4><p><a href="https://zhuanlan.zhihu.com/p/450912044">教程1</a></p><p><a href="https://blog.csdn.net/u012605037/article/details/115294898?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-115294898-blog-119606518.pc_relevant_multi_platform_whitelistv2_exp3w&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-115294898-blog-119606518.pc_relevant_multi_platform_whitelistv2_exp3w&amp;utm_relevant_index=1">基本概念</a></p><p><a href="https://zhuanlan.zhihu.com/p/178402798">教程2</a></p><p>ddp运行代码:</p><p><code>python3 -m torch.distributed.launch --nproc_per_node=8 DDP.py</code></p><p>其中<code>python3 -m</code>的意思是指run library module as a script（将模块当作脚本运行）</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 当文件作为脚本直接运行时，这段代码会产生副作用，输出字符串“模块直接运行”；</span><span class="token comment" spellcheck="true"># 当文件作为模块被导入时，不会产生副作用，不输出字符串“模块直接运行”；</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'模块直接运行'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true"># 当我们知道一个模块的名字，但不知道它的路径时，我们可以通过 -m 参数，在 shell 中将该模块当作脚本运行，例如：</span>python <span class="token operator">-</span>m module_name<span class="token comment" spellcheck="true"># 如果我们知道模块的完整路径（此处假设为"/path/to/module.py"），上述命令的效果，以下面的命令等同</span>python <span class="token operator">/</span>path<span class="token operator">/</span>to<span class="token operator">/</span>module<span class="token punctuation">.</span>py</code></pre><p>rank：用于表示进程的编号/序号（在一些结构图中rank指的是软节点，rank可以看成一个计算单位），每一个进程对应了一个rank的进程，整个分布式由许多rank完成</p><p>node：物理节点，可以是一台机器也可以是一个容器，节点内部可以有多个GPU。</p><p>rank与local_rank： rank是指在整个分布式任务中进程的序号；local_rank是指在一个node上进程的相对序号，local_rank在node之间相互独立。</p><p>nnodes、node_rank与nproc_per_node： nnodes是指物理节点数量，node_rank是物理节点的序号；nproc_per_node是指每个物理节点上面进程的数量。word size ： 全局（一个分布式任务）中，rank的数量。</p><blockquote><p>上一个运算题： 每个node包含16个GPU，且nproc_per_node=8，nnodes=3，机器的node_rank=5，请问word_size是多少？<br>答案：word_size = 3*8 = 24 </p></blockquote><p>比如分布式中有三台机器，每台机器起4个进程，每个进程占用1个GPU，如下图所示：</p><img src="/pytorch-xue-xi/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20220725162527263.png" alt="image-20220725162527263" style="zoom: 80%;"><p>Group：进程组，一个分布式任务对应了一个进程组。只有用户需要创立多个进程组时才会用到group来管理，默认情况下只有一个group。</p><h5 id="Groups"><a href="#Groups" class="headerlink" title="Groups"></a>Groups</h5><p>By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. new_group() function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a group argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).</p><pre class=" language-python"><code class="language-python">pg1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>new_group<span class="token punctuation">(</span>range<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>batch_size <span class="token operator">=</span> int<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>BATCH_SIZE <span class="token operator">/</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>feature_extractor <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>    feature_extractor<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>local_rank<span class="token punctuation">,</span>    find_unused_parameters<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> process_group<span class="token operator">=</span>pg1<span class="token punctuation">)</span>pg2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>new_group<span class="token punctuation">(</span>range<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>classifier <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>    classifier<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>local_rank<span class="token punctuation">,</span>    find_unused_parameters<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> process_group<span class="token operator">=</span>pg2<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 开启求导的异常侦测</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>set_detect_anomaly<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 保持两个进程同步</span>torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>barrier<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h5 id="Collective-functions"><a href="#Collective-functions" class="headerlink" title="Collective functions"></a>Collective functions</h5><p>``torch.distributed.barrier(group=None, async_op=False, device_ids=None)`</p><p>同步所有进程。如果async_op为False，或者在wait()上调用async工作句柄，则该集合将阻塞进程，直到整个组进入此函数。</p><h5 id="Launch-utility"><a href="#Launch-utility" class="headerlink" title="Launch utility"></a>Launch utility</h5><p><code>torch.distributed.launch</code>是一个在每个训练节点上生成多个分布式训练过程的模块。</p><p>该实用程序可用于单节点分布式训练，其中每个节点将生成一个或多个进程。该实用程序可以用于CPU训练或GPU训练。如果该实用程序用于GPU培训，则每个分布式进程将在单个GPU上运行。这可以实现明显提升单节点训练性能。它还可以用于多节点分布式训练，通过在每个节点上生成多个进程，也可以很好地提高多节点分布式训练的性能。这对于具有多个直接gpu支持的Infiniband接口的系统尤其有利，因为所有这些接口都可以用于聚合的通信带宽。</p><p>在单节点分布式训练或多节点分布式训练的两种情况下，该实用程序将启动每个节点给定数量的进程(<code>--nproc_per_node</code>)。如果用于GPU培训，这个数字需要小于或等于当前系统上的GPU数量(<code>nproc_per_node</code>)，并且每个进程将运行在从GPU 0到GPU (<code>nproc_per_node - 1</code>)的单个GPU上。</p><p><strong>How to use this module:</strong></p><p>1.Single-Node multi-process distributed training</p><pre class=" language-python"><code class="language-python">python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span>NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT<span class="token punctuation">.</span>py <span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span>arg1 <span class="token operator">-</span><span class="token operator">-</span>arg2 <span class="token operator">-</span><span class="token operator">-</span>arg3 <span class="token operator">and</span> all other arguments of your training script<span class="token punctuation">)</span></code></pre><p>2.Multi-Node multi-process distributed training: (e.g. two nodes)</p><pre class=" language-python"><code class="language-python">Node <span class="token number">1</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>IP<span class="token punctuation">:</span> <span class="token number">192.168</span><span class="token punctuation">.</span><span class="token number">1.1</span><span class="token punctuation">,</span> <span class="token operator">and</span> has a free port<span class="token punctuation">:</span> <span class="token number">1234</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span>NUM_GPUS_YOU_HAVE           <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">"192.168.1.1"</span>           <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token number">1234</span> YOUR_TRAINING_SCRIPT<span class="token punctuation">.</span>py <span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span>arg1 <span class="token operator">-</span><span class="token operator">-</span>arg2 <span class="token operator">-</span><span class="token operator">-</span>arg3 <span class="token operator">and</span> all other arguments of your training script<span class="token punctuation">)</span>Node <span class="token number">2</span><span class="token punctuation">:</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span>NUM_GPUS_YOU_HAVE           <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">"192.168.1.1"</span>           <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token number">1234</span> YOUR_TRAINING_SCRIPT<span class="token punctuation">.</span>py <span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span>arg1 <span class="token operator">-</span><span class="token operator">-</span>arg2 <span class="token operator">-</span><span class="token operator">-</span>arg3 <span class="token operator">and</span> all other arguments of your training script<span class="token punctuation">)</span></code></pre><p>3.To look up what optional arguments this module offers:</p><p><code>python -m torch.distributed.launch --help</code></p><p><strong>Important Notices:</strong></p><p>1.GPU训练目前使用$NCLL$后端达到最佳性能</p><p>2.在你的训练程序中，你必须解析命令行参数:–local_rank=LOCAL_PROCESS_RANK，<strong>它将由这个模块提供</strong>。如果你的training programs使用GPU，你应该确保你的代码只运行在LOCAL_PROCESS_RANK的GPU设备上。这可以通过以下方式实现:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> argparseparser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--local_rank"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">)</span>args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Set your device to local rank using either</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># before your code runs</span><span class="token operator">or</span><span class="token punctuation">:</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device<span class="token punctuation">(</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true"># your code to run</span></code></pre><p>3.In your training program, you are supposed to call the following function at the beginning to start the distributed backend. It is strongly recommended that <code>init_method=env://</code>. Other init methods (e.g. <code>tcp://</code>) may work, but <code>env://</code> is the one that is officially supported by this module.</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'YOUR BACKEND'</span><span class="token punctuation">,</span>init_method<span class="token operator">=</span><span class="token string">'env://'</span><span class="token punctuation">)</span></code></pre><p>4.加载数据集</p><p>多卡训练加载数据:</p><p>Dataset的设计上与单gpu一致，但是DataLoader上不一样。</p><p>首先解释下原因：多gpu训练是，我们希望同一时刻在每个gpu上的数据是不一样的，这样相当于batch size扩大了N倍，因此起到了加速训练的作用。在DataLoader时，如何做到每个gpu上的数据是不一样的，且gpu1上训练过的数据如何确保接下来不被别的gpu再次训练。这时候就得需要DistributedSampler。</p><p>Dataloader设置方式如下，注意shuffle与sampler是冲突的，并行训练需要设置sampler，此时务必要把shuffle设为False。但是这里shuffle=False并不意味着数据就不会乱序了，而是乱序的方式交给sampler来控制，实质上数据仍是乱序的。</p><pre class=" language-python3"><code class="language-python3">train_sampler = torch.utils.data.distributed.DistributedSampler(My_Dataset)dataloader = torch.utils.data.DataLoader(ds,                                         batch_size=batch_size,                                         shuffle=False,                                         num_workers=16,                                         pin_memory=True,                                         drop_last=True,                                         sampler=train_sampler)</code></pre><p>5.加载模型</p><p>多卡训练的模型设置：</p><p>最主要的是<code>find_unused_parameters</code>和<code>broadcast_buffers</code>参数；</p><p><code>find_unused_parameters</code>：如果模型的输出有不需要进行反传的(比如部分参数被冻结/或者网络前传是动态的)，设置此参数为True;如果你的代码运行</p><p>后卡住某个地方不动，基本上就是该参数的问题。</p><p><code>broadcast_buffers</code>：设置为True时，在模型执行forward之前，gpu0会把buffer中的参数值全部覆盖到别的gpu上。注意这和同步BN并不一样，同步BN应该使用SyncBatchNorm。</p><pre class=" language-python"><code class="language-python">My_model <span class="token operator">=</span> My_model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 将模型拷贝到每个gpu上.直接.cuda()也行，因为多进程时每个进程的device号是不一样的</span>My_model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">.</span>convert_sync_batchnorm<span class="token punctuation">(</span>My_model<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 设置多个gpu的BN同步</span>My_model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>My_model<span class="token punctuation">,</span>                                                      device_ids<span class="token operator">=</span><span class="token punctuation">[</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span>                                                      output_device<span class="token operator">=</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">,</span>                                                      find_unused_parameters<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                                                      broadcast_buffers<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><h3 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h3><p>网络结构图的基本构建模块<code>import torch.nn as nn</code></p><p>卷积层和线性层在<code>__init__</code>里面，而激活和池化在<code>forward</code>函数里面。</p><p><code>torch.nn</code>只支持小批量输入。整个<code>torch.nn</code>包都只支持小批量样本,而不支持单个样本。 例如,<code>nn.Conv2d</code>接受一个4维的张量,每一维分别是$Samples \times nChannels\times Height\times Width$(样本数x通道数x高x宽)。如果你有单个样本,<strong>只需使用 <code>input.unsqueeze(0) </code>来添加其它的维数</strong>.</p><h4 id="Parameter"><a href="#Parameter" class="headerlink" title="Parameter"></a>Parameter</h4><p><code>torch.nn.parameter.Parameter</code></p><p>A kind of Tensor that is to be considered as a module parameter.</p><p><strong>Parameters are <code>Tensor</code> subclasses, that have a very special property when used with <code>Module</code> s - when they’re assigned as Module attributes they are automatically added to the list of its parameters</strong>, and will appear e.g. in <code>parameters()</code> iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as <code>Parameter</code>, these temporaries would get registered too.</p><h4 id="Containers"><a href="#Containers" class="headerlink" title="Containers"></a>Containers</h4><p><strong>nn.Module</strong></p><p>所有神经网络模块的基类。你的模型也应该子类化这个类。</p><p>模块还可以包含其他模块，允许将它们嵌套在树结构中。</p><p><a href="https://www.cnblogs.com/wupiao/articles/13287061.html">Variables training 和 train()  eval()</a></p><pre class=" language-python"><code class="language-python">net<span class="token punctuation">.</span>training <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment" spellcheck="true"># 布尔值表示该模块是处于训练模式training mode还是评估模式evaluation mode。注意，对module的设置仅仅影响本层，子module不受影响</span>net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 将本层及子层的training设定为True,使用BatchNormalizetion()和Dropout()</span>net<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 将本层及子层的training设定为False,不使用BatchNormalization()和Dropout()</span></code></pre><p><code>add_module</code></p><p>在自定义网络的时候，由于自定义变量<strong>不是Module类型</strong>（例如，我们用List封装了几个网络），所以pytorch<strong>不会自动注册网络模块</strong>。<strong>add_module函数用来为网络添加模块</strong>的，所以我们可以使用这个函数手动添加自定义的网络模块。当然，这种情况，我们也可以使用ModuleList来封装自定义模块，pytorch就会自动注册了。</p><pre class=" language-python"><code class="language-python">self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># self.add_module('layers',nn.Linear(28*28,28*28)) 跟上面的方式等价</span></code></pre><p><a href="https://www.cnblogs.com/marsggbo/p/12075244.html">buffers()和parameters()的区别</a></p><p><code>buffers()</code></p><p>指那些不需要参与反向传播的参数,反向传播不需要被optimizer更新</p><pre class=" language-python"><code class="language-python">buffers<span class="token punctuation">(</span>recurse<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> → Iterator<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">for</span> buf <span class="token keyword">in</span> model<span class="token punctuation">.</span>buffers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>type<span class="token punctuation">(</span>buf<span class="token punctuation">)</span><span class="token punctuation">,</span> buf<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span> 1L<span class="token punctuation">,</span> 5L<span class="token punctuation">,</span> 5L<span class="token punctuation">)</span></code></pre><p><code>parameters()</code></p><p>是<code>nn.parameter.Paramter</code>，也就是组成Module的参数。例如一个<code>nn.Linear</code>通常由<code>weight</code>和<code>bias</code>参数组成。它的特点是默认<code>requires_grad=True</code>,也就是说训练过程中需要反向传播的，反向传播需要被optimizer更新的。</p><pre class=" language-python"><code class="language-python">parameters<span class="token punctuation">(</span>recurse<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span> Iterator<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parameter<span class="token punctuation">.</span>Parameter<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#recurse (bool)如果为True，则生成此模块和所有子模块的参数。否则，只生成此模块的直接成员参数。</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>type<span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">,</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#输出的是一个w,一个b！别忘了b！</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span> 1L<span class="token punctuation">,</span> 5L<span class="token punctuation">,</span> 5L<span class="token punctuation">)</span></code></pre><p><code>named_parameters()</code></p><p>返回模块参数的迭代器，生成参数名称和参数本身</p><pre class=" language-python"><code class="language-python">named_parameters<span class="token punctuation">(</span>prefix<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">''</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#作为所有参数名称的前缀</span>                 recurse<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>→ Iterator<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>str<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span>                <span class="token comment" spellcheck="true">#如果为真，则生成该模块和所有子模块的参数。否则，只会产生作为该模块直接成员的参数。</span><span class="token keyword">for</span> name<span class="token punctuation">,</span>parameters <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#可同时返回名字和参数</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span><span class="token string">':'</span><span class="token punctuation">,</span>parameters<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#conv1.weight : torch.Size([6, 1, 3, 3])</span><span class="token comment" spellcheck="true">#conv1.bias : torch.Size([6])</span><span class="token comment" spellcheck="true">#fc1.weight : torch.Size([10, 1350])</span><span class="token comment" spellcheck="true">#fc1.bias : torch.Size([10])                </span></code></pre><p><code>modules()</code></p><p>返回一个可以遍历网络所有模块的迭代器。</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> l <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>l<span class="token punctuation">,</span> l<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">for</span> idx<span class="token punctuation">,</span> m <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>net<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>idx<span class="token punctuation">,</span> <span class="token string">'-&gt;'</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><p><code>named_modules()</code></p><p>返回一个可以遍历网络所有模块的迭代器,产生模块的名字和模块本身。</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> l <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>l<span class="token punctuation">,</span> l<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">for</span> idx<span class="token punctuation">,</span> m <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>net<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>idx<span class="token punctuation">,</span> <span class="token string">'-&gt;'</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">,</span> Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span><span class="token string">'0'</span><span class="token punctuation">,</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># PSPNet的一段实现</span><span class="token keyword">for</span> n<span class="token punctuation">,</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token string">'conv2'</span> <span class="token keyword">in</span> n<span class="token punctuation">:</span>        m<span class="token punctuation">.</span>dilation<span class="token punctuation">,</span> m<span class="token punctuation">.</span>padding<span class="token punctuation">,</span> m<span class="token punctuation">.</span>stride <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">elif</span> <span class="token string">'downsample.0'</span> <span class="token keyword">in</span> n<span class="token punctuation">:</span>        m<span class="token punctuation">.</span>stride <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">for</span> n<span class="token punctuation">,</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>layer4<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token string">'conv2'</span> <span class="token keyword">in</span> n<span class="token punctuation">:</span>        m<span class="token punctuation">.</span>dilation<span class="token punctuation">,</span> m<span class="token punctuation">.</span>padding<span class="token punctuation">,</span> m<span class="token punctuation">.</span>stride <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">elif</span> <span class="token string">'downsample.0'</span> <span class="token keyword">in</span> n<span class="token punctuation">:</span>        m<span class="token punctuation">.</span>stride <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 注意要点有:</span><span class="token comment" spellcheck="true"># Sequential是没有name的,所有在sequential里的都按顺序从0开始编号</span><span class="token comment" spellcheck="true"># 遍历是按照深度优先遍历DFS,名字是不断叠加的，如0,0.conv1，0.conv2,之类的</span><span class="token comment" spellcheck="true"># PSP这段代码的意思就是block中的那个conv2加上空洞卷积，然后取消下采样</span>params<span class="token operator">=</span>segmodel<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#获得模型的原始状态以及参数。</span>    <span class="token keyword">for</span> k<span class="token punctuation">,</span>v <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#只打印key值，不打印具体参数。</span><span class="token triple-quoted-string string">"""conv1.weightbn1.weightbn1.biasbn1.running_meanbn1.running_varbn1.num_batches_trackedconv2.weightbn2.weightbn2.biasbn2.running_meanbn2.running_varbn2.num_batches_trackedlayer1.0.conv1.weightlayer1.0.bn1.weightlayer1.0.bn1.biaslayer1.0.bn1.running_meanlayer1.0.bn1.running_varlayer1.0.bn1.num_batches_trackedlayer1.0.conv2.weightlayer1.0.bn2.weightlayer1.0.bn2.biaslayer1.0.bn2.running_meanlayer1.0.bn2.running_varlayer1.0.bn2.num_batches_tracked"""</span>l <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>l<span class="token punctuation">,</span> l<span class="token punctuation">)</span>params<span class="token operator">=</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> k<span class="token punctuation">,</span>v <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span>v<span class="token punctuation">)</span><span class="token number">0.</span>weight<span class="token number">0.</span>bias<span class="token number">1.</span>weight<span class="token number">1.</span>bias<span class="token comment" spellcheck="true"># 所有放在Sequential里面的都是按0,1,2,3...序号排列的</span><span class="token comment" spellcheck="true"># 中间少了几层就不对不上了就少了</span></code></pre><p><code>state_dict()</code></p><p><a href="https://zhuanlan.zhihu.com/p/98563721">state_dcit和load_state_dict源码详解</a></p><p>返回一个字典，其中包含模块的整个状态,存储了网络结构的名字和对应的参数。parameters和buffers(如运行平均值)都包括在内。键是对应的parameter和buffer名称。</p><p><code>torch.nn.Module</code>模块中的<code>state_dict</code>只包含<strong>卷积层和全连接层的参数</strong>，当网络中存在batchnorm时，例如vgg网络结构，torch.nn.Module模块中的state_dict也会存放<strong>batchnorm的running_mean。</strong></p><p><code>torch.optim</code>模块中的Optimizer优化器对象也存在一个state_dict对象，此处的state_dict字典对象包含两个字典对象，<strong>key 分别为state和param_groups</strong>，param_groups对应的value也是一个字典对象，由学习率，动量等参数组成。</p><p><strong>对于module</strong></p><pre class=" language-python"><code class="language-python">state_dict<span class="token punctuation">(</span>destination<span class="token operator">=</span>None<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> keep_vars<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>dict<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'bias'</span><span class="token punctuation">,</span> <span class="token string">'weight'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># torch.nn.modules.module.py</span><span class="token keyword">class</span> <span class="token class-name">Module</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">state_dict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> destination<span class="token operator">=</span>None<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> keep_vars<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> destination <span class="token keyword">is</span> None<span class="token punctuation">:</span>            destination <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>            destination<span class="token punctuation">.</span>_metadata <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>        destination<span class="token punctuation">.</span>_metadata<span class="token punctuation">[</span>prefix<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> local_metadata <span class="token operator">=</span> dict<span class="token punctuation">(</span>version<span class="token operator">=</span>self<span class="token punctuation">.</span>_version<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># params</span>        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>_parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> param <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                destination<span class="token punctuation">[</span>prefix <span class="token operator">+</span> name<span class="token punctuation">]</span> <span class="token operator">=</span> param <span class="token keyword">if</span> keep_vars <span class="token keyword">else</span> param<span class="token punctuation">.</span>data        <span class="token comment" spellcheck="true"># buffers</span>        <span class="token keyword">for</span> name<span class="token punctuation">,</span> buf <span class="token keyword">in</span> self<span class="token punctuation">.</span>_buffers<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> buf <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                destination<span class="token punctuation">[</span>prefix <span class="token operator">+</span> name<span class="token punctuation">]</span> <span class="token operator">=</span> buf <span class="token keyword">if</span> keep_vars <span class="token keyword">else</span> buf<span class="token punctuation">.</span>data        <span class="token comment" spellcheck="true"># modules</span>        <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> module <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span>destination<span class="token punctuation">,</span> prefix <span class="token operator">+</span> name <span class="token operator">+</span> <span class="token string">'.'</span><span class="token punctuation">,</span> keep_vars<span class="token operator">=</span>keep_vars<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># </span>        <span class="token keyword">for</span> hook <span class="token keyword">in</span> self<span class="token punctuation">.</span>_state_dict_hooks<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            hook_result <span class="token operator">=</span> hook<span class="token punctuation">(</span>self<span class="token punctuation">,</span> destination<span class="token punctuation">,</span> prefix<span class="token punctuation">,</span> local_metadata<span class="token punctuation">)</span>            <span class="token keyword">if</span> hook_result <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                destination <span class="token operator">=</span> hook_result        <span class="token keyword">return</span> destination</code></pre><p>通过<code>_modules</code>递归所有子模块,再通过<code>_parameters</code>和<code>_buffers</code>获得所有parameters和buffers,注意之前的parameters()等函数也是利用他们获取相应的值。而<code>_state_dict_hooks</code>就是在读取state_dict时希望执行的操作,一般为空，所以不做考虑。另外有一点需要注意的是，在读取<code>Module</code>时采用的递归的读取方式，并且名字间使用<code>.</code>做分割，以方便后面<code>load_state_dict</code>读取参数。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>my_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 参数直接作为模型类成员变量</span>        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'my_buffer'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 参数注册为 buffer</span>        self<span class="token punctuation">.</span>my_param <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>f3 <span class="token operator">=</span> self<span class="token punctuation">.</span>fc    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> xmodel <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'my_param'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.3052</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token punctuation">(</span><span class="token string">'my_buffer'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5583</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token punctuation">(</span><span class="token string">'fc.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6322</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0255</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4747</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0530</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token punctuation">(</span><span class="token string">'conv.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.3346</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.2962</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token punctuation">(</span><span class="token string">'conv.bias'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5205</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token punctuation">(</span><span class="token string">'fc2.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4949</span><span class="token punctuation">,</span>  <span class="token number">0.2815</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span> <span class="token number">0.3006</span><span class="token punctuation">,</span>  <span class="token number">0.0768</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token punctuation">(</span><span class="token string">'f3.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6322</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0255</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4747</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0530</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><strong>对于optim</strong></p><pre class=" language-python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">state_dict</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        r<span class="token triple-quoted-string string">"""Returns the state of the optimizer as a :class:`dict`.        It contains two entries:        * state - a dict holding current optimization state. Its content            differs between optimizer classes.        * param_groups - a dict containing all parameter groups        """</span>        <span class="token comment" spellcheck="true"># Save order indices instead of Tensors</span>        param_mappings <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        start_index <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">def</span> <span class="token function">pack_group</span><span class="token punctuation">(</span>group<span class="token punctuation">)</span><span class="token punctuation">:</span>            nonlocal start_index            packed <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> group<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> k <span class="token operator">!=</span> <span class="token string">'params'</span><span class="token punctuation">}</span>            param_mappings<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">{</span>id<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">:</span> i <span class="token keyword">for</span> i<span class="token punctuation">,</span> p <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>group<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> start_index<span class="token punctuation">)</span>                                   <span class="token keyword">if</span> id<span class="token punctuation">(</span>p<span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token keyword">in</span> param_mappings<span class="token punctuation">}</span><span class="token punctuation">)</span>            packed<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>param_mappings<span class="token punctuation">[</span>id<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> group<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>            start_index <span class="token operator">+=</span> len<span class="token punctuation">(</span>packed<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> packed        param_groups <span class="token operator">=</span> <span class="token punctuation">[</span>pack_group<span class="token punctuation">(</span>g<span class="token punctuation">)</span> <span class="token keyword">for</span> g <span class="token keyword">in</span> self<span class="token punctuation">.</span>param_groups<span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Remap state to use order indices as keys</span>        packed_state <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">(</span>param_mappings<span class="token punctuation">[</span>id<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>k<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token keyword">else</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span> v                        <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>        <span class="token keyword">return</span> <span class="token punctuation">{</span>            <span class="token string">'state'</span><span class="token punctuation">:</span> packed_state<span class="token punctuation">,</span>            <span class="token string">'param_groups'</span><span class="token punctuation">:</span> param_groups<span class="token punctuation">,</span>        <span class="token punctuation">}</span></code></pre><p><code>load_state_dict()</code></p><p>将参数和缓冲区从state_dict复制到这个模块及其子模块中。</p><pre class=" language-python"><code class="language-python">load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span>str<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#传入一个state_dict</span>                strict<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#state_dict就是你之前保存的模型参数序列，而_load_from_state_dict中的local_state表示你的代码中定义的模型的结构。</span><span class="token comment" spellcheck="true">#判断上面参数拷贝过程中是否有unexpected_keys或者missing_keys,如果有就报错，代码不能继续执行。当然，如果strict=False，则会忽略这些细节。</span><span class="token comment" spellcheck="true">#missing_keys is a list of str containing the missing keys</span><span class="token comment" spellcheck="true">#unexpected_keys is a list of str containing the unexpected keys</span></code></pre><p><code>cuda</code>与<code>to</code></p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#在这没有区别</span>    <span class="token comment" spellcheck="true"># allocates a tensor on GPU 1</span>    a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>cuda<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># transfers a tensor from CPU to GPU 1</span>    b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># a.device and b.device are device(type='cuda', index=1)</span>    <span class="token comment" spellcheck="true"># You can also use ``Tensor.to`` to transfer a tensor:</span>    b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>cuda<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># b.device and b2.device are device(type='cuda', index=1)</span><span class="token comment" spellcheck="true"># .to(device)可以指定CPU或者GPU</span><span class="token comment" spellcheck="true"># 单GPU或者CPU</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># model 是 model.to(device)</span><span class="token comment" spellcheck="true"># img 是 img = img.to(device)</span><span class="token comment" spellcheck="true">#如果是多GPU</span><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>  model <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model，device_ids<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># .cuda()只能指定GPU</span><span class="token comment" spellcheck="true">#指定某个GPU</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICE'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'1'</span>model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果是多GPU</span>os<span class="token punctuation">.</span>environment<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0,1,2,3'</span>device_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span>net  <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dataparallel<span class="token punctuation">(</span>net<span class="token punctuation">,</span> device_ids <span class="token operator">=</span>device_ids<span class="token punctuation">)</span>net  <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dataparallel<span class="token punctuation">(</span>net<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 默认使用所有的device_ids </span>net <span class="token operator">=</span> net<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><code>apply(fn)</code></p><p>将<code>fn</code>递归应用于每个子模块(由<code>.children()</code>返回)以及自身self。典型用法包括初始化模型的参数(参见<code>torch.nn.init()</code>).</p><p><code>fn(Module -&gt; None)</code>:将被应用到每个子模块的函数。</p><p>Returns:<code>self</code>  Return type:<code>Module</code></p><p><code>register_buffer</code></p><p>向模块添加一个buffer.</p><p>这通常用于注册不应被视为模型参数的缓冲区。 例如,BatchNorm的running_mean不是参数,而是模块状态的一部分。 默认情况下，缓冲区是==持久性==的，并将与参数一起保存。 可以通过将persistent设置为False来更改此行为。</p><p> ==持久缓冲区和非持久缓冲区之间的唯一区别是，后者不会成为该模块的state_dict的一部分。==</p><pre class=" language-python"><code class="language-python">register_buffer<span class="token punctuation">(</span>name<span class="token punctuation">,</span>                tensor<span class="token punctuation">,</span>                persistent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#是否为持久缓冲区</span></code></pre><p><code>children</code></p><p>返回immediate 子模块的迭代器。</p><p>Yields:Module - achildren module</p><p><code>register_forward_hook(hook)</code></p><p>在模块上注册一个前向钩子。</p><p>相当于插件。可以实现一些额外的功能，而又不用修改主体代码。把这些额外功能实现了挂在主代码上，所以叫钩子，很形象。</p><p>每当<code>forward()</code>计算<code>output</code>后，该钩子都会被调用。 该钩子应该具有以下签名:</p><p><code>hook(module, input, output) -&gt; None or modified output</code></p><p>输入仅包含提供给模块的positional arguments。 Kerword arguments不会传递给钩子,而只会传递给the forward。挂钩可以修改输出。 它可以就地修改输入,但不会对正向产生影响,因为这是在调用<code>forward()</code>之后调用的。</p><p>Returns:a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p><p>ReturnType:<code>torch.utils.hooks.RemovableHandle</code></p><p><strong>nn.ModuleList</strong></p><p><a href="https://zhuanlan.zhihu.com/p/64990232">PyTorch 中的 ModuleList 和 Sequential: 区别和使用场景</a></p><p>可以把任意 nn.Module 的子类 (比如 nn.Conv2d, nn.Linear 之类的) 加到这个 list 里面，方法和 Python 自带的 list 一样，无非是 extend，append 等操作。但不同于一般的 list，加入到 nn.ModuleList 里面的 module 是会自动注册到整个网络上的，同时 module 的 parameters 也会自动添加到整个网络中。</p><p>例子1:使用 nn.ModuleList 来构建一个小型网络,包括2个全连接层</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">net1</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>net1<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">:</span>            x <span class="token operator">=</span> m<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> net1<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># net1(</span><span class="token comment" spellcheck="true">#   (modules): ModuleList(</span><span class="token comment" spellcheck="true">#     (0): Linear(in_features=10, out_features=10, bias=True)</span><span class="token comment" spellcheck="true">#     (1): Linear(in_features=10, out_features=10, bias=True)</span><span class="token comment" spellcheck="true">#   )</span><span class="token comment" spellcheck="true"># )</span><span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>type<span class="token punctuation">(</span>param<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># &lt;class 'torch.Tensor'&gt; torch.Size([10, 10])</span><span class="token comment" spellcheck="true"># &lt;class 'torch.Tensor'&gt; torch.Size([10])</span><span class="token comment" spellcheck="true"># &lt;class 'torch.Tensor'&gt; torch.Size([10, 10])</span><span class="token comment" spellcheck="true"># &lt;class 'torch.Tensor'&gt; torch.Size([10])</span></code></pre><p>网络包含两个全连接层，他们的权重 (weithgs) 和偏置 (bias) 都在这个网络之内。</p><p>例子2:使用Python自带的list</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">net2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>net2<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> <span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">:</span>            x <span class="token operator">=</span> m<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> net2<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># net2()</span><span class="token keyword">print</span><span class="token punctuation">(</span>list<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># []</span></code></pre><p><strong>使用 Python 的 list 添加的全连接层和它们的 parameters 并没有自动注册到我们的网络中。当然，我们还是可以使用 forward 来计算输出结果。但是如果用 net2 实例化的网络进行训练的时候，因为这些层的 parameters 不在整个网络之中，所以其网络参数也不会被更新，也就是无法训练。</strong></p><p>好,看到这里,我们大致明白了 nn.ModuleList 是干什么的了:它是一个<strong>储存不同 module,并自动将每个module的parameters添加到网络之中的容器</strong>。但是,我们需要注意到**,nn.ModuleList 并没有定义一个网络.它只是将不同的模块储存在一起,这些模块之间并没有什么先后顺序**可言,比如:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">net3</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>net3<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>         <span class="token keyword">return</span> xnet <span class="token operator">=</span> net3<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># net3(</span><span class="token comment" spellcheck="true">#   (linears): ModuleList(</span><span class="token comment" spellcheck="true">#     (0): Linear(in_features=10, out_features=20, bias=True)</span><span class="token comment" spellcheck="true">#     (1): Linear(in_features=20, out_features=30, bias=True)</span><span class="token comment" spellcheck="true">#     (2): Linear(in_features=5, out_features=10, bias=True)</span><span class="token comment" spellcheck="true">#   )</span><span class="token comment" spellcheck="true"># )</span>input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># torch.Size([32, 30])</span></code></pre><p>根据 net3 的结果,我们可以看出来这个 <strong>ModuleList 里面的顺序并不能决定什么,网络的执行顺序是根据 forward 函数来决定的</strong>。如果你非要 ModuleList 和 forward 中的顺序不一样,<strong>PyTorch 表示它无所谓，但以后 review 你代码的人可能会意见比较大</strong>。</p><p>我们再考虑另外一种情况,既然这个 ModuleList 可以根据序号来调用，那么一个模块是否可以在 forward 函数中被调用多次呢？答案当然是可以的，但是，<strong>被调用多次的模块，是使用同一组 parameters 的，也就是它们的参数是共享的</strong>,无论之后怎么更新。</p><p><strong>nn.sequential</strong></p><p>顺序容器。模块将按照它们在构造函数中传递的顺序被添加到它。另外，也可以传入模块的有序dict。</p><p>不同于 nn.ModuleList,它<strong>已经实现了内部的 forward 函数，而且里面的模块必须是按照顺序进行排列的，所以我们必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Example of using Sequential</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>          nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Example of using Sequential with OrderedDict</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>          <span class="token punctuation">(</span><span class="token string">'conv1'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          <span class="token punctuation">(</span><span class="token string">'relu1'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          <span class="token punctuation">(</span><span class="token string">'conv2'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          <span class="token punctuation">(</span><span class="token string">'relu2'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a>线性层</h4><p><code>nn.Linear</code>全连接层</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token punctuation">:</span> int<span class="token punctuation">,</span> out_features<span class="token punctuation">:</span> int<span class="token punctuation">,</span> bias<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#在全连接层之前通过view函数将其改为一维向量</span></code></pre><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p><code>nn.Dropout</code></p><p>在训练过程中,使用伯努利分布样本，以概率p随机地将输入张量中的一些元素置零。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#元素置0的概inplace=False)#</span><span class="token comment" spellcheck="true"># 对所有元素中每个元素按照概率0.5置为0，对点执行</span></code></pre><p>Furthermore, the outputs are scaled by a factor of $\frac{1}{1-p}$ during training. This means that during evaluation the module simply computes an identity function.</p><p><code>nn.Dropout2d</code></p><p>适用于有多个channel输出的,常用于图像处理。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 对每个通道按照概率0.5置为0，对平面执行,直接将整个channel置为0.psp中dropout值设为0.1,在最后一个预测卷积之前使用的。</span></code></pre><h4 id="Conv"><a href="#Conv" class="headerlink" title="Conv"></a>Conv</h4><p><code>nn.Conv2d()</code>卷积核是二维的</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">:</span> int<span class="token punctuation">,</span>                 out_channels<span class="token punctuation">:</span> int<span class="token punctuation">,</span>                kernel_size<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                 stride<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>                 dilation<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                 groups<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                 bias<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#是否要添加偏置参数作为可学习参数的一个，默认为True。</span>                padding_mode<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'zeros'</span><span class="token punctuation">)</span></code></pre><p>教程：<a href="https://www.jianshu.com/p/45a26d278473">https://www.jianshu.com/p/45a26d278473</a></p><p>接受$(N,C_{in},H,W)$,输出$(N,C_{out},H_{out},W_{out}$)</p><p>卷积核的规模就是kernel_size x input_channel x output_channel</p><p>$out(N_i,C_{out_j})=bias(C_{out_j})+∑<em>{k=0}^{C</em>{in-1}}weight(C_{out_j},k)⋆input(N_i,k)$</p><p>对于<strong>depthwise conv</strong>,groups这个参数是关键。当$groups=1$时,说明所有通道为一组;当$groups=in_channels$时,说明分了$in_channels$个组，即每个通道一组。然后分别对齐卷积，输出通道数为k。最后再将每组的输出串联，最后通道数为$in_channels*k$。</p><p>要实现depthwise conv,就讲groups设为in_channels,同时out_channels也设为与in_channels相同。</p><p><strong>Variables</strong></p><p>~Conv2d.weight(Tensor):维度为<code>(out_channels,in_channels/groups,kernel_size[0],kernel_size[1])</code>，权重值取样于$u(-\sqrt k,\sqrt k)$,$k=$</p><p>~Conv2d.bias(Tensor):维度为<code>(out_channels)</code>，值取样于$u(-\sqrt k,\sqrt k)$,$k=$</p><p><code>nn.ConvTranspose2d</code></p><p>在由几个输入平面组成的输入图像上应用一个二维转置卷积运算符。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">:</span> int<span class="token punctuation">,</span>                         out_channels<span class="token punctuation">:</span> int<span class="token punctuation">,</span>                         kernel_size<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                         stride<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                         padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>                         output_padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>                         groups<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                         dilation<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                         padding_mode<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'zeros'</span><span class="token punctuation">)</span></code></pre><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>Pytorch<strong>池化操作的步长默认与池化卷积核的大小一样</strong>，<strong>池化一般不考虑overlap</strong></p><p><code>nn.MaxPool2d()</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    stride<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#default value是kernel_size</span>                   padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>                    dilation<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                    return_indices<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助</span>                   ceil_mode<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果true，向上取整而不是floor向下取整</span></code></pre><p><code>nn.AvgPool2d()</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span>                   stride<span class="token operator">=</span>None<span class="token punctuation">,</span>                   padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                    ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#when True, will use ceil instead of floor to compute the output shape</span>                   count_include_pad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                   divisor_override<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p><code>torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)</code></p><p>特殊性在于，输出张量的大小都是给定的output_size，可以利用这个实现全局平均池化，将output_size设为(1,1)。</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># target output size of 5x7</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output <span class="token operator">=</span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># target output size of 7x7 (square)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output <span class="token operator">=</span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><a href="https://blog.csdn.net/u013382233/article/details/85948695">自适应池化详解1</a></p><p><a href="https://blog.csdn.net/xiaosongshine/article/details/89453037">自适应池化详解2</a></p><h4 id="Padding-Layers"><a href="#Padding-Layers" class="headerlink" title="Padding Layers"></a>Padding Layers</h4><p><code>torch.nn.ReflectionPad2d(padding)</code></p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReflectionPad2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> input <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> inputtensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h4 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h4><p><code>nn.BatchNorm2d()</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#Channel数</span>                    eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#为数值稳定性而加在分母上的值。</span>                    momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#指数加权平均的参数</span>                    affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#是否有可学习参数</span>                    track_running_stats<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这一层不用到测试</span></code></pre><p><code>nn.SyncBatchNorm</code></p><pre class=" language-python"><code class="language-python"></code></pre><p><code>nn.LayerNorm</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">,</span>                   eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span>                   elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> device<span class="token operator">=</span>None<span class="token punctuation">,</span> dtype<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># NLP和CV的应用是不同的</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># NLP Example</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> batch<span class="token punctuation">,</span> sentence_length<span class="token punctuation">,</span> embedding_dim <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> sentence_length<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># Activate module</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> layer_norm<span class="token punctuation">(</span>embedding<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># Image Example</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># Normalize over the last three dimensions (i.e. the channel and spatial dimensions)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment" spellcheck="true"># as shown in the image below</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span><span class="token punctuation">[</span>C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>input<span class="token punctuation">)</span></code></pre><h4 id="非线性激活函数"><a href="#非线性激活函数" class="headerlink" title="非线性激活函数"></a>非线性激活函数</h4><p><code>nn.ReLU()</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#inplace-选择是否进行覆盖运算 x=x+1 还是 y=x+1 x=y 节省内存</span></code></pre><p><code>nn.Softmax(dim=None)</code></p><p>对指定维度应用Softmax</p><pre class=" language-python"><code class="language-python">input <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>m0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>m1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>output0 <span class="token operator">=</span> m0<span class="token punctuation">(</span>input<span class="token punctuation">)</span>output1 <span class="token operator">=</span> m1<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"input: "</span><span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"output0: "</span><span class="token punctuation">,</span> output0<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"output1: "</span><span class="token punctuation">,</span> output1<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''input:  tensor([[1., 2., 3.],        [4., 5., 6.],        [7., 8., 9.]])dim=0,就是归一化的元素的维度1都相同,只遍历维度0output0:  tensor([[0.0024, 0.0024, 0.0024],        [0.0473, 0.0473, 0.0473],        [0.9503, 0.9503, 0.9503]])output1:  tensor([[0.0900, 0.2447, 0.6652],        [0.0900, 0.2447, 0.6652],        [0.0900, 0.2447, 0.6652]])'''</span></code></pre><p><code>nn.Softmax2d()</code></p><p>输入为(N,C,H,W),输出为(N,C,H,W) 就是你想的那样</p><p><code>nn.LogSoftmax(dim=None)</code><br>$$<br>LogSoftmax(x_i)=log(\frac{exp(x_i)}{\sum_jexp(x_j)})<br>$$</p><h4 id="距离函数"><a href="#距离函数" class="headerlink" title="距离函数"></a>距离函数</h4><p><code>nn.CosineSimilarity</code><br>$$<br>similarity=\frac{x_1\cdot x_2}{max(||x_1||_2\cdot||x_2||_2,eps)}<br>$$</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CosineSimilarity<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 计算余弦相似性的维度</span>                          eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 避免除0的小数</span></code></pre><p>Input1:(*,D,*),D是要计算的维度</p><p>Input2:(*,D,*)</p><p>Output:(*,*)</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><a href="https://www.cnblogs.com/wanghui-garcia/p/10862733.html">常见损失函数总结,讲得好啊</a></p><p><code>nn.BCELoss</code></p><p>计算二元交叉熵</p><p>$l(x,y)=L={l_1,\cdots,l_N}^T,l_n=-w_n[y_n\cdot logx_n+(1-y_n)\cdot log(1-x_n)]$</p><p>我们的解决方案是BCELoss将其对数函数输出固定为大于或等于-100。这样，我们总是可以有一个有限的损失值和一个线性的反向方法。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span>weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#手动给的权重</span>                 size_average<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#</span>                 reduce<span class="token operator">=</span>None<span class="token punctuation">,</span>                 reduction<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'mean'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#指定要应用于输出的reduction操作:' none ' | 'mean' | ' sum '。none输出向量,其他输出标量</span><span class="token comment" spellcheck="true">#“none”:表示不进行任何reduction，“mean”:输出的和除以输出中的元素数，即求平均值，“sum”:输出求和。</span><span class="token comment" spellcheck="true">#注意:size_average和reduce正在被弃用，与此同时，指定这两个arg中的任何一个都将覆盖reduction参数。默认值:“mean”</span></code></pre><p><code>nn.BCEWithLogitsLoss</code></p><p>这种损失结合了Sigmoid层和BCELoss在一个类里。这个版本通过将操作合并到一个层比使用一个简单的Sigmoid后面跟着一个BCELoss在数值上更稳定，我们利用<strong>log-sum-exp</strong>技巧的数值稳定性。</p><p>$l(x,y)=L={l_1,\cdots,l_N}^T,l_n=-w_n[y_n\cdot log\sigma(x_n)+(1-y_n)\cdot log(1-\sigma(x_n))]$</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span>weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                           size_average<span class="token operator">=</span>None<span class="token punctuation">,</span>                           reduce<span class="token operator">=</span>None<span class="token punctuation">,</span>                           reduction<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'mean'</span><span class="token punctuation">,</span>                           pos_weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#正值例子的权重，必须是有着与分类数目相同的长度的向量.可以通过增加正值示例的权重来权衡召回率和准确性。</span></code></pre><p><code>nn.NLLLoss</code></p><p>负对数似然损失。用C类来训练分类问题是有用的。</p><p>==负对数似然损失函数，就是对似然函数取负再取log==</p><p>如果提供了可选参数weight，它应该是一个一维张量，为每一类赋权。当你有一个不平衡的训练集时，这是特别有用的。</p><p>$l(x,y)=L={l_1,\cdots,l_N}^T,l_n=-w_{y_nx_{n,y_n}},w_c=weight[c]\cdot1{c\ne ignore_index}$只选择第n个数据的实际yn类别作为loss</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                 <span class="token comment" spellcheck="true"># 一个手动标给每个类别的权重，如果给定，必须是一个C大小张量,否则默认所有的权重全是1</span>                 size_average<span class="token operator">=</span>None<span class="token punctuation">,</span>                 ignore_index<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#指定一个被忽略的目标值，该目标值不影响输入梯度。</span>                 <span class="token comment" spellcheck="true"># 当size_average为真时，对非忽略目标的损失进行平均。</span>                 reduce<span class="token operator">=</span>None<span class="token punctuation">,</span>                 <span class="token comment" spellcheck="true"># 默认情况下为True，根据size_average，通过每个小批的观察来对损失进行平均或求和。</span>                 <span class="token comment" spellcheck="true"># 当reduce为False时，返回每个批处理元素的损失值，并忽略size_average</span>                 reduction<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'mean'</span><span class="token punctuation">)</span></code></pre><p>==也就是说没有特殊处理的情况下，返回的是每个mini-bacth的平均cross-entropy loss，因为loss就是一个标量嘛，再之后还需要对所有的mini-batch的cross-entropy loss求平均。==</p><p>输入<code>(N,C)</code>, <code>C</code>代表类别的数量；或者在计算高维损失函数例子中输入大小为<code>(N,C,d1,d2,...,dK),k&gt;=1</code></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#低维示例:</span>m <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># input is of size N x C = 3 x 5</span>input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>inputtensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8676</span><span class="token punctuation">,</span>  <span class="token number">1.5017</span><span class="token punctuation">,</span>  <span class="token number">0.2963</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.9431</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0929</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.3540</span><span class="token punctuation">,</span>  <span class="token number">1.0994</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1085</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4001</span><span class="token punctuation">,</span>  <span class="token number">0.0102</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.3653</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3828</span><span class="token punctuation">,</span>  <span class="token number">0.6257</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4996</span><span class="token punctuation">,</span>  <span class="token number">0.1928</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>m<span class="token punctuation">(</span>input<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.8899</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5205</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.7259</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.9653</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.1152</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#1-&gt;0.5205</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.5082</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7628</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.9707</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.2623</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8520</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#0-&gt;1.5082</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6841</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4323</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4237</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4.5490</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8566</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#4-&gt;1.8566</span>       grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>LogSoftmaxBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#each element in target has to have 0 &lt;= value &lt; C</span>target <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>output <span class="token operator">=</span> loss<span class="token punctuation">(</span>m<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>outputtensor<span class="token punctuation">(</span><span class="token number">1.2951</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#高维示例:就是逐像素返回呗,对每一个像素来说都是一个低维示例</span><span class="token comment" spellcheck="true"># 2D loss example (used, for example, with image inputs)</span>N<span class="token punctuation">,</span> C <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span><span class="token number">4</span>loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># input is of size N x channel x height x width</span>data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> C<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#输出为5*4*8*8</span>m <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># each element in target has to have 0 &lt;= value &lt; C</span>target <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>N<span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span><span class="token punctuation">.</span>random_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#target.size()=target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)</span>output <span class="token operator">=</span> loss<span class="token punctuation">(</span>m<span class="token punctuation">(</span>conv<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>outputtensor<span class="token punctuation">(</span><span class="token number">1.5501</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLoss2DBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span></code></pre><p><code>nn.CrossEntropyLoss</code></p><p>将<code>nn.LogSoftmax()</code>和<code>nn.NLLLoss()</code>方法结合到一个类中</p><p>当用C类训练分类问题时，它是有用的。如果提供了，可选的参数weight权重应该是一个一维张量，为每个类分配权重。当你有一个不平衡的训练集时，这是特别有用的。</p><p>损失函数:<br>$$<br>log(x,class)=-log(\frac{exp(x[class])}{\sum_jexp(x[j])})=-x[class]+log(\sum_jexp(x[j]))<br>$$<br>加上weight:<br>$$<br>log(x,class)=weight<a href="-x%5Bclass%5D+log(%5Csum_jexp(x%5Bj%5D))">class</a><br>$$<br>对于<code>ignore_index:</code></p><p>该ignore_index标签的样本损失不考虑</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token comment" spellcheck="true"># 假设两类{0:背景，1：前景}</span>pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>    <span class="token punctuation">[</span>        <span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.7</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># shape=(N,C)=(3,2)，N为样本数，C为类数</span>label <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># shape=(N)=(3)，3个样本的label分别为1，0，1</span>out <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> label<span class="token punctuation">,</span> ignore_index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 忽略0类</span><span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># tensor(1.0421) </span></code></pre><p>$$<br>loss=1/2{[-0.1+ln(e^{0.9}+e^{0.1})+[-0.3+ln(e^{0.7}+e^{0.3})]}=1.0421<br>$$</p><p>x选的只是class那个</p><p><strong>取平均是对每个batch维度取平均</strong></p><img src="/pytorch-xue-xi/image-20210402114047450.png" alt="shape" style="zoom:80%;"><p>reduction=none的话，说明minibatch之间不做平均，就是个向量，否则就是个标量；</p><p><strong>其实整个思路都是明白的，就看如何返回了，上面取得了形式的一致性；</strong></p><p>$(N,d1,d2,…,dn)$<strong>是如何变成(1,1)的就是对后面维度求和，对N取平均。</strong></p><h4 id="视觉层"><a href="#视觉层" class="headerlink" title="视觉层"></a>视觉层</h4><p><code>nn.Upsample</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>size<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#指定目标输出的大小</span>                  scale_factor<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#输出为输入的倍数，和size只能指定一个</span>                  mode<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'nearest'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#上采样算法，包括最近邻、线性、双线性、双三次、三线性插值算法</span>                  align_corners<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>bool<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果为True，输入的角像素将与输出张量对齐，</span><span class="token comment" spellcheck="true">#因此将保存下来这些像素的值。仅当使用的算法为'linear', 'bilinear'or 'trilinear'时可以使用。默认设置为False</span><span class="token comment" spellcheck="true">#语义分割设置为true</span>tor</code></pre><h4 id="数据并行层"><a href="#数据并行层" class="headerlink" title="数据并行层"></a>数据并行层</h4><p><code>nn.DataParallel</code></p><p>在module level实现数据并行.</p><p>该容器在batch维度上拆分,将输入分到指定的设备上,以此实现并行化(其他object每个设备都复制一次)。</p><p>前向传播过程中,module在每个设备上,每个副本处理一部分输入;</p><p>反向传播时,每个副本的梯度被累加到原始模块中。</p><p>batch size应该&gt;大于使用的GPU数量。</p><p>推荐使用<code>DistributedDataParallel</code></p><p><a href="https://zhuanlan.zhihu.com/p/86441879">pytorch多GPU并行训练</a></p><p>我一般在使用多GPU的时候, 会喜欢使用<code>os.environ['CUDA_VISIBLE_DEVICES']</code>来限制使用的GPU个数, 例如我要使用第0和第3编号的GPU, 那么只需要在程序中设置:</p><pre class=" language-python"><code class="language-python">os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0,3'</span></code></pre><p>模型加载到多GPU:</p><pre class=" language-python"><code class="language-python">model <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>对于数据:</p><pre class=" language-python"><code class="language-python">inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>如果我们不设定好要使用的<code>device_ids的</code>话, 程序会自动找到这个机器上面<strong>可以用的所有的显卡, 然后用于训练.</strong> </p><p>但是因为我们前面使用<code>os.environ['CUDA_VISIBLE_DEVICES']</code><strong>限定了这个程序可以使用的显卡, 所以这个地方程序如果自己获取的话, 获取到的其实就是我们上面设定的那几个显卡.</strong></p><p>我没有进行深入得到考究, 但是我感觉使用<code>os.environ['CUDA_VISIBLE_DEVICES']</code>对可以使用的显卡进行限定之后, 显卡的实际编号和程序看到的编号应该是不一样的, 例如上面我们设定的是<code>os.environ['CUDA_VISIBLE_DEVICES']="0,2"</code>, 但是程序看到的显卡编号应该被改成了<code>'0,1'</code>, 也就是说程序所使用的显卡编号实际上是经过了一次映射之后才会映射到真正的显卡编号上面的, 例如这里的程序看到的1对应实际的2</p><p><code>torch.nn.parallel.DistributedDataParallel</code></p><p>在module level 实现基于torch.distuributed包的分布式数据并行。这个容器通过在批处理维度上的分块，在指定的设备上分割输入，使给定模块的应用并行化。该模块被复制到每台机器和每个设备上，每个这样的副本处理一部分输入。在倒退过程中，每个节点的梯度被平均化。</p><p>batch_size应该大于本地使用的GPU数量。</p><p>创建这个类需要调用<code>torch.distributed.init_process_group()</code>使得<code>torch.distributed</code>已经被初始化了。</p><p>事实证明,在单节点多GPU的数据并行训练中,DistributedDataParallel明显比torch.nn.DataParallel快。</p><p>要在有N个GPU的主机上使用DistributedDataParallel，你应该催生N个进程，确保每个进程只在0到N-1的单个GPU上工作。这可以通过为每个进程设置CUDA_VISIBLE_DEVICES或通过调用:<code>torch.cuda.set_device(i)</code>,这里的i是从0到N-1。在每个进程中，你应该参考以下内容来构建这个模块：</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>    backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">,</span> world_size<span class="token operator">=</span>N<span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token string">'...'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>i<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>module<span class="token punctuation">,</span>                                          device_ids<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># </span>                                          output_device<span class="token operator">=</span>None<span class="token punctuation">,</span>                                          dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                                          broadcast_buffers<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#设置为True时，在模型执行forward之前，gpu0会把buffer中的参数值全部覆盖</span><span class="token comment" spellcheck="true"># 到别的gpu上。注意这和同步BN并不一样，同步BN应该使用SyncBatchNorm。</span>                                          process_group<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#用于分布式数据</span>                                          bucket_cap_mb<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span>                                           find_unused_parameters<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 如果模型的输出有不需要进行反传的(比如部分参数被冻结/</span>                                          <span class="token comment" spellcheck="true"># 或者网络前传是动态的)，设置此参数为True;如果你的代码运行后卡住某个地方不动，基本上就是该参数的问题。</span>                                          check_reduction<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                                           gradient_as_bucket_view<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                                           static_graph<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><h4 id="torch-nn-utils"><a href="#torch-nn-utils" class="headerlink" title="torch.nn.utils"></a>torch.nn.utils</h4><p><code>nn.utils.clip_grad_value_</code></p><p><a href="https://zhuanlan.zhihu.com/p/99953668">深度炼丹之梯度裁剪</a></p><p>backward之后,step之前</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_value_<span class="token punctuation">(</span>parameters<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>Iterable<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                 <span class="token comment" spellcheck="true">#将梯度归一化的张量的可迭代或单个张量 net.parameters()</span>                                clip_value<span class="token punctuation">:</span> float<span class="token punctuation">)</span> → None<span class="token comment" spellcheck="true">#梯度被裁剪到[-clip_value,clip_value]</span>eg<span class="token punctuation">:</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_value_<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span></code></pre><p><code>nn.utils.clip_grad_norm</code></p><p>这个函数的主要目的是对$parameters$里的所有参数的梯度进行规范化。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 要规范的参数</span>                               max_norm<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 所有参数的梯度的范数的上界</span>                               norm_type<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 范数的类型,"inf"为无穷范数</span>                               error_if_nonfinite<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p>设$parameters$里所有参数的梯度的范数为$total_norm$，<br>若$max_norm&gt;total_norm$, $parameters$里面的参数的梯度不做改变;<br>若$max_norm&lt;total_norm$, $parameters$里面的参数的梯度都要乘以一个系数$clip_coef$:<br>$$<br>clip_coef=max_norm/(total_norm+10^{-6})<br>$$</p><h4 id="自定义模块"><a href="#自定义模块" class="headerlink" title="自定义模块"></a>自定义模块</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DoubleConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true">#要继承</span>    <span class="token triple-quoted-string string">"""(convolution =&gt; [BN] =&gt; ReLU) * 2"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> mid_channels<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#父类__init__</span>        <span class="token keyword">if</span> <span class="token operator">not</span> mid_channels<span class="token punctuation">:</span>            mid_channels <span class="token operator">=</span> out_channels        self<span class="token punctuation">.</span>double_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> mid_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>mid_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mid_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment" spellcheck="true">#要定义一个forward函数</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>double_conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#返回自身</span></code></pre><h4 id="forward函数"><a href="#forward函数" class="headerlink" title="forward函数"></a>forward函数</h4><p>定义每次调用时执行的计算。应该被所有子类重写。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#主要是使用了__call__特殊方法,使得forward被自动调用。</span><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> init_age<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'我年龄是:'</span><span class="token punctuation">,</span>init_age<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>age <span class="token operator">=</span> init_age    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> added_age<span class="token punctuation">)</span><span class="token punctuation">:</span>        res <span class="token operator">=</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>added_age<span class="token punctuation">)</span>        <span class="token keyword">return</span> res    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'forward 函数被调用了'</span><span class="token punctuation">)</span>                <span class="token keyword">return</span> input_ <span class="token operator">+</span> self<span class="token punctuation">.</span>age<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'对象初始化。。。。'</span><span class="token punctuation">)</span>a <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#初始化</span>input_param <span class="token operator">=</span> a<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#__call__起作用</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"我现在的年龄是："</span><span class="token punctuation">,</span> input_param<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对象初始化。。。。</span><span class="token comment" spellcheck="true">#我年龄是: 10</span><span class="token comment" spellcheck="true">#forward 函数被调用了</span><span class="token comment" spellcheck="true">#我现在的年龄是： 12</span></code></pre><p>关于 <code>__call__</code> 方法，不得不先提到一个概念，就是*可调用对象callable，我们平时自定义的函数、内置函数和类都属于可调用对象，但凡是可以把一对括号<code>()</code>应用到某个对象身上都可称之为可调用对象，判断对象是否为可调用对象可以用函数 <code>callable</code>。如果在类中实现了 <code>__call__</code> 方法，那么实例对象也将成为一个可调用对象。</p><p>利用这种特性，可以实现基于类的装饰器。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Counter</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> func<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>func <span class="token operator">=</span> func        self<span class="token punctuation">.</span>count <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>count <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>func<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>@Counter<span class="token keyword">def</span> <span class="token function">foo</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    foo<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>foo<span class="token punctuation">.</span>count<span class="token punctuation">)</span></code></pre><p><a href="https://blog.csdn.net/u012436149/article/details/70145598">pytorch结构介绍</a></p><p>forward函数使用的具体流程:</p><ol><li>调用module的<code>call</code>方法</li><li><code>module</code>的<code>call</code>里面调用<code>module</code>的<code>forward</code>方法</li><li><code>forward</code>里面如果碰到<code>Module</code>的子类，回到第1步，如果碰到的是<code>Function</code>的子类，继续往下</li><li>调用<code>Function</code>的<code>call</code>方法</li><li><code>Function</code>的<code>call</code>方法调用了Function的<code>forward</code>方法。</li><li><code>Function</code>的<code>forward</code>返回值</li><li><code>module</code>的<code>forward</code>返回值</li><li>在<code>module</code>的<code>call</code>进行<code>forward_hook</code>操作，然后返回值。</li></ol><h3 id="torch-hub"><a href="#torch-hub" class="headerlink" title="torch.hub"></a>torch.hub</h3><p>Pytorch Hub is a pre-trained model repository designed to facilitate(促进) research reproducibility.</p><h3 id="torch-nn-init"><a href="#torch-nn-init" class="headerlink" title="torch.nn.init"></a>torch.nn.init</h3><p><a href="https://blog.csdn.net/luo3300612/article/details/97675312">Pytorch 默认参数初始化</a></p><p>pytorch中的各种参数层(Linear、Conv2d、BatchNorm等)在<code>__init__</code>方法中定义后，不需要手动初始化就可以直接使用，这是因为Pytorch对这些层都会进行默认初始化。但是有时候我们需要自定义参数的初始化，就需要用到torch.nn.init。具体的不同初始化，可以查看pytorch官方文档。</p><h4 id="初始化函数"><a href="#初始化函数" class="headerlink" title="初始化函数"></a>初始化函数</h4><p><strong>初始化方式</strong></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#从正态分布N(mean,std^2)中取值填充张量</span><span class="token comment" spellcheck="true">#eg:w=torch.empty(3,5)</span><span class="token comment" spellcheck="true">#nn.init.normal_(w)</span></code></pre><p><strong><code>kaiming_uniform_</code></strong></p><p>Also known as He initialization.</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">kaiming_uniform_</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> a<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_in'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'leaky_relu'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># a用于leaky_relu</span>    <span class="token keyword">if</span> <span class="token number">0</span> <span class="token keyword">in</span> tensor<span class="token punctuation">.</span>shape<span class="token punctuation">:</span>        warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span><span class="token string">"Initializing zero-element tensors is a no-op"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> tensor    fan <span class="token operator">=</span> _calculate_correct_fan<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mode<span class="token punctuation">)</span>    gain <span class="token operator">=</span> calculate_gain<span class="token punctuation">(</span>nonlinearity<span class="token punctuation">,</span> a<span class="token punctuation">)</span>    std <span class="token operator">=</span> gain <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan<span class="token punctuation">)</span>    bound <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">)</span> <span class="token operator">*</span> std  <span class="token comment" spellcheck="true"># Calculate uniform bounds from standard deviation</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span>bound<span class="token punctuation">,</span> bound<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">_calculate_correct_fan</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mode<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 计算fan_in和fan_out并返回所要的</span>    mode <span class="token operator">=</span> mode<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>    valid_modes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'fan_in'</span><span class="token punctuation">,</span> <span class="token string">'fan_out'</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> mode <span class="token operator">not</span> <span class="token keyword">in</span> valid_modes<span class="token punctuation">:</span>        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Mode {} not supported, please use one of {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>mode<span class="token punctuation">,</span> valid_modes<span class="token punctuation">)</span><span class="token punctuation">)</span>    fan_in<span class="token punctuation">,</span> fan_out <span class="token operator">=</span> _calculate_fan_in_and_fan_out<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>    <span class="token keyword">return</span> fan_in <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">'fan_in'</span> <span class="token keyword">else</span> fan_out<span class="token keyword">def</span> <span class="token function">_calculate_fan_in_and_fan_out</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    dimensions <span class="token operator">=</span> tensor<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> dimensions <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">:</span>        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 这个tensor是w的tensor</span>    num_input_fmaps <span class="token operator">=</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    num_output_fmaps <span class="token operator">=</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    receptive_field_size <span class="token operator">=</span> <span class="token number">1</span>    <span class="token keyword">if</span> tensor<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">2</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># math.prod is not always available, accumulate the product manually</span>        <span class="token comment" spellcheck="true"># we could use functools.reduce but that is not supported by TorchScript</span>        <span class="token keyword">for</span> s <span class="token keyword">in</span> tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            receptive_field_size <span class="token operator">*=</span> s    fan_in <span class="token operator">=</span> num_input_fmaps <span class="token operator">*</span> receptive_field_size    fan_out <span class="token operator">=</span> num_output_fmaps <span class="token operator">*</span> receptive_field_size    <span class="token keyword">return</span> fan_in<span class="token punctuation">,</span> fan_out</code></pre><p>Xavier在tanh中表现的很好,但在Relu激活函数中表现的很差,所以何凯明提出了针对于relu的初始化方法。</p><p><code>kaiming_uniform_</code>从$u(-bound,bound)$中采样值填充张量,$bound=gain\cdot \sqrt{3/fan_{mode}}=\sqrt{\frac{6}{(1+a^2)fan_in}}$</p><p>$f_out=out_channels×kernel_size^2,f_in=in_channels×kernel_size^2$</p><p>$fan_in$可以保持前向传播的权重方差的数量级,$fan_out$可以保持反向传播的权重方差的数量级。</p><p><a href="https://blog.csdn.net/qq_41917697/article/details/116033589">理论分析</a></p><p><strong><code>calculate_gain(nonlinearity, param=None)</code></strong></p><p>输入激活函数的名字,返回对应的$gain$值,增益值$gain$是一个比例,来调控输入数量级和输出数量级之间的关系。</p><pre class=" language-python"><code class="language-python">gain <span class="token operator">=</span> nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>calculate_gain<span class="token punctuation">(</span><span class="token string">'leaky_relu'</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># leaky_relu with </span></code></pre><p>$nonlinearity\to gain$:</p><p>$Linear/identity=1,Conv{1,2,3}D=1,Sigmoid=1,tanh=5/3,Relu=\sqrt{2}$</p><p>$Leaky Relu=\sqrt{2/(1+negative_slope^2)},SELU=3/4$</p><p><strong><code>kaiming_normal_</code></strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">kaiming_normal_</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> a<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_in'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'leaky_relu'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token number">0</span> <span class="token keyword">in</span> tensor<span class="token punctuation">.</span>shape<span class="token punctuation">:</span>        warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span><span class="token string">"Initializing zero-element tensors is a no-op"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> tensor    fan <span class="token operator">=</span> _calculate_correct_fan<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mode<span class="token punctuation">)</span>    gain <span class="token operator">=</span> calculate_gain<span class="token punctuation">(</span>nonlinearity<span class="token punctuation">,</span> a<span class="token punctuation">)</span>    std <span class="token operator">=</span> gain <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan<span class="token punctuation">)</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token punctuation">)</span></code></pre><p>从$N~(0,std^2)$中采样值填充张量,$std=\frac{gain}{\sqrt{fan_mode}}$。</p><h4 id="参数层初始化"><a href="#参数层初始化" class="headerlink" title="参数层初始化"></a>参数层初始化</h4><p><strong>Linear</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">:</span> int<span class="token punctuation">,</span> out_features<span class="token punctuation">:</span> int<span class="token punctuation">,</span> bias<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                 device<span class="token operator">=</span>None<span class="token punctuation">,</span> dtype<span class="token operator">=</span>None<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> None<span class="token punctuation">:</span>    factory_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'device'</span><span class="token punctuation">:</span> device<span class="token punctuation">,</span> <span class="token string">'dtype'</span><span class="token punctuation">:</span> dtype<span class="token punctuation">}</span>    super<span class="token punctuation">(</span>Linear<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features    self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features    self<span class="token punctuation">.</span>weight <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">**</span>factory_kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> bias<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> <span class="token operator">**</span>factory_kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">'bias'</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> None<span class="token punctuation">:</span>    init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>        fan_in<span class="token punctuation">,</span> _ <span class="token operator">=</span> init<span class="token punctuation">.</span>_calculate_fan_in_and_fan_out<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>        bound <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan_in<span class="token punctuation">)</span> <span class="token keyword">if</span> fan_in <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token number">0</span>        init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token operator">-</span>bound<span class="token punctuation">,</span> bound<span class="token punctuation">)</span></code></pre><p>$a=\sqrt{5}$,所以$bound=\sqrt{1/fan_in}$,所以$w,b\sim U(-bound,bound)$</p><p><strong>Conv</strong></p><p>比如一个输入channel为3,输出channel为64,kernel size=3的卷积层,其权值即为一个3×64×3×3的向量,它会这样进行初始化：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>        fan_in<span class="token punctuation">,</span> _ <span class="token operator">=</span> init<span class="token punctuation">.</span>_calculate_fan_in_and_fan_out<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>        bound <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan_in<span class="token punctuation">)</span>        init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token operator">-</span>bound<span class="token punctuation">,</span> bound<span class="token punctuation">)</span></code></pre><p><strong>BatchNorm</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>reset_running_stats<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">:</span>        init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>        init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span></code></pre><p>$w\sim U(0,1),bias=0$</p><h4 id="网络初始化"><a href="#网络初始化" class="headerlink" title="网络初始化"></a>网络初始化</h4><p>在各种内置的网络模型中,初始化的方法也有不同。</p><p><strong>ResNet</strong></p><p>resnet在定义各层之后，pytorch官方代码的<code>__init__</code>方法会对不同的层进行手动的初始化.</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 卷积层使用kaiming_normal_</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_out'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>    <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token punctuation">(</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># BatchNorm、GroupNorm使用常数1和0</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span></code></pre><p><strong>VGG</strong></p><p>VGG的pytorch官方初始化如下:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_initialize_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_out'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> m<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">)</span>            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span></code></pre><h4 id="设置随机数种子"><a href="#设置随机数种子" class="headerlink" title="设置随机数种子"></a>设置随机数种子</h4><p><code>torch.manual_seed(seed) → torch._C.Generator</code></p><p>在神经网络中，参数默认是进行随机初始化的。如果不设置的话每次训练时的初始化都是随机的，导致结果不确定。如果设置初始化，则每次初始化都是固定的。pytorch默认使用何恺明的初始化</p><pre class=" language-python"><code class="language-python"><span class="token keyword">if</span> args<span class="token punctuation">.</span>seed <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>    　　    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 如果使用random的随机的话，就需要设置</span>    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 如果使用np.random的随机的话，</span>    torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#为CPU设置种子用于生成随机数,以使得结果是确定的</span>    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#为当前GPU设置随机种子</span>    cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment" spellcheck="true">#使用确定性卷积</span>    <span class="token comment" spellcheck="true">#顾名思义，将这个 flag 置为True的话，每次返回的卷积算法将是确定的，即默认算法。</span>    <span class="token comment" spellcheck="true">#如果配合上设置 Torch 的随机种子为固定值的话，应该可以保证每次运行网络的时候相同输入的输出是固定的</span>    <span class="token comment" spellcheck="true">#如果使用多GPU,需要</span>    <span class="token comment" spellcheck="true">#torch.cuda.manual_seed_all() 为所有GPU设置种子</span></code></pre><h3 id="torch-nn-functioal"><a href="#torch-nn-functioal" class="headerlink" title="torch.nn.functioal"></a>torch.nn.functioal</h3><p><a href="https://www.zhihu.com/question/66782101">nn与nn.functional的区别</a></p><p><code>interpolate</code></p><p>下/上采样输入到给定的大小或给定的 scale_factor。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>input<span class="token punctuation">,</span>                                size<span class="token operator">=</span>None<span class="token punctuation">,</span>                                scale_factor<span class="token operator">=</span>None<span class="token punctuation">,</span>                                mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">,</span>                                align_corners<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># </span>                                recompute_scale_factor<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p><code>upsample</code></p><p>将输入上采样到给定的大小或给定的scale_factor</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>upsample<span class="token punctuation">(</span>input<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#输入张量</span>                             size<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#输出大小</span>                             scale_factor<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#变换因子</span>                             mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#模式</span>                             align_corners<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p><code>avg_pool2d</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span>input<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#操作的Tensor</span>                               kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>None<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> count_include_pad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> divisor_override<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensor</code></pre><p><code>max_pool2d</code></p><p><code>torch.nn.functional.adaptive_max_pool2d(*args, **kwargs)</code></p><p>在由几个输入平面组成的输入信号上应用2D自适应最大池化。</p><p><code>cross_entropy</code></p><p>See <code>CrossEntropyLoss</code></p><p><code>grid_sample</code></p><pre class=" language-python"><code class="language-python">grid_sample<span class="token punctuation">(</span>input<span class="token punctuation">,</span> grid<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> padding_mode<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># input:input参数是输入特征图tensor，也就是特征图，可以是四维或者五维张量</span><span class="token comment" spellcheck="true"># 以四维形式为例(N,C,Hin,Win)，N可以理解为Batch_size,C可以理解为通道数，Hin和Win也就是特征图高和宽。</span><span class="token comment" spellcheck="true"># grid:包含输出特征图特征图的格网大小以及每个格网对应到输入特征图的采样点位，对应四维input，其张量形式为(N,Hout,Wout,2)，其中最后一维大小必须为2</span><span class="token comment" spellcheck="true"># 如果输入为五维张量，那么最后一维大小必须为3。为什么最后一维必须为2或者3？因为grid的最后一个维度实际上代表一个坐标(x,y)或者(xy,z)，</span><span class="token comment" spellcheck="true"># 对应到输入特征图的二维或三维特特征图的坐标维度，xy取值范围一般为[-1,1]，该范围映射到输入特征图的全图。</span><span class="token comment" spellcheck="true"># mode:选择采样方法，有三种内插算法可选，分别是'bilinear'双线性差值、'nearest'最邻近插值、'bicubic' 双三次插值.</span><span class="token comment" spellcheck="true"># padding_mode:为填充模式，即当(x,y)取值超过输入特征图采样范围，返回一个特定值，有'zeros' 、 'border' 、 'reflection'三种可选，一般用zero。</span><span class="token comment" spellcheck="true"># align_corners:为bool类型，指设定特征图坐标与特征值对应方式，设定为TRUE时，特征值位于像素中心。</span></code></pre><img src="/pytorch-xue-xi/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20220909153717935.png" alt="image-20220909153717935" style="zoom:67%;"><p>经典的应用:pytorch光流函数warp</p><p><strong>关键点:图片的坐标加上光流即为在输出点的坐标</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">warp</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> flo<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        warp an image/tensor (im2) back to im1, according to the optical flow        x: [B, C, H, W] (im2)        flo: [B, 2, H, W] flow        """</span>        B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># mesh grid </span>        xx <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>H<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>        yy <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>W<span class="token punctuation">)</span>        xx <span class="token operator">=</span> xx<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>H<span class="token punctuation">,</span>W<span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>B<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>        yy <span class="token operator">=</span> yy<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>H<span class="token punctuation">,</span>W<span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>B<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>        grid <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>xx<span class="token punctuation">,</span>yy<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        grid <span class="token operator">=</span> grid<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        vgrid <span class="token operator">=</span> Variable<span class="token punctuation">(</span>grid<span class="token punctuation">)</span> <span class="token operator">+</span> flo <span class="token comment" spellcheck="true"># B,2,H,W</span>        <span class="token comment" spellcheck="true">#图二的每个像素坐标加上它的光流即为该像素点对应在图一的坐标</span> <span class="token comment" spellcheck="true"># scale grid to [-1,1] </span> <span class="token comment" spellcheck="true">##2019 code</span>        vgrid<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2.0</span><span class="token operator">*</span>vgrid<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span>max<span class="token punctuation">(</span>W<span class="token number">-1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1.0</span>         <span class="token comment" spellcheck="true">#取出光流v这个维度，原来范围是0~W-1，再除以W-1，范围是0~1，再乘以2，范围是0~2，再-1，范围是-1~1</span>        vgrid<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2.0</span><span class="token operator">*</span>vgrid<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span>max<span class="token punctuation">(</span>H<span class="token number">-1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1.0</span> <span class="token comment" spellcheck="true">#取出光流u这个维度，同上</span>        vgrid <span class="token operator">=</span> vgrid<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#from B,2,H,W -&gt; B,H,W,2，为什么要这么变呢？是因为要配合grid_sample这个函数的使用</span>        output <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>grid_sample<span class="token punctuation">(</span>x<span class="token punctuation">,</span> vgrid<span class="token punctuation">,</span>align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        mask <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>grid_sample<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> vgrid<span class="token punctuation">,</span>align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">##2019 author</span>        mask<span class="token punctuation">[</span>mask<span class="token operator">&lt;</span><span class="token number">0.9999</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        mask<span class="token punctuation">[</span>mask<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment" spellcheck="true">##2019 code</span> <span class="token comment" spellcheck="true"># mask = torch.floor(torch.clamp(mask, 0 ,1))</span> <span class="token keyword">return</span> output<span class="token operator">*</span>mask</code></pre><h4 id="Vision-functions"><a href="#Vision-functions" class="headerlink" title="Vision functions"></a>Vision functions</h4><p><code>pad</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>input<span class="token punctuation">,</span>                        pad<span class="token punctuation">,</span>                        mode<span class="token operator">=</span><span class="token string">'constant'</span><span class="token punctuation">,</span>                        value<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span></code></pre><p>从最后一个维度开始pad,例如只pad最后一个维度,pad的形式是(padding_left, padding_right);pad最后两个维度,pad的形式是</p><p>(padding_left, padding_right, padding_top, padding_bottom);(padding_left,padding_right,padding_top,padding_bottom,</p><p>padding_front,padding_back)。</p><p>假设原始是(2,3,4)</p><p>padding_left,padding_right是pad最后一个维度,即4这个维度</p><p>padding_top,padding_bottom是pad倒数第二个维度,即3这个维度</p><p>padding_front,padding_back是pad倒数第三个维度,即2这个维度</p><h3 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h3><p><a href="https://zhuanlan.zhihu.com/p/94019888">optim教程</a></p><p>是一个实现各种优化算法的包。已经支持了最常用的方法，接口也足够通用，因此将来还可以轻松地集成更复杂的方法。<br>$$<br>\begin{split}\begin{aligned} w_1 &amp;\leftarrow \left(1- \eta\lambda \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\ w_2 &amp;\leftarrow \left(1- \eta\lambda \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}\end{split}<br>$$<br><strong>整个包就是解决这一步的,就是参数优化</strong></p><p>To use <code>torch.optim</code> you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.</p><p>创建optim对象时，要给它一个包含模型参数的的可迭代对象(所有的都应该是 Variable)，然后指定learning rate,weight decay等参数.</p><pre class=" language-python"><code class="language-python">optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span>var1<span class="token punctuation">,</span> var2<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">)</span></code></pre><p>注意：由于要把模型参数传给 optim ，所以如果要使用GPU时，要在把模型参数传给 optim之前写 model().cuda()，因为调用 .cuda() 前后不是一个参数对象，在此optimize期间，要保证 optimized parameters 在同一位置，不要 .cpu()或 .cuda() 乱用，注意下顺序 <strong>！！！有待验证！！！</strong></p><p>也支持为每个参数单独设置选项。若想这么做，不要直接传入Variable的iterable，而是传入dict的iterable。这种方法在对每层分别指定learning rate时很有用:</p><pre class=" language-python"><code class="language-python">optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span>                <span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>base<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">,</span>                <span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>classifier<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">}</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span></code></pre><p>上面这样写，表示 $model.base$ 的 $lr$ 是 1e-2，$model.classifier$ 的 lr 是 $1e-3$，$momentum=0.9$ 同时用于这两个参数</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Optimizer</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> defaults<span class="token punctuation">)</span><span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_log_api_usage_once<span class="token punctuation">(</span><span class="token string">"python.optimizer"</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>defaults <span class="token operator">=</span> defaults        self<span class="token punctuation">.</span>_hook_for_profile<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>params<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">"params argument given to the optimizer should be "</span>                            <span class="token string">"an iterable of Tensors or dicts, but got "</span> <span class="token operator">+</span>                            torch<span class="token punctuation">.</span>typename<span class="token punctuation">(</span>params<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>state <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>dict<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>param_groups <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># 在源码中有一个参数param_groups来存储params</span>        <span class="token comment" spellcheck="true"># 以resnet18为例,params是generator,list完之后维度为(64,),param_groups[0]的维度为torch.Size([64, 3, 7, 7])</span>        param_groups <span class="token operator">=</span> list<span class="token punctuation">(</span>params<span class="token punctuation">)</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>param_groups<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"optimizer got an empty parameter list"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 成为一个字典</span>        <span class="token keyword">if</span> <span class="token operator">not</span> isinstance<span class="token punctuation">(</span>param_groups<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dict<span class="token punctuation">)</span><span class="token punctuation">:</span>            param_groups <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> param_groups<span class="token punctuation">}</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> param_group <span class="token keyword">in</span> param_groups<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>add_param_group<span class="token punctuation">(</span>param_group<span class="token punctuation">)</span></code></pre><h4 id="torch-optim-Optimizer"><a href="#torch-optim-Optimizer" class="headerlink" title="torch.optim.Optimizer"></a>torch.optim.Optimizer</h4><p>所有优化器的基类。</p><p><code>zero_grad(set_to_none: bool = False)</code></p><p><a href="https://www.jb51.net/article/189433.htm">zero_grad教程</a></p><p>设置被优化的张量的梯度为0,显然，我们进行下一次batch梯度计算的时候，前一个batch的梯度计算结果，没有保留的必要了。所以在下一次梯度更新的时候，先使用optimizer.zero_grad把梯度信息设置为0。</p><p>唯一一个参数意思是不设为0而设置为None</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">zero_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> set_to_none<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        r<span class="token triple-quoted-string string">"""Sets the gradients of all optimized :class:`torch.Tensor` s to zero.        Args:            set_to_none (bool): instead of setting to zero, set the grads to None.                This will in general have lower memory footprint, and can modestly improve performance.                However, it changes certain behaviors. For example:                1. When the user tries to access a gradient and perform manual ops on it,                a None attribute or a Tensor full of 0s will behave differently.                2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\ s                are guaranteed to be None for params that did not receive a gradient.                3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None                (in one case it does the step with a gradient of 0 and in the other it skips                the step altogether).        """</span>        <span class="token keyword">if</span> <span class="token operator">not</span> hasattr<span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"_zero_grad_profile_name"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>_hook_for_profile<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>profiler<span class="token punctuation">.</span>record_function<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_zero_grad_profile_name<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> group <span class="token keyword">in</span> self<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>                <span class="token keyword">for</span> p <span class="token keyword">in</span> group<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                    <span class="token comment" spellcheck="true"># p.grad可以直接查看梯度，只有不是None的时候才设为0,是None的话保持None就行</span>                    <span class="token comment" spellcheck="true"># a = [None, None] a[0]=1 =&gt;a = [1, None]</span>                    <span class="token keyword">if</span> p<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                        <span class="token keyword">if</span> set_to_none<span class="token punctuation">:</span>                            p<span class="token punctuation">.</span>grad <span class="token operator">=</span> None                        <span class="token keyword">else</span><span class="token punctuation">:</span>                            <span class="token keyword">if</span> p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>grad_fn <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                                p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span>                            <span class="token keyword">else</span><span class="token punctuation">:</span>                                p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>                            p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>optimizer执行的两种方式:<code>optimizer.step()</code>和``optimizer.step(closure)`，</p><p>所有的optim 都实现了前一种方法，第一种方法会更新所有参数，这是大多数 optim 都支持的方法，只要损失反向传播后就可以调用此函数:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> input<span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 求出每一阶段的损失</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 更新参数</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>关于第二种：optimizer.step(closure) 一些优化算法，例如 Conjugate Gradient 和 LBFGS 需要重复多次计算，因此你需要传入一个 closure 去允许它们重新计算你的模型。这个closure 应当清空梯度，计算损失，然后返回</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> input<span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">closure</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span>closure<span class="token punctuation">)</span></code></pre><p><code>torch.optim.RMSprop</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>RMSprop<span class="token punctuation">(</span>params<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#用于优化的参数iterable或定义参数组的dicts</span>                    lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>                    alpha<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">,</span>                    eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span>                    weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#权重衰减</span>                    momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#动量</span>                    centered<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p><code>torch.optim.SGD</code></p><p>实现随机梯度下降(可选动量)。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 待优化参数的可迭代对象或者是定义了参数组的dict</span>                lr<span class="token operator">=</span><span class="token operator">&lt;</span>required parameter<span class="token operator">&gt;</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#学习率</span>                momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#动量</span>                dampening<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#</span>                weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#权重衰减</span>                nesterov<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> loss_fn<span class="token punctuation">(</span>model<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 随机梯度下降,就比较依赖于数据的随机程度,如果不对数据进行打乱处理,可能异常值集中在数据某一块,会对算法收敛拟合造成干扰。</span></code></pre><p><code>torch.optim.Adam</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>params<span class="token punctuation">,</span>                 lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span>                 betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#</span>                 eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span>                 weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                 amsgrad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><h3 id="How-to-adjust-learning-rate"><a href="#How-to-adjust-learning-rate" class="headerlink" title="How to adjust learning rate?"></a>How to adjust learning rate?</h3><p><code>torch.optim.lr_scheduler</code> provides several methods to adjust the learning rate based on the number of epochs. <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau"><code>torch.optim.lr_scheduler.ReduceLROnPlateau</code></a> allows dynamic learning rate reducing based on some validation measurements.</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Learning rate scheduling should be applied after optimizer’s update:</span>model <span class="token operator">=</span> <span class="token punctuation">[</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>optimizer <span class="token operator">=</span> SGD<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>scheduler <span class="token operator">=</span> ExponentialLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> input<span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Most learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). </span><span class="token comment" spellcheck="true"># The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it.</span>model <span class="token operator">=</span> <span class="token punctuation">[</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>optimizer <span class="token operator">=</span> SGD<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>scheduler1 <span class="token operator">=</span> ExponentialLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>scheduler2 <span class="token operator">=</span> MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> input<span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    scheduler1<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    scheduler2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># In many places in the documentation, we will use the following template to refer to schedulers algorithms.</span>scheduler <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    train<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    validate<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><code>lr_scheduler.LambdaLR</code></p><p>将每个参数组的学习率设置为初始 lr 乘以给定函数。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Assuming optimizer has two groups.</span>lambda1 <span class="token operator">=</span> <span class="token keyword">lambda</span> epoch<span class="token punctuation">:</span> epoch <span class="token operator">//</span> <span class="token number">30</span>lambda2 <span class="token operator">=</span> <span class="token keyword">lambda</span> epoch<span class="token punctuation">:</span> <span class="token number">0.95</span> <span class="token operator">**</span> epochscheduler <span class="token operator">=</span> LambdaLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> lr_lambda<span class="token operator">=</span><span class="token punctuation">[</span>lambda1<span class="token punctuation">,</span> lambda2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    train<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    validate<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="torch-utils-tensorboard"><a href="#torch-utils-tensorboard" class="headerlink" title="torch.utils.tensorboard"></a>torch.utils.tensorboard</h3><p>一旦你安装了TensorBoard，这些工具可以让你将PyTorch模型和指标记录到一个目录中，以便在TensorBoard UI中可视化。标量、图像、直方图、图形和嵌入可视化都支持PyTorch模型和张量以及Caffe2 网络和blobs。</p><p>安装<code>pip install tensorboardX</code> 还需要安装tensorflow <code>pip install tensorflow</code></p><p>除此之外还需要设置隧道映射,多换几个端口试试</p><p><a href="https://blog.csdn.net/bigbennyguo/article/details/87956434">tensorboardX介绍</a></p><h4 id="SummaryWriter"><a href="#SummaryWriter" class="headerlink" title="SummaryWriter"></a>SummaryWriter</h4><p><code>CLASS torch.utils.tensorboard.writer.SummaryWriter</code></p><p>直接将条目写入log_dir中的事件文件中，以供TensorBoard使用。</p><p>SummaryWriter类提供了一个高级API，用于在<strong>给定目录中创建事件文件并向其添加摘要和事件</strong>。该类异步更新文件内容。这<strong>允许训练程序调用方法直接从训练循环向文件添加数据，而不会减慢训练速度。</strong></p><p><code>__init__</code>创建一个SummaryWriter，将事件和摘要写入事件文件中。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter__init__<span class="token punctuation">(</span>log_dir<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#保存目录位置，默认值是'run/CURRENT_DATETIME_HOSTNAME',所以每次运行后都会更改，日期时间肯定会变</span>         comment<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#注解添加到默认log_dir的后缀，若log_dir被指定，则此参数不起作用。</span>         purge_step<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#</span>         max_queue<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#在一个'add'调用强制刷新磁盘之前，挂起事件和汇总的队列的大小。默认为10项。</span>         flush_secs<span class="token operator">=</span><span class="token number">120</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将挂起事件和摘要刷新到磁盘的频率(以秒为单位)。默认每120s一次</span>         filename_suffix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#添加到log_dir目录中所有事件文件名的后缀。</span></code></pre><p>SummaryWriter类是您记录TensorBoard使用和可视化数据的主要入口。例如:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torchvision<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms<span class="token comment" spellcheck="true"># Writer will output to ./runs/ directory by default</span>writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token punctuation">)</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>trainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">'mnist_train'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet50<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Have ResNet model take in grayscale rather than RGB</span>model<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> next<span class="token punctuation">(</span>iter<span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span><span class="token punctuation">)</span>grid <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">)</span>writer<span class="token punctuation">.</span>add_image<span class="token punctuation">(</span><span class="token string">'images'</span><span class="token punctuation">,</span> grid<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>writer<span class="token punctuation">.</span>add_graph<span class="token punctuation">(</span>model<span class="token punctuation">,</span> images<span class="token punctuation">)</span>writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这可以通过TensorBoard进行可视化，TensorBoard安装和运行:</span>pip install tensorboardtensorboard <span class="token operator">-</span><span class="token operator">-</span>logdir<span class="token operator">=</span><span class="token comment" spellcheck="true">#包含记录文件的文件夹路径</span><span class="token comment" spellcheck="true">#记录文件名类似如下:events.out.tfevents.1616293286.vision806-desktop</span><span class="token comment" spellcheck="true"># 默认的终端号是6006，要是一台服务器多人使用，很麻烦，可以指定端口，输入</span>tensorboard <span class="token operator">-</span><span class="token operator">-</span>logdir<span class="token operator">=</span>logs <span class="token operator">-</span><span class="token operator">-</span>port<span class="token operator">=</span><span class="token number">6007</span></code></pre><p>一个实验可以记录很多信息。为了避免UI的混乱和更好的结果聚类，我们可以通过分级命名来对图进行分组。例如， Loss/train 和Loss/test 被分组在一起，Accuracy/train和Accuarcy/test被分组在一起。</p><p><code>add_scalar</code></p><pre class=" language-python"><code class="language-python">add_scalar<span class="token punctuation">(</span>tag<span class="token punctuation">:</span>string<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 数据标识,train/loss,分成了两级</span>           scalar_value<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 数字常量值,一定要是float类型</span>           <span class="token comment" spellcheck="true"># 如果是 Pytorch scalar tensor,则需要调用.item()方法获取其数值。</span>           global_step<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 训练的step,作为横坐标</span>           walltime<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 记录发生的时间，默认为time.time()</span></code></pre><p>添加标量,一般使用其<strong>记录训练过程的 loss、accuracy、learning rate 等数值的变化，直观地监控训练过程。</strong></p><p>如果tag相同标量会被放在一个图里，如<code>writer.add_scalar(‘y=2x’, i * 2, i)</code>和<code>writer.add_scalar('y=2x, i * i, i)</code></p><p><code>add_image</code></p><pre class=" language-python"><code class="language-python">add_image<span class="token punctuation">(</span>self<span class="token punctuation">,</span> tag<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#就是保存图片的名称</span>          img_tensor<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#图形:torch.Tensor numpy.array or string</span>          global_step<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 训练的step 作为横坐标</span>          walltime<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 记录发生的时间，默认为time.time()</span>          dataformats<span class="token operator">=</span>‘CHW’<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#默认为CHW tensor是CHW numpy是HWC</span></code></pre><p>也可以画特征图的变化</p><h3 id="visdom"><a href="#visdom" class="headerlink" title="visdom"></a>visdom</h3><p>必须提前开启:</p><p><code>python -m visdom server -p 10086</code></p><h3 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h3><p>PyTorch数据加载工具的核心是torch.utils.data.DataLoader类。它表示一个数据集上的Python可迭代对象。</p><p>pytorch输入数据PipeLine一般遵循一个“三步走”的策略，一般pytorch 的数据加载到模型的操作顺序是这样的：<br>① 创建一个 Dataset 对象。必须实现<code>__len__()</code>、<code>__getitem__()</code>这两个方法，这里面会用到transform对数据集进行扩充。<br>② 创建一个 DataLoader 对象。它是对DataSet对象进行迭代的，一般不需要事先里面的其他方法了。<br>③ 循环遍历这个 DataLoader 对象。将img, label加载到模型中进行训练</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> MyDataset<span class="token punctuation">(</span><span class="token punctuation">)</span>           <span class="token comment" spellcheck="true"># 第一步：构造Dataset对象</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第二步：通过DataLoader来构造迭代对象</span>num_epoches <span class="token operator">=</span> <span class="token number">100</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_epoches<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># 第三步：逐步迭代数据</span>    <span class="token keyword">for</span> img<span class="token punctuation">,</span> label <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 训练代码</span></code></pre><h4 id="Map-style-datasets"><a href="#Map-style-datasets" class="headerlink" title="Map-style datasets"></a><strong>Map-style datasets</strong></h4><p>map风格的数据集是一个实现<code>__getitem__()</code>和<code>__len__()</code>协议的数据集，并表示从(可能非整数)索引/键到数据样本的映射。</p><p>例如，当使用dataset[idx]访问这样的数据集时，可以从磁盘上的文件夹中读取idx-th映像及其对应的标签。</p><p>查看Dataset了解更多细节。</p><h4 id="Iterable-style-datasets"><a href="#Iterable-style-datasets" class="headerlink" title="Iterable-style datasets"></a>Iterable-style datasets</h4><p>iterable风格的数据集是IterableDataset的一个子类的实例，它实现了<code>__iter__()</code>协议，并表示数据样本上的一个iterable。这种类型的数据集特别适合于这样的情况:随机读取代价很高，甚至不太可能，而且批大小取决于所获取的数据。</p><p>例如，当调用iter(dataset)时，这样的数据集可以返回从数据库、远程服务器甚至实时生成的日志读取的数据流。</p><p>查看IterableDataset了解更多细节。</p><h4 id="Data-Loading-Order-and-Sampler"><a href="#Data-Loading-Order-and-Sampler" class="headerlink" title="Data Loading Order and Sampler"></a>Data Loading Order and Sampler</h4><h4 id="Memory-Pinning"><a href="#Memory-Pinning" class="headerlink" title="Memory Pinning"></a>Memory Pinning</h4><p>pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。</p><p>主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。</p><p>而显卡中的显存全部是锁页内存！</p><p>当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。</p><h4 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a>torch.utils.data.DataLoader</h4><p>数据加载程序。<strong>组合数据集dataset和采样器sampler</strong>,并提供给定数据集上的迭代。</p><p>在训练模型时使用到此函数，用来把训练数据分成多个小组，此函数每次抛出一组数据。直至把所有的数据都抛出。就是做一个数据的初始化。</p><p>DataLoader支持map-style和iterable-style的数据集，具有单进程或多进程加载、自定义加载顺序和可选的自动批处理(整理)和内存固定.</p><p><strong>DataLoader本质是一个可迭代对象，所以:</strong></p><ol><li>可以使用<code>for inputs, labels in dataloaders</code>进行可迭代对象的访问</li><li>先使用iter对dataloader进行第一步包装，使用<code>iter(dataloader)</code>返回的是一个迭代器，然后就可以可以使用next访问了。Dataloader的<code>__iter__()</code>根据num_workers的数量返回单线程或多线程的迭代器</li><li>我们一般不需要再自己去实现DataLoader的方法了，只需要在构造函数中指定相应的参数即可，比如常见的batch_size，shuffle等等参数。所以使用DataLoader十分简洁方便。</li><li>DataLoader实际上一个较为高层的封装类，它的功能都是通过更底层的<code>_DataLoader</code>来完成的，但是<code>_DataLoader</code>类较为低层，这里就不再展开叙述了。DataLoaderIter就是<code>_DataLoaderIter</code>的一个框架, 用来传给<code>_DataLoaderIter</code> 一堆参数, 并把自己装进DataLoaderIter 里。</li></ol><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>Dataset<span class="token punctuation">[</span>T_co<span class="token punctuation">]</span><span class="token punctuation">,</span>                            batch_size<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>int<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                            shuffle<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>                    <span class="token comment" spellcheck="true">#表示每一个epoch之后是否对样本进行随机打乱,所有的先打乱,再取batch.具体解析见下文</span>                            sampler<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>Sampler<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                            <span class="token comment" spellcheck="true">#自定义从数据集中抽取样本的策略，如果指定这个参数，那么shuffle必须为False。</span>                            batch_sampler<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>Sampler<span class="token punctuation">[</span>Sequence<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                            num_workers<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#要使用多少子进程装载数据。0表示数据将在主进程中加载。</span>                            collate_fn<span class="token punctuation">:</span> Callable<span class="token punctuation">[</span>List<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">,</span> Any<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                   <span class="token comment" spellcheck="true"># 合并一个list的samples以形成mini-batch的Tensors</span>                   <span class="token comment" spellcheck="true"># collate_fn这个函数的输入就是一个list，list的长度是一个batch size，list中的每个元素都是__getitem__得到的结果。</span>                            pin_memory<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#是否锁页内存</span>                            drop_last<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#如果数据集大小不能被批大小整除，则设置为True可删除最后一个不完整的批处理。</span><span class="token comment" spellcheck="true">#如果为False，并且dataset的大小不能被batch-size整除，那么最后一批将变小。(默认值:False)</span>                            timeout<span class="token punctuation">:</span> float <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>           <span class="token comment" spellcheck="true">#timeout (numeric, optional): 如果是正数，表明等待从worker进程中收集一个batch等待的时间，若超出设定的时间还没有收集到，那就不收集这个内容了。这个numeric应总是大于等于0。默认为0</span>                            worker_init_fn<span class="token punctuation">:</span> Callable<span class="token punctuation">[</span>int<span class="token punctuation">,</span> None<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                            multiprocessing_context<span class="token operator">=</span>None<span class="token punctuation">,</span>                            generator<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span>                            prefetch_factor<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>                            persistent_workers<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""    批训练，把数据变成一小批一小批数据进行训练。    DataLoader就是用来包装所使用的数据，每次抛出一批数据"""</span><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> DataBATCH_SIZE <span class="token operator">=</span> <span class="token number">5</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 把数据放在数据库中</span>torch_dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>    <span class="token comment" spellcheck="true"># 从数据库中每次抽出batch size个样本</span>    dataset<span class="token operator">=</span>torch_dataset<span class="token punctuation">,</span>    batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span>    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">show_batch</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># training</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"steop:{}, batch_x:{}, batch_y:{}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>step<span class="token punctuation">,</span> batch_x<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    show_batch<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#loader在这里就是迭代器,在for里面一旦取用自动更新</span><span class="token comment" spellcheck="true">#for data in testloader: 这里的data就是一个batch的数据,是一个tuple,既有train_X,也有train_Y.</span></code></pre><p><a href="https://blog.csdn.net/qq_27825451/article/details/96130126">pytorch数据预处理三剑客之Dataset，DataLoader，Transform</a></p><p>上面这篇讲的非常好</p><p><a href="https://www.cnblogs.com/hesse-summer/p/11343870.html">num_worker的作用</a></p><p><strong>为什么机器学习需要打乱数据?</strong></p><p>防止数据按一定规律排列,这样神经网络学习时会把这种规律当做一种特征学习,从而过拟合。这样做使得数据更接近于真实分布。</p><p>比如随机梯度下降,就比较依赖于数据的随机程度,如果不对数据进行打乱处理,可能异常值集中在数据某一块,会对算法收敛拟合造成干扰。</p><h4 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h4><p>表示一个Dataset的抽象类。</p><p><strong>所有表示从键到数据样本映射的数据集都应该子类化它</strong>。所有子类都应该覆盖<code>__getitem__()</code>，从而支持获取一个给定键的数据样本。子类也可以选择性地覆盖<code>__len__()</code>，许多Sampler实现和DataLoader默认选项都希望它返回数据集的大小。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#dataset的抽象父类定义如下</span><span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> T_co<span class="token punctuation">:</span>        <span class="token keyword">raise</span> NotImplementedError<span class="token keyword">def</span> <span class="token function">__add__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">:</span> <span class="token string">'Dataset[T_co]'</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token string">'ConcatDataset[T_co]'</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> ConcatDataset<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">,</span> other<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><code>__init__(self)</code>:主要是数据的获取，比如从某个文件中获取</p><p><code>__len__(self)</code>:整个数据集的长度</p><p><code>__getitem__(self,index)</code>:这个是最重要的，一般情况下它会包含以下几个业务需要处理</p><ol><li>比如如果我们需要在读取数据的同时对图像进行增强的话，当然，图像增强的方法可以使用Pytorch内置的图像增强方式，也可以使用自定义或者其他的图像增强库,这个很灵活。</li><li>在Pytorch中得到的图像必须是tensor，也就是说我们必须要将数据格式转化成pytorch的tensor格式才行。</li></ol><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># coding: utf-8</span><span class="token keyword">import</span> os<span class="token keyword">import</span> torch<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image<span class="token keyword">import</span> cv2<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToTensor<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms<span class="token keyword">import</span> random<span class="token keyword">class</span> <span class="token class-name">LaneDataSet</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataset<span class="token punctuation">,</span> transform<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        param：            detaset: 实际上就是tusimple数据集的三个文本文件train.txt、val.txt、test.txt三者的文件路径            transform: 决定是否进行变换,它其实是一个函数或者是几个函数的组合        构造三个列表，存储每一张图片的文件路径                  '''</span>        self<span class="token punctuation">.</span>_gt_img_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>_gt_label_binary_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>_gt_label_instance_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform        <span class="token keyword">with</span> open<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> file<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 打开其实是那个 training下面的那个train.txt 文件</span>            <span class="token keyword">for</span> _info <span class="token keyword">in</span> file<span class="token punctuation">:</span>                info_tmp <span class="token operator">=</span> _info<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info_tmp<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info_tmp<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info_tmp<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_shuffle<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">_shuffle</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 将gt_image、binary_image、instance_image三者所对应的图片路径组合起来，再进行随机打乱</span>    c <span class="token operator">=</span> list<span class="token punctuation">(</span>zip<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">)</span><span class="token punctuation">)</span>    random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>c<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_instance_list <span class="token operator">=</span> zip<span class="token punctuation">(</span><span class="token operator">*</span>c<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">assert</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">)</span> \               <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 读取图片</span>    img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>IMREAD_COLOR<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#真实图片 (720,1280,3)</span>    label_instance_img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>IMREAD_UNCHANGED<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># instance图片 （720,1280）</span>    label_binary_img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>IMREAD_GRAYSCALE<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#binary图片 （720,1280)</span>    <span class="token comment" spellcheck="true"># optional transformations,裁剪成（256,512）</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">:</span>        img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        label_binary_img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>label_binary_img<span class="token punctuation">)</span>        label_instance_img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>label_instance_img<span class="token punctuation">)</span>    img <span class="token operator">=</span> img<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#（3,720,1280） 这里都没有问题</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>img<span class="token punctuation">,</span> label_binary_img<span class="token punctuation">,</span> label_instance_img<span class="token punctuation">)</span>  </code></pre><h4 id="torch-utils-data-TensorDataset-tensors"><a href="#torch-utils-data-TensorDataset-tensors" class="headerlink" title="torch.utils.data.TensorDataset(*tensors)"></a>torch.utils.data.TensorDataset(*tensors)</h4><p>TensorDataset 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能。该类通过每一个 tensor 的第一个维度进行索引。因此，该类中的 tensor 第一维度必须相等。</p><p>举个例子,六张图片,六个label,维度分别是(6,H,W,C)和(6,)</p><h4 id="torch-utils-data-Concat-datasets"><a href="#torch-utils-data-Concat-datasets" class="headerlink" title="torch.utils.data.Concat(datasets)"></a>torch.utils.data.Concat(datasets)</h4><p>连接多个数据集产生一个新的数据集,该类用于组装不同的现有数据集。</p><p>datasets:(<em>sequence</em>) – List of datasets to be concatenated</p><h4 id="torch-utils-data-Subset"><a href="#torch-utils-data-Subset" class="headerlink" title="torch.utils.data.Subset"></a>torch.utils.data.Subset</h4><p><code>torch.utils.data.random_split</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>random_split<span class="token punctuation">(</span>dataset<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>Dataset<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#要被划分的数据集</span>                              lengths<span class="token punctuation">:</span> Sequence<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#要生成的切片长度</span>                              generator<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>Generator<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">&lt;</span>torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>Generator object<span class="token operator">&gt;</span><span class="token punctuation">)</span> → List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>Subset<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#用于随机排列的生成器。</span><span class="token comment" spellcheck="true">#eg:random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))</span><span class="token comment" spellcheck="true">#eg:train, val = random_split(dataset, [n_train, n_val])</span></code></pre><h4 id="torch-utils-data-Sampler"><a href="#torch-utils-data-Sampler" class="headerlink" title="torch.utils.data.Sampler"></a>torch.utils.data.Sampler</h4><p>所有采样器的基类。</p><p>每个采样器子类都必须提供一个<code>__iter__()</code>方法，提供一种遍历数据集元素索引的方法，以及一个<code>__len__()</code>方法，该方法返回返回的迭代器的长度。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#pytorch采样器</span><span class="token keyword">class</span> <span class="token class-name">Sampler</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">class</span> <span class="token class-name">SequentialSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">RandomSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">SubsetRandomSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">WeightedRandomSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">BatchSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#Dataloader中的采样器</span><span class="token keyword">if</span> sampler <span class="token keyword">is</span> None<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># give default samplers</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>_dataset_kind <span class="token operator">==</span> _DatasetKind<span class="token punctuation">.</span>Iterable<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># See NOTE [ Custom Samplers and IterableDataset ]</span>        sampler <span class="token operator">=</span> _InfiniteConstantSampler<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># map-style</span>        <span class="token keyword">if</span> shuffle<span class="token punctuation">:</span>            sampler <span class="token operator">=</span> RandomSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            sampler <span class="token operator">=</span> SequentialSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span></code></pre><h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><p><a href="https://www.cnblogs.com/houjun/p/10406640.html">torchvision简介</a></p><p>该库是Pytorch项目的一部分。安装pytorch时，torchvision独立于torch。torchvision包由流行的数据集（torchvision.datasets）、模型架构(torchvision.models)和用于计算机视觉的常见图像转换组成t(torchvision.transforms)。</p><h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionmnist <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">"path/to/mnist/"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transforms<span class="token punctuation">,</span> target_transform<span class="token operator">=</span>None<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token punctuation">:</span> str<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#存放training.pt和test.pt的root directory</span>                           train<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#从training.pt创建数据集,否则从test.pt</span>                           transform<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>Callable<span class="token punctuation">,</span> NoneType<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#转换操作</span>                           target_transform<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>Callable<span class="token punctuation">,</span> NoneType<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                           download<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span> → None<span class="token comment" spellcheck="true">#下载到本地并存放到root directory</span></code></pre><h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionvgg16 <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#pretrained=True加载别人预训练好的模型,否则就是权重随机初始化的模型</span></code></pre><h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p><a href="https://blog.csdn.net/qq_38410428/article/details/94719553">transforms的二十二个方法</a></p><p><a href="https://blog.csdn.net/qq_27825451/article/details/97145592?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control">第三篇-pytorch数据预处理三剑客</a></p><p>变换是常见的图像变换。它们可以使用<code>Compose</code>链接在一起。此外，还有<code>torchvision.transforms.functional</code>模块。<strong>Functional transforms可以对转换进行细粒度控制</strong>。如果您必须构建更复杂的转换管道（例如，在分割任务的情况下），这将非常有用。</p><p>所有的转换都接受<strong>PIL Image，Tensor Image或batch of tensor Image</strong>作为输入。Tensor Image是一个具有(C, H, W)形状的张量，其中C是通道，H和W是图像的高度和宽度。Batch of Tensor Images是(B, C, H, W) 形状的张量，其中B是一个Batch中图像的个数。对Batch of Tensor Image应用确定或随机变换，就能对这批图像的所有图像进行相同的变换。</p><p>注意事项：</p><p>（1）transfroms中的数据增强操作针对的是pillow的Image图像格式，而我们很多时候在使用opencv读进去的又是ndarray格式，所以需要第一步先将ndarray转化成Image格式，即<code>transforms.ToPILImage()</code>.</p><p>（2）但是我们后需要的数据又是需要ndarray格式或者是tensor格式，故而有需要将Image转换回来，即<code>transforms.ToTensor()</code>。</p><p>自从v0.8.0以来，所有的<strong>随机转换都使用torch默认的随机生成器来采样随机参数</strong>。这是一个破坏后项兼容的更改，后向兼容是指向低版本兼容，用户应该设置随机状态如下:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Previous versions</span><span class="token comment" spellcheck="true"># import random</span><span class="token comment" spellcheck="true"># random.seed(12)</span><span class="token comment" spellcheck="true"># Now</span><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">17</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#请记住，pytorch随机数生成器和Python随机数生成器的相同种子不会产生相同的结果。</span></code></pre><p><code>torchvision.transforms.Compose(transforms)</code><br>将几个变换组合在一起</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Compose</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transforms<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>transforms <span class="token operator">=</span> transforms    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#self.transform(img)实际上是一个函数调用形式,果然实现了__call__()s</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>transforms<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#从这可以看出,传入的是一个容器,列表就可以</span>            img <span class="token operator">=</span> t<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token keyword">return</span> img <span class="token comment" spellcheck="true">#返回的直接就是img,注意,处理的是单张img,返回的也是单张img</span>    <span class="token keyword">def</span> <span class="token function">__repr__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        format_string <span class="token operator">=</span> self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__ <span class="token operator">+</span> <span class="token string">'('</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>transforms<span class="token punctuation">:</span>            format_string <span class="token operator">+=</span> <span class="token string">'\n'</span>            format_string <span class="token operator">+=</span> <span class="token string">'    {0}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>t<span class="token punctuation">)</span>        format_string <span class="token operator">+=</span> <span class="token string">'\n)'</span>        <span class="token keyword">return</span> format_string<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>     transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>     transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h4 id="裁剪-Crop"><a href="#裁剪-Crop" class="headerlink" title="裁剪(Crop)"></a>裁剪(Crop)</h4><h4 id="翻转和旋转-Flip-and-Rotation"><a href="#翻转和旋转-Flip-and-Rotation" class="headerlink" title="翻转和旋转(Flip and Rotation)"></a>翻转和旋转(Flip and Rotation)</h4><pre class=" language-python"><code class="language-python"></code></pre><h4 id="图像变换-resize"><a href="#图像变换-resize" class="headerlink" title="图像变换(resize)"></a>图像变换(resize)</h4><pre class=" language-python"><code class="language-python">torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 1.转换通道顺序:HWC-&gt;CHW</span><span class="token comment" spellcheck="true"># 2.将PIL Image或者 ndarray 转换为tensor float</span><span class="token comment" spellcheck="true"># 3.归一化至[0-1] 注意事项：归一化至[0-1]是直接除以255，若自己的ndarray数据尺度有变化，则需要自行修改。</span>torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token punctuation">,</span> std<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#用平均值和标准偏差归一化张量图像。给定mean：(M1,…,Mn)和std：(S1,…,Sn)对于n通道，此变换将标准化输入的每个通道，torch.*Tensor即 input[channel] = (input[channel] - mean[channel]) / std[channel]</span><span class="token comment" spellcheck="true">#mean(sequence)-每个通道的均值序列。</span><span class="token comment" spellcheck="true">#std(sequence)-每个通道的标准偏差序列。</span>torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToPILImage<span class="token punctuation">(</span>mode<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape H x W x C to a PIL Image while preserving the value range.</span></code></pre><p>对transforms操作，使数据增强更灵活</p><p>Normalize参数解惑<a href="https://blog.csdn.net/xys430381_1/article/details/85724668">https://blog.csdn.net/xys430381_1/article/details/85724668</a></p><h4 id="functional"><a href="#functional" class="headerlink" title="functional"></a>functional</h4><p><code>torchvision.transforms.functional.adjust_gamma(img: torch.Tensor, gamma: float, gain: float = 1) → torch.Tensor</code></p><p>对图片进行gamma校正<br>$$<br>I_{out}=255\cdot gain \cdot (\frac{I_{in}}{255})^y<br>$$<br>img:PIL Image或Tensor </p><h3 id="torchvision-utils"><a href="#torchvision-utils" class="headerlink" title="torchvision.utils"></a>torchvision.utils</h3><p><code>torchvision.utils.make_grid()</code></p><p>制作图像网格</p><pre class=" language-python"><code class="language-python">torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>tensor<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true">#4D mini-batch Tensor of shape (B x C x H x W) or a list of images all of the same size.</span>                            nrow<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true">#网格中每一行中显示的图像数,最终尺寸为(B/nrow, nrow)</span>                            padding<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true">#子图像与子图像之间的pad有多宽。</span>                            normalize<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true">#If True, 归一化图像到(0, 1)区间</span>                            value_range<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> int<span class="token punctuation">]</span><span class="token punctuation">,</span> NoneType<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>              <span class="token comment" spellcheck="true"># 用来normalize</span>                            scale_each<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>                            pad_value<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>                            <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> → torch<span class="token punctuation">.</span>Tensor</code></pre><p><code>torchvision.utils.save_image()</code></p><h2 id="Docs"><a href="#Docs" class="headerlink" title="Docs"></a>Docs</h2><h3 id="Features-for-large-scale-deployments"><a href="#Features-for-large-scale-deployments" class="headerlink" title="Features for large-scale deployments"></a>Features for large-scale deployments</h3><h4 id="API-usage-logging"><a href="#API-usage-logging" class="headerlink" title="API usage logging"></a>API usage logging</h4><p>When running in a broader ecosystem, for example in managed job scheduler, it’s often useful to track which binaries invoke particular PyTorch APIs. There exists simple instrumentation injected at several important API points that triggers a given callback. Because usually PyTorch is invoked in <em>one-off 一次性的</em> python scripts, the callback fires only once for a given process for each of the APIs.</p><p>Note for developers: new API trigger points can be added in code with <code>C10_LOG_API_USAGE_ONCE("my_api")</code> in C++ or <code>torch._C._log_api_usage_once("my.api")</code> in Python.</p><p><code>eg:torch._C._log_api_usage_once("python.optimizer")</code></p><p><strong>回调函数:被中间函数回调的函数</strong></p><blockquote><p>作者：no.body<br>链接：<a href="https://www.zhihu.com/question/19801131/answer/27459821">https://www.zhihu.com/question/19801131/answer/27459821</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p><strong>什么是回调函数？</strong></p><p>我们绕点远路来回答这个问题。</p><p>编程分为两类：系统编程（system programming）和应用编程（application programming）。所谓系统编程，简单来说，就是编写<strong>库</strong>；而应用编程就是利用写好的各种库来编写具某种功用的程序，也就是<strong>应用</strong>。系统程序员会给自己写的库留下一些接口，即API（application programming interface，应用编程接口），以供应用程序员使用。所以在抽象层的图示里，库位于应用的底下。</p><p>当程序跑起来时，一般情况下，应用程序（application program）会时常通过API调用库里所预先备好的函数。但是有些库函数（library function）却要求应用先传给它一个函数，好在合适的时候调用，以完成目标任务。这个被传入的、后又被调用的函数就称为<strong>回调函数</strong>（callback function）。</p><p>打个比方，有一家旅馆提供叫醒服务，但是要求旅客自己决定叫醒的方法。可以是打客房电话，也可以是派服务员去敲门，睡得死怕耽误事的，还可以要求往自己头上浇盆水。这里，“叫醒”这个行为是旅馆提供的，相当于库函数，但是叫醒的方式是由旅客决定并告诉旅馆的，也就是回调函数。而旅客告诉旅馆怎么叫醒自己的动作，也就是把回调函数传入库函数的动作，称为<strong>登记回调函数</strong>（to <strong>register a callback function</strong>）。如下图所示（图片来源：维基百科）：</p><p><img src="/pytorch-xue-xi/new1\jianguoyun_posts\pytorch学习\image-20210909144400210.png" alt="回调函数"></p><p>可以看到，回调函数通常和应用处于同一抽象层（因为传入什么样的回调函数是在应用级别决定的）。而回调就成了一个高层调用底层，底层再<strong>回</strong>过头来<strong>调</strong>用高层的过程。（我认为）这应该是回调最早的应用之处，也是其得名如此的原因。</p><p><strong>回调机制的优势</strong></p><p>从上面的例子可以看出，回调机制提供了非常大的灵活性。请注意，从现在开始，我们把图中的库函数改称为<strong>中间函数</strong>了，这是因为回调并不仅仅用在应用和库之间。任何时候，只要想获得类似于上面情况的灵活性，都可以利用回调。</p><p>这种灵活性是怎么实现的呢？乍看起来，回调似乎只是函数间的调用，但仔细一琢磨，可以发现两者之间的一个关键的不同：在回调中，我们利用某种方式，把回调函数像参数一样传入中间函数。可以这么理解，在传入一个回调函数之前，中间函数是不完整的。换句话说，程序可以在运行时，通过登记不同的回调函数，来决定、改变中间函数的行为。这就比简单的函数调用要灵活太多了。请看下面这段Python写成的回调的简单示例：</p><p><code>even.py</code></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#回调函数1</span><span class="token comment" spellcheck="true">#生成一个2k形式的偶数</span><span class="token keyword">def</span> <span class="token function">double</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x <span class="token operator">*</span> <span class="token number">2</span><span class="token comment" spellcheck="true">#回调函数2</span><span class="token comment" spellcheck="true">#生成一个4k形式的偶数</span><span class="token keyword">def</span> <span class="token function">quadruple</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> x <span class="token operator">*</span> <span class="token number">4</span></code></pre><p><code>callback_demo.py</code></p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> even <span class="token keyword">import</span> <span class="token operator">*</span><span class="token comment" spellcheck="true">#中间函数</span><span class="token comment" spellcheck="true">#接受一个生成偶数的函数作为参数</span><span class="token comment" spellcheck="true">#返回一个奇数</span><span class="token keyword">def</span> <span class="token function">getOddNumber</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> getEvenNumber<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">+</span> getEvenNumber<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#起始函数，这里是程序的主函数</span><span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        k <span class="token operator">=</span> <span class="token number">1</span>    <span class="token comment" spellcheck="true">#当需要生成一个2k+1形式的奇数时</span>    i <span class="token operator">=</span> getOddNumber<span class="token punctuation">(</span>k<span class="token punctuation">,</span> double<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#当需要一个4k+1形式的奇数时</span>    i <span class="token operator">=</span> getOddNumber<span class="token punctuation">(</span>k<span class="token punctuation">,</span> quadruple<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#当需要一个8k+1形式的奇数时</span>    i <span class="token operator">=</span> getOddNumber<span class="token punctuation">(</span>k<span class="token punctuation">,</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    main<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 最终输出: 3 5 9</span></code></pre><p>上面的代码里，给<code>getOddNumber</code>传入不同的回调函数，它的表现也不同，这就是回调机制的优势所在。值得一提的是，上面的第三个回调函数是一个匿名函数。</p><p><strong>易被忽略的第三方</strong></p><p>通过上面的论述可知，中间函数和回调函数是回调的两个必要部分，不过人们往往忽略了回调里的第三位要角，就是中间函数的调用者。绝大多数情况下，这个调用者可以和程序的主函数等同起来，但为了表示区别，我这里把它称为<strong>起始函数</strong>（如上面的代码中注释所示）。</p><p>之所以特意强调这个第三方，是因为我在网上读相关文章时得到一种印象，很多人把它简单地理解为两个个体之间的来回调用。譬如，很多中文网页在解释“回调”（callback）时，都会提到这么一句话：“If you call me, I will call you back.”我没有查到这句英文的出处。我个人揣测，很多人把起始函数和回调函数看作为一体，大概有两个原因：第一，可能是“回调”这一名字的误导；第二，给中间函数传入什么样的回调函数，是在起始函数里决定的。实际上，回调并不是“你我”两方的互动，而是ABC的三方联动。有了这个清楚的概念，在自己的代码里实现回调时才不容易混淆出错。</p><p>另外，回调实际上有两种：阻塞式回调和延迟式回调。两者的区别在于：<strong>阻塞式回调里，回调函数的调用一定发生在起始函数返回之前；而延迟式回调里，回调函数的调用有可能是在起始函数返回之后</strong>。这里不打算对这两个概率做更深入的讨论，之所以把它们提出来，也是为了说明强调起始函数的重要性。网上的很多文章，提到这两个概念时，只是笼统地说阻塞式回调发生在主调函数返回之前，却没有明确这个主调函数到底是起始函数还是中间函数，不免让人糊涂，所以这里特意说明一下。另外还请注意，本文中所举的示例均为阻塞式回调。延迟式回调通常牵扯到多线程，我自己还没有完全搞明白，所以这里就不多说了。</p></blockquote><h3 id="REPRODUCIBILITY"><a href="#REPRODUCIBILITY" class="headerlink" title="REPRODUCIBILITY"></a>REPRODUCIBILITY</h3><p>终归无法一模一样!</p><p>torch.manual_seed() :<a href="https://blog.csdn.net/X_singing/article/details/104447052?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">https://blog.csdn.net/X_singing/article/details/104447052?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><p>为了保证能够复现，初始随机梯度是固定的</p><p><strong>随机数种子</strong></p><pre class=" language-python"><code class="language-python">cudnn<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">True</span>cudnn<span class="token punctuation">.</span>benchmark <span class="token operator">=</span> <span class="token boolean">True</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">if</span> args<span class="token punctuation">.</span>seed <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span>    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span>    torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span>    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span></code></pre><p><strong>Data Loader</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">seed_worker</span><span class="token punctuation">(</span>worker_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    worker_seed <span class="token operator">=</span> torch<span class="token punctuation">.</span>initial_seed<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">32</span>    numpy<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>worker_seed<span class="token punctuation">)</span>    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>worker_seed<span class="token punctuation">)</span>DataLoader<span class="token punctuation">(</span>    train_dataset<span class="token punctuation">,</span>    batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span>    num_workers<span class="token operator">=</span>num_workers<span class="token punctuation">,</span>    worker_init_fn<span class="token operator">=</span>seed_worker<span class="token punctuation">)</span></code></pre><h2 id="相关库"><a href="#相关库" class="headerlink" title="相关库"></a>相关库</h2><h3 id="torchstat-amp-thop"><a href="#torchstat-amp-thop" class="headerlink" title="torchstat&amp;thop"></a>torchstat&amp;thop</h3><p><a href="https://blog.csdn.net/qq_35407318/article/details/109359006">相关教程</a></p><p>FLOPs&amp;MACs</p><p><a href="https://zhuanlan.zhihu.com/p/137719986">FLOPs详解</a></p><p><a href="https://zhuanlan.zhihu.com/p/364543528">FLOPs与MACs的一些区分</a></p><p>FLOPS：注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。</p><p>FLOPs：==注意s小写，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。==</p><p>我们知道，通常我们去<strong>评价一个模型时，首先看的应该是它的精确度</strong>，当你精确度不行的时候，你和别人说我的模型预测的多么多么的快，部署的时候占的内存多么多么的小，都是白搭。但当你模型达到一定的精确度之后，就需要更<strong>进一步的评价指标来评价你模型</strong>：1）<strong>前向传播时所需的计算力</strong>，它反应了对硬件如GPU性能要求的高低；2）<strong>参数个数</strong>，它反应所占内存大小。为什么要加上这两个指标呢？因为这事关你模型算法的落地。比如你要在手机和汽车上部署深度学习模型，对模型大小和计算力就有严格要求。模型参数想必大家都知道是什么怎么算了，而前向传播时所需的计算力可能还会带有一点点疑问。所以这里总计一下前向传播时所需的计算力。它正是由<strong>FLOPs</strong>体现，那么<strong>FLOPs</strong>该怎么计算呢？</p><p>==时间复杂度是看计算的量级,FLOPs应该是具体把计算次数算出来,应该是这个意思==</p><p>MACs(Multiply-Accumulate Operations):乘加累计操作数,1个MACs包含一个乘法操作与一个加法操作,大约包含2FLOPs.</p><p>而人们常常将<strong>MACs</strong>与<strong>FLOPs</strong>混淆使用，甚至一些代码与论文中，都没对两个概念进行区分.</p><h4 id="torchstat基本介绍"><a href="#torchstat基本介绍" class="headerlink" title="torchstat基本介绍"></a>torchstat基本介绍</h4><p>[委实拉胯]</p><p>这是基于PyTorch的轻量级神经网络分析库。 它旨在使您能够快速，轻松地构建网络并进行调试。 注意：此存储库当前正在开发中。 因此，某些API可能会更改。</p><p>该工具可以显示</p><ul><li>网络参数总数  Total number of network parameters</li><li>FLOPs  Theoretical amount of floating point arithmetics (FLOPs)</li><li>理论乘积MAdd   Theoretical amount of multiply-adds (MAdd)</li><li>内存使用情况</li></ul><p>应用示例:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> torchstat <span class="token keyword">import</span> stat<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> modelsmodel <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span><span class="token punctuation">)</span>stat<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p><strong>Support Layers</strong></p><img src="/pytorch-xue-xi/image-20210409152152829.png" alt="Support Layers" style="zoom:50%;"><h4 id="thop基本介绍"><a href="#thop基本介绍" class="headerlink" title="thop基本介绍"></a>thop基本介绍</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> thop <span class="token keyword">import</span> profileinput <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>flops<span class="token punctuation">,</span> params <span class="token operator">=</span> profile<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token operator">=</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>flops<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>params<span class="token punctuation">)</span></code></pre><p><strong>卷积核FLOPs计算</strong></p><h3 id="torchsummary"><a href="#torchsummary" class="headerlink" title="torchsummary"></a>torchsummary</h3><h3 id="wandb"><a href="#wandb" class="headerlink" title="wandb"></a>wandb</h3><p><a href="https://docs.wandb.ai/">官方文档</a></p><p><a href="https://blog.csdn.net/weixin_42686816/article/details/123953150">CSDN教程</a></p><h4 id="基础结构"><a href="#基础结构" class="headerlink" title="基础结构"></a>基础结构</h4><blockquote><p>代码大致可分为以下步骤：<br>①导包<br>②初始化一个项目<br>③设置参数<br>④设定好模型和数据集<br>⑤追踪模型参数并记录<br>⑥储存模型</p></blockquote><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># import the library</span><span class="token keyword">import</span> wandb<span class="token comment" spellcheck="true"># start a new experiment</span>wandb<span class="token punctuation">.</span>init<span class="token punctuation">(</span>project<span class="token operator">=</span><span class="token string">"new-sota-model"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># capture a dictionary of hyperparameters with config</span>wandb<span class="token punctuation">.</span>config <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"learning_rate"</span><span class="token punctuation">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span> <span class="token string">"epochs"</span><span class="token punctuation">:</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token string">"batch_size"</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true"># set up model and data</span>model<span class="token punctuation">,</span> dataloader <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> get_data<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># optional: track gradients</span>wandb<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token keyword">for</span> batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>  metrics <span class="token operator">=</span> model<span class="token punctuation">.</span>training_step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># log metrics inside your training loop to visualize model performance</span>  wandb<span class="token punctuation">.</span>log<span class="token punctuation">(</span>metrics<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># optional: save model at the end</span>model<span class="token punctuation">.</span>to_onnx<span class="token punctuation">(</span><span class="token punctuation">)</span>wandb<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"model.onnx"</span><span class="token punctuation">)</span></code></pre><p>The most commonly used functions/objects are:</p><ul><li>wandb.init — initialize a new run at the top of your training script</li><li>wandb.config — track hyperparameters and metadata</li><li>wandb.log — log metrics and media over time within your training loop</li></ul><h4 id="函数释义"><a href="#函数释义" class="headerlink" title="函数释义"></a>函数释义</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Hooks into the torch model to collect gradients and the topology. 记录梯度</span>wandb<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>models<span class="token punctuation">,</span> criterion<span class="token operator">=</span>None<span class="token punctuation">,</span> log<span class="token operator">=</span><span class="token string">"gradients"</span><span class="token punctuation">,</span> log_freq<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> idx<span class="token operator">=</span>None<span class="token punctuation">,</span>log_graph<span class="token operator">=</span><span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="albumentations"><a href="#albumentations" class="headerlink" title="albumentations"></a>albumentations</h3><p><a href="https://albumentations.ai/docs/">官方文档</a></p><p><a href="https://blog.csdn.net/u014264373/article/details/114144303">最快最好用的数据增强库</a></p><p>There are two types of image augmentations: pixel-level augmentations and spatial-level augmentations.</p><p>Pixel-level augmentations change the values of pixels of the original image, but they don’t change the output mask. Image transformations such as changing brightness or contrast of adjusting values of the RGB-palette of the image are pixel-level augmentations.</p><p>On the contrary, spatial-level augmentations change both the image and the mask. When you apply image transformations such as mirroring or rotation or cropping a part of the input image, you also need to apply the same transformation to the output label to preserve its correctness.</p><h4 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h4><p>If the image has one associated mask, you need to call <code>transform</code> with two arguments: <code>image</code> and <code>mask</code>. In <code>image</code> you should pass the input image, in <code>mask</code> you should pass the output mask. <code>transform</code> will return a dictionary with two keys: <code>image</code> will contain the augmented image, and <code>mask</code> will contain the augmented mask.</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># </span>transformed <span class="token operator">=</span> transform<span class="token punctuation">(</span>image<span class="token operator">=</span>image<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>transformed_image <span class="token operator">=</span> transformed<span class="token punctuation">[</span><span class="token string">'image'</span><span class="token punctuation">]</span>transformed_mask <span class="token operator">=</span> transformed<span class="token punctuation">[</span><span class="token string">'mask'</span><span class="token punctuation">]</span></code></pre><h4 id="Core"><a href="#Core" class="headerlink" title="Core"></a>Core</h4><h4 id="Augmentations"><a href="#Augmentations" class="headerlink" title="Augmentations"></a>Augmentations</h4><p>只有物体的位置(平移变换)和朝向(旋转变换)发生改变，而形状不变，得到的变换称为刚性变换。非刚性变换就是比这更复杂的变换，如伸缩，仿射，透射，多项式等一些比较复杂的变换。</p><p>albumentations 中主要提供了三种非刚体变换方法：ElasticTransform、GridDistortion 和 OpticalDistortion。</p><pre class=" language-python"><code class="language-python">albumentations<span class="token punctuation">.</span>augmentations<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>GridDistortion <span class="token punctuation">(</span>num_steps<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> distort_limit<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> interpolation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> border_mode<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> value<span class="token operator">=</span>None<span class="token punctuation">,</span> mask_value<span class="token operator">=</span>None<span class="token punctuation">,</span> always_apply<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span></code></pre><h4 id="ImgAug-Helpers"><a href="#ImgAug-Helpers" class="headerlink" title="ImgAug Helpers"></a>ImgAug Helpers</h4><h4 id="Pytorch-Helpers"><a href="#Pytorch-Helpers" class="headerlink" title="Pytorch Helpers"></a>Pytorch Helpers</h4><h3 id="segmentation-models-pytorch"><a href="#segmentation-models-pytorch" class="headerlink" title="segmentation_models_pytorch"></a>segmentation_models_pytorch</h3><p><a href="https://smp.readthedocs.io/en/latest/">文档</a></p><h3 id="cupy"><a href="#cupy" class="headerlink" title="cupy"></a>cupy</h3><p><a href="https://docs.cupy.dev/en/stable/">官方文档</a></p><p>CuPy 项目的目标是为 Python 用户提供 GPU 加速能力,无需深入了解底层 GPU 技术。 CuPy 团队专注于提供：</p><ul><li><strong>完整的 NumPy 和 SciPy API 覆盖成为完全的替代品</strong>，以及高级 CUDA 功能以最大限度地提高性能。</li><li>成熟且优质的库，作为所有需要加速的项目的基础包，从实验室环境到大规模集群。</li></ul><h3 id="NNI"><a href="#NNI" class="headerlink" title="NNI"></a>NNI</h3><p><a href="https://nni.readthedocs.io/zh/stable/index.html">官方文档</a></p><h3 id="yacs"><a href="#yacs" class="headerlink" title="yacs"></a>yacs</h3><p><a href="https://blog.csdn.net/gefeng1209/article/details/90668882">文档</a></p><h3 id="einops"><a href="#einops" class="headerlink" title="einops"></a>einops</h3><p><a href="https://einops.rocks/">文档</a></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># concatenate images along height (vertical axis), 960 = 32 * 30</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> rearrange<span class="token punctuation">(</span>images<span class="token punctuation">,</span> <span class="token string">'b h w c -&gt; (b h) w c'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape <span class="token comment" spellcheck="true"># 括号表示为b和h相乘</span><span class="token punctuation">(</span><span class="token number">960</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></code></pre><h2 id="奇迹淫巧"><a href="#奇迹淫巧" class="headerlink" title="奇迹淫巧"></a>奇迹淫巧</h2><p><a href="https://zhuanlan.zhihu.com/p/77952356">pytorch常见的坑汇总</a></p><h4 id="GPU利用率不高-gpu显存占用浪费"><a href="#GPU利用率不高-gpu显存占用浪费" class="headerlink" title="GPU利用率不高+gpu显存占用浪费"></a>GPU利用率不高+gpu显存占用浪费</h4><p> 1 主函数前面加(这个会牺牲一点点显存提高模型精度):</p><pre class=" language-python"><code class="language-python">cudnn<span class="token punctuation">.</span>benchmark <span class="token operator">=</span> <span class="token boolean">True</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">False</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">True</span></code></pre><p><a href="https://zhuanlan.zhihu.com/p/73711222">设置torch.backends.cudnn.benchmark</a></p><p>通过预先搜索最适合的卷积算法的实现，因为卷积算法有很多种不同的实现，最简单的实现方式就是使用多层循环嵌套，对于每张输入图像，对于每个要输出的通道，对于每个输入的通道，选取一个区域，同指定卷积核进行卷积操作，然后逐行滑动，直到整张图像都处理完毕，这个方法一般被称为 direct 法，这个方法虽然简单，但是看到这么多循环，我们就知道效率在一般情况下不会很高了。除此之外，实现卷积层的算法还有基于 GEMM (General Matrix Multiply) 的，基于 FFT 的，基于 Winograd 算法的等等，而且每个算法还有自己的一些变体。在一个开源的 <a href="https://link.zhihu.com/?target=https://www.scss.tcd.ie/~andersan/projects/live/triNNity.html">C++ 库 triNNity</a> 中，就实现了接近 80 种的卷积前向传播算法！</p><p>要求有：</p><ol><li>网络结构固定</li><li>输入尺寸固定</li></ol><p>2 训练时,epoch前面加:(定期清空模型，效果感觉不明显）</p><p><code>torch.cuda.empty_cache()</code></p><p>3 无用变量前面加:(同上，效果某些操作上还挺明显的）</p><p><code>del xxx(变量名)</code></p><p>4 dataloader的长度<code>__len__</code>设置:(dataloader会间歇式出现卡顿,设置成这样会避免不少）</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>images<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></code></pre><p>5 dataloader的预加载设置:(会在模型训练的时候加载数据，提高一点点gpu利用率）</p><pre class=" language-python"><code class="language-python">train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>        train_dataset<span class="token punctuation">,</span>        pin_memory<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span></code></pre><p>6 loss的item()</p><p><img src="/pytorch-xue-xi/image-20211009185157414.png" alt="image-20211009185157414"></p><p>没有item()的话会导致显存占用变高</p><p>7 混合精度计算</p><h4 id="加速Pytorch训练"><a href="#加速Pytorch训练" class="headerlink" title="加速Pytorch训练"></a>加速Pytorch训练</h4><p><a href="https://zhuanlan.zhihu.com/p/97190313">加速pytorch训练</a></p><h5 id="prefetch-generator"><a href="#prefetch-generator" class="headerlink" title="prefetch_generator"></a>prefetch_generator</h5><p>使用 prefetch_generator库在后台加载下一batch的数据，原本Pytorch默认的DataLoader会创建一些worker线程来预读取新的数据，但是除非这些线程的数据全部都被清空，这些线程才会读下一批数据。使用prefetch_generator，我们可以保证线程不会等待，每个线程都总有至少一个数据在加载。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#使用</span><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token keyword">from</span> prefetch_generator <span class="token keyword">import</span> BackgroundGenerator<span class="token keyword">class</span> <span class="token class-name">DataLoaderX</span><span class="token punctuation">(</span>DataLoader<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> BackgroundGenerator<span class="token punctuation">(</span>super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__iter__<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#然后用DataLoaderX替换原本的DataLoader</span></code></pre><h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h3><p>获取参数量</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">print_network</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token punctuation">:</span>    num_params <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        num_params <span class="token operator">+=</span> param<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Total number of parameters: %d'</span> <span class="token operator">%</span> num_params<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>numpy、pandas、matplotlib学习</title>
      <link href="np-pd-plt/"/>
      <url>np-pd-plt/</url>
      
        <content type="html"><![CDATA[<h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><p>菜鸟教程：<a href="https://www.runoob.com/numpy/numpy-ndarray-object.html">https://www.runoob.com/numpy/numpy-ndarray-object.html</a></p><p><a href="https://docs.scipy.org/doc/numpy-1.9.0/genindex.html">numpy API</a></p><p>Python<code>list</code>只能与整数相乘，在这种情况下，将<code>list</code>重复的元素：</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span></code></pre><p>如果要进行矢量运算，请<code>numpy.ndarray</code>改用：</p><pre class=" language-python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ar <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ar <span class="token operator">*</span> <span class="token number">3</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#numpy数组索引</span>pre<span class="token operator">=</span>pre<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#不用三个中括号，只用一个，里面只用一个逗号</span></code></pre><p><a href="https://zhuanlan.zhihu.com/p/60365398">numpy广播机制</a></p><h4 id="axis详解"><a href="#axis详解" class="headerlink" title="axis详解"></a>axis详解</h4><p><a href="https://www.jianshu.com/p/30b40b504bae">虽然是tf的教程，但是类似</a></p><ol><li>0维，又称0维张量，数字，标量：1   <code>()</code></li><li>1维，又称1维张量，数组，vector：[1, 2, 3]  <code>(3)</code></li><li>2维，又称2维张量，矩阵，二维数组：[[1,2], [3,4]] <code>(2,2)</code></li><li>3维，又称3维张量，立方（cube），三维数组：[[[5,6], [7,8]]]  <code>(1,2,2)</code> ,越往里axis越大，axis是<code>(0,1,2)</code>每个3维的有一个2维的，每个2维的有2个1维的，每个1维的有2个标量,数字7的坐标是[0,1,0]</li><li>n维：你应该get到点了吧~</li></ol><p>本来是2x3x4,<code>tf.reduce_sum(tensor, axis=0)</code>，加完了就变成了3x4</p><h4 id="文件存储npy-npz"><a href="#文件存储npy-npz" class="headerlink" title="文件存储npy\npz"></a>文件存储npy\npz</h4><p>Numpy能够读写磁盘上的文本数据或二进制数据。</p><p><strong>将数组以二进制格式保存到磁盘</strong></p><p><code>np.load</code>和<code>np.save</code>是读写磁盘数组数据的两个主要函数，默认情况下，数组是以未压缩的原始二进制格式保存在扩展名为.npy的文件中。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 读取</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npa<span class="token operator">=</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'test.npy'</span><span class="token punctuation">,</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 存储</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npa<span class="token operator">=</span>np<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'test.npy'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span></code></pre><p><code>np.savez</code>以未压缩的 .npz 格式将多个数组保存到单个文件中。</p><pre class=" language-python"><code class="language-python">np<span class="token punctuation">.</span>savez<span class="token punctuation">(</span>outfile<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>npzfile <span class="token operator">=</span> np<span class="token punctuation">.</span>load<span class="token punctuation">(</span>outfile<span class="token punctuation">)</span>npzfile<span class="token punctuation">.</span>files<span class="token punctuation">[</span><span class="token string">'arr_0'</span><span class="token punctuation">,</span> <span class="token string">'arr_1'</span><span class="token punctuation">]</span>np<span class="token punctuation">.</span>savez<span class="token punctuation">(</span>outfile<span class="token punctuation">,</span> x<span class="token operator">=</span>x<span class="token punctuation">,</span> y<span class="token operator">=</span>y<span class="token punctuation">)</span>npzfile <span class="token operator">=</span> np<span class="token punctuation">.</span>load<span class="token punctuation">(</span>outfile<span class="token punctuation">)</span>sorted<span class="token punctuation">(</span>npzfile<span class="token punctuation">.</span>files<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">,</span> <span class="token string">'y'</span><span class="token punctuation">]</span></code></pre><h4 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h4><pre class=" language-python"><code class="language-python">a <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回a列表的元素总数：60</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>size<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回a列表的维度大小：(3,4,5)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回a列表的第一维大小：3</span><span class="token keyword">print</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>list<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将列表转化为数组</span>np<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#求标准差</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#矩阵乘</span>np<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#element wise</span>a<span class="token operator">*</span>b<span class="token comment" spellcheck="true">#element wise</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>condition<span class="token punctuation">[</span><span class="token punctuation">,</span>x<span class="token punctuation">,</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回根据条件从x或y中选择的元素。条件为真，返回x，否则返回y。</span><span class="token comment" spellcheck="true">#&lt;&lt;&lt;a</span><span class="token comment" spellcheck="true">#array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span><span class="token comment" spellcheck="true">#&lt;&lt;&lt;(a &lt; 5, a, 10*a)</span><span class="token comment" spellcheck="true">#array([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#显示np数组中的unique值，还有返回index值等功能</span>np<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#向上取整 </span>np<span class="token punctuation">.</span>floor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#向下取整</span>np<span class="token punctuation">.</span>round<span class="token punctuation">(</span>a<span class="token punctuation">,</span>decimals<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>out<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将数组四舍五入到指定的小数上</span>np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>a<span class="token punctuation">,</span>axis<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>ndarray    eg<span class="token punctuation">:</span>img_nd <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>img_nd<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#img_nd原先是二维的(0,1),扩张完变(0,1,2)</span><span class="token comment" spellcheck="true">#调整数组维度顺序</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>a<span class="token punctuation">,</span>axes<span class="token operator">=</span>None<span class="token punctuation">)</span> eg<span class="token punctuation">:</span>img_nd<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 或 np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>img_nd<span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#(a,b,c)-&gt;(c,a,b)</span>ndarray<span class="token punctuation">.</span>max<span class="token punctuation">(</span>axis<span class="token operator">=</span>None<span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回指定维度的最大值,没指定的话就是所有的最大值</span>np<span class="token punctuation">.</span>newaxis<span class="token comment" spellcheck="true">#作用是增加一个维度</span><span class="token comment" spellcheck="true">#a=np.array([1,2,3,4,5])</span><span class="token comment" spellcheck="true">#aa=a[:,np.newaxis]</span><span class="token comment" spellcheck="true">#print(aa.shape)  (5,1) 现有5个,5个里每个再有1个</span><span class="token comment" spellcheck="true">#print (aa)      [[1],[2],[3],[4],[5]] </span>np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>a1<span class="token punctuation">,</span> a2<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将数组序列拼接在一起</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#(2,2)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#(1,2)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#拼接完-&gt;(3,2)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>numpy<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span>a<span class="token punctuation">,</span> order<span class="token operator">=</span><span class="token string">'C'</span><span class="token punctuation">)</span>ndarray<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>order<span class="token operator">=</span><span class="token string">'C'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#都是将多维数组降为一维，区别为:</span><span class="token comment" spellcheck="true">#ndarray.flatten(order='C')返回拷贝，对拷贝的修改不会影响原数组</span><span class="token comment" spellcheck="true">#numpy.ravel(a, order='C')返回视图，对视图的修改会影响原数组</span><span class="token comment" spellcheck="true">#order:使用这个索引顺序读取a的元素。有'C' 'F' 'A' 'K'</span><span class="token comment" spellcheck="true">#'C'意味着以行主、C风格的顺序索引元素，最后一个轴索引变化最快，回到第一个轴索引变化最慢。eg:[0,0,0],[0,0,1],[0,0,2],[0,1,0],[0,1,1],...</span><span class="token comment" spellcheck="true">#'F'表示以fortran风格的列主顺序索引元素，第一个索引变化最快，最后一个索引变化最慢。注意，' C '和' F '选项不考虑底层数组的内存布局，只参考轴索引的顺序。eg:[0,0,0],[1,0,0],[2,0,0],[0,1,0],[1,1,0],[2,1,0],...</span><span class="token comment" spellcheck="true">#'A'表示如果a在内存中是Fortran连续的，则以类似Fortran的索引顺序读取元素，否则以类似c的顺序读取</span><span class="token comment" spellcheck="true">#'K'的意思是按照元素在内存中出现的顺序读取它们，除非在步数为负时反转数据。默认情况下，使用'C'索引顺序。</span>np<span class="token punctuation">.</span>spacing<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回x和最近相邻数字之间的距离</span><span class="token comment" spellcheck="true">#可以视为eps的一个繁华</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>array<span class="token punctuation">,</span>axis<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#用于返回一个numpy数值中最大值的索引,当一组中同时出现几个最大值时，返回第一个最大值的索引值。</span>np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 和/作用相同,就是除以</span>np<span class="token punctuation">.</span>bincount<span class="token punctuation">(</span>x<span class="token punctuation">,</span> weights<span class="token operator">=</span>None<span class="token punctuation">,</span> minlength<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 计算非负整数数组中每个值的出现次数。</span><span class="token comment" spellcheck="true"># bin的个数至少比最大的x的值大一，如果设置了minlength,则至少有minlength个bin</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> np<span class="token punctuation">.</span>bincount<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出:array([1, 3, 1, 1, 0, 0, 0, 1])</span>np<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>array<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 当array是一个一维数组时,会返回一个以array为对角线元素的矩阵</span><span class="token comment" spellcheck="true"># 当array是一个二维矩阵时，会返回该矩阵的对角线元素</span>np<span class="token punctuation">.</span>nanmean<span class="token punctuation">(</span>array<span class="token punctuation">,</span> axis<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 计算指定axis的平均,忽略nan</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>nan<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> np<span class="token punctuation">.</span>nanmean<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token number">2.6666666666666665</span><span class="token comment" spellcheck="true"># 从一个可迭代对象创建一个新的一维数组。</span>numpy<span class="token punctuation">.</span>fromiter<span class="token punctuation">(</span>iter<span class="token punctuation">,</span> dtype<span class="token punctuation">,</span> count<span class="token operator">=</span><span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> like<span class="token operator">=</span>None<span class="token punctuation">)</span>iterable <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token operator">*</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>fromiter<span class="token punctuation">(</span>iterable<span class="token punctuation">,</span> float<span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">16</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># asfortranarray()返回在内存中以Fortran顺序排列的数组(ndim&gt;= 1)</span>numpy<span class="token punctuation">.</span>asfortranarray<span class="token punctuation">(</span>a<span class="token punctuation">,</span> dtype<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> like<span class="token operator">=</span>None<span class="token punctuation">)</span>x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>y <span class="token operator">=</span> np<span class="token punctuation">.</span>asfortranarray<span class="token punctuation">(</span>x<span class="token punctuation">)</span>x<span class="token punctuation">.</span>flags<span class="token punctuation">[</span><span class="token string">'F_CONTIGUOUS'</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token boolean">False</span>y<span class="token punctuation">.</span>flags<span class="token punctuation">[</span><span class="token string">'F_CONTIGUOUS'</span><span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token boolean">True</span><span class="token comment" spellcheck="true"># 用0替换nan,用有限数字替换inf。</span>np<span class="token punctuation">.</span>nan_to_num<span class="token punctuation">(</span>x<span class="token punctuation">)</span>x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>np<span class="token punctuation">.</span>inf<span class="token punctuation">,</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>inf<span class="token punctuation">,</span> np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> np<span class="token punctuation">.</span>nan_to_num<span class="token punctuation">(</span>x<span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>  <span class="token number">1.79769313e+308</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">1.79769313e+308</span><span class="token punctuation">,</span>   <span class="token number">0.00000000e+000</span><span class="token punctuation">,</span>        <span class="token operator">-</span><span class="token number">1.28000000e+002</span><span class="token punctuation">,</span>   <span class="token number">1.28000000e+002</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 计算数据的直方图</span>hist<span class="token punctuation">,</span>bins<span class="token operator">=</span>np<span class="token punctuation">.</span>histogram<span class="token punctuation">(</span>a <span class="token comment" spellcheck="true"># array-like数据,但是必须被flattened</span>             <span class="token punctuation">,</span> bins<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 如果是int,给定等宽bin的数量</span>             range<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># range的上下界,左开右闭,除了最后一个</span>             normed<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> weights<span class="token operator">=</span>None<span class="token punctuation">,</span> density<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># hist是每个bin的数量，bins是边界 如</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> np<span class="token punctuation">.</span>histogram<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bins<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Return the cumulative sum of the elements along a given axis. 给定轴的累加</span>np<span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 输入的array</span>          axis<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 所指定的轴,如果未指定,就是对flatten array操作</span>          dtype<span class="token operator">=</span>None<span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 上下翻转数组。</span>np<span class="token punctuation">.</span>flipud<span class="token punctuation">(</span>m<span class="token punctuation">:</span>array_like<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 左右翻转数组。</span>np<span class="token punctuation">.</span>fliplr<span class="token punctuation">(</span>m<span class="token punctuation">:</span>array_like<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将数组逆时针旋转90度。前两个维度旋转。</span>np<span class="token punctuation">.</span>rot90<span class="token punctuation">(</span>m<span class="token punctuation">:</span>array_like<span class="token punctuation">,</span>k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">:</span>旋转次数<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># </span>np<span class="token punctuation">.</span>flatnonzero<span class="token punctuation">(</span><span class="token punctuation">)</span>numpy<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>A<span class="token punctuation">,</span> reps<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Construct an array by repeating A the number of times given by reps.</span><span class="token comment" spellcheck="true"># 和torch.clamp一样的用法,就是把数据夹紧到一个区间范围内</span>numpy<span class="token punctuation">.</span>clip<span class="token punctuation">(</span>a<span class="token punctuation">,</span> a_min<span class="token punctuation">,</span> a_max<span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 数组的拼接</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> marray<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> n <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> narray<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 水平拼接,输出为:</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 拼接的方法有：</span>np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>m<span class="token punctuation">,</span>n<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>c_<span class="token punctuation">[</span>m<span class="token punctuation">,</span> n<span class="token punctuation">]</span>np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 竖直拼接,输出为：</span><span class="token comment" spellcheck="true"># 拼接的方法有</span>np<span class="token punctuation">.</span>r_<span class="token punctuation">[</span>m<span class="token punctuation">,</span> n<span class="token punctuation">]</span>np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 数组生成</span><span class="token comment" spellcheck="true"># np.linspace()和np.arrange()一个是样本数量，一个步长</span><span class="token comment" spellcheck="true">#调整数组顺序</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h2><p>教程：<a href="https://www.yiibai.com/pandas/python_pandas_environment_setup.html">https://www.yiibai.com/pandas/python_pandas_environment_setup.html</a></p><p>API:<a href="https://pandas.pydata.org/pandas-docs/stable/reference/index.html">https://pandas.pydata.org/pandas-docs/stable/reference/index.html</a></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 查看序列的index</span>Series<span class="token punctuation">.</span>index<span class="token comment" spellcheck="true"># Return a Series containing counts of unique values.结果对象将按降序排列，因此第一个元素是最常出现的元素。 默认情况下排除 NA 值。</span>index <span class="token operator">=</span> pd<span class="token punctuation">.</span>Index<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>nan<span class="token punctuation">]</span><span class="token punctuation">)</span>index<span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token number">3.0</span>    <span class="token number">2</span><span class="token number">1.0</span>    <span class="token number">1</span><span class="token number">2.0</span>    <span class="token number">1</span><span class="token number">4.0</span>    <span class="token number">1</span>dtype<span class="token punctuation">:</span> int64Series<span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> sort<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> ascending<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> bins<span class="token operator">=</span>None<span class="token punctuation">,</span> dropna<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取DataFrame的行数和列数</span>df<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">//</span>行数df<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">//</span>列数<span class="token comment" spellcheck="true"># 读取csv文件,</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'train.csv'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用于返回DataFrame类型的数据，如果不使用该函数,则输出结果为数据的前面五行和末尾五行。中间部分以...代替。</span><span class="token keyword">print</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>to_string<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将DataFrame保存为csv文件</span>df<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">'site.csv'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># head()读取前面的n行,如果不填参数n,默认返回5行。</span><span class="token keyword">print</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回每个group的前5行</span>GroupBy<span class="token punctuation">.</span>head<span class="token punctuation">(</span>n<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># tail()读取尾部的n行,如果不填参数n，默认返回5行，空行各个字段的值返回NaN。</span><span class="token keyword">print</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>tail<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># info()返回表格的一些基本信息</span><span class="token keyword">print</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'pandas.core.frame.DataFrame'</span><span class="token operator">&gt;</span>RangeIndex<span class="token punctuation">:</span> <span class="token number">458</span> entries<span class="token punctuation">,</span> <span class="token number">0</span> to <span class="token number">457</span>          <span class="token comment" spellcheck="true"># 行数，458 行，第一行编号为 0</span>Data columns <span class="token punctuation">(</span>total <span class="token number">9</span> columns<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 列数，9列</span> <span class="token comment" spellcheck="true">#   Column    Non-Null Count  Dtype       # 各列的数据类型</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>  <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>    <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>  <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>   <span class="token number">0</span>   Name      <span class="token number">457</span> non<span class="token operator">-</span>null    object  <span class="token number">1</span>   Team      <span class="token number">457</span> non<span class="token operator">-</span>null    object  <span class="token number">2</span>   Number    <span class="token number">457</span> non<span class="token operator">-</span>null    float64 <span class="token number">3</span>   Position  <span class="token number">457</span> non<span class="token operator">-</span>null    object  <span class="token number">4</span>   Age       <span class="token number">457</span> non<span class="token operator">-</span>null    float64 <span class="token number">5</span>   Height    <span class="token number">457</span> non<span class="token operator">-</span>null    object  <span class="token number">6</span>   Weight    <span class="token number">457</span> non<span class="token operator">-</span>null    float64 <span class="token number">7</span>   College   <span class="token number">373</span> non<span class="token operator">-</span>null    object         <span class="token comment" spellcheck="true"># non-null，意思为非空的数据    </span> <span class="token number">8</span>   Salary    <span class="token number">446</span> non<span class="token operator">-</span>null    float64dtypes<span class="token punctuation">:</span> float64<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> object<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>                 <span class="token comment" spellcheck="true"># 类型</span><span class="token comment" spellcheck="true"># 删除'B','C'两列</span>df<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token string">'C'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># </span>df<span class="token punctuation">.</span>columns<span class="token comment" spellcheck="true"># duplicated()返回数据是否重复</span>person <span class="token operator">=</span> <span class="token punctuation">{</span>  <span class="token string">"name"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'Google'</span><span class="token punctuation">,</span> <span class="token string">'Runoob'</span><span class="token punctuation">,</span> <span class="token string">'Runoob'</span><span class="token punctuation">,</span> <span class="token string">'Taobao'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token string">"age"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">]</span>  <span class="token punctuation">}</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>person<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>duplicated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token number">0</span>    <span class="token boolean">False</span><span class="token number">1</span>    <span class="token boolean">False</span><span class="token number">2</span>     <span class="token boolean">True</span><span class="token number">3</span>    <span class="token boolean">False</span>dtype<span class="token punctuation">:</span> bool<span class="token comment" spellcheck="true"># drop_duplicates()删除重复数据</span>DataFrame<span class="token punctuation">.</span>drop_duplicates<span class="token punctuation">(</span>subset<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 仅考虑用于识别重复项的某些列，默认情况下使用所有列。 </span>                          keep<span class="token operator">=</span><span class="token string">'first'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># {'first', 'last', False},分别是除第一次出现全删除,除最后一次出现全删除,删除所有重复项</span>                          inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># Whether to drop duplicates in place or to return a copy.</span>                          ignore_index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 1.删除完全重复数据</span>data<span class="token punctuation">.</span>drop_duplicates<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 数据清洗时，会将带空值的行删除，此时DataFrame或Series类型的数据不再是连续的索引，可以使用reset_index()重置索引。</span>df<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span><span class="token punctuation">)</span>    index   <span class="token keyword">class</span>  <span class="token class-name">max_speed</span> <span class="token comment" spellcheck="true"># 之前的index被添加成一个column</span><span class="token number">0</span>  falcon    bird      <span class="token number">389.0</span><span class="token number">1</span>  parrot    bird       <span class="token number">24.0</span><span class="token number">2</span>    lion  mammal       <span class="token number">80.5</span><span class="token number">3</span>  monkey  mammal        NaN<span class="token comment" spellcheck="true"># 如果设置df.reset_index(drop=True),则之前的index不被添加成一个column</span><span class="token comment" spellcheck="true"># 利用groupby之后也可以利用reset_index来回转</span>DataFrame<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span>by<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#用于确定groupby的组:mapping,function,label,or list of labels</span>                  axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#Split along rows (0) or columns (1):{0 or ‘index’, 1 or ‘columns’}</span>                  level<span class="token operator">=</span>None<span class="token punctuation">,</span> as_index<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> sort<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> group_keys<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> squeeze<span class="token operator">=</span>NoDefault<span class="token punctuation">.</span>no_default<span class="token punctuation">,</span> observed<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> dropna<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'Animal'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'Falcon'</span><span class="token punctuation">,</span> <span class="token string">'Falcon'</span><span class="token punctuation">,</span><span class="token string">'Parrot'</span><span class="token punctuation">,</span> <span class="token string">'Parrot'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                   <span class="token string">'Max Speed'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">380</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">370</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'Animal'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        Max SpeedAnimalFalcon      <span class="token number">375.0</span>Parrot       <span class="token number">25.0</span><span class="token comment" spellcheck="true"># agg()在指定轴上使用一项或多项操作进行聚合。</span>DataFrame<span class="token punctuation">.</span>agg<span class="token punctuation">(</span>func<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 用于聚合数据的函数。如果是函数,则必须在传递DataFrame或传递给DataFrame.apply时工作：function, str, list or dict</span>              axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># {0 or ‘index’,1 or ‘columns’}, default 0</span>              <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># The return can be:</span><span class="token comment" spellcheck="true"># scalar : when Series.agg is called with single function</span><span class="token comment" spellcheck="true"># Series : when DataFrame.agg is called with a single function</span><span class="token comment" spellcheck="true"># DataFrame : when DataFrame.agg is called with several functions</span><span class="token comment" spellcheck="true"># Return scalar, Series or DataFram</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">"id"</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">"name"</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token string">"id"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>agg<span class="token punctuation">(</span>list<span class="token punctuation">)</span><span class="token punctuation">)</span>id<span class="token number">1</span>    <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token number">2</span>    <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token number">3</span>    <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token number">4</span>    <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span>Name<span class="token punctuation">:</span> name<span class="token punctuation">,</span> dtype<span class="token punctuation">:</span> objectDataFrame<span class="token punctuation">.</span>copy<span class="token punctuation">(</span>deep<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># deep=True,深拷贝,deep=False,浅拷贝</span><span class="token keyword">print</span><span class="token punctuation">(</span>type<span class="token punctuation">(</span>df<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># &lt;class 'pandas.core.series.Series'&gt; 调用一列就是一个Series</span><span class="token comment" spellcheck="true"># map()根据输入对应关系映射Series的值。用于将Series中的每个值替换为另一个值,该值可能来自function,dict</span>Series<span class="token punctuation">.</span>map<span class="token punctuation">(</span>arg<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># Mapping correspondence. </span>           na_action<span class="token operator">=</span>None<span class="token punctuation">)</span>s <span class="token operator">=</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'cat'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> <span class="token string">'rabbit'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>s<span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'cat'</span><span class="token punctuation">:</span> <span class="token string">'kitten'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">:</span> <span class="token string">'puppy'</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 通过标签或布尔数组访问一组行和列。</span>            max_speed  shieldcobra               <span class="token number">1</span>       <span class="token number">2</span>viper               <span class="token number">4</span>       <span class="token number">5</span>sidewinder          <span class="token number">7</span>       <span class="token number">8</span>df<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token string">'viper'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># return a Series</span>df<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'viper'</span><span class="token punctuation">,</span> <span class="token string">'sidewinder'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># [[]]returns a DataFrame</span>df<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token string">'cobra'</span><span class="token punctuation">,</span> <span class="token string">'shield'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># single label for row and column 返回类型是string</span>df<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token string">"id"</span><span class="token punctuation">,</span><span class="token string">"cell_type"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 先行后列,逗号分割</span><span class="token comment" spellcheck="true"># 纯粹基于整数位置的索引，用于按位置选择。</span>df<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 只要每个group的第一个,和那个drop_duplicates起到的作用是一样的。</span>df<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token string">'id'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>agg<span class="token punctuation">(</span><span class="token string">'first'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># DataFrame.iterrows(),Iterate over DataFrame rows as (index, Series) pairs.</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'int'</span><span class="token punctuation">,</span> <span class="token string">'float'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>row <span class="token operator">=</span> next<span class="token punctuation">(</span>df<span class="token punctuation">.</span>iterrows<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>row<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> int      <span class="token number">1.0</span> float    <span class="token number">1.5</span> Name<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> dtype<span class="token punctuation">:</span> float64<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 要以 Pandas 的方式迭代遍历DataFrame的行，可以使用：</span><span class="token keyword">for</span> index<span class="token punctuation">,</span> row <span class="token keyword">in</span> df<span class="token punctuation">.</span>iterrows<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span> <span class="token punctuation">(</span>row<span class="token punctuation">[</span><span class="token string">"c1"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> row<span class="token punctuation">[</span><span class="token string">"c2"</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 将Series转换成DataFrame</span>Series<span class="token punctuation">.</span>to_frame<span class="token punctuation">(</span>name<span class="token operator">=</span>NoDefault<span class="token punctuation">.</span>no_default<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>s <span class="token operator">=</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token string">"b"</span><span class="token punctuation">,</span> <span class="token string">"c"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              name<span class="token operator">=</span><span class="token string">"vals"</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>s<span class="token punctuation">.</span>to_frame<span class="token punctuation">(</span><span class="token punctuation">)</span>  vals<span class="token number">0</span>    a<span class="token number">1</span>    b<span class="token number">2</span>    c </code></pre><p><code>groupby</code>详解</p><p><a href="https://zhuanlan.zhihu.com/p/101284491">https://zhuanlan.zhihu.com/p/101284491</a></p><img src="/np-pd-plt/image-20211106164247955.png" alt="image-20211106164247955" style="zoom:50%;"><pre class=" language-python"><code class="language-python">group <span class="token operator">=</span> data<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token string">"company"</span><span class="token punctuation">)</span>group<span class="token operator">&lt;</span>pandas<span class="token punctuation">.</span>core<span class="token punctuation">.</span>groupby<span class="token punctuation">.</span>generic<span class="token punctuation">.</span>DataFrameGroupBy object at <span class="token number">0x000002B7E2650240</span><span class="token operator">&gt;</span></code></pre><p>生成了一个<code>DataFrameGroupBy</code>对象,为了看看<code>group</code>内部究竟是什么,将其转换成<code>list</code>来查看:</p><pre class=" language-python"><code class="language-python">In <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">:</span> list<span class="token punctuation">(</span>group<span class="token punctuation">)</span>Out<span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'A'</span><span class="token punctuation">,</span>   company  salary  age  <span class="token number">3</span>       A      <span class="token number">20</span>   <span class="token number">22</span>  <span class="token number">6</span>       A      <span class="token number">23</span>   <span class="token number">33</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token punctuation">(</span><span class="token string">'B'</span><span class="token punctuation">,</span>   company  salary  age  <span class="token number">4</span>       B      <span class="token number">10</span>   <span class="token number">17</span>  <span class="token number">5</span>       B      <span class="token number">21</span>   <span class="token number">40</span>  <span class="token number">8</span>       B       <span class="token number">8</span>   <span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token punctuation">(</span><span class="token string">'C'</span><span class="token punctuation">,</span>   company  salary  age  <span class="token number">0</span>       C      <span class="token number">43</span>   <span class="token number">35</span>  <span class="token number">1</span>       C      <span class="token number">17</span>   <span class="token number">25</span>  <span class="token number">2</span>       C       <span class="token number">8</span>   <span class="token number">30</span>  <span class="token number">7</span>       C      <span class="token number">49</span>   <span class="token number">19</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><p>列表由三个元组组成,每个元组中,第一个元素是组别（这里是按照<code>company</code>进行分组，所以最后分为了<code>A</code>,<code>B</code>,<code>C</code>），第二个元素的是对应组别下的<code>DataFrame</code>,整个过程可以图解如下：</p><img src="/np-pd-plt/image-20211106164453476.png" alt="image-20211106164453476" style="zoom: 67%;"><img src="/np-pd-plt/image-20211106171634345.png" alt="image-20211106171634345" style="zoom:67%;"><p><strong>索引实际上变成了A,group by就是把index由0~N变成了A这一列</strong></p><p><code>agg</code>聚合操作</p><p>聚合操作是<code>groupby</code>后非常常见的操作，会写<code>SQL</code>的朋友对此应该是非常熟悉了。聚合操作可以用来求和、均值、最大值、最小值等。</p><img src="/np-pd-plt/image-20211106171013110.png" alt="image-20211106171013110" style="zoom:67%;"><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># tqdm对于pandas提供了支持</span><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdmdf <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">100000</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Apply a function along an axis of the DataFrame.</span>DataFrame<span class="token punctuation">.</span>apply<span class="token punctuation">(</span>func<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 应用于每一列或每一行的函数。</span>                axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># {0 or ‘index’, 1 or ‘columns’}, default 0</span><span class="token comment" spellcheck="true">#  or ‘columns’:apply function to each row. 0 or ‘index’: apply function to each column.</span>                raw<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> result_type<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># {‘expand’, ‘reduce’, ‘broadcast’, None}, default None</span><span class="token comment" spellcheck="true"># These only act when axis=1 (columns):None取决于函数的返回值类型： list-like results will be returned as a Series of those. However if the apply function returns a Series these are expanded to columns.</span><span class="token comment" spellcheck="true">#‘expand’ : list-like results will be turned into columns.</span><span class="token comment" spellcheck="true">#‘reduce’ : returns a Series if possible rather than expanding list-like results. This is the opposite of ‘expand’.</span><span class="token comment" spellcheck="true">#‘broadcast’ : results will be broadcast to the original shape of the DataFrame, the original index and columns will be retained.</span>                args<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`</span><span class="token comment" spellcheck="true"># (can use `tqdm.gui.tqdm`, `tqdm.notebook.tqdm`, optional kwargs, etc.)</span>tqdm<span class="token punctuation">.</span>pandas<span class="token punctuation">(</span>desc<span class="token operator">=</span><span class="token string">"my bar!"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Now you can use `progress_apply` instead of `apply`</span><span class="token comment" spellcheck="true"># and `progress_map` instead of `map`</span>df<span class="token punctuation">.</span>progress_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># can also groupby:</span><span class="token comment" spellcheck="true"># df.groupby(0).progress_apply(lambda x: x**2)</span><span class="token comment" spellcheck="true"># Merge DataFrame or named Series objects with a database-style join.</span><span class="token comment" spellcheck="true"># A named Series object is treated as a DataFrame with a single named column.</span>df<span class="token punctuation">.</span>apply<span class="token punctuation">(</span><span class="token punctuation">)</span>DataFrame<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>right<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># DataFrame or named Series 合并的对象</span>                how<span class="token operator">=</span><span class="token string">'inner'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># {‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’</span>                on<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># label or list. Column or index level names to join on. </span>                <span class="token comment" spellcheck="true"># 必须在两个 DataFrame 中都可以找到。 如果 on 是 None 并且不合并索引，则默认为两个 DataFrame 中列的交集。</span>                left_on<span class="token operator">=</span>None<span class="token punctuation">,</span> right_on<span class="token operator">=</span>None<span class="token punctuation">,</span> left_index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> right_index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> sort<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> suffixes<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'_x'</span><span class="token punctuation">,</span> <span class="token string">'_y'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> copy<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> indicator<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> validate<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 合并方式</span>left<span class="token punctuation">:</span> use only keys <span class="token keyword">from</span> left frame<span class="token punctuation">,</span> similar to a SQL left outer join<span class="token punctuation">;</span> preserve key order<span class="token punctuation">.</span>right<span class="token punctuation">:</span> use only keys <span class="token keyword">from</span> right frame<span class="token punctuation">,</span> similar to a SQL right outer join<span class="token punctuation">;</span> preserve key order<span class="token punctuation">.</span>outer<span class="token punctuation">:</span> use union of keys <span class="token keyword">from</span> both frames<span class="token punctuation">,</span> similar to a SQL full outer join<span class="token punctuation">;</span> sort keys lexicographically<span class="token punctuation">.</span>inner<span class="token punctuation">:</span> use intersection of keys <span class="token keyword">from</span> both frames<span class="token punctuation">,</span> similar to a SQL inner join<span class="token punctuation">;</span> preserve the order of the left keys<span class="token punctuation">.</span>cross<span class="token punctuation">:</span> creates the cartesian product <span class="token keyword">from</span> both frames<span class="token punctuation">,</span> preserves the order of the left keys<span class="token punctuation">.</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>df1 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'a'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'foo'</span><span class="token punctuation">,</span> <span class="token string">'bar'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>df2 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'a'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'foo'</span><span class="token punctuation">,</span> <span class="token string">'baz'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>df1<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>df2<span class="token punctuation">,</span> how<span class="token operator">=</span><span class="token string">'inner'</span><span class="token punctuation">,</span> on<span class="token operator">=</span><span class="token string">'a'</span><span class="token punctuation">)</span>      a  b  c<span class="token number">0</span>   foo  <span class="token number">1</span>  <span class="token number">3</span><span class="token comment" spellcheck="true"># 使用布尔表达式查询DataFrame的列。</span>df<span class="token punctuation">.</span>query<span class="token punctuation">(</span>expr<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># expr 要评估的查询字符串;可以在环境中引用变量,在变量前面加上@字符(@a+b);</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'A'</span><span class="token punctuation">:</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   <span class="token string">'B'</span><span class="token punctuation">:</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                   <span class="token string">'C C'</span><span class="token punctuation">:</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>query<span class="token punctuation">(</span><span class="token string">'A &gt; B'</span><span class="token punctuation">)</span>   A  B  C C<span class="token number">4</span>  <span class="token number">5</span>  <span class="token number">2</span>    <span class="token number">6</span>df<span class="token punctuation">.</span>isna<span class="token punctuation">(</span><span class="token punctuation">)</span>df<span class="token punctuation">.</span>isnull<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 都是用来判断是否为nan，完全相同，两个函数是为了模仿R的DataFrame</span></code></pre><h3 id="pandas读取表格"><a href="#pandas读取表格" class="headerlink" title="pandas读取表格"></a>pandas读取表格</h3><pre class=" language-python"><code class="language-python">df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_excel<span class="token punctuation">(</span><span class="token string">'data/dataset_example.xlsx'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># sheet_name(str, int, list, or None, default 0) : 讀取指定的工作表</span>a_int <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_excel<span class="token punctuation">(</span><span class="token string">'data/dataset_example.xlsx'</span><span class="token punctuation">,</span> sheet_name <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span></code></pre><h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>易百教程：<a href="https://www.yiibai.com/matplotlib">https://www.yiibai.com/matplotlib</a></p><p>API：<a href="https://matplotlib.org/stable/api/index.html">https://matplotlib.org/stable/api/index.html</a></p><h3 id="plg-fig-ax的区别"><a href="#plg-fig-ax的区别" class="headerlink" title="plg/fig/ax的区别"></a>plg/fig/ax的区别</h3><p><a href="https://zhuanlan.zhihu.com/p/93423829">知乎教程</a></p><p>在matplotlib中,有两种画图方式：</p><ul><li><p><code>plt.figure()</code>：<code>plt.xxx</code>系列。通过<code>plt.xxx</code>来画图,其实是取了一个捷径。这是通过<code>matplotlib</code>提供的一个api,这个<code>plt</code>提供了很多基本的function可以让你很快的画出图来,但是如果你想要更细致的精调,就要使用另外一种方法。</p><pre class=" language-python"><code class="language-python">plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">211</span><span class="token punctuation">)</span>   plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">)</span>   plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre></li><li><p><code>fig, ax = plt.subplots()</code>: 这个就是正统的稍微复杂一点的画图方法了。指定figure和axes,然后对axes单独操作。等下就讲figure和axes都神马意思。</p><pre class=" language-python"><code class="language-python">fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token punctuation">)</span>   ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">)</span></code></pre></li></ul><p>强烈建议在初学matplotlib的时候，<strong>尽量避免使用<code>plt.xxx</code>系列</strong>。当你明白<code>figure/axes/axis</code>都是控制什么的时候，如果你想要简单的制作一个quick and dirty的图，用<code>plt.xxx</code>才是OK。</p><p>matplotlib的名词定于对于非英语母语的人来说实在是太不友好了,尤其是axes。仰天长啸。</p><img src="/np-pd-plt/image-20211106220938936.png" alt="image-20211106220938936" style="zoom:80%;"><p><img src="/np-pd-plt/image-20220512103718744.png" alt="image-20220512103718744"></p><ul><li><strong>Figure</strong>:<code>fig = plt.figure()</code>: 可以解释为画布。</li><li><ul><li>画图的第一件事，就是创建一个画布figure，然后在这个画布上加各种元素。</li></ul></li><li><strong>Axes</strong>:<code>ax = fig.add_subplot(1,1,1)</code>: 不想定义,没法定义,就叫他axes！</li><li><ul><li>首先，这个不是你画图的xy坐标抽！</li><li>希望当初写这个lib的时候他们用一个更好的名字。。。</li><li>可以把<code>axes</code>理解为你要放到画布上的各个物体。比如你要画一个太阳,一个房子,一个车在画布上,那么太阳是一个<code>axes</code>,房子是一个<code>axes</code>,etc。</li><li>如果你的<code>figure</code>只有一张图,那么你只有一个<code>axes</code>。如果你的<code>figure</code>有<code>subplot</code>，那么每一个<code>subplot</code>就是一个<code>axes</code>。</li><li><code>axes</code>是<code>matlibplot</code>的宇宙中心!<code>axes</code>下可以修改编辑的变量非常多,基本上能包含你的所有需求。</li></ul></li><li><strong>Axis</strong>:<code>ax.xaxis/ax.yaxis</code>: 对,这才是你的xy坐标轴。</li><li><ul><li>每个坐标轴实际上也是<strong>由竖线和数字组成的</strong>,<strong>每一个竖线其实也是一个axis的subplot</strong>，因此<code>ax.xaxis</code>也存在<code>axes</code>这个对象。对这个<code>axes</code>进行编辑就会修改<code>xaxis</code>图像上的表现。</li></ul></li><li><strong>Artist</strong>:基本上，在 Figure上可见的所有东西都是 Artist（甚至是 Figure、Axes 和 Axis 对象）。 这包括 Text 对象、Line2D 对象、集合对象、Patch 对象等。当Figure被渲染时，所有的Artists都被绘制到画布上。 大多数Artists都被绑在axes上； 这样的artist不能被多个轴共享，也不能从一个轴移动到另一个轴。</li></ul><p><strong>「为什么plt没有指定画布和区域也能作图？」</strong></p><p>因为<code>matplotlib</code><strong>默认在最近创建的画布上绘制</strong>,而当你没有指定区域,告诉它去画图,他就会自动去生成一个<code>Axes</code>去绘制,进一步没有画布,也会自动去创建一个<code>Figure</code>,也称为<strong>隐式绘制</strong>。    </p><p><strong>图像的各个部位名称</strong></p><img src="/np-pd-plt/image-20211106221423457.png" alt="image-20211106221423457" style="zoom:80%;"><p><strong>使用ax标准的流程</strong>：</p><ol><li><strong>创建一个画布(Figure)</strong></li><li><strong>创建一个或者多个Axes</strong></li><li><strong>使用<code>ax.xxxx</code>在指定Axes上绘图</strong></li></ol><p><strong>首先，搞个画布</strong></p><p>我喜欢用<code>subplots</code>这个命令来开始画图。哪怕你没有subplot,也可以用这个subplots来创建一个画布。</p><p>这个function创建了一个大小为<code>(14,7)</code>的画布,把这个画布赋值给变量<code>fig</code>,同时在这个画布上创建了一个<code>axes</code>,把这个<code>axes</code>赋值给<code>ax</code>。这样,所有未来的<code>fig.xxx</code>都是对这个画布的操作，所有<code>ax.xxx</code>都是对这个<code>axes</code>的操作。</p><p>如果你有两个图,那么<code>ax</code>是一个有两个元素<code>ax[0],ax[1]</code>的<code>list</code>。<code>ax[0]</code>就对应第一个<code>subplot</code>的<code>ax</code>。</p><pre class=" language-python"><code class="language-python">fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">14</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># fig, ax = plt.subplots(2,1,figsize=(14,7))</span><span class="token comment" spellcheck="true"># ax[0].xxx</span><span class="token comment" spellcheck="true"># ax[1].xxx</span></code></pre><p><strong>好了画布搞好了，画数据。</strong></p><p>注意,我们这里依然不使用plt!因为我们要在这个axes上画数据,因此就用ax.plot()来画。画完第一个再call一次,再画第二个。</p><pre class=" language-python"><code class="language-python">ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">)</span>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>B<span class="token punctuation">,</span>A<span class="token punctuation">)</span></code></pre><p><strong>下面开始细节的处理</strong></p><p>数据画好了就可以各种细调坐标轴啊,tick啊之类的。首先把标题和xy坐标轴的标题搞定。Again,不用plt。直接在axes上进行设定。</p><pre class=" language-python"><code class="language-python">ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Title'</span><span class="token punctuation">,</span>fontsize<span class="token operator">=</span><span class="token number">18</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'xlabel'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">18</span><span class="token punctuation">,</span>fontfamily <span class="token operator">=</span> <span class="token string">'sans-serif'</span><span class="token punctuation">,</span>fontstyle<span class="token operator">=</span><span class="token string">'italic'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'ylabel'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token string">'x-large'</span><span class="token punctuation">,</span>fontstyle<span class="token operator">=</span><span class="token string">'oblique'</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>后是xy坐标轴的一些属性设定, 也是在axes level上完成的。</p><pre class=" language-python"><code class="language-python">ax<span class="token punctuation">.</span>set_aspect<span class="token punctuation">(</span><span class="token string">'equal'</span><span class="token punctuation">)</span> ax<span class="token punctuation">.</span>minorticks_on<span class="token punctuation">(</span><span class="token punctuation">)</span> ax<span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">16</span><span class="token punctuation">)</span> ax<span class="token punctuation">.</span>grid<span class="token punctuation">(</span>which<span class="token operator">=</span><span class="token string">'minor'</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token string">'both'</span><span class="token punctuation">)</span></code></pre><p>最后是坐标轴tick和细节,这个在<code>axes.xaxis</code>or<code>axes.yaxis</code>上完成。</p><pre class=" language-python"><code class="language-python">ax<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_tick_params<span class="token punctuation">(</span>rotation<span class="token operator">=</span><span class="token number">45</span><span class="token punctuation">,</span>labelsize<span class="token operator">=</span><span class="token number">18</span><span class="token punctuation">,</span>colors<span class="token operator">=</span><span class="token string">'w'</span><span class="token punctuation">)</span> start<span class="token punctuation">,</span> end <span class="token operator">=</span> ax<span class="token punctuation">.</span>get_xlim<span class="token punctuation">(</span><span class="token punctuation">)</span> ax<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_ticks<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>start<span class="token punctuation">,</span> end<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> ax<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>tick_right<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>画图的时候,请坚持使用<code>ax</code>格式。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 依次创建fig,ax</span>fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>a1 <span class="token operator">=</span> fig<span class="token punctuation">.</span>add_axes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span>a1<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>a1<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'exp'</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>labels <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'exp'</span><span class="token punctuation">,</span><span class="token string">'log'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>loc<span class="token operator">=</span><span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 利用subplots直接获得fig,ax</span>N <span class="token operator">=</span> <span class="token number">45</span>x<span class="token punctuation">,</span> y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span>c <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> size<span class="token operator">=</span>N<span class="token punctuation">)</span>s <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">220</span><span class="token punctuation">,</span> size<span class="token operator">=</span>N<span class="token punctuation">)</span>fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token punctuation">)</span>scatter <span class="token operator">=</span> ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> c<span class="token operator">=</span>c<span class="token punctuation">,</span> s<span class="token operator">=</span>s<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># produce a legend with the unique colors from the scatter</span>legend1 <span class="token operator">=</span> ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token operator">*</span>scatter<span class="token punctuation">.</span>legend_elements<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    loc<span class="token operator">=</span><span class="token string">"lower left"</span><span class="token punctuation">,</span> title<span class="token operator">=</span><span class="token string">"Classes"</span><span class="token punctuation">)</span>ax<span class="token punctuation">.</span>add_artist<span class="token punctuation">(</span>legend1<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># produce a legend with a cross section of sizes from the scatter</span>handles<span class="token punctuation">,</span> labels <span class="token operator">=</span> scatter<span class="token punctuation">.</span>legend_elements<span class="token punctuation">(</span>prop<span class="token operator">=</span><span class="token string">"sizes"</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.6</span><span class="token punctuation">)</span>legend2 <span class="token operator">=</span> ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> loc<span class="token operator">=</span><span class="token string">"upper right"</span><span class="token punctuation">,</span> title<span class="token operator">=</span><span class="token string">"Sizes"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="可视化中间特征图"><a href="#可视化中间特征图" class="headerlink" title="可视化中间特征图"></a>可视化中间特征图</h3><p>把网络中间某层的输出的特征图按通道作为图片进行可视化展示即可，如下述代码所示：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token comment" spellcheck="true">#get feature map of layer_activation</span>plt<span class="token punctuation">.</span>matshow<span class="token punctuation">(</span>layer_activation<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'viridis'</span><span class="token punctuation">)</span></code></pre><p><a href="https://blog.csdn.net/ztf312/article/details/102474190">cmap参数设置</a></p><h3 id="疑难解惑"><a href="#疑难解惑" class="headerlink" title="疑难解惑"></a>疑难解惑</h3><h4 id="1-整数还是浮点"><a href="#1-整数还是浮点" class="headerlink" title="1.整数还是浮点"></a>1.整数还是浮点</h4><p>为什么例如cityscape的labelID图片用<code>plt.imread</code>读取后再输出无法输出整数而是输出浮点数？</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">imread</span><span class="token punctuation">(</span>fname<span class="token punctuation">,</span> format<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">from</span> urllib <span class="token keyword">import</span> parse    <span class="token keyword">if</span> format <span class="token keyword">is</span> None<span class="token punctuation">:</span>        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>fname<span class="token punctuation">,</span> str<span class="token punctuation">)</span><span class="token punctuation">:</span>            parsed <span class="token operator">=</span> parse<span class="token punctuation">.</span>urlparse<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># If the string is a URL (Windows paths appear as if they have a</span>            <span class="token comment" spellcheck="true"># length-1 scheme), assume png.</span>            <span class="token comment" spellcheck="true"># 如果不是windows路径而是url,则假设是png图片</span>            <span class="token keyword">if</span> len<span class="token punctuation">(</span>parsed<span class="token punctuation">.</span>scheme<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>                ext <span class="token operator">=</span> <span class="token string">'png'</span>            <span class="token comment" spellcheck="true"># 是windows路径,取后缀</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                ext <span class="token operator">=</span> Path<span class="token punctuation">(</span>fname<span class="token punctuation">)</span><span class="token punctuation">.</span>suffix<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>        <span class="token keyword">elif</span> hasattr<span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'geturl'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Returned by urlopen().</span>            ext <span class="token operator">=</span> <span class="token string">'png'</span>        <span class="token keyword">elif</span> hasattr<span class="token punctuation">(</span>fname<span class="token punctuation">,</span> <span class="token string">'name'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            ext <span class="token operator">=</span> Path<span class="token punctuation">(</span>fname<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">.</span>suffix<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            ext <span class="token operator">=</span> <span class="token string">'png'</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        ext <span class="token operator">=</span> format    <span class="token comment" spellcheck="true"># 是png的话采用PIL.PngImagePlugin.PngImageFile而不是PIL.Image.open</span>    img_open <span class="token operator">=</span> <span class="token punctuation">(</span>        PIL<span class="token punctuation">.</span>PngImagePlugin<span class="token punctuation">.</span>PngImageFile <span class="token keyword">if</span> ext <span class="token operator">==</span> <span class="token string">'png'</span> <span class="token keyword">else</span> PIL<span class="token punctuation">.</span>Image<span class="token punctuation">.</span>open<span class="token punctuation">)</span>    <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>fname<span class="token punctuation">,</span> str<span class="token punctuation">)</span><span class="token punctuation">:</span>        parsed <span class="token operator">=</span> parse<span class="token punctuation">.</span>urlparse<span class="token punctuation">(</span>fname<span class="token punctuation">)</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>parsed<span class="token punctuation">.</span>scheme<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Pillow doesn't handle URLs directly.</span>            _api<span class="token punctuation">.</span>warn_deprecated<span class="token punctuation">(</span>                <span class="token string">"3.4"</span><span class="token punctuation">,</span> message<span class="token operator">=</span><span class="token string">"Directly reading images from URLs is "</span>                <span class="token string">"deprecated since %(since)s and will no longer be supported "</span>                <span class="token string">"%(removal)s. Please open the URL for reading and pass the "</span>                <span class="token string">"result to Pillow, e.g. with "</span>                <span class="token string">"``PIL.Image.open(urllib.request.urlopen(url))``."</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># hide imports to speed initial import on systems with slow linkers</span>            <span class="token keyword">from</span> urllib <span class="token keyword">import</span> request            ssl_ctx <span class="token operator">=</span> mpl<span class="token punctuation">.</span>_get_ssl_context<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> ssl_ctx <span class="token keyword">is</span> None<span class="token punctuation">:</span>                _log<span class="token punctuation">.</span>debug<span class="token punctuation">(</span>                    <span class="token string">"Could not get certifi ssl context, https may not work."</span>                <span class="token punctuation">)</span>            <span class="token keyword">with</span> request<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span>fname<span class="token punctuation">,</span> context<span class="token operator">=</span>ssl_ctx<span class="token punctuation">)</span> <span class="token keyword">as</span> response<span class="token punctuation">:</span>                <span class="token keyword">import</span> io                <span class="token keyword">try</span><span class="token punctuation">:</span>                    response<span class="token punctuation">.</span>seek<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>                <span class="token keyword">except</span> <span class="token punctuation">(</span>AttributeError<span class="token punctuation">,</span> io<span class="token punctuation">.</span>UnsupportedOperation<span class="token punctuation">)</span><span class="token punctuation">:</span>                    response <span class="token operator">=</span> io<span class="token punctuation">.</span>BytesIO<span class="token punctuation">(</span>response<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">return</span> imread<span class="token punctuation">(</span>response<span class="token punctuation">,</span> format<span class="token operator">=</span>ext<span class="token punctuation">)</span>    <span class="token keyword">with</span> img_open<span class="token punctuation">(</span>fname<span class="token punctuation">)</span> <span class="token keyword">as</span> image<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 是png采用_pil_png_to_float_array()而不是pil_to_array()</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>_pil_png_to_float_array<span class="token punctuation">(</span>image<span class="token punctuation">)</span>                <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>image<span class="token punctuation">,</span> PIL<span class="token punctuation">.</span>PngImagePlugin<span class="token punctuation">.</span>PngImageFile<span class="token punctuation">)</span> <span class="token keyword">else</span>                pil_to_array<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">_pil_png_to_float_array</span><span class="token punctuation">(</span>pil_png<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Convert a PIL `PNGImageFile` to a 0-1 float array."""</span>    mode <span class="token operator">=</span> pil_png<span class="token punctuation">.</span>mode    rawmode <span class="token operator">=</span> pil_png<span class="token punctuation">.</span>png<span class="token punctuation">.</span>im_rawmode    <span class="token keyword">if</span> rawmode <span class="token operator">==</span> <span class="token string">"1"</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Grayscale.</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>pil_png<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token keyword">if</span> rawmode <span class="token operator">==</span> <span class="token string">"L;2"</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Grayscale.</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>pil_png<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">2</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token keyword">if</span> rawmode <span class="token operator">==</span> <span class="token string">"L;4"</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Grayscale.</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>pil_png<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">4</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># rawmode是L,所以被除以了255。</span>    <span class="token keyword">if</span> rawmode <span class="token operator">==</span> <span class="token string">"L"</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Grayscale.</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>pil_png<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">8</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token keyword">if</span> rawmode <span class="token operator">==</span> <span class="token string">"I;16B"</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Grayscale.</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>pil_png<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">16</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">"RGB"</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># RGB.</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>pil_png<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">8</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">"P"</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Palette.</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>pil_png<span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">"RGBA"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">8</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">"LA"</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># Grayscale + alpha.</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>pil_png<span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">"RGBA"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">8</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">"RGBA"</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># RGBA.</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>pil_png<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">8</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>f<span class="token string">"Unknown PIL rawmode: {rawmode}"</span><span class="token punctuation">)</span></code></pre><p><code>pil_png.png</code>是<code>&lt;PIL.PngImagePlugin.PngStream object</code>,<code>a = np.array(pil_png) or plt.imshow(pil_png)</code>之后,<code>pil_png.png</code>都会变为None(暂时不知道为什么)</p><p>所以其实乘以255就会得到正确的</p><h4 id="2-cmap问题"><a href="#2-cmap问题" class="headerlink" title="2.cmap问题"></a>2.cmap问题</h4><p>为什么<code>plt.imshow()</code>显示上面的被变成小数的array会是彩色的?</p><pre class=" language-python"><code class="language-python">matplotlib<span class="token punctuation">.</span>pyplot<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>X<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># array-like or PIL image</span>                         cmap<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 颜色图谱 </span>                         norm<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 归一化,将scalar归一化到[0,1],默认的话使用线性缩放，最小值映射到0,最大值映射到1</span>                         aspect<span class="token operator">=</span>None<span class="token punctuation">,</span> interpolation<span class="token operator">=</span>None<span class="token punctuation">,</span> alpha<span class="token operator">=</span>None<span class="token punctuation">,</span> vmin<span class="token operator">=</span>None<span class="token punctuation">,</span> vmax<span class="token operator">=</span>None<span class="token punctuation">,</span> origin<span class="token operator">=</span>None<span class="token punctuation">,</span> extent<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> filternorm<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> filterrad<span class="token operator">=</span><span class="token number">4.0</span><span class="token punctuation">,</span> resample<span class="token operator">=</span>None<span class="token punctuation">,</span> url<span class="token operator">=</span>None<span class="token punctuation">,</span> data<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre><p>Display data as an image, i.e., on a 2D regular raster.</p><p><strong>The input may either be actual RGB(A) data, or 2D scalar data, which will be rendered as a pseudocolor伪彩色 image. For displaying a grayscale image set up the colormapping using the parameters <code>cmap='gray', vmin=0, vmax=255</code>.cmap其实不就是PIL的调色板模式吗？！</strong></p><p>对于X,the image data. Supported array shapes are:</p><ul><li>(M, N): an image with scalar data. The values are <strong>mapped to colors using normalization and a colormap</strong>. See parameters <strong><em>norm</em>, <em>cmap</em>, <em>vmin</em>, <em>vmax</em>.</strong></li><li>(M, N, 3): an image with RGB values (0-1 float or 0-255 int).</li><li>(M, N, 4): an image with RGBA values (0-1 float or 0-255 int), i.e. including transparency.</li></ul><p>The first two dimensions (M, N) define the rows and columns of the image.</p><p>Out-of-range RGB(A) values are clipped.</p><table><thead><tr><th>autumn</th><th>红-橙-黄</th></tr></thead><tbody><tr><td>bone</td><td>黑-白，x线</td></tr><tr><td>cool</td><td>青-洋红</td></tr><tr><td>copper</td><td>黑-铜</td></tr><tr><td>flag</td><td>红-白-蓝-黑</td></tr><tr><td>gray</td><td>黑-白</td></tr><tr><td>hot</td><td>黑-红-黄-白</td></tr><tr><td>hsv</td><td>hsv颜色空间， 红-黄-绿-青-蓝-洋红-红</td></tr><tr><td>inferno</td><td>黑-红-黄</td></tr><tr><td>jet</td><td>蓝-青-黄-红</td></tr><tr><td>magma</td><td>黑-红-白</td></tr><tr><td>pink</td><td>黑-粉-白</td></tr><tr><td>plasma</td><td>绿-红-黄</td></tr><tr><td>prism</td><td>红-黄-绿-蓝-紫-…-绿模式</td></tr><tr><td>spring</td><td>洋红-黄</td></tr><tr><td>summer</td><td>绿-黄</td></tr><tr><td>viridis</td><td>蓝-绿-黄</td></tr><tr><td>winter</td><td>蓝-绿</td></tr></tbody></table><p><code>cmap</code>不输入的话默认是<code>viridis</code>模式</p><p>When <strong>using scalar data</strong> and <strong>no explicit norm,</strong> vmin and vmax <strong>define the data range</strong> that the <strong>colormap covers</strong>. By default, <strong>the colormap covers the complete value range of the supplied data</strong>. It is <strong>deprecated to use vmin/vmax when norm is given</strong>. When <strong>using RGB(A) data</strong>, parameters <strong>vmin/vmax are ignored</strong>.</p><p><img src="/np-pd-plt/np&amp;pd&amp;plt%5Cimage-20210921200133539.png" alt="vmin&amp;vmax"></p><p>颜色</p><img src="/np-pd-plt/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20230113063319761.png" alt="image-20230113063319761" style="zoom:67%;"><h3 id="基本函数"><a href="#基本函数" class="headerlink" title="基本函数"></a>基本函数</h3><p><a href="https://www.fontke.com/tool/rgb/800000/">根据RGB  CMYK  HSL  HSV  XYZ 值来查询颜色</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>image <span class="token keyword">as</span> imgplt<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltx <span class="token operator">=</span> imgplt<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'label.png'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">300</span><span class="token punctuation">:</span><span class="token number">350</span><span class="token punctuation">,</span><span class="token number">150</span><span class="token punctuation">:</span><span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#matplotlib显示图像，注意x只用一个[]，不同维度用逗号分割！</span><span class="token comment" spellcheck="true">#imread的返回值是图片数据data，数据类型是class:`numpy.array`。这个图片数据data的维度如下：</span><span class="token comment" spellcheck="true">#- (M, N) 对于灰度级图片</span><span class="token comment" spellcheck="true">#- (M, N, 3) 对于RGB彩色图片.</span><span class="token comment" spellcheck="true">#- (M, N, 4) 对于RGBA彩色图片</span>plt<span class="token punctuation">.</span>colorbar<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># colorbar()如下图中红框标出</span></code></pre><img src="/np-pd-plt/image-20210422211101827.png" alt="colorbar" style="zoom:67%;"><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Create a new figure, or activate an existing figure.</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>num<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># A unique identifier for the figure.</span>           figsize<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 指定figure的宽和高，单位为英寸；</span>           dpi<span class="token operator">=</span>None<span class="token punctuation">,</span>           facecolor<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 背景颜色</span>           edgecolor<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 边界颜色</span>           frameon<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 是否显示边框</span>           FigureClass<span class="token operator">=</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'matplotlib.figure.Figure'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> clear<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 图片的保存</span>plt<span class="token punctuation">.</span>imsave<span class="token punctuation">(</span>filename<span class="token punctuation">,</span>X<span class="token punctuation">,</span>format<span class="token operator">=</span><span class="token string">'png'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建单个子图</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span>nrows<span class="token punctuation">,</span>ncows<span class="token punctuation">,</span>index<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#nrows:行数 ncols:列数 index:索引值,图片放在第几个窗格</span><span class="token comment" spellcheck="true"># Create a figure and a set of subplots.</span><span class="token comment" spellcheck="true"># 创建多个子图</span>pyplot<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>nrows<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 行数</span>                ncols<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 列数</span>                <span class="token operator">*</span><span class="token punctuation">,</span> sharex<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> sharey<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> squeeze<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> subplot_kw<span class="token operator">=</span>None<span class="token punctuation">,</span> gridspec_kw<span class="token operator">=</span>None<span class="token punctuation">,</span>                          <span class="token operator">**</span>fig_kw<span class="token comment" spellcheck="true"># 所有其他关键字参数都传递给 pyplot.figure 调用。</span>               <span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span> fig<span class="token punctuation">,</span> ax<span class="token comment" spellcheck="true"># ticks:设置刻度 labels:设置刻度标签</span>plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span>ticks<span class="token operator">=</span>None<span class="token punctuation">,</span>labels<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># xlabel: label的文本</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span>xlabel<span class="token punctuation">:</span>str<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将图片展示到figure上</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>train_images<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span>plt<span class="token punctuation">.</span>cm<span class="token punctuation">.</span>binary<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#train_images[i]:必须是array-like或PIL image</span><span class="token comment" spellcheck="true">#cmap:用于将标量数据映射到颜色的Colormap实例或已注册的colormap名称</span><span class="token comment" spellcheck="true"># 显示figure</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将当前Axes设置为ax,将当前Figure设置为ax的父级。</span>matplotlib<span class="token punctuation">.</span>pyplot<span class="token punctuation">.</span>sca<span class="token punctuation">(</span>ax<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># set current ax</span><span class="token comment" spellcheck="true"># tight_layout会自动调整子图参数，使之填充整个图像区域。这是个实验特性，可能在一些情况下不工作。它仅仅检查坐标轴标签、刻度标签以及标题的部分。</span><span class="token comment" spellcheck="true"># 因为当你拥有多个子图时，你会经常看到不同轴域的标签叠在一起。</span>plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 去掉坐标轴</span>ax<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">"off"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 去掉刻度</span>plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="各种图形画法"><a href="#各种图形画法" class="headerlink" title="各种图形画法"></a>各种图形画法</h3><p>散点图</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">plot_embedding</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> label<span class="token punctuation">,</span> title<span class="token punctuation">)</span><span class="token punctuation">:</span>    fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 归一化</span>    x_min<span class="token punctuation">,</span> x_max <span class="token operator">=</span> np<span class="token punctuation">.</span>min<span class="token punctuation">(</span>data<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>data<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>    data <span class="token operator">=</span> <span class="token punctuation">(</span>data <span class="token operator">-</span> x_min<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>x_max <span class="token operator">-</span> x_min<span class="token punctuation">)</span>    data_s <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>    data_d <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>    data_n <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>    ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>data_s<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>data_s<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c<span class="token operator">=</span><span class="token string">"red"</span><span class="token punctuation">,</span>s<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>alpha<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"S"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#label的设置很重要,label是标记了legend</span>    ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>data_d<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>data_d<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c<span class="token operator">=</span><span class="token string">"green"</span><span class="token punctuation">,</span>s<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>alpha<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"D1"</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>data_n<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>data_n<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c<span class="token operator">=</span><span class="token string">"blue"</span><span class="token punctuation">,</span>s<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>alpha<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"D2"</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span>title<span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> figfig <span class="token operator">=</span> plot_embedding<span class="token punctuation">(</span>result<span class="token punctuation">,</span> label<span class="token punctuation">,</span><span class="token string">'t-SNE embedding of the digits'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h2><p><a href="https://pillow.readthedocs.io/en/stable/">PIL Api</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np   <span class="token comment" spellcheck="true">#可以这样用PIL读取处理图片,然后最后再转成ndarray</span><span class="token keyword">from</span> PIL <span class="token keyword">import</span> Imageimg <span class="token operator">=</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span><span class="token string">'label.png'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 转换成img2之后,此时输出的值是真实的图片的值</span><span class="token comment" spellcheck="true"># PIL读取的图片不能输出，用np.array()转化会变成单通道的灰度图</span><span class="token comment" spellcheck="true"># PIL图像在转换为numpy.ndarray后，格式为(h,w,c)，像素顺序为RGB；</span><span class="token comment" spellcheck="true"># OpenCV在cv2.imread()后数据类型为numpy.ndarray，格式为(h,w,c)，像素顺序为BGR。</span>img2 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>img<span class="token punctuation">)</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>img2<span class="token punctuation">)</span>img<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 在windows使用自带的图像显示程序显示图像，在linux使用display\eog\xv,具体取决于找到哪个</span><span class="token comment" spellcheck="true"># 直接输出img如下</span><span class="token keyword">print</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=2048x1024 at 0x1C137643C70&gt;</span><span class="token keyword">print</span><span class="token punctuation">(</span>img<span class="token punctuation">.</span>format<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 打印出格式信息，此处输出PNG</span><span class="token keyword">print</span><span class="token punctuation">(</span>img<span class="token punctuation">.</span>mode<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># L</span>img3<span class="token operator">=</span>img<span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">"RGB"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 转换图片的mode</span><span class="token comment" spellcheck="true"># 将array转换成图片</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">import</span> PIL<span class="token punctuation">.</span>Image <span class="token keyword">as</span> IMG<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npa <span class="token operator">=</span> IMG<span class="token punctuation">.</span>fromarray<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 必须转换成整数</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># PIL.Image.Image是一个类实例，包含自己的属性,通道顺序是(H,W,C)</span><span class="token comment" spellcheck="true"># &lt;PIL.Image.Image image mode=RGB size=2x3 at 0x7F51B6B9AF10&gt;</span><span class="token comment" spellcheck="true"># 转为调色板模式</span>new_mask <span class="token operator">=</span> Image<span class="token punctuation">.</span>fromarray<span class="token punctuation">(</span>mask<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'P'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 模式'L'为灰色图像，它的每个像素用8个bit表示，0表示黑，255表示白，其他数字表示不同的灰度。</span><span class="token comment" spellcheck="true"># 模式“P”为8位彩色图像，它的每个像素用8个bit表示，其对应的彩色值是按照调色板索引值查询出来的。</span><span class="token comment" spellcheck="true"># 0-255,每个对应一个颜色。</span><span class="token comment" spellcheck="true"># 标签图像的模式正是'P'模式，因此测试时要生成对应标签图像的图片的话，构建一个调色板然后上色即可。</span>new_mask<span class="token punctuation">.</span>putpalette<span class="token punctuation">(</span>palette<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 保存图片,以给定的文件名保存此图像.如果未指定格式,则使用的格式由文件扩展名决定(如果可能)。</span>Image<span class="token punctuation">.</span>save<span class="token punctuation">(</span>fp<span class="token punctuation">,</span> format<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">**</span>params<span class="token punctuation">)</span>img<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'F:/360data/重要数据/桌面/att/050_1.png'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#保存</span>img <span class="token operator">=</span> img<span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span>ow<span class="token punctuation">,</span> oh<span class="token punctuation">)</span><span class="token punctuation">,</span> Image<span class="token punctuation">.</span>BILINEAR<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># mask用最近邻插值,直接选最近的数据点赋值</span>mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span>ow<span class="token punctuation">,</span> oh<span class="token punctuation">)</span><span class="token punctuation">,</span> Image<span class="token punctuation">.</span>NEAREST<span class="token punctuation">)</span>Image<span class="token punctuation">.</span>crop<span class="token punctuation">(</span>left<span class="token punctuation">,</span> up<span class="token punctuation">,</span> right<span class="token punctuation">,</span> below<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 切割图片</span><span class="token comment" spellcheck="true"># left：与左边界的距离 up：与上边界的距离 right：还是与左边界的距离 below：还是与上边界的距离</span><span class="token comment" spellcheck="true">#旋转图像,逆时针旋转90度,expand默认为0,表示旋转前后尺度不变,从HxW-&gt;HxW,expand=1就是HxW-&gt;WxH</span>yourimage<span class="token punctuation">.</span>rotate<span class="token punctuation">(</span><span class="token number">90</span><span class="token punctuation">,</span>expand<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p>图像的模式,常见的<strong>mode</strong> 有如下:</p><img src="/np-pd-plt/image-20210921164138454.png" alt="图像的mode" style="zoom:80%;"><p><strong>jpg都是RGB格式</strong> </p><p><strong>png是RGBA格式</strong></p><p><strong>P模式可把我骗惨了，一个Png的图片,使用PIL.Image读取,可能是P模式的，虽然这个png图片你看着是有颜色的,但是其实读出来是1,2,3这种label。如果你把他转为RGB格式，则会变成对应label变成的对应palette</strong></p><p>1:位图 L:灰度模式 </p><p>三通道RGBnumpy读取显示是0-1，而不是0-255，这个是归一化的RGB！！正好和概率二分类对应！</p><p>==<strong>使用FastStone查看图片,图像是24bit是3通道的,8bit的是1通道的！</strong>==如果是8bit却是彩色值的话,那就说明是调色板模式</p><p><strong>Snipaste竟然可以实时获取颜色值！！！如下：</strong></p><p><img src="/np-pd-plt/image-20201207093513840.png" alt="采集颜色值"></p><p><strong>tensor与pil相互转换</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># PIL to tensor </span><span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image<span class="token keyword">import</span> torch<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transformstrans <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>             transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>             transform<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> img <span class="token operator">=</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>img_path<span class="token punctuation">)</span>img <span class="token operator">=</span> trans<span class="token punctuation">(</span>img<span class="token punctuation">)</span>img <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 填充一维</span><span class="token comment" spellcheck="true"># tensor to PIL</span>image <span class="token operator">=</span> im_tensor<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>image <span class="token operator">=</span> image<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 压缩一维</span>image <span class="token operator">=</span> transforms<span class="token punctuation">.</span>ToPILImage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>image<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 自动转换为0-255 并变换通道为HWC</span></code></pre><h3 id="有用的函数"><a href="#有用的函数" class="headerlink" title="有用的函数"></a>有用的函数</h3><p><strong>可视化tensor</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">tensor2im</span><span class="token punctuation">(</span>input_image<span class="token punctuation">,</span> imtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">""""    Parameters:        input_image (tensor) --  输入的tensor，维度为CHW，注意这里没有batch size的维度        imtype (type)        --  转换后的numpy的数据类型    """</span>    mean <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.485</span><span class="token punctuation">,</span> <span class="token number">0.456</span><span class="token punctuation">,</span> <span class="token number">0.406</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># dataLoader中设置的mean参数，需要从dataloader中拷贝过来</span>    std <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># dataLoader中设置的std参数，需要从dataloader中拷贝过来</span>    <span class="token keyword">if</span> <span class="token operator">not</span> isinstance<span class="token punctuation">(</span>input_image<span class="token punctuation">,</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>input_image<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 如果传入的图片类型为torch.Tensor，则读取其数据进行下面的处理</span>            image_tensor <span class="token operator">=</span> input_image<span class="token punctuation">.</span>data        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> input_image        image_numpy <span class="token operator">=</span> image_tensor<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># convert it into a numpy array</span>        <span class="token keyword">if</span> image_numpy<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># grayscale to RGB</span>            image_numpy <span class="token operator">=</span> np<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>image_numpy<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>mean<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 反标准化，乘以方差，加上均值</span>            image_numpy<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> image_numpy<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> std<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> mean<span class="token punctuation">[</span>i<span class="token punctuation">]</span>        image_numpy <span class="token operator">=</span> image_numpy <span class="token operator">*</span> <span class="token number">255</span> <span class="token comment" spellcheck="true">#反ToTensor(),从[0,1]转为[0,255]</span>        image_numpy <span class="token operator">=</span> np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>image_numpy<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 从(channels, height, width)变为(height, width, channels)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 如果传入的是numpy数组,则不做处理</span>        image_numpy <span class="token operator">=</span> input_image    <span class="token keyword">return</span> image_numpy<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>imtype<span class="token punctuation">)</span></code></pre><p><strong>特征图热力图</strong></p><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">get_heatmap</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>        data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># heatmap = cv2.applyColorMap(np.uint8(255*data), cv2.COLORMAP_JET)</span>        <span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns        <span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt        <span class="token comment" spellcheck="true"># 引入 FigureCanvasAgg</span>        <span class="token keyword">from</span> matplotlib<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>backend_agg <span class="token keyword">import</span> FigureCanvasAgg        plt<span class="token punctuation">.</span>clf<span class="token punctuation">(</span><span class="token punctuation">)</span>        sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>data<span class="token punctuation">,</span> center<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 引入 Image</span>        <span class="token keyword">import</span> PIL<span class="token punctuation">.</span>Image <span class="token keyword">as</span> Image        <span class="token comment" spellcheck="true"># 将plt转化为numpy数据</span>        canvas <span class="token operator">=</span> FigureCanvasAgg<span class="token punctuation">(</span>plt<span class="token punctuation">.</span>gcf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 绘制图像</span>        canvas<span class="token punctuation">.</span>draw<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 获取图像尺寸</span>        w<span class="token punctuation">,</span> h <span class="token operator">=</span> canvas<span class="token punctuation">.</span>get_width_height<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 解码string 得到argb图像</span>        buf <span class="token operator">=</span> np<span class="token punctuation">.</span>fromstring<span class="token punctuation">(</span>canvas<span class="token punctuation">.</span>tostring_argb<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 重构成w h 4(argb)图像</span>        buf<span class="token punctuation">.</span>shape <span class="token operator">=</span> <span class="token punctuation">(</span>w<span class="token punctuation">,</span> h<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 转换为 RGBA</span>        buf <span class="token operator">=</span> np<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>buf<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 得到 Image RGBA图像对象 (需要Image对象的同学到此为止就可以了)</span>        image <span class="token operator">=</span> Image<span class="token punctuation">.</span>frombytes<span class="token punctuation">(</span><span class="token string">"RGBA"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>w<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token punctuation">,</span> buf<span class="token punctuation">.</span>tobytes<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 转换为numpy array rgba四通道数组</span>        image <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>image<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 转换为rgb图像</span>        heatmap <span class="token operator">=</span> image<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> heatmap</code></pre><h2 id="cv2"><a href="#cv2" class="headerlink" title="cv2"></a>cv2</h2><p><a href="https://docs.opencv.org/4.x/index.html">opencv的API</a></p><h3 id="基本函数-1"><a href="#基本函数-1" class="headerlink" title="基本函数"></a>基本函数</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token comment" spellcheck="true"># Loads an image from a file.</span>cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>filename<span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># Name of file to be loaded.</span>          flags<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># Flag that can take values of cv2::ImreadModes</span>         <span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回值:&lt;class 'numpy.ndarray'&gt;,如果读取文件错误,那么返回空矩阵</span><span class="token comment" spellcheck="true"># In the case of color images, the decoded images will have the channels stored in B G R order.</span><span class="token comment" spellcheck="true"># 返回的通道顺序为 H W C</span>cv2<span class="token punctuation">.</span>imwrite<span class="token punctuation">(</span>filename<span class="token punctuation">,</span> img<span class="token punctuation">[</span><span class="token punctuation">,</span> params<span class="token punctuation">]</span>    <span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 报错,module 'cv2' has no attribute 'imread'</span><span class="token comment" spellcheck="true"># 版本问题,这个算法被申请了专利，把版本降下去就可以了。依次执行以下操作</span>pip uninstall opencv<span class="token operator">-</span>pythonpip install opencv<span class="token operator">-</span>python<span class="token operator">==</span><span class="token number">3.4</span><span class="token punctuation">.</span><span class="token number">2.16</span>pip install opencv<span class="token operator">-</span>contrib<span class="token operator">-</span>python<span class="token operator">==</span><span class="token number">3.4</span><span class="token punctuation">.</span><span class="token number">2.16</span></code></pre><img src="/np-pd-plt/image-20211114001200331.png" alt="目前支持的图片格式" style="zoom:67%;"><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Converts an image from one color space to another.</span><span class="token comment" spellcheck="true"># OpenCV默认颜色格式为BGR</span>cv<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>src<span class="token punctuation">,</span> code<span class="token punctuation">[</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 色彩空间转换代码</span>            dst<span class="token punctuation">[</span><span class="token punctuation">,</span> dstCn<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>image<span class="token operator">=</span>cv<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>image<span class="token punctuation">,</span>cv<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span></code></pre><p>R、G、B通道值的常规范围是:</p><ul><li>0 to 255 for CV_8U images</li><li>0 to 65535 for CV_16U images</li><li>0 to 1 for CV_32F images</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 将多通道的array分为几个单通道的array</span>cv<span class="token punctuation">.</span>split<span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将几个单通道的array合并为一个多通道的array</span>cv<span class="token punctuation">.</span>merge<span class="token punctuation">(</span><span class="token punctuation">(</span>a1<span class="token punctuation">,</span>a2<span class="token punctuation">,</span>a3<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p><code>ColorConvensionCodes</code></p><ul><li>COLOR_BGR2RGB</li></ul><pre class=" language-python"><code class="language-python"></code></pre><h3 id="实用功能小组件"><a href="#实用功能小组件" class="headerlink" title="实用功能小组件"></a>实用功能小组件</h3><p><strong>show image and mask</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">show_img</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> mask<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>    clahe <span class="token operator">=</span> cv2<span class="token punctuation">.</span>createCLAHE<span class="token punctuation">(</span>clipLimit<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span> tileGridSize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#     img = clahe.apply(img)</span><span class="token comment" spellcheck="true">#     plt.figure(figsize=(10,10))</span>    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'bone'</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># plt.imshow(np.ma.masked_where(mask!=1, mask), alpha=0.5, cmap='autumn')</span>        plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>        handles <span class="token operator">=</span> <span class="token punctuation">[</span>Rectangle<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span> color<span class="token operator">=</span>_c<span class="token punctuation">)</span> <span class="token keyword">for</span> _c <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0.667</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.667</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.667</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"Large Bowel"</span><span class="token punctuation">,</span> <span class="token string">"Small Bowel"</span><span class="token punctuation">,</span> <span class="token string">"Stomach"</span><span class="token punctuation">]</span>        plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token punctuation">,</span>labels<span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span></code></pre><p><a href="https://www.cnblogs.com/juzicode/p/15659437.html">直方图均衡教程,包括上图的createCLAHE的自适应局部直方图均衡</a></p><p><strong>Histogram Equalization</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> cv2 <span class="token keyword">as</span> cv<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> pltpath <span class="token operator">=</span> <span class="token string">"./1.jpeg"</span>img <span class="token operator">=</span> cv<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">)</span>cv<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'image'</span><span class="token punctuation">,</span>img<span class="token punctuation">)</span>cv<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>cv<span class="token punctuation">.</span>destroyAllWindows<span class="token punctuation">(</span><span class="token punctuation">)</span>hist<span class="token punctuation">,</span>bins <span class="token operator">=</span> np<span class="token punctuation">.</span>histogram<span class="token punctuation">(</span>img<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">]</span><span class="token punctuation">)</span>cdf <span class="token operator">=</span> hist<span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token punctuation">)</span>cdf_normalized <span class="token operator">=</span> cdf <span class="token operator">*</span> float<span class="token punctuation">(</span>hist<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> cdf<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>cdf_normalized<span class="token punctuation">,</span> color <span class="token operator">=</span> <span class="token string">'b'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>img<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color <span class="token operator">=</span> <span class="token string">'r'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlim<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'cdf'</span><span class="token punctuation">,</span><span class="token string">'histogram'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> loc <span class="token operator">=</span> <span class="token string">'upper left'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># equ = cv.equalizeHist(img)</span><span class="token comment" spellcheck="true">#</span>R<span class="token punctuation">,</span> G<span class="token punctuation">,</span> B <span class="token operator">=</span> cv<span class="token punctuation">.</span>split<span class="token punctuation">(</span>img<span class="token punctuation">)</span>output1_R <span class="token operator">=</span> cv<span class="token punctuation">.</span>equalizeHist<span class="token punctuation">(</span>R<span class="token punctuation">)</span>output1_G <span class="token operator">=</span> cv<span class="token punctuation">.</span>equalizeHist<span class="token punctuation">(</span>G<span class="token punctuation">)</span>output1_B <span class="token operator">=</span> cv<span class="token punctuation">.</span>equalizeHist<span class="token punctuation">(</span>B<span class="token punctuation">)</span>equ <span class="token operator">=</span> cv<span class="token punctuation">.</span>merge<span class="token punctuation">(</span><span class="token punctuation">(</span>output1_R<span class="token punctuation">,</span> output1_G<span class="token punctuation">,</span> output1_B<span class="token punctuation">)</span><span class="token punctuation">)</span>cv<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'equ.png'</span><span class="token punctuation">,</span>equ<span class="token punctuation">)</span>cv<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>cv<span class="token punctuation">.</span>destroyAllWindows<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><strong>cv2利用sobel算子检测边缘</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npimg <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">"D:/gui.jpg"</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>x <span class="token operator">=</span> cv2<span class="token punctuation">.</span>Sobel<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>CV_16S<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>y <span class="token operator">=</span> cv2<span class="token punctuation">.</span>Sobel<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>CV_16S<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>absX <span class="token operator">=</span> cv2<span class="token punctuation">.</span>convertScaleAbs<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 转回unit8</span>absY <span class="token operator">=</span> cv2<span class="token punctuation">.</span>convertScaleAbs<span class="token punctuation">(</span>y<span class="token punctuation">)</span>dst <span class="token operator">=</span> cv2<span class="token punctuation">.</span>addWeighted<span class="token punctuation">(</span>absX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> absY<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">"absX"</span><span class="token punctuation">,</span> absX<span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">"absY"</span><span class="token punctuation">,</span> absY<span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">"Result"</span><span class="token punctuation">,</span> dst<span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>cv2<span class="token punctuation">.</span>destroyAllWindows<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="plotly"><a href="#plotly" class="headerlink" title="plotly"></a>plotly</h2><p><a href="https://plotly.com/python/">api</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pdpd<span class="token punctuation">.</span>options<span class="token punctuation">.</span>plotting<span class="token punctuation">.</span>backend <span class="token operator">=</span> <span class="token string">"plotly"</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>dict<span class="token punctuation">(</span>a<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> b<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig <span class="token operator">=</span> df<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="jcm"><a href="#jcm" class="headerlink" title="jcm"></a>jcm</h2>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> numpy </tag>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>托福笔记</title>
      <link href="tuo-fu-bi-ji/"/>
      <url>tuo-fu-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="曲根词汇"><a href="#曲根词汇" class="headerlink" title="曲根词汇"></a>曲根词汇</h2><h4 id="阅读记忆"><a href="#阅读记忆" class="headerlink" title="阅读记忆"></a>阅读记忆</h4><p>经济学人 时代 卫报</p><p>生词圈出，反复研读句子。勾画词组，了解替换，熟词辟意。</p><h4 id="词根词缀"><a href="#词根词缀" class="headerlink" title="词根词缀"></a>词根词缀</h4><p>prefix </p><ol><li>肯定，否定</li><li>方向</li><li>数字</li></ol><p>suffix</p><ol><li>词性</li></ol><p>root-根的逆推/想到熟词</p><ol><li>意义</li></ol><h4 id="词源-Etymology"><a href="#词源-Etymology" class="headerlink" title="词源 Etymology"></a>词源 Etymology</h4><p><strong>看读音，看拼写，看意思</strong></p><p>元音和元音字母组合之间可以替换：<strong>a,e,i,o,u,y</strong></p><p>辅音之间：<strong>p\b,t\d,k\g\c\qu\x,f\v,s\x\z\th</strong></p><p>形近字母的互换：<strong>u/v/w(特征是去掉元音),m/n（m/n可以省略）</strong></p><p>字母<strong>g、h</strong>的脱落（不发音）</p><p>造新词往往在<strong>单词结尾加轻辅音</strong></p><p>固定转换：s/t/d， p/b/ph/f/v        amorphous：form的倒写 </p><h4 id="字母组合（单音节，无词根词缀，-大体意思）"><a href="#字母组合（单音节，无词根词缀，-大体意思）" class="headerlink" title="字母组合（单音节，无词根词缀， 大体意思）"></a>字母组合（单音节，无词根词缀， 大体意思）</h4><p>sp表示发出，散开，产生</p><p>scr、cr多和手上的动作有关（注意：s在造词的时候无意义，只起到加强语气的作用）</p><p>词根词源字典：<a href="http://www.etymon.cn/index.html">http://www.etymon.cn/index.html</a></p><h4 id="联想法记单词"><a href="#联想法记单词" class="headerlink" title="联想法记单词"></a>联想法记单词</h4><p>单音节词汇 —-形近词  终点记不一样的地方</p><p>多音节词汇 —拆词，拆成认识的  词根词缀   拼音   熟词  与熟词形近的部分 等</p><p>​    </p><p>im/in: 1 into 2 not</p><p>vis: to see</p><p>it: to go</p><p>fact: to make</p><p>dis:1 not 2 apart</p><p>re:1 again 2 back</p><p>aneous:整体的形容词后缀</p><p>ade:名词后缀</p><p>er：n或v后缀，…的人</p><p>le:如果是动词，那么le就是动词后缀，往往表示动作的反复行为</p><p>ate:动词后缀</p><p>uous/ious/ous：形容词后缀</p><p>ish：形容词后缀，像$\cdots$一样的</p><p><strong>词根arch,archy= government,to rule统治</strong></p><p> ——词根arch 来自希腊语的arkhos，一般构成名词，亦可以当词根讲 意为government,to rule。arch 还有chief,first,old的含义。它们属于一对同源异形根，在派生词中，arch 常指统治的人物，作 ruler 统治者讲；而 archy 常指统治这一行为、方式，作rule 管理/管辖/统治或 government 政体讲。同义词根有来自希腊语的cracy/crat 和来自拉丁语的reg。</p><p>acc/app/ass/att，a+辅音双写表动作的加强</p><p>an/a：not</p><p><strong>后缀-ence,-ency的含义、词源和例词</strong></p><p>汉：来源于拉丁语及法语的名词后缀-ence(-ency)的用法与<a href="http://www.etymon.cn/yingyucizhui/yingyuhouzhui/205.html">-ance</a>(-ancy)基本相同。它们加在动词或动词词根后，意为the act or fact of ～ing或者the quality or condition of ～ing,即表示行为或该行为的性质状态等。这些名词往往有与之对应的以-ent结尾的形容词。</p><p> -ence,-ency与形容词后缀-ent相对应（如 difference-different；urgency-urgent），表示性质、状态、行为，后缀-ence和-ency义同，有些英语单词具有-ence和-ency两种形式（如 innocence = innocency ; persistency = persistence）</p><p>bene：词根 good male：词根evil</p><p>dict: to say</p><p>sion/dion: 名词后缀</p><p>vol:will 意愿</p>]]></content>
      
      
      <categories>
          
          <category> 英语 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tofel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>颜色恒常性之Bayesiancolorconstancy</title>
      <link href="yan-se-heng-chang-xing-zhi-bayesiandecisiontheory/"/>
      <url>yan-se-heng-chang-xing-zhi-bayesiandecisiontheory/</url>
      
        <content type="html"><![CDATA[<h2 id="论文思路"><a href="#论文思路" class="headerlink" title="论文思路"></a>论文思路</h2><p>论文：&lt;&lt;Bayesian color constancy &gt;&gt;</p><h3 id="Title-amp-amp-Abstract"><a href="#Title-amp-amp-Abstract" class="headerlink" title="Title&amp;&amp;Abstract"></a>Title&amp;&amp;Abstract</h3><p>MSE:</p><p><a href="https://blog.csdn.net/qq_36512295/article/details/86526799">https://blog.csdn.net/qq_36512295/article/details/86526799</a></p><p>MMSE:</p><p><a href="https://blog.csdn.net/tanghonghanhaoli/article/details/82751690?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522160395601819724842903308%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=160395601819724842903308&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v28-28-82751690.first_rank_ecpm_v3_pc_rank_v2&amp;utm_term=MMSE&amp;spm=1018.2118.3001.4187">https://blog.csdn.net/tanghonghanhaoli/article/details/82751690?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522160395601819724842903308%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=160395601819724842903308&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v28-28-82751690.first_rank_ecpm_v3_pc_rank_v2&amp;utm_term=MMSE&amp;spm=1018.2118.3001.4187</a></p><blockquote><p>the Maximum Local Mass Estimate</p></blockquote><p><strong>什么是局部最大质量估计？</strong></p><h3 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h3><p><strong>Figure1</strong></p><blockquote><p>We assume that each surface is <strong>flat and matte</strong>, so that it may be characterized by a single spectral reflectance function</p></blockquote><p><strong>为什么要flat and matte?</strong></p><p>平坦是保证处处一致，无光泽不光滑是保证是漫反射吧应该？</p><blockquote><p>The spectral power distri- bution of the light reaching the observer from each surface is given as the <strong>wavelength-by-wavelength product</strong> of the illuminant spectral power distribution and the surface reflectance function.</p></blockquote><p><strong>什么是wavelength-by-wavelength product?</strong></p><p>二者直接相乘</p><p><strong>Figure2</strong></p><p>Fig2（a）:</p><p>对于简单的乘积例子，给定高斯噪声时，后验分布的图像，最优解在岭处</p><p>Fig2（b）：</p><p>横截面表明，即使在岭处都有最大的固定值，一些局部区域也会含有不同的概率质量</p><hr><p><strong>什么是parametre vector？参数是什么意思？</strong></p><p>Y=ax+b Y是因变量，x是自变量，a,b就是参数</p><p><strong>什么是rendering equation？</strong></p><p><a href="https://zhuanlan.zhihu.com/p/52497510">https://zhuanlan.zhihu.com/p/52497510</a></p><p><strong>什么是Gaussian observation noise？高斯噪声？</strong></p><p>高斯噪声百度百科：<a href="https://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0/8587563?fr=aladdin">https://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0/8587563?fr=aladdin</a></p><p>为什么深度学习去噪都采用高斯白噪声？<a href="https://www.zhihu.com/question/67938028">https://www.zhihu.com/question/67938028</a></p><p>高斯白噪声解释：<a href="https://blog.csdn.net/szlcw1/article/details/41758711">https://blog.csdn.net/szlcw1/article/details/41758711</a></p><p><strong>Figure3</strong></p><hr><p><strong>delta loss function损失函数？</strong></p><p>常见损失函数：<a href="https://blog.csdn.net/perfect1t/article/details/88199179">https://blog.csdn.net/perfect1t/article/details/88199179</a></p><p>应该就是指单峰函数，0-1函数$\delta(\widetilde{x}-x)=0\quad if(\widetilde{x}=x)$ 相同的是预判正确的，所以是没有损失的</p><p><strong>期望损失？</strong></p><p><a href="https://blog.csdn.net/hx14301009/article/details/79870851">https://blog.csdn.net/hx14301009/article/details/79870851</a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="A-Why-Color-Constancy-is-Difficult"><a href="#A-Why-Color-Constancy-is-Difficult" class="headerlink" title="A. Why Color Constancy is Difficult"></a>A. Why Color Constancy is Difficult</h4><p>1.Problem Statement</p><blockquote><p>The entries of sj specify the fraction of incident light reflected in Nl evenly spaced wavelength bands throughout the visible spectrum</p></blockquote><p><strong>是不是错了，应该是reflected in</strong> $N_j$<strong>?这句话怎么理解?</strong></p><p>应该意思是$N_l$个均匀排列波长带</p><p>2.Why It Is Difficult</p><blockquote><p>It is underdetermined and it is nonlinear.</p></blockquote><p><strong>什么是欠定的？</strong></p><p><a href="http://blog.sina.com.cn/s/blog_531bb7630100xx6c.html">http://blog.sina.com.cn/s/blog_531bb7630100xx6c.html</a></p><blockquote><p>If we have data from N image locations (say, 10) and assume one illuminant, then we have NNr measurements (e.g., 10 x 3 = 30) available to estimate Nl(N + 1) scene parameters [e.g., 31 x (10 + 1) = 341].</p></blockquote><p><strong>这句话怎么理解？</strong></p><blockquote><p>To address the underdeterminancy of color constancy, previous investigators have described spectral functions by using low-dimensional linear models</p></blockquote><p>什么是低维线性模型？</p><p><a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1604305021&amp;ver=2681&amp;signature=FecbuKdwyqukrDBQO*pJ3q2jZFxLcCWxabeUx7eeSsOp9MNNUxijWNnlaNGRWguX2sl69suc3xXZInrRwvy-CsE1AVD*Vr3NvyjihI-8QmMzO04JeBJpKBXi75iy89*z&amp;new=1">https://mp.weixin.qq.com/s?src=11&amp;timestamp=1604305021&amp;ver=2681&amp;signature=FecbuKdwyqukrDBQO<em>pJ3q2jZFxLcCWxabeUx7eeSsOp9MNNUxijWNnlaNGRWguX2sl69suc3xXZInrRwvy-CsE1AVD</em>Vr3NvyjihI-8QmMzO04JeBJpKBXi75iy89*z&amp;new=1</a></p><blockquote><p>The columns of $B_e$ are the basis functions of the linear model, since the matrix product $B_ew_e$ expresses a weighted sum of these columns.</p></blockquote><p><strong>什么是基函数？</strong></p><p><a href="https://www.jianshu.com/p/5cc427f0df33">https://www.jianshu.com/p/5cc427f0df33</a></p><blockquote><p>If we assume that a population of spectra lie within an $N_m$-dimensional linear model, then we can parameterize the spectra by specifying the model weights.</p></blockquote><p><strong>这句话如何理解？</strong></p>]]></content>
      
      
      <categories>
          
          <category> Color Constancy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CC/AWB </tag>
            
            <tag> Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>随想与感悟</title>
      <link href="sui-xiang-yu-gan-wu/"/>
      <url>sui-xiang-yu-gan-wu/</url>
      
        <content type="html"><![CDATA[<p><a href="https://medium.com/@mohamedalihabib7/advice-on-building-a-machine-learning-career-and-reading-research-papers-by-prof-andrew-ng-f90ac99a0182#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImQ0ZTA2Y2ViMjJiMDFiZTU2YzIxM2M5ODU0MGFiNTYzYmZmNWE1OGMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2Mzc4MTAzNzYsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwNzI2MTQzOTI0NzMwNzM0MDk2MCIsImVtYWlsIjoiZG9uZ2NoZW5naGFvenh5QGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJuYW1lIjoi6JGj5oiQ6LGqIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hLS9BT2gxNEdnbTlSb0ZISXpZazBRQ0x5RnFXTTlPOUlCaVpheWNvTmFadDVFZD1zOTYtYyIsImdpdmVuX25hbWUiOiLmiJDosaoiLCJmYW1pbHlfbmFtZSI6IuiRoyIsImlhdCI6MTYzNzgxMDY3NiwiZXhwIjoxNjM3ODE0Mjc2LCJqdGkiOiI5NTJhNGI5YTRkNTNkZjFlNTMzZDg5ZjBiZGJmOTlhNzliZDcyODcxIn0.UetNw0Zm-knHazJclZlLykTPWgj1C6gcwQ6yE-TfAXoZ_GmffD9GNpd3vZ-dpF-OmMJToxVFU2gu5NQOtJZ0VshzlEKYRB-nQwIggiCDXz5GVYJfZmhmsmxR41JU1p16S-kojaJVoQqB3K92lsrmnkwwiBxQHH3yg0odXzeLxjsV07F9oJEPyzKiJDgiEuU3aSwEmcEreth51stgrfoRgjh74rCrssJHnPJaOobwuEjah9xrKy3Dt2FNAE5uGKxofwGIP7Mc6WYDC86mnZa754mm7ZLto-iQP8T4JzPpfRrglIrZg3O8Q2gzlytnLYtmtB2llhP7MuRDolUWbq3CeA">吴恩达阅读论文的建议</a></p><h2 id="☆科研到底应该如何入门？"><a href="#☆科研到底应该如何入门？" class="headerlink" title="☆科研到底应该如何入门？"></a>☆科研到底应该如何入门？</h2><p>很多科研小白都有过这样的苦恼，刚读研时老师给了个领域方向、几个关键词，让看看论文，然后就去云游四海了，一两个月见不到一次，留下你在浩瀚如繁星的论文库里挣扎狗刨；虽说师傅领进门，修行在个人，但很多人连门都没有摸着，组会分享做的ppt常常不着边际、让人一头雾水。时光匆匆流去，走了不少弯路，科研到底应该如何入门？</p><p>其实，<strong>科研创新最重要的是找创新点，而找点最需要的是思考方式上的提升。</strong></p><p>导师忘记给你的发论文秘籍，今天我总结出来以下五个步骤给你。</p><h3 id="☆1-问题，问题，还是问题："><a href="#☆1-问题，问题，还是问题：" class="headerlink" title="☆1. 问题，问题，还是问题："></a>☆1. 问题，问题，还是问题：</h3><p>刚读研的时候，拿到一篇论文，算法、模型、实验、成篇成篇的推导和公式，看不懂是一件很正常的事情。但是，你一定要看懂的一点，是一篇论文所解决的问题和挑战。因此，第一步是，每当看完一篇论文，问一下自己，它解决了什么问题，如果回答不上来，请再看一遍，重点看摘要和第二章动机。</p><p>发现问题、解决问题是科研创新最重要的思考方式，也是很多国内的学生所欠缺的。因为国内的评价体系过于单一，我们从小习惯了刷题、看标准答案、考试的应试教育，惯性的认为考高分就是优秀。而在国外的多元评价体系里，考满分和考六十分都算是通过，不需要拿时间去刷题、拿分数去麻痹自己，而是拿时间去思考自己热爱什么，把时间投入到自己热爱的事。</p><p><strong>展现人生价值的，不是别人口中的优秀，而是自己心中的热爱。</strong></p><p>因此，看论文的过程中除了发现问题，还要思考一件事，就是要不断摸索自己所在领域的研究价值，并找到真正值得研究的好问题。</p><h3 id="☆2-建立知识图谱："><a href="#☆2-建立知识图谱：" class="headerlink" title="☆2.建立知识图谱："></a>☆2.建立知识图谱：</h3><p>现在你已经看了半年论文，了解了很多问题，可它们就像一团乱麻，让人感觉“找不到创新点，看再多的论文也只是浪费时间”，怎么办？这个时候，你需要做第二步，建立知识图谱。它有很多的形式，比如思维导图、目录，而我最推荐的是excel表格，创建一个表格，一共十列“场景/问题/机制/模型/算法/实验/工具/结论/未来工作/创新点”，然后把看过的觉得好的论文按行放进去。这样既能保证你看论文的效率，想到某篇论文时不用再从头看一遍，只需看看excel，又能帮助你抽象事物的本质，并建立联系。</p><p>很多关于提高记忆力、理解能力的书，比如教你如何一晚上记住七百个人的名字，如何三天背一千个单词，其实都是一个道理。我曾经看一篇论文看了三天看不懂，组会给导师汇报时，导师说你把论文发我，他当场开始看，鼠标滑了滑，花了三分钟，就给我讲清楚了这篇论文。为什么他只用花三分钟？因为他能快速抓住本质，并与已有的知识建立联系。</p><p><strong>学习、记忆、理解的过程，其实就是抽象事物的本质，并建立联系。</strong></p><p>如果你经常练习抽象事物的本质，并建立联系，比如做成知识图谱，你将拥有比别人更快的学习能力、和把异常复杂的难题化简的能力。这是你读了十几年的书一直在做却浑然不觉的事，也是读博要培养的核心能力。</p><h3 id="☆3-相中一篇论文："><a href="#☆3-相中一篇论文：" class="headerlink" title="☆3.相中一篇论文："></a>☆3.相中一篇论文：</h3><p>现在你已经有了一个长长的excel表格，觉得别人什么都对，什么都好，就是不知道自己应该做什么发论文，怎么办？第三步，相中一篇论文，它可能发表于三到五年前，比较新，但引用量还不错，你觉得它写的特别好、很有价值，或者它是其他很多篇论文的“爸爸”。那么，请跟它软磨硬泡、死缠烂打，彻底把它吃透，做到了然于胸。然后，它的问题就成了你的问题，它的相关工作就是你的相关工作，它的模型、算法、实验工具也都可以借鉴，比如复现它的算法作为你的对比算法。</p><p>也就是俗话说的，站在巨人的肩膀上。</p><h3 id="☆4-组合式创新："><a href="#☆4-组合式创新：" class="headerlink" title="☆4.组合式创新："></a>☆4.组合式创新：</h3><p>科研小白的第一篇论文，很难做到颠覆式创新，大多是组合式创新。第四步，即，有一个A，有一个B，你做了A+B，然后把它命名为C，并证明A+B=C且C带来了好处。组合，可以是增、删、改、换，比如增加约束条件、删除不合理的前提条件、改变优化目标、改变求解方法、换个应用场景。</p><p>就好比，有人喝奶，有人喝茶，把奶和茶倒一块，就成了风靡全国的奶茶，属实组合式创新；用什么奶，用什么茶，什么样的奶茶比例，加多少冰、加多少糖、加不加珍珠、奶盖、椰肉、水果，口感不一样，又可以有好多创新，各家各有配方，所以有了喜茶、一点点、茶百道、CoCo、烧仙草等等。</p><h3 id="☆5-占卜师-vs-淘金者："><a href="#☆5-占卜师-vs-淘金者：" class="headerlink" title="☆5.占卜师 vs 淘金者："></a>☆5.占卜师 vs 淘金者：</h3><p>现在你已经大概有了创新点，你需要实现它，但突然觉得自己好菜、实现它好难，做不出来，怎么办？很多人都有畏难心理，所以往往拿着锤子找钉子，认为我会什么，我就只做什么，比如只会某个算法或者某个工具，就用那个东西。其实这样不好，我把这样的心态比作淘金者心态，即你找个坑就开始往下挖，不知道该挖多深，挖浅了挖偏了挖不到金子，挖深了又发现自己掉进了自己挖的坑里爬不出来。</p><p>相比于淘金者，我更推荐占卜师的心态。一个领域就是一颗星球，你不要着急拿着铁锹开始干，你先占卜一下，根据这个星球的年龄、地质成分、气候条件、元素分布，看看哪个地方多深的位置会有金子，储金量有多少。</p><p>什么意思呢，两方面，一是说论文的实验仿真可以是最小实现，实验是辅助证明正确性的，如果你能通过理论上证明很多部分的正确性，比如证明了金子的正确位置且比较浅，实验可以做得相对简单，稍稍挖一下，有金子露出来了实验部分就可以了；二是说，如果你占卜出来金子埋得很深、储金量很大，那么，不要一个人埋头苦干，可以给导师汇报，拉着师兄师姐学弟学妹一起搞，有坑一起挖，出坑一起填，实验实现只是时间问题。</p><p>我个人实践，对于科研小白，按照以上五步，坚持半年到一年，发一篇六页左右的会议论文，或者写一篇综述，应该不成问题。</p><p>如果你的论文依然被拒了，请不要灰心，最后，奉上发论文的终极秘诀：</p><p><strong>“好论文都是改出来的”。</strong></p><p><strong>点赞，做科研我个人感受最深的也是找问题，问题的新颖性决定了motivation的高度，而method可以为了这个motivation无限调整。我曾经有个误区就是看论文只看别人怎么去解决问题，不看别人的问题是什么，走了很多弯路。</strong></p><h3 id="☆6"><a href="#☆6" class="headerlink" title="☆6"></a>☆6</h3><p>大家都是玩模块，如何你的能比别人的好呢？是要<strong>有一个物理概念</strong></p><p><strong>踏踏实实的做好每一件事，专注，谦虚</strong></p><p><strong>把结果看清，这是做好的过程中自然而然得到的</strong></p><p><strong>相关工作不是你的工作，写论文先写自己的工作，相关工作可以最后写用来凑字数</strong></p><p>☆7</p><p>研究别人的模型为什么好,而不是研究差的模型为什么不好</p><h2 id="论文的几个部分如何读？"><a href="#论文的几个部分如何读？" class="headerlink" title="论文的几个部分如何读？"></a>论文的几个部分如何读？</h2><p>* <strong>Abstract</strong> 我一般在自己决定是否要读一篇论文的时候会读。但是如果是已经确定是一篇要读的论文（经典论文或者他人推荐的），我经常跳过。一个好的电影谁想先看故事梗概呢？</p><p>* <strong>Introduction</strong> 是我最喜欢读的部分。它可以让你了解作者对论题甚至该领域的真知灼见，加深自己对领域的理解。通常要真的弄懂一个领域，2-3年的积累和沉淀是要的。</p><p>一个好的引言是什么样的？Don Geman 给过一个标准：如果读者是论文reviewer，看完引言就可以决定是否accept or reject了。计算机科学很多时候是一层窗户纸，Introduction会点破这层窗户纸，同行一看就明白整个论文在做什么，意义有多大了。一篇好的Introduction，一定会对读者带来或多或少的心里冲击。</p><p><strong>Related Works</strong> 可以帮助我们查查漏读了哪些文章。如果你对当前领域已经很熟悉了，也可以跳过。另外在阅读相关文献时，高频出现的名字或单位一般是这个领域的关键贡献者，我们平时就可以多跟踪他们的出品。</p><p>*<strong>Approach</strong> 部分阅读的原则是先整体，后部分。一般论文会有一个概况文字描述，或者配有一个系统框图，我们先快速了解全貌，特别是部分之间的关系。然后再具体研读每一个部分。</p><p>* <strong>Experiment</strong> 部分如果和我们手头的工作很相关，就要非常仔细的看细节。否则，就重点看看实验结果是否有力地支持了文章的论点。</p><p>* <strong>Conclusion</strong> 可以看看作者指出的方法局限性和下一步工作。但是要小心，一般比较明显的下一步工作要么作者已经做了，要么可能是个大坑。</p><p><strong>读完论文后找人交流一下读后感会很有益处</strong>。旷视研究院的 Paper Reading 论坛是个很好的交流方式。大家会每天分享论文解读和进行评论，甚至还经常有人报告复现结果和复现中发现的问题。通过了解别人的观点，就能够更全面的理解一篇文章，同时也增强了自己全面看问题的能力</p><p><strong>做什么事情就要遵循什么套路，就比如系统的撰写，大家都是做个形式，吹，就看谁吹得好看，套路就是这样，没花也得说出花，不然就过不了，就没奖。</strong></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><strong>模型的你的主要创新点需要改，剩下的尽量和你想要对比的算法是一致的，否则你做的实验就成了白做了！</strong></p><h2 id="另一个思路"><a href="#另一个思路" class="headerlink" title="另一个思路"></a>另一个思路</h2><p>记录知乎<a href="https://www.zhihu.com/people/yi-ge-ke-xue-jia">一个科学家</a>的分享，我觉得说的很棒</p><h3 id="提升逻辑思维能力"><a href="#提升逻辑思维能力" class="headerlink" title="提升逻辑思维能力"></a>提升逻辑思维能力</h3><p>很多人都误以为自己的逻辑思维能力很强，其实不是的。逻辑思维能力，不是你把1+2=3给算出来就是有能力。而是，你可以一点点通过构建自己的知识系统来理解别人的论文，发现别人论文中隐含的一些信息，并最终利用你的逻辑把你的理解讲出来，让大家都能听懂并且接受。</p><p>拥有强大逻辑思维能力的人可以把一个方法的理解出三种层次：<strong>1，数学层次，2，几何层次，3，物理层次</strong></p><p>比如说，现在有一篇论文介绍了如何利用主成分分析（PCA）来寻找城市的发展模式。作者拿到了100个城市数据（GPA，人口，公路里程，铁路里程，耗电量等），然后在上面运行了PCA，之后得到了几种不同的发展模式并且提供了一些合理的分析和解释。在研究这篇论文里面的方法的时候，很多学生上来就被一堆数学公式给吓唬住了。然后开始研究这些公式到底是干什么用的。稍加思考，发现这些公式是主成分分析的步骤。然后为了学会主成分分析，这名学生开始按照论文里面的公式，一步步进行推导。过了几天以后，经过推导，这个学生发现论文里面的公式是正确的，然后按照公式可以一步步明白作者的方法。这时候理解的层次就是数学层次。在这个层次的理解下，如果让这个学生给大家介绍这篇论文，那么他就会长篇大论开始介绍公式1推导出了公式2，然后公式2又算出了公式3等等。然而除了这个学生自己，没人能听懂他到底是在做什么。</p><p>稍微高级一些的层次就是几何层次。如果一个学生对于个方法的理解达到了这个层次，那么他应该可以想象出来这个方法在处理数据的时候，数据是怎么变化的。比如还是刚才的这个例子里面，PCA会对数据进行线性变换。而线性变化可以理解为一堆采样点在空间上的旋转和拉伸。这样通过PCA的处理，原本在三维空间上的采样点就会被变换到一个二维的平面上，因为这些数据点本身就是三维空间里面的一个平面而不是一团。于是乎，PCA就发现了原来这个数据只需要二维的信息就可以表示了，这样就实现了降低维度的操作。当一个学生理解到了这个层次的时候，就可以直观地想象出数据的样子还有数据被操作的过程了。</p><p>对于一种方法的理解，最高级的就是物理层次了。在这个层次上，你不光可以想象到数据在空间中的样子，你还可以明白这些数据在每一步被处理的时候到底是什么含义。比如在刚才的例子里，数据是代表着各个城市的一些特征。那么经过PCA的处理以后，我们就找到了真正决定一个城市发展模式的核心特征。在这个层次的基础上，领导们就可以抓住核心的特征来改变一个城市的规划进而提高市民们的生活水平了。</p><p>大家习惯了不去思考更深一层的东西，而是幻想着去通过体力上的努力来弥补脑力上的思考。因为不会正确的逻辑思维，一个人就会懒于思考，他就会误以为思考这些太浪费时间。这样是不行的。</p><p>最后我想说一下<strong>逻辑思维对于写作的重要性</strong>。比如我做了一项研究，制造出了一个机械零件，为此我写了第一个版本的论文：</p><blockquote><p>我设计了一个机械零件，他包含：15根不锈钢条，空气，橡胶膜，还有一个不锈钢的50cm直径的圆环。其中，不锈钢条有3mm粗20cm长。现在我把这几个部件按照以下顺序组装起来：不锈钢条插在圆环里面，圆环外边套上橡胶膜，最后在橡胶模里面冲上空气。于是这个零件造好了。好了，到现在为止，你能明白我要做什么吗？</p></blockquote><p>我要是换成下面这个版本呢？</p><blockquote><p>我设计了一个机械零件，我给它起名字叫轮子。轮子可以用来帮助人们搬运货物，可以把人类文明推进100万年。那为了达到帮运货物节省力气的这个目的，我们找了一块大石头，把石头磨圆。这时候我们确实可以用这个圆形的石头来搬货物，可是这个太沉了，不太省力。为了节省力气，我们把石头改成木头。可是我们发现还有点沉，于是把圆球改成了圆环。可是我们发现圆环不太结实，于是我们把木头圆环改成了铁的圆环。但是走起来有点颠簸，于是我们加了一层橡胶。为了进一步减震，我们罐装了空气。这么一来是不是好理解多了？</p></blockquote><p>可惜，往往现实是大家写的都是第一个让人摸不清头脑的版本，然后提交了上去。第二个版本是非常容易理解的，但是这个包含的信息太多了。这个版本其实是我们在学习制造轮子这个方法的时候，我们自己应该整理出来的一套逻辑。我写这两个版本就是为了让大家看一看<strong>不同的逻辑思维在写作和理解上带来的差距。</strong></p><p>看到这里，大家肯定会问逻辑思维的能力应该怎么才能训练呢？逻辑思维的能力的训练是因人而异的，我会在以后慢慢讲。不过请大家放心，逻辑思维能力不是一个超能力，而是一个我们每个人通过训练都可以获得的能力。这个训练不会经历三年五年，只要掌握了正确的方法，两三个月就可以让你自己的逻辑思维能力大大提升。最终这个良好的逻辑思维能力足以支撑你把能够发表的论文的档次一点点提高。</p><h3 id="怎么写论文"><a href="#怎么写论文" class="headerlink" title="怎么写论文"></a>怎么写论文</h3><h3 id="为什么你提出的idea会和别人重合？怎么写introduction和abstract？"><a href="#为什么你提出的idea会和别人重合？怎么写introduction和abstract？" class="headerlink" title="为什么你提出的idea会和别人重合？怎么写introduction和abstract？"></a>为什么你提出的idea会和别人重合？怎么写introduction和abstract？</h3><p>一个idea里面包含的信息是很多的。而且这些信息表达的顺序一般来说也是比较固定的。之所以要按照一定顺序将这些信息表达出来，就是为了让你的idea变得有逻辑，让读者能够更加轻松地理解你的idea。因此，一个合格的idea，应该按照下面的顺序将以下的信息逐个表达出来：1，idea的背景。2，一个具体的研究问题。3，别人是怎么做的。4，别人的方法中存在什么问题。5，别人的方法出现这些问题的原因是什么。6，你发现了什么新的现象或者信息，并利用这一现象提出一个新的方法来避免前人的问题。 7，你的方法里面真正的挑战是什么。8，为了克服挑战，你提出的具体的设计是什么。下面我详细给大家讲一下。</p><p>1, <strong>idea的背景。</strong> 我们在做研究都有一个大的背景。比如说我现在研发一个大推力的火箭。那么我的背景如果用一句话来讲，可以是：将货物或者人类，通过火箭送入太空已经不再是一间稀奇的事情。</p><p>2, <strong>一个具体的研究问题</strong>。具体的研究问题和背景有时候容易混淆。这个具<strong>体的问题指的是你的论文直接要解决的一个问题，而不是一个特别宽泛的问题。在我们这个大推力火箭的idea里面，一个具体的研究问题可以是：随着火箭发射技术的成熟，人们利用火箭实现了登陆月球和建立空间站。为了建实现征服太空的梦想，如何一次性将更多的货物送入太空成为了一个亟待解决的问题。</strong> 当然了，这个具体的研究问题还可以换成别的。我暂时拿这个举这个例子。其实，==一个idea可以对应解决不同的研究问题。而在论文里面写的不同的研究问题，就将决定这个idea的定位，还有这篇论文的档次高低。==以后我会找出一个新的说法让大家看一下区别。</p><p>3，<strong>别人是怎么做的。</strong> 这里面要列举一下别人的科研工作。请大家注意，<strong>这个可不是参考文献里面随随便便列举一些论文。这里应该是你真正理解的几种方法。</strong>比如在我的idea中，我可以说：为了一次发送更多的货物，现如全世界设计了三种大推力的火箭：第一种是前苏联的联盟号火箭，这个火箭利用了多个发动机的技术将100吨的货物送入了和平号空间站。第二种是美国的土星五号，利用了分级火箭的技术将100吨的货物送去了月球。第三种是中国的长征系列火箭利用了捆绑火箭的技术，将100吨货物送入了火星。</p><p>4，<strong>别人的方法中存在什么问题。</strong> 这里我们需要把<strong>别人的方法中的最致命的问题提出来</strong>。这个问题将是我们的论文真正解决的一个问题！这个十分的关键！千万别和2混淆！在我们这个idea里面，我们找到的上述三种方案中的问题是：随着火箭运送货物重量的增加，需要携带的燃料也越来越多，相应的火箭整体的质量也越来越大。这样一来，火箭推力的要求将变得更大，对于火箭设计的难度将大大增加。</p><p>5，<strong>别人的方法中出现问题的原因是什么。</strong> 我们一定要在<strong>idea中将别人方法中出现问题的原因找出来。只有找到了问题的原因，我们才有可能解决这个问题。</strong>如果没有这一条，那么读者就会不明白为什么你的方法一定从理论上就可以解决这个问题。如果没有这一条，即使我们做了大量实验证明你的方法比别人好，读者也不会相信我的方法一定比别人的好，因为原理上说不通。</p><p>回到我们的例子，我们发现导致别人方法里面的问题的根本原因是：人们希望火箭发射以后就会一直将货物送到太空，发射过程中火箭不会停下来休息，因此火箭发射一次就需要装载好全程的燃料。比如说发射100吨的货物到500公里外的太空，一趟需要200吨的燃料，于是乎火箭的推力就得至少是100+300=300吨。火箭的推力有2/3消耗在了推进燃料上面。推进的效率只有1/3。</p><p>6，<strong>insight!</strong> 你发现了什么新的现象或者信息，并利用这一现象提出一个新的方法来避免前人的问题。这一条就很有意思了。<strong>这一条是我们的idea的核心。只有引入了新的信息和观察到了别人没有观察到的现象</strong>，并加以利用，我们的方法才可能比别人的方法好！</p><p>在我们这个火箭发射的idea里面，我找到了这么一个有趣的信息：<strong>如果我们不是用火箭径直将货物一次性送入太空，而是通过接力的方法去运送货物，那么每一段接力的火箭携带的燃料可以减半，因此火箭的推力就不需要那么大了，火箭推进的效率就提高了。</strong>具体地讲，发射100吨的货物到500公里外的太空，一趟需要200吨的燃料。现在我们分两阶段接力。第一阶段由火箭将货物运到250公里高度。由于火箭不需要飞500公里高，因此燃料只要之前的一半，也就是100吨。于是乎第一段火箭的推力只需要100吨货物+100吨燃料=200吨。当第一段火箭发射的同时，第二段的火箭也一同发射，只不过第二段火箭这时候只装载着燃料。第二段火箭飞到了250公里高度的时候，接替第一段火箭，然后将货物继续送入500公里的太空。由于第二段火箭在前250公里飞行时没有货物，而加载货物以后也只需要飞250公里，因此第二段火箭消耗的燃料肯定小于一次性将100吨的货物发送到500公里外的太空所需燃料，也就是200吨。所以，第二段火箭的推力肯定小于300吨！于是乎，我们找到了一种节省火箭推力，提高推进效率的方法！这一方法里面虽然两段接力下来，总体消耗的燃料肯定要大于200吨，但是每一段火箭的效率提高了，因此，两段接力的火箭的推力都可以小于300吨。换个角度讲，如果我们两段火箭的推力都是300吨，那么我们肯定可以将大于100吨的货物发送到太空！</p><p>7，<strong>你的方法里面真正的挑战是什么。</strong> <strong>如果一个idea里面没有任何挑战，那将这个idea的创新性将大打折扣。同时，一个没有挑战的idea一定很容易被别人想到。然而，我不是在这里劝大家一定要去硬生生地编造一些挑战出来，而是要真真切切地有挑战才可以</strong>。 比如在我们这个火箭发射的故事里面，我刚刚讲了我们可以用接力的方法去提高火箭发射的效率。然而这个idea并没那么简单，如果你仔细思考你就会发现，火箭接力的核心是火箭可以在空中完成接力的动作。这一动作的完成相当的困难。想要完成这个动作，我们一定需要对火箭的姿态有非常精准的控制才可以。如果控制出现微小的偏差，别说接力了，火箭可能直接就歪倒掉落下来。</p><p>8，为了克服挑战，你提出的<strong>具体的设计</strong>是什么。这个就是你的idea最最最关键，也是你论文里面真正的<strong>干货</strong>了。</p><p>对了，刚才这8条信息，按照我这个写作的方法，再稍微加一些实验的结果，就是我们的论文的introduction了。如果将这8条信息，每一条浓缩成一句话，然后把8句话合在一起，那就是abstract了。在此，我想我不光介绍了什么才是一个idea，我还告诉了大家应该怎么写abstract和introduction。</p><p>请大家记住，今天我在这这个文章的时候，我的逻辑是先讲idea有什么信息和内容，然后自然而然就把introduction以及abstract的写作也给大家讲完了。如果我换一个顺序呢？如果我跟大家讲：今天我想跟大家分享一下如何写论文的introduction和abstract。那么我将遇到一个尴尬就是我需要把introduction的结构给大家先讲一下。然后我还得去介绍idea是什么，因为idea是introduction的子集。introduction的结构其实只比idea的信息要多了一个实验结果。如果我先介绍introduction后讲idea，我的文字里面就会有大量的重复的内容，因为idea包含在introduction里面了。</p><p>所以，<strong>通过这次分享，我也想让大家知道一下，不同写作顺序带来的影响和好处。也想告诉大家一下，一个正确的逻辑是多么重要。一个错误的逻辑将会让整个idea变得混乱不堪，无法理解，十分罗嗦。</strong></p><h3 id="怎么判断你提出来的idea的档次高低"><a href="#怎么判断你提出来的idea的档次高低" class="headerlink" title="怎么判断你提出来的idea的档次高低"></a>怎么判断你提出来的idea的档次高低</h3><img src="/sui-xiang-yu-gan-wu/image-20210608162711853.png" alt="image-20210608162711853" style="zoom:50%;"><p>如果仔细地看这个树，树根的部位代表着一些公理，比如1+1=2.稍微高一层的就是一些基础的定理，包括相对论，牛顿三大定律之类的。在这个基础之上，我们形成了我们的几个大的学科，包括数学，物理，化学，生物。在此之上，我们诞生出了EE， CS等专业。在计算机专业里面，我们还有很多研究方向，包括机器学习，系统，计算机视觉等。在系统这个方向上，我们有云计算，无线，还有计算机网络等子方向。在无线这个子方向上面，我们有无线感知和无线通信两大领域。在无线通讯领域里面，我们还可以细分成5G，WiFi，还有低功耗的IoT通讯。最后我们在WiFi这个分支上，还可以看到MIMO，60GHZ等具体的研究方向。</p><p>我画了这么一个树，我主要是想表达的意思是：人<strong>类的知识是一点点建立起来的。每一篇论文的创新也都只是在这一颗树上不断分叉，不断细化。那么一个idea的创新程度，就可以反映在这个idea处于这棵树的层级。</strong></p><p>我们知道，<strong>任何idea都有一定的假设条件。有的idea里面的假设条件很明显，有的则是很隐晦。每当有一个创新性很强的idea出现的时候，它都会打破原有方向上的某些假设条件。</strong>比如说，量子力学的诞生，就打破了牛顿的经典力学的假设条件。再比如说我们制造芯片，芯片上面的集成的晶体管不能做到无限小，因为当晶体管尺寸小于XX纳米以后，它将不遵守经典物理的规律，反而表现出量子物理的一些特征。这是因为经典物理的规律对尺寸有一定的要求和假设。同样，咱们高中化学课上面讲过，化学反应不会制造新的元素。这也就成为了我们化学专业的一个假设条件。如果你能打破这个假设，你在你的反应里面制造出了新的元素，那么恭喜你，你发明了核裂变或者核聚变。你想想你这个idea得多么厉害。</p><p><strong>一个idea的创新程度，取决于它打破了这个人类知识树的哪一层上的假设。</strong></p><p>如果现在我们提出了一个能够推翻相对论的idea，那么idea这个将处于知识树很底层。所以这种idea将值得无数个诺贝尔奖。如果我们的idea在计算机这个专业下面，打破了现有科研方向的界限，我们制造出了一个新的方向，那么我们的idea将是图灵奖级别的。依此类推，当你打破的假设条件越接近树的叶子，那么这个idea的创新程度就越小。</p><p>最终，如果你的idea连一个假设条件也没有打破，只是应用几篇论文里面的方法，东拼西凑出一个新的方法，然后应用这个东拼西凑去识别一个图像，检测一个异常，挖掘什么数据，那么这个就叫做<strong>incremental work。这种idea如果包装的好一点，注意，我说的是包装，不是写作，那么有可能发到CCF 的C类会议上，或者一些三四区的sci上面。</strong>当然了，我本人对于sci的档次一直不看好，这种idea好好弄弄发表在什么一区二区也是常有的事。</p><h3 id="一个好的idea是怎么提出来的？为什么你研究了好几年却什么也没提出来？"><a href="#一个好的idea是怎么提出来的？为什么你研究了好几年却什么也没提出来？" class="headerlink" title="一个好的idea是怎么提出来的？为什么你研究了好几年却什么也没提出来？"></a>一个好的idea是怎么提出来的？为什么你研究了好几年却什么也没提出来？</h3><p>我们应该怎么提出一个idea呢？对于这个问题，我先讲一种思路吧。一种95%的导师都会告诉大家的思路，那就是：<strong>多读论文，看看别人的论文里面有什么毛病，然后你去把这个毛病给解决了，你就有了论文了。</strong></p><p>==这是标准的错误答案！这是标准的错误答案！这是标准的错误答案！==</p><p>扪心自问，<strong>大家有多少人都是按照这个思路去做研究的？有多少人真的根据这个思路发表了论文？又有多少人按照这种思路能发表真正有价值的顶尖的论文？</strong></p><p>据我所知，<strong>按照这种思路做研究的学生占90%以上。可悲！可怜！可恨！</strong>恨的是这些没有责任心的导师和网上答虚假信息。诺大的知乎，居然没有人来告诉大家这么做不对。居然很多人还倡导这种思路。在介绍真正的思路之前，我先给大家分析一下为什么这种思路的错误的。</p><p>我一直在强调逻辑。那我就从最最最最最基本的逻辑来给大家证明一下这种思路的错误。首先，你去读一篇论文。你是读者。你再怎么聪明，你对于一篇论文的理解，肯定不如作者理解的深。何况正如我之前写的文章里面讲的一样，如果你的逻辑思维能力不够强大，很大的概率你会被自己错误的思路给误导。那么你的理解就浅薄了。那么在这种情况下，你认为你找到的毛病，作者会不知道么？你能发现一篇论文里面存在的问题，那么作者肯定也明白这个问题。那么为什么作者不去把这个问题解决了呢？为什么作者要把这个机会留给你呢？你难道认为你比作者聪明，比作者对他的论文理解还深入么？</p><p>当然了，肯定有人会说，我要是作者的话，我会留一手，等下一篇论文再解决这个问题。我可以负责的告诉你，没有作者会这么想。作者都是会把所有的子弹打光，把能做的都做好，这样才能保证文章录用。你要是作者，你在投论文的时候内心肯定是担心论文会被拒稿。作者不会自信到把贡献分摊到下一篇论文，然后给这一篇论文的投稿带来更大的不确定性。这是不太可能发生的。当然了也有例外，我以后会给大家讲。但是总的来说，这种情况不存在。</p><p>那么如果你按照这种思路走下去，你找到了一个毛病。你开始研究解决这个毛病。现在我说一个情况，你看看你有没有发生过？你开始着手解决别人论文里的毛病，然后你发现没有思路，然后你再去看论文，再去找相关的资料，然后再去做实验。反复折腾了一年，最后还是没解决。或者是，你发现这个毛病非常迅速的就被你解决了。两个礼拜以后你成功地解决了这个问题，你非常开心。这两种情况，无论是哪一种，你都发表不了论文。因为第一种情况说明，你这个问题非常可能它压根就解决不了。人家作者没去解决这个问题正是因为这个问题是非常难以解决的，除非有爱因斯坦级别的人物才有可能解决。第二种情况说明，这个问题作者之所以没去解决，是因为这个问题太小了。就算解决了也没啥贡献。这两种情况无论是哪一种，都将是你按照挑别人论文毛病的思路去做科研所带来的恶果！</p><p>现在我来告诉大家一种思路，看完这种思路以后，大家自行判断我说的思路对不对。</p><p>首先我们得明白，<strong>一篇论文里面其实很多部分都有误导性，甚至是错误。尤其是档次比较差的论文。当然了，有些所谓的档次高的论文，比如饱受大家诟病的infocom，虽然是CCF A类会议，里面的论文很大一部分都有误导性。这种论文的实验都很难复现，就更不用说什么创新了。所以挑出来这些文章的毛病，其实是一件很容易的事情。但是这些毛病也会把你误导的。</strong></p><p>我在这个文章里面给大家讲了一个idea应该传达的8条信息。这8条信息中，最关键的其实是一个idea的insight，或者叫intuition。也就是第6条信息：你发现了什么新的现象或者信息，并利用这一现象提出一个新的方法来避免前人的问题。</p><p>我可以从审稿人的角度，负责任的告诉大家，一篇论文哪怕里面的算法设计的一塌糊涂，只要论文里面的insight能够给大家带来新的启发，带来新的知识，带来新的理解，那么这一篇论文就一定会让审稿人眼前一亮。这时候，哪怕实验效果再假，论文只要写得清清楚楚，大概率也是会被录用的。所以，insight才是一个文章的核心！一篇文章哪怕到处都是错误，但是文章的insight很大概率是不会错的！而且一篇论文，哪怕是infocom的论文，里面的insight也都会描述的比较清晰，比较直观。因此，我们在读论文的时候，核心是要理解这个论文的insight。</p><p>掌握了一个论文的insight，就相当于掌握了一件武器。==当你读了你的方向上的5篇很有代表性的论文以后，你应该深入理解并且掌握了5个insight。这时候摆在你面前的就是5个不同的武器。那你现在需要做的就是，再深入思考思考这些insight之间的关联。然后你要想一想，这些insight背后的深层次的含义是什么？是什么导致了这些现象？能不能从中发现一些更加有规律性的现象？这时候，你就离提出一个有价值的idea不远了。==</p><p>以SVM为例，SVM的第一个insight：线性不可分的数据，我们可以把他们映射到一个高维空间上，让他们在高维空间做到线性可分。当然了，SVM还有两个insights，在这里我就不过多展开了。我想说的是，根据这个insight后面诞生了一个新的科研方向，在这方向上诞生了非常多的idea。而这些idea是怎么样提出来的呢？</p><p>我们可以深入思考一下SVM的insight。我们可以想象一下，一个低维空间的数据，被我们用一些方法转换成了高维空间的数据。这种转换不一定非得是SVM才可以完成，你看现在的深度学习，神经网络，都属于是把一个输入的数据，映射到了高维的空间中。只不过是我们可以把这些神经网络的节点来看作成高维度的空间。那么我们现在跳出了SVM的局限，我们可以想象，凡是把低维度数据转换到高维空间，我们都可以有类似的insight。这种低维到高维的转换，有没有缺点呢 ？我们可以想象一下，原本就是二维的信息，现在我们凭空变出第三个维度，如果这个维度设计的不太好，那么会不会使得这种转换对于噪声和微小扰动的容忍度变得很低？比如说刚才的例子中，如果红色的点被放在高度是0.1的平面上，蓝色的点还在高度是0的平面上：</p><p><img src="/sui-xiang-yu-gan-wu/image-20210608184643793.png" alt="image-20210608184643793"></p><p>那么只要有超过0.1的误差就会使得样本被错误的分类了。那么现在可以把这个insight再往前推进一步，我们可以想象出下面这种情况：我们故意在低维数据上面增加一些噪声，这样的数据经过转换以后，在高维空间上表现得跟没有加入噪声之前非常不一样，以至于被错误的分类了。在论文“DeepFool: a simple and accurate method to fool deep neural networks”里面，作者就是用这种insight：在鲸鱼的图片上插入人眼很难识别的微小扰动，以至于这些高纬度的分类器会将鲸鱼错误分类成乌龟。用这个insight到声音里面，我们也可以得到类似的insight：人类对于声音中的微小扰动也无法识别，但是深度学习网络会将这个错误判断。于是就有人提出出了一个攻击语言识别的idea。等等。</p><p>==我们不应该从挑别人论文的毛病开始，而是恰巧相反，<strong>我们应该从理解别人论文的优点开始</strong>！我们将别人论文中的优点，主要是insight，深入理解以后，在你自己的领域和研究的背景下面加以扩展，我们就可以得到一些新的现象，进而提出新的idea！==</p><h3 id="写给还在为论文纠结的博士和硕士们：怎么提出idea-一个提出idea的过程是什么？"><a href="#写给还在为论文纠结的博士和硕士们：怎么提出idea-一个提出idea的过程是什么？" class="headerlink" title="写给还在为论文纠结的博士和硕士们：怎么提出idea?一个提出idea的过程是什么？"></a>写给还在为论文纠结的博士和硕士们：怎么提出idea?一个提出idea的过程是什么？</h3><p>大家明白我的意思了吧。找idea的过程需要不断地反复。在这一过程中，你自己的逻辑也会得到提升。但是千万别想着，你只要练习了100小时瞄准，你射的第一支箭就能命中靶心。实际上，不管你怎么努力练习瞄准，你都可能射不中，甚至脱靶。但是不能因为这种不确定性，我们放弃了射箭。我们需要的不断的尝试！快速的迭代！快速从错误和失败中走出来，不断地提出新的idea。一个不行就下一个。我们需要不断地思考，我们不能畏首畏尾。创新性强的idea可能提不出来，那么我们从创新性小的idea开始做起。上来就想发CCF A类论文是不太现实的，我们可以从C类开始。对不对？</p><h3 id="你还在为idea纠结么？这里告诉你两种提idea的思路"><a href="#你还在为idea纠结么？这里告诉你两种提idea的思路" class="headerlink" title="你还在为idea纠结么？这里告诉你两种提idea的思路"></a>你还在为idea纠结么？这里告诉你两种提idea的思路</h3><p>在我们做完一项研究工作以后，就需要开始撰写论文。写论文的时候，<strong>第一个需要写的部分不是introduction和abstract，而应该是自己提出的设计，也就是所谓的design部分。</strong>Design部分的写作其实与自己提出的idea很相关<strong>。不同类型的idea，其实对应的design部分的写作也会不同。</strong></p><p>为了在下一次给大家讲清楚design部分的写作思路，我需要先给大家介绍一下idea的两种类型。这两种类型的idea其实对应着是两种提出idea的思路。当然了，我在这里说的思路都是延续着我之前讲的从发掘并且深入理解别人论文的insight开始的。</p><p>现在我们回到我们掌握了几个insights后，开始想自己的idea的那个时刻。我们可能会提出两种类型的idea：一种是应用型的idea。另一种是颠覆性的idea。</p><p><strong>颠覆性的idea引申出的几个designs其实是在完成不同的任务。这些任务全都成功地完成了以后，我们这个idea所期盼的研究的目标才得以完成。</strong></p><p>而<strong>应用型的idea最后引申出来的的几个designs其实是在做相同的任务，只不过是每个design对应着不同的场景。</strong></p><p>下面我给大家详细地讲讲这两种idea是怎么回事，然后我会告诉大家一般都需要怎么思考才能够提出这两种idea。</p><p><strong>首先我们来看看应用型的idea</strong>。我先举一个通俗的例子。我们现在要设计一种轮子，这种轮子是需要在山林间运送货物用的。如果我们想在树林里面方便地运送货物，我们就不能选择普通的轮子。因为山林里面很多石头和树杈，普通的汽车轮胎还没开出去十米就被扎爆胎了。这时候，我们的第一个设计，也叫basic design就是我们用钢铁制造的轮子来代替橡胶制造的轮子。这个basic design一旦做好了，装在车上，我们的确可以在山林间运送货物了。然而，这个还没完，我们在运输货物的时候，发现这种钢铁的轮子虽然不会被扎破了，但是由于轮子是金属的，一旦下雨就会打滑。等到下雨天的时候，车辆在树林间的移动速度就会大大降低。这时候我们需要在basic design的基础上再做一些改进，用来适应在下雨天防滑的这个需求或者说是场景。那我们的第二个设计，也可以叫advanced design可以是这样的：我们在金属轮胎外层安装上一堆钢钉就可以了。然后我们测试了一下，不同尺寸（长度，直径）和不同的分布的钢钉，对于轮胎抓地力的影响，最终找到一个优化的设计。</p><p>在这个idea里面，我们有两个设计：basic design 和advanced design。其中，basic design就可以满足我们最基本的需求：支持车辆在山林间行驶。只不过是basic design的性能在某些条件下不太好：下雨天就会打滑，或者抓地力不足导致不能爬陡峭的山坡。但是这种basic design在大部分时候是可以用的。为了增加轮胎的抓地力，我们在轮胎外侧设计了一堆钢钉。这种advanced design存在的目的只是为了更大地扩展basic design的应用场景或者提高一下basic design的性能。即使我们没有Advanced design，我们最初的设计也是可以用的，只不过是效果稍微差了一点点而已。</p><p><img src="/sui-xiang-yu-gan-wu/image-20210608185449854.png" alt="image-20210608185449854"></p><p><strong>那我们现在来看看什么样的idea是颠覆式的idea。</strong> 请看上面上面张图：这是NASA在火星上使用的无人机。如果我们现在需要提出一个idea，目的是为NASA设计这么一款可以在火星飞行的无人机，我们应该怎么设计呢？我们先想想，有没有什么insight是我们可以直接用的？比如说，我们直接把咱们在地球上的无人机直接放到火星上面，会有什么问题？如果真这么做的话，会产生两个问题。第一个问题是：火星上面空气密度是地球的1%，那么地球上面使用的无人机的螺旋桨，放在火星上无法产生那么大的动力。也就是根本飞不起来。为了解决这个问题，我们不得不将火星无人机的螺旋桨设计的更大，然后发动机的转速更快。这样的话对于发动机的设计，还有螺旋桨的材料要求就很高了。我们希望无人机尽量轻，发动机动力尽量强劲。这么一来，似乎无人机可以在火星飞起来了。</p><p>然而，实际上没这么简单。无人机的飞行需要用电池来提供能量。我们的无人机在从地球发射到火星这8个月的飞行中，电池的电量早就耗没了。无人机一旦到达火星地面，它是没有电力的。于是乎。我们面临的第二个问题就是无人机到火星以后需要充电。可是上火星没有插座。那我们是不是可以让无人机飞到火星车上，然后在火星车上面设计一个插座不就可以了么？这么做好像也不太行，因为火星总会刮大风，无人机可能没法那么精准地落在插座上面。那么无线充电呢？这个好像也不太行，因为现在的手机无线充电需要手机和充电器特别近。为了使得无人机能飞起来，火星上面的无人机被设计得实在是太大了，因此无线充电的信号衰减太大了，以至于根本充不进去电。那么只能最后一个办法了，那就是设计一个太阳能电池板放在无人机的头顶。但是这种太阳能电池板也不是那么容易设计的，因为火星离地球太远了，太阳能充电效率太低了。所以我们第二个设计就是一种能够在火星上快速充电的太阳能电池板。</p><p>在这个idea里面，我们有两个设计。第一个设计是针对无人机的发动机和螺旋桨提出的设计。这个设计可以让无人机在火星上飞起来。可是这个设计使得无人机的体积过大，无线充电和插座充电都不能用，我们不得不用太阳能给无人机充电。但是由于火星离太阳太远了，充电效率过低，我们于是就提出了第二个设计。这两个设计的关系不是应用型的idea里面basic design和advanced design的关系。因为这两个设计是相互影响，缺一不可的。少了任何一个设计，无人机都无法在火星飞起来。所以这种idea里面结局的问题往往是非常复杂而且交织在一起的。一旦这种科研项目做成了，其成功往往是颠覆性的。</p><p><strong>那么这两种idea一般都是怎么想出来的呢？</strong> 其实大家可以看出来，这两种idea里面，应用型的idea应该比颠覆性的idea容易想到。这是为什么呢？因为应用型的idea里面，我们只需要想出一个简单但是有效的设计作为basic design就好了。至于advanced design，我们可以多换一些场景，没准就可以想出来。换句话说，即使我们想不出来advanced design，我们至少还有basic design。按照世俗的眼光来看，只要basic design说的通，我们计算机的同学们至少可以试着往CCF C类会议投稿。如果大家对一些C类会议有所了解的话，就知道一篇C类会议的论文，很多都是6页以内的。相比与A和B类会议要求的10页甚至12页的篇幅，C类会议也就差不多是一个 basic design的量。因此，如果有的同学就是想发一篇CCF C类会议的话，不妨照着下面的思路试试：</p><p><strong>一个应用型的idea一般是这样提出来的：</strong> 我看了几篇论文。经过我的思考和理解，我发现了这几篇论文的几个insights，然后我开始想怎么能够扩展一下这些insights？这时候，我可能想到，有一些insights稍加修改，就可以用于别的场景了。于是我为了在新的场景里面使用这个改进后的insight，我就编造出了一个有意思的设计和场景。这样一来，我就有了basic design。之后，我就稍微做一些实验，看看basic design的效果。一般来说，这种稍微改进后的insight都可以使得你的实验效果变得很好。于是乎，我们为了给论文增加一些分量，我们试着找一些新的场景或者需求。就相当于刚才我那个轮胎的例子里面，我们为了让轮胎在丛林中适应下雨路滑的场景，给轮胎加装钢钉。总而言之，应用型的idea之所以被我称为是应用型，是因为这种idea是建立在深入思考并且理解了别人论文的insight以后，把别人的insight加以扩展，并且应用到了新的场景下面，解决的是新场景下的一些问题。</p><p>讲过的，提出idea的过程其实是很随机的。我们只要掌握了一个核心的思路以后，具体的思考过程可能很随机。比如说还是上面的那个轮胎的例子。我可能是先想到了advanced design，但是当时我没意识到这其实是advanced design。然后我在后期再仔细思考的时候，才发现，原来这种设计有更加简化的版本。那么我们完全可以把简化的版本当作basic design。然后给深入思考一下们最开始提出的那个设计的特殊的地方，然后找到合适的场景，然后把这个设计可能再稍微改进一下。最后组开始的design就可能成为了advanced design。</p><p>其中一篇论文是介绍如何设计汽车的雪地胎。这篇论文交给我的insight是：只要轮胎的花纹设计的合理，轮胎的抓地力就会增强。于是现在我脑子里想的是，怎么应用这个insight或者怎么扩展这个insight呢？花纹可以增加抓地力，那么除了花纹还有啥可以增加抓地力？花纹是在轮胎上面雕刻，是在轮胎的表面刻一些凹痕。那么如果我们不是雕刻一些凹痕，而是让轮胎有一些突起是不是也可以增加抓地力？那么我们怎么增加突起呢？往轮胎上面安装钉子？要是装钉子的话，那是不是橡胶的轮胎就不合适了？那我们换成钢铁的轮胎不就好了么？那么钢铁的轮胎加上钢铁的钉子，这种轮胎谁要呀？我们要实在森林里面开车，应该就用的上了。有了钢钉的加持，这种轮胎应该可以在山林中越野。咦？既然要越野的话，没有钢钉应该也行。那就把没有钢钉的版本当作basic design，把有钢钉的版本当作advanced design。于是乎，一个idea和它所带来的设计就这样完成了</p><h2 id="如何从别人的论文里获取自己的idea？"><a href="#如何从别人的论文里获取自己的idea？" class="headerlink" title="如何从别人的论文里获取自己的idea？"></a>如何从别人的论文里获取自己的idea？</h2><p>作者：Cheng Li<br>链接：<a href="https://www.zhihu.com/question/353691411/answer/900046621">https://www.zhihu.com/question/353691411/answer/900046621</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>发现看的人有点多 （为了避免误导）感觉要补充说明下</p><p>如果只是A+B需要说明为什么是non-trivial的</p><p>一般至少要改成A+B’</p><p>或者A+B+C比较容易发</p><p>比如之前搞过一篇</p><p>其实是Unsupervised Landmark + VUNet的decompose + CycleGAN</p><p>如果只是前面两个term做到一半的时候试着投ICLR 就没成。。。</p><p>后来把CycleGAN的部分做完CVPR就中了。。。</p><p>（因为好像点赞的人很多我们后面整理一下自己的publication</p><p>，其实很多时候A+B’也可以做出还不错的想法）</p><hr><p><strong>原来回答：</strong></p><p>我其实有个不错的想法。。。</p><p>找40篇比较新的oral paper</p><p>最好是开源的、你能看懂的、尽可能时髦的、大佬点赞的。</p><p>然后画一个40*40的矩阵。。</p><p>对角线上的元素不看，还剩下1560个元素</p><p>每个元素看看A+B是不是靠谱</p><p>虽然可能99%都不靠谱。。。</p><p>但是还是有可能筛出来15篇左右的idea。。。。</p><p>（如果考虑交换性可能只有7篇也够了。。。）</p><p>或者你找40篇比较新的不是你发的oral paper，</p><p>再找K篇自己的paper，也可以做这个事情。</p><p>这样就不用排除对角元素了</p><hr><p>个人的publication水平还不高</p><p>不过很多其实也不是A+B产生的。。</p><p>比如CNN之前的话</p><p>有一些是发数据集的</p><p>Pixel-level hand detection in ego-centric videos</p><p>有一些其实是一个经典pipeline里面有A+B+C很多步</p><p>别人讨论B，C等步骤比较多，但是A步骤也很重要</p><p>想出一个A的trick最后发展出一片文章</p><p>Face alignment by coarse-to-fine shape searching</p><p>A+B也可以有一些跨度大的时候，也能产生一些还比较有趣的想法。。</p><p>并不是简简单单的incremental work</p><p>比如把推荐系统用在分类器推荐（CNN时代之前）。。</p><p>Model recommendation with virtual probes for egocentric hand detection</p><p>分而治之也是常见思路，任何topic都可以加（CNN时代之前）</p><p>Unconstrained face alignment via cascaded compositional learning</p><p>还有有的时候看到别人RL+tracking的文章，想到手里的聚类也可以这么做，就搞了一个</p><p>A+B（不过步子扯有点大老是被拒后来就投了AAAI）</p><p>Merge or not? learning to group faces via imitation learning</p><p>今年还看到有人用GCN聚类所以结合GCN重新投了一篇。。。</p><p>（还没release）</p><p>类似这样。。</p><p>还有有时候可以做一些哲学讨论，就不是简单的A+B了</p><p>The devil of face recognition is in the noise</p><h2 id="如何找到研究的突破点"><a href="#如何找到研究的突破点" class="headerlink" title="如何找到研究的突破点"></a>如何找到研究的突破点</h2><p>我刚做研究的时候，也有找不到研究突破点的感觉。但是随着经验的增长，还是找到了一些技巧，这里分享给大家。</p><p>1.<strong>多读文献加总结</strong>是科研有突破的重要途径。建议从早期的文章开始读，读的时候<strong>想象自己穿越回当时</strong>，能不能发现问题在哪里，有没有什么新的想法。然后再找后面的文章查对一下，看看自己的想法对不对。通过这种方式促使自己学会发现问题，找到创新点。</p><p>2.多阅读一些<strong>专业的审稿意见</strong>，训练自己的鉴赏能力。很多未解决的问题都能从审稿意见中发掘出来。欢迎关注我的系列视频栏目「从审稿到中稿」，带大家从审稿意见中发现问题，最终实现中稿。</p><p>3.多读读arxiv，最好培养<strong>定期翻看arxiv最新文章</strong>的习惯。很多厉害的文章都会先放在arxiv上，而做研究掌握一个好的timing至关重要，早点入局就能抢到先手。另一方面，arxiv上很多文章做的不够好，问题没做完，自己多想想努力一下就能有更好的结果。当然别忘了引用arxiv的文章并讨论区别。</p><p>4.培养自己对科研问题的<strong>格局把握</strong>，具体说，判断一个问题是三个月之内可以解决的，还是三年可以解决的，还是三十年也解决不了的。不要凭自己的主观臆断来判断，多看看大佬怎么说。如果很多大佬都觉得一个问题三年之内可以解决，并且很有趣，那么就是一个很好的入手问题。</p><p>5.题主说，看到很多论文准确率都很高了，不知道该咋办。这是个新手非常常见的问题，我想引用一个著名的比喻来回答。一个问题的模型，就好比一架很复杂的波音飞机，有几百上千个按钮，有的重要，有的不重要。你现在看到的，就好比看到有那么一架飞机飞的很快。但是更有意义的问题是，你对这架飞机的性能完全了解吗？哪些部分是真正起作用的？能不能造一架更简单但是飞的更快的飞机？如果不能，瓶颈在哪里？这架飞机是不是足够鲁棒，在任何条件下都能飞得很好？</p><p>6.题主说准确率已经很高了，比如说达到了95%。一个事实：如果把5%的错误样本拿出来构成一个数据集，那么它的正确率将是0%。</p><p>所以关键不在准确率多高，而在失败的样本是不是重要。在真实的自动驾驶或者机器人中，有1%的失误可能会导致很快报废。</p><p>7.永远关注交叉领域或者新领域，并大胆提出新想法。可能新想法会被另一个领域的人爆锤，不要灰心，这是常态，<strong>并不要怕碰壁成为作为阻止你学习的理由。不要自己给自己设置壁垒。</strong></p><p>8.可能导师会觉得研究另一个领域会投入很多资源（比如算力或者他开会的时间），但是他挡不住你学习另一个领域的论文。资源可以受限，但是思想不会受限。特别是一个领域的突破性进展，往往可以影响其他领域。</p><p>9.<strong>把卡住自己的问题记录下来</strong>，之后会有意想不到的影响。我跟swin transformer(今年获得了比best paper很难得的马尔奖)的作者聊过，他们是怎么想到swin transformer的？他们说，之前就有想过把self attention替换cnn，但是瓶颈是计算量会大很多，看到谷歌出了vit，算的很快，解决了他们之前的瓶颈，就是self-attention也可以很快还很好，这样他们就产生了swin的想法。</p><p>10.人的创造力是无穷的，科研更多是一种文化，比赛才要刷点。比如同一届的会议有很多论文在imagenet上点数都差不多，但是都各有亮点。百花齐放，百家争鸣，而不要内卷。</p><p>11.多复现一些代码，很多论文都是表面好看，实质代码一跑就有很多问题。</p><p>12.历史是会循环的，旧的领域的突破往往可以适用于新的领域。因此多看看经典论文很有帮助。</p><p>13.要掌握一门到两门可以泛化的，经过大量事实验证有效的方法论。举个例子，比如加速计算的cuda编程等一系列方法，在不同的领域都可以试用。</p><p>14、很多好的idea都是在讨论和碰撞中产生的。尽量多和研究者讨论，方式不限于</p><p>（1）实验室交流</p><p>（2）参加讨论会或者研讨会</p><p>（3）微信群组讨论，可以私信加入我的讨论群</p><p>（4）参加一些国际顶级学术会议，不需要发表论文也可以参加</p><p>（5）网上看workshop的录像</p><p>15、没有比较就没有伤害，把同一个顶会做同一个任务的论文放到一起，高下立判。</p><p>16、想到什么好的idea，<strong>马上去试</strong>，不要犹豫。<strong>没有不好的idea，只有不明确的idea</strong>。</p><h2 id="分清主次太重要了"><a href="#分清主次太重要了" class="headerlink" title="分清主次太重要了"></a>分清主次太重要了</h2><p>不要由着自己性子来,先做重要的,不然很大可能是做无用功!!!!</p><p>如果有一种思想,先用简单的模型去把这种思想实现,如果好像有用,就再调模型！！！！反正最开始一定要设计的简单,突出主要的insight！</p><p><strong>做科研其实就是先大量阅读论文,看看别人怎么做的，然后提出一个新颖的理论可行的猜想，然后拿出一个月的时间来设计网络来调参,基于结果决定该idea可行与否。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 奇奇怪怪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 奇怪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>奇奇怪怪不成体系问题合集</title>
      <link href="qi-qi-guai-guai-bu-cheng-ti-xi-wen-ti-he-ji/"/>
      <url>qi-qi-guai-guai-bu-cheng-ti-xi-wen-ti-he-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="HP打印机状态需要注意、打印显示用户干预如何处理？"><a href="#HP打印机状态需要注意、打印显示用户干预如何处理？" class="headerlink" title="HP打印机状态需要注意、打印显示用户干预如何处理？"></a>HP打印机状态需要注意、打印显示用户干预如何处理？</h2><p>和打印机连接在<strong>同一Wifi并已经添加了打印机</strong>的情况下，<strong>右键打印机</strong>-&gt;<strong>属性</strong>-&gt;<strong>Web服务</strong>，获得<strong>打印机IP</strong>，然后<strong>右键打印机</strong>-&gt;<strong>打印机属性</strong>-&gt;<strong>端口</strong>-&gt;<strong>添加端口</strong>-&gt;<strong>Standard TCP/IP Port</strong>-&gt;<strong>输入你的打印机的IP</strong>-&gt;<strong>填写随便一个端口名</strong>-&gt;<strong>应用</strong>-&gt;<strong>问题解决</strong>！</p><h2 id="文件扩展名？什么是-mat文件？"><a href="#文件扩展名？什么是-mat文件？" class="headerlink" title="文件扩展名？什么是.mat文件？"></a>文件扩展名？什么是.mat文件？</h2><p>文件扩展名查询：<a href="https://www.reviversoft.com/zh-cn/file-extensions/mat">https://www.reviversoft.com/zh-cn/file-extensions/mat</a></p><h2 id="Mathtype破解问题"><a href="#Mathtype破解问题" class="headerlink" title="Mathtype破解问题"></a>Mathtype破解问题</h2><p><a href="https://blog.csdn.net/weixin_43115631/article/details/110067650">首先是这篇博客</a></p><p><a href="https://blog.csdn.net/qq_40750329/article/details/102858972">其次是自己的CSDN</a></p><h2 id="IDM老是弹出更新界面？"><a href="#IDM老是弹出更新界面？" class="headerlink" title="IDM老是弹出更新界面？"></a>IDM老是弹出更新界面？</h2><p><a href="https://www.cnblogs.com/jingtaoxin/p/13773077.html">最新的破解</a></p>]]></content>
      
      
      <categories>
          
          <category> 奇奇怪怪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 奇怪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>肤色分级与系统</title>
      <link href="fu-se-fen-ji-yu-xi-tong/"/>
      <url>fu-se-fen-ji-yu-xi-tong/</url>
      
        <content type="html"><![CDATA[<h2 id="论文阅读-lt-lt-Brief-overview-of-PANTONE-SkinTone-Guide-chart-in-CIEL-a-b-color-space-gt-gt"><a href="#论文阅读-lt-lt-Brief-overview-of-PANTONE-SkinTone-Guide-chart-in-CIEL-a-b-color-space-gt-gt" class="headerlink" title="论文阅读-<<Brief overview of PANTONE SkinTone Guide chart in CIEL*a*b* color space>>"></a>论文阅读-&lt;&lt;Brief overview of PANTONE SkinTone Guide chart in CIEL*a*b* color space&gt;&gt;</h2><h3 id="Title-amp-Keywords-amp-Abstract-amp-Conclusion"><a href="#Title-amp-Keywords-amp-Abstract-amp-Conclusion" class="headerlink" title="Title&amp;Keywords&amp;Abstract&amp;Conclusion"></a>Title&amp;Keywords&amp;Abstract&amp;Conclusion</h3><h4 id="CIEL-a-b-color-space"><a href="#CIEL-a-b-color-space" class="headerlink" title="CIEL*a*b* color space?"></a>CIEL*a*b* color space?</h4><p><a href="https://blog.csdn.net/lxw907304340/article/details/46437953">彩色空间转换公式</a></p><p><strong>颜色开发培训讲义</strong>：<a href="https://www.zhihu.com/column/cxqingzong-color">https://www.zhihu.com/column/cxqingzong-color</a>     <strong>这篇不能更赞</strong></p><p><strong>可见光谱:</strong></p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014213348777.png" alt="可见光谱"></p><p>我们所说的<strong>颜色主要分两种</strong>：</p><blockquote><p><strong>光源色（light source color）</strong>：来自发光体的颜色。如太阳，灯泡，led灯，等等。</p><p><strong>表面色（surface color</strong>）：不是来自发光体的物体色。物体本身不发光，但能看到物体的颜色，是因为这些物体能对来自于其他发光体的光的选择性的吸收和反射。</p></blockquote><p>颜色也可以简单分为两大类：</p><blockquote><p><strong>非彩色</strong>：黑白灰</p><p><strong>彩色</strong>：红黄蓝绿等</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014210822647.png" alt="彩色与非彩色"></p><p><strong>颜色感知的三要素，光源，物体和观察者，缺一不可，缺少一个要素，我们看不到颜色，或者其中一个要素发生改变，我们看到的颜色都会不一样。</strong></p><p><strong>1光源</strong></p><p><strong>不同光源性质是不一样的</strong>，有些光源会亮一点白一点，如太阳，或者就是太阳光，一天内不同时间段的太阳光也会有很大差异，导致在这些不同的光源下看相同一个颜色都会有很大差异.</p><p>所以我们在<strong>颜色开发或者颜色沟通交流的时候，会指定一个标准光源</strong>，这样能确保我们双方看颜色条件的一致性。我们比对颜色使用的光源都是有标准规定的光源。通常在下图所示的对色灯箱里看颜色。灯箱里面装有不同的常用光源。</p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014215437189.png" alt="对色灯塔及常见光源" style="zoom: 67%;"><blockquote><p><strong>Light sources：发光体（照明体）</strong>。泛指能发出光（可见光）的物体，如太阳，蜡烛，灯泡等。但是有些发光体发出的光是变化的不稳定的。例如太阳光，就算在同一天光照辐射都是不一样的，是变化的，更何况在不同的天气，不同的季节。所以很难用这些不稳定的光源来进行对颜色的描述和交流。</p><p><strong>lluminants：光源</strong>。是一个可以定量描述的发光体。是国际照明委员会 CIE（Commission Internationale de L’Eclairage）为了对颜色的评估和计算而定义了不同类型的，能用数学表（相对能量和波长)表示的标准光源。</p></blockquote><p><strong>色温Color temperature是照明光学中用于定义光源颜色的一个物理量。光源的色温是以光源发光时所显现的颜色与一个绝对黑体被高温燃烧时所显现的颜色相一致时的燃烧温度来定义的，它的单位是绝对温度Kelvin开尔文【K】。是为了量化光源色彩的一个物理量</strong></p><p>开尔文与摄氏度的转换关系如下：</p><p><strong>K(开尔文）=273.15+T(摄氏度）</strong></p><p>K值越高，显现的颜色就愈趋向于白蓝色；K值越低，显现的颜色就愈趋向于黄红色。</p><p>开尔文认为，假定纯黑体，能够将落在其上的所有热量吸收，而没有损失，同时又能够将热量生成的能量全部以“光”的形式释放出来的话，它产生辐射最大强度的波长随温度变化而变化。</p><p><strong>显色指数color rendering index (CRI)</strong> :<strong>与标准的参考光源相比较，一个光源对物体颜色外貌所产生的效果</strong>。换句话说，是<strong>一个光源与标准光源（例如日光）相比较下在颜色辨认方面的一种测量方式</strong>。CRI是一种得到普遍认可的度量标准，也是目前评价与报告<strong>光源显色性</strong>的惟一途径。</p><p><strong>Ra</strong> <strong>=</strong> <strong>物体在某一光源照射下所显现的颜色 ÷ 物体本身所具有的颜色</strong>。</p><p>Ra表示某光源的显色指数。Ra愈接近100%，<strong>表明在该光源照射下，物体所显现的颜色与物体本身所具有的颜色的差异就愈小</strong>。</p><p><strong>标准光源</strong>的光谱要求如下：</p><p>（1）光源的<strong>色温必须是5000K-6500K</strong>，在这种光源色温下观察颜色的效果基本类似于中国大部分地区上午8点至10点，下午3点至5点的自然光下的观察效果。</p><p>（2）光源的<strong>显色指数Ra&gt;90</strong></p><p>光源的性质，可以通过<strong>光谱功率分布曲线（SPD）</strong>来描述。不同的光源有着不同的光谱功率分布曲线。光谱功率分布（SPD）的意思就是光源发出可见光的不同光谱波长（400nm~700nm）的功率是不同的。功率可以理解成强度的大小。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015094547985.png" alt="常用光源的光谱功率分布曲线"></p><p><strong>2物体</strong></p><blockquote><p><strong>物体关于颜色的性质是对不同波长的电磁波的选择性吸收</strong>，所以我们用<strong>光谱反射率曲线</strong>来表达物体的这种性质。<strong>红色绿色蓝色</strong>的光谱反射率曲线的<strong>最大特征</strong>是，它<strong>有明显的波峰</strong>。<strong>波峰所在的位置的电磁波波长代表着这个物体的颜色</strong>。</p><p>但是<strong>黑白灰</strong>就不一样。物体之所以能够呈现出<strong>白色</strong>，是因为这个物体对<strong>不同波长的电磁波几乎都不吸收</strong>，所以都被反射出来。<strong>黑色</strong>跟白色刚好相反，黑色物体<strong>几乎完全吸收所有波长的电磁波</strong>，所以从黑色的光谱反射率曲线来看，所有波长的光谱的反射率都很低很低。</p></blockquote><p><strong>3观察者人眼：</strong></p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015095247048.png" alt="人眼观察黄光" style="zoom:67%;"><p>类似人眼三种视锥细胞对不同波长的光的响应，研究人员也得到一个标准观察者的三刺激值（x，y，z），作为测色仪辨别颜色的视锥细胞。通过这三个参数xyz，来描述一个颜色，也就是后面将要介绍的<strong>CIE-XYZ颜色空间</strong>。</p><p><strong>CIE-XYZ颜色空间：</strong></p><p>我们将<strong>光源</strong>、<strong>物体</strong>和<strong>观察者</strong>这三要素的性质相乘，也就是光源的光谱功率分布曲线乘以物体的光谱反射率曲线乘以标准观察者，得到三个参数<strong>X，Y，Z</strong>（都是大写字母），不同的颜色，有着不同XYZ值。</p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100147722.png" alt="CIEXYZ" style="zoom:80%;"><p>按照下图里的公式算出<strong>x</strong>（小写X），<strong>y</strong>（小写Y）。<strong>xyz值（小写）代表着XYZ（大写）的占比</strong>，这样三个参数缩减到两个参数，<strong>两个参数形成一个平面的二维颜色空间，也就是CIE-XYZ颜色空间</strong>。CIE XYZ颜色空间具有不均匀性。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100255681.png" alt="CIE-XYZ颜色空间"></p><p><strong>缺点：不容易对颜色差异的大小进行判定</strong>，<strong>无法非常直观的判定这个颜色</strong>就是我需要的颜色，不知道这个颜色跟我需要的颜色的差异的大小。</p><p>我们把<strong>人眼感觉不出的色彩差别量（变化范围）叫做颜色的宽容量</strong>。颜色的宽容量反映在<strong>CIExy色度图上即为两个色度点之间的距离</strong>。因为，每种颜色在色度图上是一个点，但<strong>对人的视感觉来说，当这种颜色的色度坐标位置变化很小时，人眼仍认为它是原来的颜色，感觉不出它的变化。</strong>所以，对视感觉效果来说，<strong>在这个变化的距离（或范围）以内的色彩差别量，在视觉效果上是等效的。</strong>对色彩复制和其它颜色工业部门来说这种位于人眼宽容量范围之内的色彩差别量是允许存在的。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015220402922.png" alt="不同标准色度点的颜色宽容量"></p><p><strong>CIE-L*a*b*颜色空间：</strong></p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201016174528078.png" alt="介绍"></p><p>跟之前介绍的孟塞尔颜色体系的颜色空间是一样，是三维空间中立体的球形。空间中有三个维度，形成三个互相垂直的轴，分别是：</p><ul><li>L*轴：从上到下；<strong>表示明度</strong>，范围由0到100，表示颜色从深（黑）到浅（白）。</li><li>a*轴：从左到右；<strong>表示红绿</strong>，数值变化由正到负，表示颜色从红（正）到绿（负）。a值越大颜色越红，a值越小颜色越绿。</li><li>b*轴：从里到外。<strong>表示黄蓝</strong>，数值变化由正到负，表示颜色从黄（正）到蓝（负）。b值越大颜色越黄，b值越小颜色越蓝。</li><li><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100941807.png" alt="CIE-L*a*b*"></li></ul><p><strong>两个颜色之间的差异大小</strong>。引入一个概念——<strong>色差△E</strong>。<strong>两个颜色的差异大小，就是这两个颜色的在颜色空间上两个点的距离</strong>。色差的计算公式如下：</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100802146.png" alt="色差的计算公式"></p><p><strong>同色异谱：</strong></p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019221820491.png" alt="同色异谱"></p><blockquote><p>有时候我们看两个物体的颜色，在某种场景下，比如上图左边的两个物体在室外太阳光底线看起来颜色的一样的，但是一旦我们拿到室内，如上图右边，在荧光灯管底线发现，其实这两个物体的颜色是相差非常大的。这就是同色异谱现象，<strong>同色异谱也叫做条件对色</strong>，顾名思义，这两个颜色只有符合一定观察条件下颜色才能相等，实际上这两个颜色并非完全一样。</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019222411178.png" alt="同色异谱"></p><blockquote><p><strong>同色异谱（条件等色）的根源在于两物体的光谱反射率曲线不同</strong>，也就是说有不同的颜色色粉配方。同色异谱中的<strong>“谱”指的就是光谱反射率曲线</strong>。如下图，就是上面两个颜色色卡的光谱反射率曲线，可以看到两者的差异是非常大的。也可以看到两个光谱反射率曲线的交叉点很多。</p><p>《颜色技术原理》中提到过史泰鲁斯 （stiles）和 维 泽 斯 基（ wyszecki）发现两个同色异谱的颜色的光谱反射曲线在可见光谱波段 （400~700nm） 内， 至少在三个不同波长上必须具有相同的反射率。也就是两者的光谱反射率曲线至少要有三个交叉点 。</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019222914891.png" alt="不同的光谱反射率曲线"></p><p>常见颜色空间介绍：<a href="https://blog.csdn.net/JiangHui1211/article/details/84592774?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">https://blog.csdn.net/JiangHui1211/article/details/84592774?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><p><strong>RGB颜色空间</strong>:</p><p><strong>任意色光F</strong>都可以用RGB<strong>三种颜色不同分量的相加混合而成</strong>：**F=r[R]+g[G]+b[B]**。</p><p><strong>一般我们读取图片获得的三维矩阵是RGB空间</strong></p><p><strong>色度学规则</strong>：<br>　　(1)通过<strong>R,G,B这三种颜色能产生任何颜色</strong>，并且<strong>这三种颜色混合后产生的颜色是唯一</strong>的。<br>　　(2)如果<strong>两个颜色相等，这三个颜色分量再乘以或者除以相同的数，得到的颜色仍然相等</strong>。<br>　　(3)<strong>混合色的亮度等于每种颜色亮度的和</strong>。</p><p><strong>RGB颜色空间</strong>的<strong>均匀性非常差，且两种颜色之间的知觉差异色差不能表示为该颜色空间中两点间的距离</strong>，但是<strong>利用线性或非线性变换</strong>，则<strong>可以从RGB颜色空间推导出其他的颜色特征空间</strong>。</p><p><strong>CMYK模式：</strong></p><p>俗称<strong>四色打印模式</strong>，是最佳的打印模式。因为在实际应用中，青色、洋红色和黄色很难叠加形成真正的黑色，最多不过是褐色而已。因此才引入了K——黑色。黑色的作用是强化暗调，加深暗部色彩。</p><p><strong>HSV颜色空间：</strong></p><p>感觉和孟塞尔颜色体系很像。**HSV即色相(Hue)、饱和度(Saturation)、明度(Value)，又称HSB(B即Brightness)**。</p><p>RGB和CMYK<strong>面向硬件</strong>，可用于<strong>图片编码</strong>；HSV<strong>面向用户</strong>，可用于<strong>图片编辑软件</strong>。</p><p><strong>sRGB色彩空间：</strong></p><p>standard Red Green Blue，<strong>标准红绿蓝色彩空间</strong>是惠普与微软于1996年一起开发的用于<strong>显示器、打印机以及因特网的一种标准RGB</strong>色彩空间。这种标准得到了W3C、Exif、英特尔、Pantone、Corel以及其它许多业界厂商的支持。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015222536629.png" alt="sRGB色域"></p><p>维基百科：<a href="https://zh.wikipedia.org/wiki/Lab%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4">https://zh.wikipedia.org/wiki/Lab%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4</a></p><p>在<strong>RGB</strong>或<strong>CMYK</strong>值与<strong>L*a*b*</strong> 之间没有转换的简单公式，因为<strong>RGB和CMYK色彩空间是设备依赖的</strong>。RGB或CMYK值<strong>首先必须被变换到特定绝对色彩空间中，比如sRGB或Adobe RGB</strong>。这种调整将是设备依赖的，但是<strong>变换的结果数据是设备无关的</strong>，允许把数据变换成<strong>CIE 1931色彩空间</strong>并接着变换成<strong>L*a*b*</strong>。</p><h4 id="Hue-Angle色相角"><a href="#Hue-Angle色相角" class="headerlink" title="Hue Angle色相角?"></a>Hue Angle色相角?</h4><blockquote><p>孟塞尔颜色体系：</p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014214736980.png" alt="image-20201014214736980" style="zoom:50%;"><p><strong>1）色相/色调/Hue</strong>，对于色相我们比较熟悉的是这个色环，色环上不同位置代表不同色相。色相的排列顺序是按照可见光波长从低到高，逆时针分布。这是色相环的概念。把一周均分成五5种主色互相调和成五种中间色，相邻的两个位置之间再均分10份，共100份</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015221249004.png" alt="色相带"></p><p><strong>2）明度/Value/Lightness</strong>，很容易理解，就是一个颜色中含有白和黑的比例：白越多黑越少，这个颜色的明度就越高。</p><p><strong>3）色度chroma</strong>。是一个颜色里面含有这个色相的浓度。<strong>很多人容易把饱和度和明度的概念混淆。是因为他们不理解饱和度和明度在色彩空间中的位置。明度在色彩空间中的位置是从顶部到底部，明度从高到低。而饱和度在色彩空间中的位置是从里到外，饱和度从低到高。</strong></p></blockquote><h4 id="色度-色域？"><a href="#色度-色域？" class="headerlink" title="色度?色域？"></a>色度?色域？</h4><p>研究颜色测量的学科叫做<strong>色度学</strong>，色度学的任务就是用数量化来表征色觉特性。色度”中的“度”是度量的意思。 类似于长度，高度等等概念。度量长度或高度使用的工具是尺子，而度量颜色的工具就是<strong>颜色感知三要素</strong>。</p><p><strong>色域</strong>是对一种颜色进行编码的方法，也指一个技术系统能够产生的颜色的总合。在计算机图形处理中，色域是<strong>颜色的某个完全的子集</strong>。颜色子集最常见的应用是用来精确地代表一种给定的情况。例如一个给定的色彩空间或是某个输出装置的呈色范围。</p><h3 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h3><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170452039.png" alt="Table1" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170512027.png" alt="Table2" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170533519.png" alt="Figure1" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170555412.png" alt="Figure2" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170613588.png" alt="Figure3" style="zoom:50%;"><blockquote><p>lightness and chroma:亮度和色度</p><p>yellow and red categories:Hue色相</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019215839363.png" alt="观察者"></p><blockquote><p>用于色彩排列和分类的这种三维系统已经融入目前广泛使用的<strong>色彩空间模型、色差公式和色容差系统</strong>。</p></blockquote><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019223925779.png" alt="转换" style="zoom:67%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019224245507.png" alt="从色彩到色彩测量" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019224402876.png" alt="从色彩到色彩测量" style="zoom:50%;"><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><blockquote><p><strong>Applications</strong>:</p><ol><li>diagnosis and treatments of cutaneous disorders </li><li>matching our body color to get maxillofacial soft tissue prostheses</li><li>face detection and recognition</li><li>cosmetics</li></ol><p>CIEL*a*b* is a <strong>device-independent color space</strong> which we used in our study.</p></blockquote><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><blockquote><p>110 colors numbered from 1Y01 SP to 4R15 SP</p><p> the first number <strong>indicates the chroma, varies from 1 to 5</strong> which it is the highest</p><p> the rigid represents <strong>yellowness (Y)</strong> or <strong>redness (R) as hue</strong> and </p><p><strong>two last numbers show the lightness, varies from 1 to 15</strong> which it is the darkest. These samples are sorted in <strong>decreasing order</strong> of lightness (5).</p></blockquote><h2 id="小实验"><a href="#小实验" class="headerlink" title="小实验"></a>小实验</h2><blockquote><p>MATLAB Api：<a href="https://www.mathworks.com/help/">https://www.mathworks.com/help/</a></p></blockquote><pre class=" language-matlab"><code class="language-matlab"><span class="token comment" spellcheck="true">% shows example of illuminant estimation based on Grey-World, Shades of</span><span class="token comment" spellcheck="true">% Gray, max-RGB, and Grey-Edge algorithm</span><span class="token comment" spellcheck="true">%example images</span>input_im<span class="token operator">=</span><span class="token function">double</span><span class="token punctuation">(</span><span class="token function">imread</span><span class="token punctuation">(</span><span class="token string">'test_3.jpg'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>input_im<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">title</span><span class="token punctuation">(</span><span class="token string">'input image'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% %Grey-World</span><span class="token comment" spellcheck="true">% [wR,wG,wB,out]=general_cc(input_im,0,1,0);</span><span class="token comment" spellcheck="true">% figure(2);</span><span class="token comment" spellcheck="true">% imshow(uint8(out));</span><span class="token comment" spellcheck="true">% title(</span><span class="token string">'Grey-World'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%max-RGB</span><span class="token comment" spellcheck="true">% [wR,wG,wB,out]=general_cc(input_im,0,-1,0);</span><span class="token comment" spellcheck="true">% figure(3);imshow(uint8(out));</span><span class="token comment" spellcheck="true">% title(</span><span class="token string">'max-RGB'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% Shades of Grey</span>mink_norm<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">% any number between 1 and infinity</span><span class="token punctuation">[</span>wR<span class="token punctuation">,</span>wG<span class="token punctuation">,</span>wB<span class="token punctuation">,</span>out<span class="token punctuation">]</span><span class="token operator">=</span><span class="token function">general_cc</span><span class="token punctuation">(</span>input_im<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>mink_norm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">title</span><span class="token punctuation">(</span><span class="token string">'Shades of Grey'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% Grey-Edge</span><span class="token comment" spellcheck="true">% mink_norm=5;    % any number between 1 and infinity</span><span class="token comment" spellcheck="true">% sigma=2;        % sigma </span><span class="token comment" spellcheck="true">% diff_order=1;   % differentiation order (1 or 2)</span><span class="token comment" spellcheck="true">% [wR,wG,wB,out]=general_cc(input_im,diff_order,mink_norm,sigma);</span><span class="token comment" spellcheck="true">% figure(5);imshow(uint8(out));</span><span class="token comment" spellcheck="true">% title(</span><span class="token string">'Grey-Edge'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% 截取皮肤区域</span><span class="token comment" spellcheck="true">% out=out(375:480,185:290,:);</span><span class="token comment" spellcheck="true">% out=out(360:420,360:420,:);</span>out<span class="token operator">=</span><span class="token function">out</span><span class="token punctuation">(</span><span class="token number">165</span><span class="token operator">:</span><span class="token number">240</span><span class="token punctuation">,</span><span class="token number">230</span><span class="token operator">:</span><span class="token number">320</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% 展示截取皮肤区域</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% convert to lab</span>labI <span class="token operator">=</span> <span class="token function">rgb2lab</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%seperate l,a,b</span><span class="token comment" spellcheck="true">%matlab的下标从1开始</span>l <span class="token operator">=</span> <span class="token function">labI</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>a <span class="token operator">=</span> <span class="token function">labI</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>b <span class="token operator">=</span> <span class="token function">labI</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% % 显示各维度直方图</span><span class="token comment" spellcheck="true">% figure(4);</span><span class="token comment" spellcheck="true">% hist(l);</span><span class="token comment" spellcheck="true">% figure(5);</span><span class="token comment" spellcheck="true">% hist(a);</span><span class="token comment" spellcheck="true">% figure(6);</span><span class="token comment" spellcheck="true">% hist(b);</span><span class="token comment" spellcheck="true">%网格曲面图</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">subplot</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">meshc</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>l<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">subplot</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">meshz</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>l<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><img src="/fu-se-fen-ji-yu-xi-tong/result_0.png" alt="result_0" style="zoom: 50%;"><img src="/fu-se-fen-ji-yu-xi-tong/result_1.png" alt="result_1" style="zoom: 50%;"><p><img src="/fu-se-fen-ji-yu-xi-tong/result_3.png" alt="result_3" style="zoom: 67%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019214729213.png" alt="image-20201019214729213"></p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019214825960.png" alt="三维图" style="zoom:80%;"><h2 id="接下来打算要做"><a href="#接下来打算要做" class="headerlink" title="接下来打算要做"></a>接下来打算要做</h2><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019155029016.png" alt="计划流程图"></p><h2 id="Pantone-Skintone-Review"><a href="#Pantone-Skintone-Review" class="headerlink" title="Pantone Skintone Review"></a>Pantone Skintone Review</h2><p>彩通肤色指南[PANTONE SkinTone Guide]是根据科学测量各种人类皮肤类型中数干种实际肤色而建立。这个色库为再现实体肤色而配制，是人类肤色的完整视觉参考，适用于与肤色相关的任何市场。</p><p>首个匹配和再现逼真肤色的科学指南，适用于各个行业。1000多种人体皮肤测量值，收集于不同年龄和种族的参与者。为获得一致的样品精确度．采用几种高端X—Rite分光光度计和小巧的手持PANTONE CAPSURE分光光度计来测量皮肤样本。根据这些测量值，Pantone<br>建立了精确的皮肤色彩空间．并创建了彩通肤色色库(PANTONE SkinToneLibrary)．它确定了1 10种再现性最强的色彩。<br><strong>现有的主要问题是，这110种肤色是如何确定的？</strong></p><p><strong>这也是我们需要做的！</strong></p>]]></content>
      
      
      <categories>
          
          <category> 肤色分级 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 颜色空间 </tag>
            
            <tag> 肤色分级 </tag>
            
            <tag> MATLAB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献图书资源搜索与使用管理</title>
      <link href="wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/"/>
      <url>wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/</url>
      
        <content type="html"><![CDATA[<h2 id="文献引用信息有误"><a href="#文献引用信息有误" class="headerlink" title="文献引用信息有误"></a>文献引用信息有误</h2><blockquote><p>进入Mendeley网站，搜索出错的文献，然后“+Add to library”</p></blockquote><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201012160759194.png" alt="image-20201012160759194" style="zoom:67%;"><blockquote><p>回到Mendeley sync一下，出现该项，但是此时无法打开，如下图将你的论文添加：</p></blockquote><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201012160634578.png" alt="image-20201012160634578"></p><h2 id="SCI一区、二区、影响因子？"><a href="#SCI一区、二区、影响因子？" class="headerlink" title="SCI一区、二区、影响因子？"></a>SCI一区、二区、影响因子？</h2><p>一般SCI论文分四个区，一区都是国际顶级期刊，二区次之，三区和四区是一般的SCI期刊，有两种，一种是web of science的JCR（journal citation report）分区，在web of science 搜索论文下面会有按钮显示期刊影响力。另一种是中科院分区，请在高校或中科院内登录<a href="http://www.fenqubiao.com/">http://www.fenqubiao.com</a>。</p><p>影响因子（英文：Impact Factor），简称IF，是汤森路透（Thomson Reuters）出品的期刊引证报告（Journal Citation Reports，JCR）中的一项数据。 即某期刊前两年发表的论文在该报告年份（JCR year）中被引用总次数除以该期刊在这两年内发表的论文总数。这是一个国际上通行的期刊评价指标。影响因子现已成为国际上通用的期刊评价指标，它不仅是一种测度期刊有用性和显示度的指标，而且也是测度期刊的学术水平，乃至论文质量的重要指标。影响因子是一个相对统计量</p><h2 id="知名会议和期刊"><a href="#知名会议和期刊" class="headerlink" title="知名会议和期刊"></a>知名会议和期刊</h2><p>查找会议论文：<a href="https://blog.csdn.net/qq_35091353/article/details/107209512">https://blog.csdn.net/qq_35091353/article/details/107209512</a></p><p><strong>ICCV</strong>:International Conference on Computer Vision</p><p><strong>CVPR</strong>:International Conference on Computer Vision and Pattern Recognition</p><p><strong>ECCV</strong>:Europeon Conference on Computer Vision</p><p><strong>TPAMI</strong>:IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</p><p><strong>TIP</strong>:IEEE TRANSACTIONS ON IMAGE PROCESSING</p><p><strong>TSP</strong>: IEEE TRANSACTIONS ON SIGNAL PROCESSING</p><p><strong>ICLR：International Conference on Learning Representations</strong>   顶级会议，大佬背书</p><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201119155123687.png" alt="不完整列表"></p><h2 id="图书搜索"><a href="#图书搜索" class="headerlink" title="图书搜索"></a>图书搜索</h2><blockquote><p><strong>虫部落快搜</strong>：<a href="https://search.chongbuluo.com/">https://search.chongbuluo.com/</a></p><p>集合了很多搜索，包括<strong>鸠摩搜书</strong></p><p><strong>Z-Library：</strong><a href="https://1lib.net/">https://1lib.net/</a>   <strong>真的是宝藏</strong></p><p>Library Genesis：<a href="https://libgen.lc/">https://libgen.lc/</a></p><p>全国图书管参考咨询联盟：<a href="http://www.ucdrs.superlib.net/">http://www.ucdrs.superlib.net</a></p><p>中国国家图书管： <a href="http://www.nlc.cn/">http://www.nlc.cn</a></p><p>英文书必应搜：书名 pdf</p><p>很明显好用的google，filetype:pdf 书名啊，同样适用于百度</p></blockquote><h2 id="如何快速有效的读论文？"><a href="#如何快速有效的读论文？" class="headerlink" title="如何快速有效的读论文？"></a>如何快速有效的读论文？</h2><p><strong>进入新方向先看综述！！！</strong></p><h3 id="误区？"><a href="#误区？" class="headerlink" title="误区？"></a>误区？</h3><p><strong>从头到尾恨不得嚼透每个单词</strong>，读完后束之高阁让记忆随风飘摇，<strong>无选择的精读</strong></p><p>读论文和学英语区分开！！！读论文不是来学英语！<strong>那些在阅读中遇到的生单词（学术词汇除外）真的会对通篇的理解形成严重障碍吗？</strong></p><p><strong>只看不记</strong></p><h3 id="如何读一篇论文？"><a href="#如何读一篇论文？" class="headerlink" title="如何读一篇论文？"></a>如何读一篇论文？</h3><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201103091823835.png" alt="论文结构" style="zoom:67%;"><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201103092224607.png" alt="是否值得读"></p><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201103092352775.png" alt="如何读"></p><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201103092602961.png" alt="如何读"> 很多人对于做笔记到底写什么各执一词，这里我觉得每个人在科研的不同阶段对于文章的关注点可能不尽相同，所以很难一言以蔽之。</p><p>比如<strong>初涉科研的小白</strong>，文献阅读能力和论文写作能力比较欠缺，那么可以在<strong>笔记中[用一句话（英文）概括实验、结果、讨论章节中的每一段内容，组成一个阅读笔记]。</strong>这样既可以锻炼英语书写表达能力，也可以逼迫自己[在理解的基础上进行一定量的输出，这是一个加深理解和记忆的过程」。<br>对于<strong>阅读科研文献比较熟练，有一些科研工作经历的人来说</strong>，这个笔记的内容可能是<strong>文中某个新的实验方法、异于其他研究的实验条件、阅读时自己的新想法等等</strong>。精读文献并认真做笔记并不代表读者对于这篇文章的消化过程就此终止，我个人觉得优秀的科研论文、大牛的研究著作依然是常读常新，每位从事科研学习和工作的人在不同的时期都能从中汲取养分。</p><h3 id="总结汇报"><a href="#总结汇报" class="headerlink" title="总结汇报"></a>总结汇报</h3><ul><li>基本信息(标题、作者、作者单位、发表期刊/会议、发表时间)</li><li>核心问题</li><li>主要思路/创新</li><li>存在问题与改进思路</li></ul><h3 id="技巧？"><a href="#技巧？" class="headerlink" title="技巧？"></a>技巧？</h3><p>review论文</p><p>与论文相应的PPT、博客、视频、课程、代码等</p><p>有代码的文章重点看！有代码的话, 一方面便于自己将一些核心思想或者基础知识理解透彻; 另一方面, 将来写文章,做对比试验也方便</p><p>英文论文中，<strong>每段话第一句一般都是主旨句</strong>，剩下内容都是围绕第一句展开（自己写论文时也可以这样，先写一段中心句，下面内容围绕它展开）</p><h2 id="要数据和源码"><a href="#要数据和源码" class="headerlink" title="要数据和源码"></a>要数据和源码</h2><blockquote><p>Xiaodan学姐您好！</p><p>冒昧打扰您，目前我就读于北京邮电大学，研究生一年级，此前一直对于图像美学和图像质量评价非常感兴趣。</p><p>有幸拜读过您的最新的文章 “Beyond Vision: A Multimodal Recurrent Attention Convolutional Neural Network for Unified Image Aesthetic Prediction Tasks”，其工作将多种方法巧妙运用，行文风格清晰，解决了我之前自学时的很多疑问，让我颇为受用，并且诞生出了以您这篇文章为基础进行后续学习和研究的想法。</p><p>因此在开学前的这几个月时间中，我也着手努力复现您这篇文章的架构，并寄希望于复现后，进一步研究学姐在总结部分提出的几个设想，但是因为个人知识的欠缺，在复现结构时遇到了很多困难，难以解决，我身边的同学和老师因为鲜有涉及这方面的研究，也不能给我提供相应的帮助。</p><p>综合上述两点，我想咨询学姐是否方便将源码提供给我这个后辈学习和研究，我必遵守相应的科研道德，以及您的相关要求合理使用这份宝贵的资源。如有搅扰，望您海涵。</p><p>最后，祝学姐工作顺利，科研顺心，生活美满！</p><p>耕耘前辈您好！</p><p>冒昧打扰您，我是就读于北京邮电大学的一名研究生，此前一直对于图像美学和图像质量评价非常感兴趣。</p><p>有幸拜读过您19年的文章 “Theme Aware Aesthetic Distribution Prediction with Full Resolution Photos”，您的工作利用巧妙的方法解决了美学图像多尺寸的输入问题，且行文风格清晰，让我颇为受用。</p><p>在研究生的初期，自己所在项目组一直在开展医学影像相关的工作，但是我个人对这一方面不感兴趣，在与导师交流后，同意我以计算机摄影学和图像美学评价为毕设课题。在看到您的文章过后，文章中的算法我较为熟悉，因此诞生出了以您这篇文章为基础进行后续学习和研究的想法。</p><p>在今年疫情的半年内，我也着手努力复现您这篇文章的架构，但是因为个人知识的欠缺，在复现结构时遇到了很多困难，难以解决，我身边的同学和老师因为鲜有涉及这方面的研究，也不能给我提供相应的帮助。</p><p>综合上述两点，我想咨询前辈是否方便将源码提供给我这个后辈学习和研究，我必遵守相应的科研道德，以及您的相关要求合理引用您的工作，倍加珍惜这份宝贵的资源。如有搅扰，望您海涵。</p><p>最后，祝前辈工作顺利，科研顺心，生活美满！</p></blockquote><h2 id="导出参考文献"><a href="#导出参考文献" class="headerlink" title="导出参考文献"></a>导出参考文献</h2><p>参考该博客：<a href="https://blog.csdn.net/xlcaoyi/article/details/90511973">https://blog.csdn.net/xlcaoyi/article/details/90511973</a></p>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mendeley </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>颜色恒常性之&lt;&lt;A Multi-Hypothesis Approach to Color Constancy&gt;&gt;</title>
      <link href="yan-se-heng-chang-xing-zhi-mhcc/"/>
      <url>yan-se-heng-chang-xing-zhi-mhcc/</url>
      
        <content type="html"><![CDATA[<h2 id="论文思路"><a href="#论文思路" class="headerlink" title="论文思路"></a>论文思路</h2><p>论文：&lt;&lt;A Multi-Hypothesis Approach to Color Constancy&gt;&gt;</p><h3 id="Title-amp-Abstract-amp-Conclusion"><a href="#Title-amp-Abstract-amp-Conclusion" class="headerlink" title="Title&amp;Abstract&amp;Conclusion"></a>Title&amp;Abstract&amp;Conclusion</h3><blockquote><p>Multi-Hypothesis？</p></blockquote><p><strong>多假设都有什么假设？</strong></p><ol><li>Under the prevalent assumption that the scene is illuminated by a single or dominant light source, the observed pixels of an image are typically modelled using the physical model of Lambertian image formation captured under a trichromatic photosensor:</li><li>we assume that the color of the light and the surface reflectance are independent.</li><li>the function modelling the prior also depends on factors such as the environment (indoor / outdoor), the time of day, ISO etc. However, the size of currently available datasets prevent us from modelling more complex proxies.</li></ol><blockquote><p><strong>Our likelihood estimator</strong> learns to answer <strong>a camera-agnostic question</strong> and thus enables <strong>effective multi-camera training</strong> by disentangling illuminant estimation from the supervised learning task.</p><p>learning from image samples that were <strong>captured by multiple cameras</strong></p></blockquote><p><strong>相机无关和多相机图片训练到底是如何实现的？</strong></p><p>只是这个似然估计器与相机无关，多个相机获得多个数据集，对每个数据集利用KMeans找出候选光源，然后都喂入网络，实现了多相机图片训练</p><h3 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h3><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201019104708991.png" alt="Figure1"></p><blockquote><p><strong><u>（d）use an illuminant candidate set per camera</u></strong>. <strong><u>[ r/g ,b/g ]</u></strong></p></blockquote><p><strong>每个摄像机获得一个候选集吗？最后是如何训练的？对于每个摄像机的候选集，是如何选取划分的？</strong></p><p>每个摄像机有自己的一个照片集，对这里的图片进行分类，每个摄像机获得一个候选集。训练应该就是将这些候选集都喂入。</p><p><strong>[r/g,b/g]这个图如何读?</strong></p><p>图上一个点应该是代表一个光源，<strong>为了将三维降成二维？</strong>，显示了光源的分布</p><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201018164327078.png" alt="pipeline"></p><p><strong>如果现在有一张待还原的照片，如何还原，都要生成n个候选光源吗？怎么生成？</strong></p><p>n个候选光源已经生成好了，现在训练网络是需要不同光源的权重配比不同！所以到时候对于待还原照片，还是相同的过程，用每个候选光源修正图片，然后放入网络，得到权重。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>$$<br>\rho_{k}(X)=\int_{\Omega}E(\lambda)S(\lambda,X)C_{k}(\lambda)d\lambda\quad\quad\quad k\in{R,G,B}<br>$$</p><p><strong>为什么要积分？</strong></p><p>积分是因为比如绿色，打个比方是755~760这个频段的波长共同作用生成的，所以需要进行积分。<br>$$<br>\rho_{k}^E=\int_{\Omega}E(\lambda)C_{k}(\lambda)d\lambda\quad\quad\quad k\in{R,G,B}<br>$$</p><p>The goal of computational CC then becomes estimation of the <u><strong>global illumination color</strong></u>$\rho_k^E$？</p><p><strong>为什么变成了这个形式？</strong></p><p>对于物体成像的颜色，$S(\lambda,X)$表示物体本身的影响，$\rho_k^E$就表示光源的影响。后边我们说的光源就是指$\rho_k^E$整体。</p><blockquote><p>due to the <strong>ill-posed</strong> nature of the problem, <strong>multiple illuminant solutions are often possible with varying probability</strong>.</p></blockquote><p><strong>什么是ill-posed ？</strong></p><p>适定问题是指定解满足下面三个要求的问题：① 解是存在的；② 解是唯一的；③ 解连续依赖于定解条件，即解是稳定的。这三个要求中，只要有一个不满足，则称之为不适定问题</p><blockquote><p>avoid <u><strong>distribution shift</strong></u> and <u><strong>resulting domain gap problems</strong></u> [1, 41, 22], associated with camera specific training, and propose a well-founded strategy to leverage multiple data.</p></blockquote><p><strong>什么是distributin shift&amp;domain gap？</strong></p><p><strong>distribution shift</strong>: <a href="https://zh.d2l.ai/">https://zh.d2l.ai/</a> </p><p><strong>domain gap problem</strong>:<a href="https://zhuanlan.zhihu.com/p/195704051">https://zhuanlan.zhihu.com/p/195704051</a></p><blockquote><p>Principled combination of datasets is of high value for learning based color constancy given the typically small nature of individual color constancy datasets (on the order of only hundreds of images).</p></blockquote><p><strong>这句话在说啥？</strong></p><blockquote><p>We provide <strong><u>a training-free model adaptation strategy for new cameras</u></strong>.</p></blockquote><p><strong>加入一个新的摄像机，如何改进模型？</strong></p><p>新加入一个摄像机，只要这个摄像机的候选光源已知了，就可以直接拿这个网络训练了，所以不需要再重新训练或微调。</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="Bayesian-framework"><a href="#Bayesian-framework" class="headerlink" title="Bayesian framework"></a>Bayesian framework</h4><blockquote><p> They <strong>model the prior of the illuminant and the surface reflectance as a <u>truncated multivariate normal distribution</u> on the weights of a linear model</strong></p></blockquote><p><strong>什么是truncated multivariate normal distribution on the weights of a linear model?</strong></p><p>截断正态分布：指限制变量x取值范围(scope)的一种分布。例如，限制x取值在0到50之间，即{0&lt;x&lt;50}。</p><blockquote><p>Bayesian works [44, 23], <strong>discretise the illuminant space</strong> and <strong>model the surface reflectance priors</strong> by <u><strong>learning real world histogram frequencies</strong></u>;</p></blockquote><p>通过学习真实世界的直方图频率，来离散化光源空间和对表面反射率进行先验建模。<strong>可以查看它如何学习真实世界的直方图频率应用到肤色定级。</strong></p><blockquote><p>in [44] the prior is modelled as a <strong><u>uniform distribution over a subset of illuminants</u></strong> while [23] uses the <strong><u>empirical distribution of the training illuminants</u></strong>.</p></blockquote><p><strong>对于光源概率44和23有两种想法：直接建模成均匀分布和利用训练光源的经验分布。</strong></p><p>经验分布函数：<a href="https://zh.wikipedia.org/zh-hans/%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0#:~:text=%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%EF%BC%88%E8%8B%B1%E8%AA%9E%EF%BC%9Aempirical,%E6%A0%B7%E6%9C%AC%E6%89%80%E5%8D%A0%E7%9A%84%E6%AF%94%E4%BE%8B%E3%80%82">https://zh.wikipedia.org/zh-hans/%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0#:~:text=%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%EF%BC%88%E8%8B%B1%E8%AA%9E%EF%BC%9Aempirical,%E6%A0%B7%E6%9C%AC%E6%89%80%E5%8D%A0%E7%9A%84%E6%AF%94%E4%BE%8B%E3%80%82</a></p><h4 id="Fully-supervised-methods"><a href="#Fully-supervised-methods" class="headerlink" title="Fully supervised methods"></a>Fully supervised methods</h4><blockquote><p>frame color constancy as a classification problem：CCC and FCCC using a color space that identifies image re-illumination with a histogram shift. </p></blockquote><p><strong>CCC和FCCC待看</strong></p><h4 id="Multi-device-training"><a href="#Multi-device-training" class="headerlink" title="Multi-device training"></a>Multi-device training</h4><blockquote><p> [37] affords <strong>fast adaptation to previously unseen cameras</strong>, and robustness to changes in capture device by leveraging annotated samples across different cameras and datasets in a <strong><u>meta-learning</u></strong> framework</p></blockquote><p><strong>meta-learning?</strong></p><blockquote><p>A recent approach [8], makes an assumption that sRGB images collected from the web are well white balanced, therefore, they apply <strong><u>a simple de-gamma correction</u></strong> to approximate an <strong><u>inverse tone mapping</u></strong> and then find achromatic pixels with a CNN to predict the illuminant. </p></blockquote><p><strong>de-gamma correction？inverse tone mapping？</strong></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><blockquote><p>Let y = (yr, yg, yb) be a pixel from an input image Y in <strong><u>linear RGB space</u></strong>. </p></blockquote><p><strong>线性RGB空间？</strong></p><p><a href="https://www.cnblogs.com/guanzz/p/7416821.html">https://www.cnblogs.com/guanzz/p/7416821.html</a></p><p>gamma校正将把线性颜色空间转变为非线性空间</p><blockquote><p>We model the global illumination, Eq. (2), with the <strong><u>standard linear model</u></strong> [51] such that each pixel y is the product of the surface reflectance r = (rr, rg, rb) and a global illuminant ? = (?r, ?g, ?b) shared by all pixels such that</p></blockquote><p><strong>标准线性模型？</strong></p><p>可能就是三个函数相乘得到一个线性模型？</p><blockquote><p>we propose to frame the CC problem with a <strong><u>probabilistic generative model</u></strong> with unknown surface re- flectances and illuminant</p></blockquote><p><strong>概率生成模型？</strong></p><p><strong>公式推导</strong></p><p>$$<br>P(l|Y)=\frac{P(Y|l)P(l)}{P(Y)}<br>$$</p><p>$$<br>P(Y|l)=\int_rP(Y|l,R=r)P(R=r)dr<br>$$</p><p>公式(4)利用了全概率公式<br>$$<br>\int_rP(Y|l,R=r)P(R=r)dr=P(R=diag(l)^{-1}Y)<br>$$<br>公式(5),由于$y_k=r_k\cdot l_k\quad\quad k\in R,G,B$ 所以当且仅当$R=diag(l)^{-1}$时，才能生成Y,所以此时$P(Y|l,R=diag(l)^{-1})=1$,$P(Y|l,R=else)=0$,所以只剩下一项$P(R=diag(l)^{-1}Y)$</p><blockquote><p>We highlight that learned affine transformation parameters are training <strong>camera-dependent and provide further discussion</strong> on camera agnostic considerations in Section</p></blockquote><p><strong>为什么这个参数是摄像机依赖的？</strong></p><p>因为$B_l$是光源的先验估计，由公式二，全局光源由光源功率和接收函数决定。所以是摄像机依赖的。</p><blockquote><p>In order to estimate the illuminant  l*, we optimise the quadratic cost (minimum MSE Bayesian estimator), minimised by the mean of the posterior distribution:<br>$$<br>l^*=\int_l l\cdot P(l|Y)dl<br>$$</p></blockquote><p><strong>为什么是这个公式？</strong></p><p>我们现在获得了n个光源$l_0、l_1\cdots l_n$和n个概率$p_0、p_1\cdots p_n$,我们如何确定最优光源$l^*$?该论文就是简单使MSE最小，当$l^*$是期望时MSE最小，如果你忘了为啥了可以列个二次函数求导！</p><blockquote><p>We require <strong>a differentiable method</strong> in order to train our model end-to-end, and therefore the use of <strong>a simple Maximum a Posteriori （MAP）inference strategy is not possible</strong>. Therefore to estimate the illuminant l*, we use the minimum mean square error Bayesian estimator, which is minimised by the posterior mean of l (c.f. Eq. (6))”</p></blockquote><p><strong>为什么MAP不行？</strong></p><p>因为反向传播我们是需要求导的，而如果用极大后验估计求$l^*$，似然是用网络得到的，是没有办法求导的；所以我们需要采取一个办法他不需要对网络那一块求导就能得到$l^*$，所以使用最简单的方法-使MSE最小，$l^*$就是各个候选光源的期望。<br>$$<br>l^*=\sum_{i=1}^n l_i\cdot softmax(log(P(l_i|Y)))\<br>=\frac{1}{\sum e^{log(P(l_i|Y))}}\sum_{i=1}^nl_i\cdot e^{log(P(l_i|Y))}\<br>=\frac{1}{\sum P(l_i|Y)}\sum_{i=1}^nl_i\cdot P(l_i|Y)<br>$$</p><blockquote><p>The resulting vector $l^*$ is l2-normalised.</p></blockquote><p><strong>l2-normalised？</strong></p><p><a href="https://blog.cweihang.io/ml/trick/l2_normalize">https://blog.cweihang.io/ml/trick/l2_normalize</a></p><p>？？？</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><strong>Gehler-Shi</strong> dataset存在非一致真实值的情况 2个摄像机，分别为Canon 1D和Canon 5D 室内室外组合 佳能RAW格式保存，并提供了tiff格式，还提供了颜色检查板的坐标 因为自带程序包含非线性处理，所以使用Dcraw转换为tiff格式，并且只对RGGB的两个G取了平均，没有进行去马赛克，12位</p><p>NUS 8个摄像机</p><p>Cube+  Canon550D 主要室外</p><p>NUS Shi均为3折 用之前工作提供的划分 Cube+没提供，所以用所有的图像训练，用比赛数据集测试，还跟人家的比赛结果比了比</p><p>NUS加了个多摄像机模式 自己弄了个划分</p><p><strong>Trimean？</strong></p><p>三均值<br>$$<br>TM=\frac{Q_1+2Q_2+Q_3}{4}<br>$$<br>Q1,Q3为数据的两个四分位点，Q2为中位数</p><h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3><p><strong>1x1 Conv?</strong></p><p>也叫Network in Network,添加了一个非线性运算，可用于压缩信道或增加信道</p><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201027160655612.png" alt="吴恩达课程"></p><blockquote><p>Towards reproducibility, and fair comparison, our suppplementary material provides the cross validation splits, used in the main paper, for multi-device training</p></blockquote><p><strong>Cross validation？</strong></p><p>交叉验证：<a href="https://zhuanlan.zhihu.com/p/24825503">https://zhuanlan.zhihu.com/p/24825503</a></p><h2 id="个人思路"><a href="#个人思路" class="headerlink" title="个人思路"></a>个人思路</h2><h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a><strong>创新点</strong></h3><ol><li><p>提供了最佳光源的多个可能性</p></li><li><p>采用分类的方法而不是回归</p></li><li><p>设计的网络是摄像机无关的，可以使用多设备数据集进行训练，对于新型设备的泛化性比较好</p></li></ol><h3 id="多假设"><a href="#多假设" class="headerlink" title="多假设"></a>多假设</h3><ol><li>Under the prevalent assumption that the scene is illuminated by a single or dominant light source, the observed pixels of an image are typically modelled using the physical model of Lambertian image formation captured under a trichromatic photosensor:</li><li>we assume that the color of the light and the surface reflectance are independent.</li><li>the function modelling the prior also depends on factors such as the environment (indoor / outdoor), the time of day, ISO etc. However, the size of currently available datasets prevent us from modelling more complex proxies.</li></ol><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029093444705.png" alt="朗伯特模型"></p><p>对此公式可以进行简化，原式$=S(X)\int_\Omega E(\lambda)C_k(\lambda)d\lambda\quad k\in {R,G,B}$</p><p><strong>公式简化的两种解释</strong></p><p>1：相机R、G、B光谱敏感函数是<strong>狄拉克δ函数</strong>，就是说，每个相机的光敏R、G、B三通道每个只能感应波长的一个值</p><p>2：RGB的能感知的光谱构成可见光的一个划分,$Sup(Rc)$支撑集表示Rc能感知的光谱。对于每个支撑集，<strong>假设反射率函数与波长无关</strong></p><p>我们对该公式用如下形式表示：$y_k=r_k\cdot l_k \quad k\in{R,G,B}$ </p><p>已知一个参数$y_k$,即<strong>我们已经知道的照片</strong>，求两个参数$r_k,l_k$,分别为<strong>物体对成像的影响</strong>和<strong>光照对成像的影响</strong>。</p><p>已知一个参数，求两个参数，约束过少。</p><p><strong>琅伯特模型经典假设：</strong></p><p>  1：<strong>固定相机拍摄的固定场景物体颜色的改变只能由改变光照实现</strong></p><p>  2：<strong>固有物体反射率图像可以通过过滤光照颜色来实现</strong></p><p>过滤光照颜色即除$l_k$,即$r_k=\frac{y_k}{l_k}$,只要获得$r_k$再乘以标准光照，就能获得白平衡图像。所以我们需要做的就是<strong>估计光照</strong>$l_k$</p><p>曾经困扰过我的是$l_k=E\cdot C_k$，<strong>可以变换乘法的位置吗？</strong>后来明白，<strong>我们实际进行颜色还原时，是对每一个像素点进行处理，那么每一个E和C都是一个标量！所以自然可以变换位置</strong></p><p>之前的<strong>回归方法</strong>，是利用网络直接学习$l_k$,这样提供一个点估计，但是<strong>颜色还原问题本身具有不适定性</strong>，可能有多个$l_k$符合条件,每个$l_k$的概率不同。</p><p>所以作者想的是对于图像数据集利用K_means对光源进行聚类，获得的聚类中心点就是候选的光源，也就是多个可能性，解决了上面回归方法的单个点估计的考量。具体做法见下图:</p><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029101605001.png" alt="网络架构"><br>$$<br>P(l|Y)=\frac{P(Y|l)P(l)}{P(Y)}<br>$$</p><p>$$<br>P(Y|l)=\int_rP(Y|l,R=r)P(R=r)dr=P(R=diag(l)^{-1}Y)<br>$$</p><p>第一步变换应用了<strong>全概率公式</strong></p><p><strong>第二步变换</strong>由于$y_k=r_k\cdot l_k\quad\quad k\in R,G,B$ 所以当且仅当$R=diag(l)^{-1}$时，才能生成Y,所以此时$P(Y|l,R=diag(l)^{-1})=1$,$P(Y|l,R=else)=0$,所以只剩下一项$P(R=diag(l)^{-1}Y)$</p><p>所以我们这个<strong>CNN网络</strong>为$f^W$,则$log(P(Y|l))=log(P(R=diag(l)^{-1}Y))=f^W(diag(l)^{-1}Y)$，即每个候选光源是场景光源的概率。</p><p>另外，在实际场景中，<strong>不同候选光源出现的概率</strong>是不同的，即$P(l)$不同,基于此我们添加了两个参数$G_l、B_l$，分别为增益系数和$log(P(l))$<br>$$<br>log(P(l|Y))=Gl\cdot log(P(Y|l))+B_l<br>$$<br>而引入这两个参数会带来问题！</p><p>本来我们的网络是<strong>摄像机无关</strong>的,因为没有$log(P(l))$ ,为什么说摄像机无关呢？</p><p>多个相机获得多个数据集，对每个数据集利用KMeans找出候选光源，然后都喂入网络，实现了多相机图片训练。假设我们现在引入了 一个新的摄像机，并获得一个该摄像机的数据集，我们要做的就是对该摄像机进行K-Means，然后测试时，对于一张图片 ，我们现在修正图片需要做的是之前的加上新的候选光源一起修正分别得到概率就行，不需要重新训练或微调。</p><p>而引入$log(P(l))$,之前我们的公式表明，<strong>全局光源由光源功率和接收函数决定</strong>，这个时候就必然引入了摄像机关联。</p><p>所以如果要多设备训练的话，就不引入这两个参数，这样虽<strong>然降低了灵活性，少了两个学习参数，但是现在可用的数据集变多了，大数据集弥补了</strong>。</p><p>我们现在获得了n个光源$l_0、l_1\cdots l_n$和n个概率$p_0、p_1\cdots p_n$,我们如何确定最优光源$l^*$?</p><p>首先<strong>简单的MAP是不行的</strong>，因为反向传播我们是需要求导的，而如果用极大后验估计求$l^*$，似然是用网络得到的，是没有办法求导的；</p><p>所以我<strong>们需要采取一个办法他不需要对网络那一块求导就能得到</strong>$l^*$，该论文就是利用简单的线性组合获得$l^*$，使MSE最小，当$l^*$是期望时MSE最小，如果你忘了为啥了可以列个二次函数求导！<br>$$<br>l^*=\sum_{i=1}^n l_i\cdot softmax(log(P(l_i|Y)))\<br>=\frac{1}{\sum e^{log(P(l_i|Y))}}\sum_{i=1}^nl_i\cdot e^{log(P(l_i|Y))}\<br>=\frac{1}{\sum P(l_i|Y)}\sum_{i=1}^nl_i\cdot P(l_i|Y)<br>$$<br>上式<strong>使用softmax是使概率归一。</strong></p><p><strong>CNN结构：</strong></p><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029112002665.png" alt="Architecture"></p><h3 id="失败案例"><a href="#失败案例" class="headerlink" title="失败案例"></a>失败案例</h3><p><strong>1 GT在光源分布外</strong></p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029112126449.png" alt="failure-1" style="zoom:50%;"><p>因为是线性内插，分布外的点得不到</p><p><strong>2 打破了单一光源假设</strong></p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029113004477.png" alt="failure-2" style="zoom:67%;"><p><strong>3 问题的不适定本性</strong></p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029113057076.png" alt="failure-3" style="zoom: 80%;"><p>学到了看起来非常可信的白平衡图片，认为石头是白的，其实石头是黄的</p>]]></content>
      
      
      <categories>
          
          <category> Color Constancy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CC/AWB </tag>
            
            <tag> Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo-git-github博客搭建</title>
      <link href="hexo-git-github-bo-ke-da-jian/"/>
      <url>hexo-git-github-bo-ke-da-jian/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考：洪卫的博客教程<a href="https://sunhwee.com/posts/6e8839eb.html">https://sunhwee.com/posts/6e8839eb.html</a>   <a href="https://zhuanlan.zhihu.com/p/80140564">https://zhuanlan.zhihu.com/p/80140564</a></p><p>参考：hexo-theme-matery主题<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md#%E9%85%8D%E7%BD%AE%E5%9F%BA%E6%9C%AC%E8%8F%9C%E5%8D%95%E5%AF%BC%E8%88%AA%E7%9A%84%E5%90%8D%E7%A7%B0%E8%B7%AF%E5%BE%84url%E5%92%8C%E5%9B%BE%E6%A0%87icon">https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md#%E9%85%8D%E7%BD%AE%E5%9F%BA%E6%9C%AC%E8%8F%9C%E5%8D%95%E5%AF%BC%E8%88%AA%E7%9A%84%E5%90%8D%E7%A7%B0%E8%B7%AF%E5%BE%84url%E5%92%8C%E5%9B%BE%E6%A0%87icon</a></p><p>参考：过客～励む的博客<a href="https://yafine-blog.cn/posts/4ab2.html">https://yafine-blog.cn/posts/4ab2.html</a></p><p>参考:大佬的artitalk的教程<a href="https://zhangxiaocai.cn/posts/7404e01a.html">https://zhangxiaocai.cn/posts/7404e01a.html</a></p></blockquote><h2 id="按流程搭建遇到的问题："><a href="#按流程搭建遇到的问题：" class="headerlink" title="按流程搭建遇到的问题："></a>按流程搭建遇到的问题：</h2><h3 id="搭建hexo博客时，到了最后一步，hexo-s后只出现代码，而不是首页？"><a href="#搭建hexo博客时，到了最后一步，hexo-s后只出现代码，而不是首页？" class="headerlink" title="搭建hexo博客时，到了最后一步，hexo s后只出现代码，而不是首页？"></a>搭建hexo博客时，到了最后一步，hexo s后只出现代码，而不是首页？</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012224004113.png" alt="错误代码"></p><p><strong>在npm install安装依赖时出现了错误</strong></p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012224051241.png" alt="错误为第10行"></p><p>仔细查看错误信息，我们不难发现是ejs出现了问题。我们可以先执行以下代码后再继续后续操作。</p><pre><code>npm install ejs@2.7.4 --ignore-scripts</code></pre><p><strong><em>注意：之后所有的Bash命令都在最后一个MyBlog文件夹下操作，也就是你之前安装hexo那个文件夹！</em></strong></p><h3 id="什么是github-io？"><a href="#什么是github-io？" class="headerlink" title="什么是github.io？"></a>什么是github.io？</h3><blockquote><p>官网的一句话来形容 Websites for you and your projects</p></blockquote><h3 id="购买个人域名之后打开失败？"><a href="#购买个人域名之后打开失败？" class="headerlink" title="购买个人域名之后打开失败？"></a>购买个人域名之后打开失败？</h3><blockquote><p>极有可能是你未设置域名解析！</p></blockquote><h3 id="写文章发布文章不生成文件夹及图片无法显示？"><a href="#写文章发布文章不生成文件夹及图片无法显示？" class="headerlink" title="写文章发布文章不生成文件夹及图片无法显示？"></a>写文章发布文章不生成文件夹及图片无法显示？</h3><h4 id="不生成文件夹？"><a href="#不生成文件夹？" class="headerlink" title="不生成文件夹？"></a>不生成文件夹？</h4><p>首先，新建博客一定要用hexo new post命令，不然很多信息识别不出来</p><p>然后将_config.yml文件中的post asset folder设置为true，之后会出现文件夹</p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012224940500.png" alt="设置为true"></p><h4 id="图片不显示？"><a href="#图片不显示？" class="headerlink" title="图片不显示？"></a>图片不显示？</h4><p>首先下载依赖</p><pre><code>npm install hexo-asset-image --save</code></pre><p>然后对于typora编辑，偏好设置为：</p><p>然后图片编写时，使用相对路径，例如：</p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012225527582.png" alt="例子"></p><p><strong><em>另外注意：千万不要错误使用转义符’'!!!!</em></strong></p><h3 id="数学公式块无法正常显示？"><a href="#数学公式块无法正常显示？" class="headerlink" title="数学公式块无法正常显示？"></a>数学公式块无法正常显示？</h3><blockquote><p>后面配置了主题就可以了！</p><p>但是注意：<strong>数学公式中如果出现了连续两个{，中间一定要加空格！</strong></p></blockquote><h3 id="菜单导航配置在哪？"><a href="#菜单导航配置在哪？" class="headerlink" title="菜单导航配置在哪？"></a>菜单导航配置在哪？</h3><blockquote><p>菜单导航配置在themes/hexo-theme-matery/__config.yml</p></blockquote><h2 id="什么是TOC"><a href="#什么是TOC" class="headerlink" title="什么是TOC?"></a>什么是TOC?</h2><h3 id="什么是RSS订阅？"><a href="#什么是RSS订阅？" class="headerlink" title="什么是RSS订阅？"></a>什么是RSS订阅？</h3><blockquote><p>​        RSS也称为RSS订阅或RSS提要，博客和新闻网站的一个常见做法是联合其内容。Web联合是指来自网站的内容可供其他站点或远程应用程序使用。Web联合的最常用方法是使用称为<strong>ReallySimpleSyndication</strong>的协议。RSS是一种协议，允许网站将其内容或其部分内容提供给其他网站或应用程序。</p></blockquote><h3 id="DaoVoice"><a href="#DaoVoice" class="headerlink" title="DaoVoice?"></a>DaoVoice?</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013144940429.png" alt="设置"></p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013144951426.png" alt="设置"></p><h3 id="新建文章模板修改失败？"><a href="#新建文章模板修改失败？" class="headerlink" title="新建文章模板修改失败？"></a>新建文章模板修改失败？</h3><blockquote><p>莫名其妙post.md上下都变成了两个—，奇怪</p></blockquote><h3 id="修改页脚"><a href="#修改页脚" class="headerlink" title="修改页脚?"></a>修改页脚?</h3><h3 id="修改社交链接？"><a href="#修改社交链接？" class="headerlink" title="修改社交链接？"></a>修改社交链接？</h3><h3 id="不蒜子？不蒜子访问量和人数无法区分问题？"><a href="#不蒜子？不蒜子访问量和人数无法区分问题？" class="headerlink" title="不蒜子？不蒜子访问量和人数无法区分问题？"></a>不蒜子？不蒜子访问量和人数无法区分问题？</h3><blockquote><p>是一个极简网页计数器</p></blockquote><h3 id="添加动漫人物"><a href="#添加动漫人物" class="headerlink" title="添加动漫人物?"></a>添加动漫人物?</h3><blockquote><p>由于一直要安包，不敢继续弄了</p></blockquote><h3 id="gitalk-error-not-found？"><a href="#gitalk-error-not-found？" class="headerlink" title="gitalk error not found？"></a>gitalk error not found？</h3><blockquote><p><del>既有可能是yml的设置错误！</del></p><p><del><strong>owner和admin都填写github用户名</strong>，<strong>repo填我们的博客github仓库名</strong></del></p><p>问题终于解决！</p></blockquote><h3 id="yml、yaml格式不正确？"><a href="#yml、yaml格式不正确？" class="headerlink" title="yml、yaml格式不正确？"></a>yml、yaml格式不正确？</h3><blockquote><p>使用这个在线校验器校验：<a href="http://www.bejson.com/validators/yaml_editor/">http://www.bejson.com/validators/yaml_editor/</a></p></blockquote><p>教程：<a href="https://www.runoob.com/w3cnote/yaml-intro.html">https://www.runoob.com/w3cnote/yaml-intro.html</a></p><p>缩进不允许使用tab，只允许空格</p><h3 id="gitalk未找到相关issues？"><a href="#gitalk未找到相关issues？" class="headerlink" title="gitalk未找到相关issues？"></a>gitalk未找到相关issues？</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013183732541.png" alt="URL"></p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013183843079.png" alt="回调函数"></p><blockquote><p>一定要<strong>使用https而不是http</strong></p><p>还是不行？？？</p><p>失败！</p></blockquote><h3 id="网站根目录在哪里？"><a href="#网站根目录在哪里？" class="headerlink" title="网站根目录在哪里？"></a>网站根目录在哪里？</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013200031772.png" alt="网站根目录"></p><h3 id="如何删除文章："><a href="#如何删除文章：" class="headerlink" title="如何删除文章："></a>如何删除文章：</h3><p>先hexo clean，然后在直接删除，如果不hexo clean的话，还是会再生成。</p><h2 id="js、ejs学习："><a href="#js、ejs学习：" class="headerlink" title="js、ejs学习："></a>js、ejs学习：</h2><blockquote><p>js:<a href="https://www.runoob.com/js/js-tutorial.html">https://www.runoob.com/js/js-tutorial.html</a></p></blockquote><p>JavaScript 是<strong>脚本语言，浏览器会在读取代码时，逐行地执行脚本代码</strong>。而对于传统编程来说，会在执行前对所有代码进行编译。</p><p>对象最好使用**.**来调用，防止方法调用失败！</p><h2 id="博客编写规范"><a href="#博客编写规范" class="headerlink" title="博客编写规范"></a>博客编写规范</h2><p><strong>不做内容的搬运工！</strong>而是</p><ol><li><p><strong>论文：记录遇到的问题，只针对问题进行解答</strong></p></li><li><p><strong>汇报：对于组会汇报内容进行详细编写</strong></p></li><li><p><strong>相应知识点：除了贴教程，最好加一两句自己的心得体会！</strong></p></li></ol><p>复制的话先清除样式，不然容易出问题</p><h2 id="图片loading不显示？"><a href="#图片loading不显示？" class="headerlink" title="图片loading不显示？"></a>图片loading不显示？</h2><p>gulp加速关掉图片压缩！！！！最好不使用gulp压缩，最好使用hexo-neat</p><h2 id="网页代码显示莫名其妙格式错误？"><a href="#网页代码显示莫名其妙格式错误？" class="headerlink" title="网页代码显示莫名其妙格式错误？"></a>网页代码显示莫名其妙格式错误？</h2><p>语言指定成c++就会出错，指定成c没事，所以还是指定成c，如果还是不行的话可以不指定语言，那样就没有高亮</p>]]></content>
      
      
      <categories>
          
          <category> 博客搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> git </tag>
            
            <tag> ejs </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
