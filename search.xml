<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ONNX</title>
      <link href="onnx/"/>
      <url>onnx/</url>
      
        <content type="html"><![CDATA[<img src="/onnx/image-20210326105103774.png" alt="图标很好看的样子" style="zoom:50%;"><p><a href="https://zhuanlan.zhihu.com/p/41255090">onnx教程</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semantic-Segmentation-2</title>
      <link href="semantic-segmentation-2/"/>
      <url>semantic-segmentation-2/</url>
      
        <content type="html"><![CDATA[<h2 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h2><h2 id="Non-Local"><a href="#Non-Local" class="headerlink" title="Non-Local"></a>Non-Local</h2><h2 id="DANet"><a href="#DANet" class="headerlink" title="DANet"></a>DANet</h2><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>19CVPR <a href="https://blog.csdn.net/wumenglu1018/article/details/95949039">DANet阅读笔记</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>基于self-attention捕捉丰富的上下文相关性。与以往通过多尺度特征融合获取上下文的研究不同，提出了一种双注意网络(DANet)来自适应地整合局部特征及其全局依赖关系。</p><ul><li>位置注意力模块通过所有位置处的特征的加权和来选择性地聚合每个位置的特征。无论距离如何，类似的特征都将彼此相关。</li><li>通道注意力模块通过整合所有通道映射之间的相关特征来选择性地强调存在相互依赖的通道映射</li><li>将两个注意模块的输出相加以进一步改进特征表示，这有助于更精确的分割结果</li></ul><p>在Cityscapes、PASCAL Context、COCO Stuff取得sota</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>为了增强特征表达，</p><ol><li>利用多尺度上下文融合，如结合不同扩展卷积和池操作生成的特征映射来聚合多尺度上下文</li><li>使用分解结构增大卷积核尺寸或在网络顶部引入有效的编码层，来捕获更丰富的全局信息。</li><li>编码器-解码器结构来融合中层和高层语义特征。</li><li>使用RNN捕捉长程依赖关系，从而提高分割精度。</li></ol><p>1、2、3尽管上下文融合有助于捕获不同尺度的对象，但它不能充分利用全局视图中对象或事物之间的关系，这也是场景分割的关键。4利用递归神经网络隐含地捕捉全局关系，其有效性严重依赖于长期记忆的学习结果。</p><p><img src="/semantic-segmentation-2/image-20210323162023746.png" alt="Figure1"></p><p>在处理复杂多样的场景时，DANet比以往的方法更加有效和灵活。</p><p>以图1中的街景为例。首先,第一排的一些人和红绿灯由于光线和视野的原因是不显眼或不完整的物体。</p><p>如果采用简单的上下文嵌入方法，则显著对象(如汽车、建筑)的上下文会对不显著对象的标记造成伤害。相比之下，我们的注意模型选择性地聚集不显著对象的相似特征，以突出其特征表征，避免显著对象的影响。</p><p>其次,车和人的尺度是不同的，识别这些不同的物体需要不同尺度上的上下文信息。也就是说，不同尺度的特征应该被平等对待，以代表相同的语义。我们的注意机制模型从全局的角度自适应地集成任何尺度上地相似特征,在一定程度上可以解决上述问题。</p><p>第三，我们明确地将空间和通道关系考虑在内，以便场景理解可以从长期依赖中受益。</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h3 id="DANet-1"><a href="#DANet-1" class="headerlink" title="DANet"></a>DANet</h3><h4 id="3-1Overview"><a href="#3-1Overview" class="headerlink" title="3.1Overview"></a>3.1Overview</h4><p>因为卷积操作产生的是局部感受野，导致相同标签的像素对应特征可能不同，这种差异会进而导致类内的不一致性，影响识别的准确率。所以本文提出：在特征之间使用注意力机制建立关联以获取全局上下文信息。</p><img src="/semantic-segmentation-2/image-20210323184906645.png" alt="DANet" style="zoom:80%;"><h4 id="3-2PAM"><a href="#3-2PAM" class="headerlink" title="3.2PAM"></a>3.2PAM</h4><p>观察:传统FCNs生成的局部特征会导致objects和stuff的错误分类。</p><p>解决:引入位置注意模块在局部特征上建立丰富的上下文关系，将更广泛的上下文信息编码为局部特征，进而增强他们的表示能力。</p><img src="/semantic-segmentation-2/image-20210323192129272.png" alt="PAM" style="zoom:80%;"><p>公式一中$s_{ji}$表示第j个点对第i个点的影响，两个位置的特征表示越相似，它们之间的相关性就越大。向量相乘就是余弦相似度。</p><p>由公式二，每个位置得到的特征E是所有位置特征与原始特征的加权和。因此，它具有全局语境观，并根据空间注意特征选择性地聚合上下文。相似的语义特征相互受益，提高了类内的紧凑性和语义一致性。相似的语义特征相互受益，提高了类内的紧凑性和语义一致性。</p><p>PAM：每个维度所有点需要注意的</p><h4 id="3-3CAM"><a href="#3-3CAM" class="headerlink" title="3.3CAM"></a>3.3CAM</h4><p>观察:每个high level特征的通道图都可以看作是一个特定于类的响应，通过挖掘通道图之间的相互依赖关系，可以突出相互依赖的特征图，提高特定语义的特征表示。</p><p>解决:建立一个通道注意力模块来显式地建模通道之间的依赖关系。</p><img src="/semantic-segmentation-2/image-20210323195612525.png" alt="CAM" style="zoom:80%;"><p>实现了对特征图之间的长程语义依赖关系建模，有助于提高特征的辨别性。</p><p>公式三中，$x_{ji}$表示第j个维度所有点对第i个维度所有点的影响。</p><p>注意，在计算两个通道的关系之前，我们不使用卷积层来嵌入特征，因为它可以保持不同通道映射之间的关系。此外，与最近的作品[27]通过全局池化或编码层探索信道关系不同，我们利用所有对应位置的空间信息来建模信道相关性。</p><p><strong>CAM是,一个点有三个维度，每个维度要注意什么</strong></p><h4 id="3-4Attention-Module-Embedding-with-Networks"><a href="#3-4Attention-Module-Embedding-with-Networks" class="headerlink" title="3.4Attention Module Embedding with Networks"></a>3.4Attention Module Embedding with Networks</h4><p>为了充分利用长程上下文信息，所以将这2个注意力模块的特征进行了聚合。即通过卷积层对两个注意力模块的输出进行转换，并执行一个element-wise的求和来实现特征融合。最后接一个卷积得到最后的预测特征图。</p><p>该注意力模块很简单，可以直接插入到现在的FCN中。而且它们不会增加太多参数，还能有效地增强特征表示。我们不采用级联操作，因为它需要更多的GPU内存。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="/semantic-segmentation-2/image-20210323200616217.png" alt="PAM"></p><p><img src="/semantic-segmentation-2/image-20210323200636411.png" alt="CAM"></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文提出了一种双注意网络(DANet)用于场景分割，该网络利用自注意机制自适应地整合局部语义特征。具体来说，引入了位置注意模块和通道注意模块来分别捕捉空间维度和通道维度的全局相关性。消融实验表明，双注意模块能有效地捕获长时间上下文信息，并给出更精确的分割结果。此外，降低计算复杂度和增强模型的鲁棒性也很重要，这将在以后的工作中进行研究。</p>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>变帅秘籍</title>
      <link href="bian-shuai-mi-ji/"/>
      <url>bian-shuai-mi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="发型"><a href="#发型" class="headerlink" title="发型"></a>发型</h2><p>两边太鼓不好看 头发太厚去薄 头发往后留  </p>]]></content>
      
      
      <categories>
          
          <category> 变帅 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 变帅 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN_tricks</title>
      <link href="cnn-tricks/"/>
      <url>cnn-tricks/</url>
      
        <content type="html"><![CDATA[<h2 id="Bag-of-Tricks"><a href="#Bag-of-Tricks" class="headerlink" title="Bag of Tricks"></a>Bag of Tricks</h2><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>19CVPR</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>图像分类研究的最新进展主要归功于<strong>训练过程的改进，如数据增强和优化方法的改变。</strong>然而这些改进大部分作为实验细节提一嘴，要么只在源代码中可见。本文通过消融研究来检验这些改进的影响。通过这些trick集合，能够<strong>显著改进各种CNN模型</strong>，同时<strong>对对象检测和语义分割带来更好的迁移学习性能。</strong></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>改进包括模型架构、损失函数、数据预处理、优化方法，这些改进提高模型精度，但几乎不改变计算复杂度。</p><h3 id="Training-Procedures-基线"><a href="#Training-Procedures-基线" class="headerlink" title="Training Procedures/基线"></a>Training Procedures/基线</h3><h4 id="2-1Baseline-Training-Procedure"><a href="#2-1Baseline-Training-Procedure" class="headerlink" title="2.1Baseline Training Procedure"></a>2.1Baseline Training Procedure</h4><p>采用Resnet作为基线</p><p>训练过程执行而验证过程不执行随机增强。</p><p>卷积层和全连接层的<strong>权值均采用Xavier算法进行初始化</strong>。</p><h3 id="Efficient-Training-训练速度"><a href="#Efficient-Training-训练速度" class="headerlink" title="Efficient Training/训练速度"></a>Efficient Training/训练速度</h3><p>在本节中，我们将回顾在不牺牲模型精度的情况下实现低精度和大批量训练的各种技术。有些技术甚至可以提高准确性和训练速度。</p><h4 id="3-1Large-batch-training"><a href="#3-1Large-batch-training" class="headerlink" title="3.1Large-batch training"></a>3.1Large-batch training</h4><ul><li>batch数太小，而类别又比较多的时候，真的可能会导致loss函数震荡而不收敛，尤其是在你的网络比较复杂的时候。</li><li>随着batchsize增大，处理相同的数据量的速度越快，跑完一个epoch所需迭代次数变少</li><li>一定范围内，batchsize越大，其确定的下降方向就越准，不容易被noise影响，引起训练振荡就越小</li><li>而batchsize越大的话，显存又可能受不住，模型收敛速度可能变慢，因为每次迭代用时更长了，学习率不变的话，需要更多次迭代</li><li>具体的batch size的选取和训练集的样本数目相关。</li><li>所以实际工程最常用的就是mini-batch，GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16,32,64…往往比整10、整100更优</li></ul><p><strong>Linear scaling learning rate</strong></p><p>大批量的批处理减少了梯度中的噪声，所以我们可以增加学习速率，使梯度方向相反的方向取得更大的进步。<strong>增大batch size，单batch数据中噪声的影响会更小，此时就可以使用大的学习率步长</strong>。比如ResNet-50网络，官方采用的batch size为256，初始学习率为0.1，那么当使用更大的batch size，符号表示为b时，那么可以设置初始学习率为$0.1\cdot \frac{b}{256}$。</p><p><strong>Learning rate warmup</strong></p><p>在训练开始时，所有参数通常都是随机值，因此远离最终解。使用过大的学习率可能会导致数值不稳定。<strong>学习率热启动策略具体操作为，将实验中用的学习率参数从0逐渐增大到初始学习率大小，然后再采用常规的学习率衰减方案，逐渐增大学习率的过程被称作warmup阶段</strong>。</p><p>假设使用前m个batch来预热，初始学习率为l，那么对于第i个批次,$1\le i\le m$,学习率为$l\cdot \frac{i}{m}$</p><p><strong>这种策略的好处：防止训练过程中出现的instablility</strong>。</p><p><strong>Zero</strong> $\gamma$</p><p>Resnet网络结构中包含了多个residual blocks，记某个block的输入为x，那么经过残差求和后的结果为$x+block(x)$,而block的最后一层为batch norm层，batch norm层先将该层的输入数据标准化，记作$\hat{x}$ ,那么batch norm层的输出为$ \gamma \hat{x}+\beta$ ，其中，$\gamma$和$\beta$为可训练参数，因此也需要在模型训练之前做初始化，<strong>通常的做法是将它们分别初始化为</strong>1和0。Zero $\gamma$ 的做法是将它们均初始化为0。</p><p><strong>这种策略的好处：将所有residual blocks中的最后一个batch norm层的$\gamma$和$\beta$参数设置为0，也即residual block的输出和输入相等，可以使模型在初始阶段更容易训练。</strong><br><strong>No bias decay</strong></p><p>权值衰减通常适用于所有可学习参数，包括权值和偏差。这相当于对所有参数应用L2正则化，使它们的值趋近于0。但是为了避免过拟合，建议对权重应用正则化。bias,和BN层中的$\gamma$和$\beta$都不正则化。</p><h4 id="3-2-Low-precision-training"><a href="#3-2-Low-precision-training" class="headerlink" title="3.2 Low-precision training"></a>3.2 Low-precision training</h4><p>通常，计算机使用32-bit浮点精度（FP32）做训练，也就是说，所有的数字都以FP32格式存储，算术运算的输入和输出也是FP32数字。英伟达的部分显卡针对FP16做了定制化优化，能够达到更快的计算速度，比如最新的显卡V100。另外，关于低精度训练相关的算法理论，可以参见笔者之前写的一篇文章。</p><p>建议在FP16中存储所有参数和激活，并使用FP16来计算梯度。同时，所有参数都有一个用于参数更新的FP32的副本。此外，用一个标量与损失相乘以更好地将梯度范围对齐到FP16也是一种实用的解决方案。</p><h4 id="3-3Experiment-Results"><a href="#3-3Experiment-Results" class="headerlink" title="3.3Experiment Results"></a>3.3Experiment Results</h4><p>技巧确实有用</p><h3 id="Model-Tweaks-网络结构"><a href="#Model-Tweaks-网络结构" class="headerlink" title="Model Tweaks/网络结构"></a>Model Tweaks/网络结构</h3><p>A model tweak is a minor adjustment to the network architecture。这样的调整通常几乎不会改变计算复杂度，但可能对模型精度有不可忽视的影响。</p><p>剩余内容见backbone中的Resnet部分。</p><h3 id="Training-Refinement-过程优化"><a href="#Training-Refinement-过程优化" class="headerlink" title="Training Refinement/过程优化"></a>Training Refinement/过程优化</h3><h4 id="5-1-Cosine-Learning-Rate-Decay"><a href="#5-1-Cosine-Learning-Rate-Decay" class="headerlink" title="5.1 Cosine Learning Rate Decay"></a>5.1 Cosine Learning Rate Decay</h4><p>广泛使用的策略是指数衰减的学习速率。何的ResNet每30个周期以0.1降低速率，我们称之为“阶跃衰减step decay”。</p><p>$η_t=\frac{1}{2}(1+cos(\frac{t\pi}{T}))η$</p><p>T为batch总数(不包括预热),t为当前批次，η为初始速率。与步长衰减相比，余弦衰减从一开始就开始衰减学习，但一直很大，直到步长衰减使lr降低了10倍，从而潜在地提高了训练进度。</p><h4 id="5-2-Label-Smoothing"><a href="#5-2-Label-Smoothing" class="headerlink" title="5.2 Label Smoothing"></a>5.2 Label Smoothing</h4><p>$z_i$表示对class i的预测分数，则$q=softmax(z)$,即:<br>$$<br>q_i=\frac{exp(z_i)}{\sum_{j=1}^Kexp(z_j)}<br>$$<br>假设真实label为y,则可以构造真实概率分布，$p_i=1\quad if\quad i=y$,否则为0，交叉熵为:<br>$$<br>l(p,q)=-\sum_{i=1}^Kp_ilogq_i\<br>=-logp_y\<br>=-z_y+log(\sum_{i=1}^Kexp(z_i))<br>$$<br>最优解是$z_y = inf$同时保持其他值足够小。换句话说，它鼓励输出分数显著不同，这可能导致过拟合。</p><p>标签平滑改变了真实概率的构造:<br>$$<br>q_i=c(u)=\begin{cases}1-\varepsilon \quad  if \quad i=y,\\varepsilon/(K-1) \quad otherwise \end{cases}<br>$$<br>$\varepsilon$是一个小常数</p><h4 id="5-3-Knowledge-Distillation"><a href="#5-3-Knowledge-Distillation" class="headerlink" title="5.3 Knowledge Distillation"></a>5.3 Knowledge Distillation</h4><p>Knowledge Distillation，可以翻译成知识蒸馏，最早由Hinton在2015年提出，它包含了一个”教师网络”和一个”学生网络“。其中，“教师网络”通常采用预训练好的模型，而“学生网路”中包含了待学习的参数。记标签的真实概率分布为p ，“学生网络”的预测概率分布为z ，“教师网络”的预测概率分布为r，优化的目标是，让“学生网络”在学习标签的真实概率分布的同时，还要学习“教师网络”的先验知识，公式表示如下:<br>$$<br>l(p,softmax(z))+T^2l(softmax(\frac{r}{T}),softmax(\frac{z}{T}))<br>$$<br>其中，前者表示学习标签的真实概率分布，后者表示学习“教师网络”的先验知识，T为超参数。</p><p>为什么学习的目标是这样子呢？</p><p><strong>标签的真实概率分布提供了hard label，准确率高但是信息熵很低，而“教师网络”提供了soft label，准确率相对较低但是信息熵高。这里的信息熵怎么理解呢，比如一副马的图片，可以想象的到它看上去也有点像牛，而hard label给的标签是[1, 0]，soft label给的标签是[0.7, 0.3]，显然，soft label提供了类别之间的关联，提供的信息量更大，有助于模型在学习时增强类间区分度，hard label和soft label结合等价于在学习真实标签的同时，补充类间信息。</strong></p><h4 id="5-4-Mixup-Training"><a href="#5-4-Mixup-Training" class="headerlink" title="5.4 Mixup Training"></a>5.4 Mixup Training</h4><p>mixup的数据增强方法。每次随机抽取两个样本$(x_i,y_i)$和$x_j,y_j$,然后由这两个样本生成新样本:</p><p>$\hat{x}=\lambda x_i+(1-\lambda)x_j$ $\hat{y}=\lambda y_i+(1-\lambda)y_j$    $\lambda \in [0,1] from\quad Beta(\alpha,\alpha) $</p><p>在mixup training,只使用$(\hat{x},\hat{y})$</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>backbone</title>
      <link href="backbone/"/>
      <url>backbone/</url>
      
        <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>LeNet：5层轻量级网络，一般用来验证小型数据；经典的手写数字识别模型</p><p>AlexNet/VGGNet：把网络层数加深；</p><p>GoogLeNet/Inception：结合1x1卷积并采用带有不同kernel和池化的多分支策略进行特征提取；</p><p>ResNet：Residual block，使训练更深层的网络变得可能；</p><p>RexNeXt：引入组卷积，在精度基本不降的情况下速度超过ResNet；</p><p>DenseNet：主要是特征复用的思想，参数量虽小计算量不敢恭维；</p><p>Res2Net：基于ResNet引入多尺度；</p><p>SENet：基于通道矫正，强化重要特征，抑制非重要特征，重点是轻便可以随意嵌入；</p><p>SKNet：引入特征图注意力，使卷积核的感受野能够自适应</p><p>DCNet：引入可变性卷积，提高了泛化能力；</p><p>SqueezeNet、ShuffleNet、MobileNet：轻量级网络；</p><p>CSPNet：利用跨阶段特征融合策略和截断梯度流来增强不同层次特征的可变性解决冗余梯度信息，提高推理速度；</p><p>EfficientNet：E0-E7的进化之路号称无人能敌，配合谷歌刚出的Lite，实现精度、延迟两不误的移动端新SOTA；</p><p>RegNet：FBAI力作，号称超越EfficientNet，GPU上提速5倍的神作；</p><p>ResNeSt：刚出来的Backbone，乍眼一看是一个ResNeXt和SKNet的结合体，论文写着刷爆各大榜单；具体效果还未使用不从得知，直观感觉是个好的神器，留待时间去考证</p><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><img src="/backbone/image-20210324170556751.png" alt="LeNet" style="zoom:80%;"><p>网络结构如上，基本不会用到，了解即可</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><ol><li>成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。</li><li>训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。</li><li>在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。</li><li>出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。(后来好像被证明没用)</li><li>用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算。AlexNet使用了两块GTX 580 GPU进行训练，单个GTX 580只有3GB显存，这限制了可训练的网络的最大规模。因此作者将AlexNet分布在两个GPU上，在每个GPU的显存中储存一半的神经元的参数。因为GPU之间通信方便，可以互相访问显存，而不需要通过主机内存，所以同时使用多块GPU也是非常高效的。同时，AlexNet的设计让GPU之间的通信只在网络的某些层进行，控制了通信的性能损耗。</li><li>数据增强，随机地从256<em>256的原始图像中截取224</em>224大小的区域（以及水平翻转的镜像），相当于增加了2*(256-224)^2=2048倍的数据量。如果没有数据增强，仅靠原始的数据量，参数众多的CNN会陷入过拟合中，使用了数据增强后可以大大减轻过拟合，提升泛化能力。进行预测时，则是取图片的四个角加中间共5个位置，并进行左右翻转，一共获得10张图片，对他们进行预测并对10次结果求均值。同时，AlexNet论文中提到了会对图像的RGB数据进行PCA处理，并对主成分做一个标准差为0.1的高斯扰动，增加一些噪声，这个Trick可以让错误率再下降1%。</li><li><img src="/backbone/image-20210324170958652.png" alt="AlexNet" style="zoom:67%;"></li></ol><h2 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h2><p><img src="/backbone/image-20210226194943546.png" alt="VGG6种结构配置"></p><ul><li>16个权重层，也是VGG16名称的来源</li><li>conv3-64表示，kernel size为3x3，64个filter，stride=1，padding=same，池化层均为2</li></ul><p><img src="/backbone/image-20210226195521085.png" alt="VGG16模块图"></p><p><img src="/backbone/image-20210226195459799.png" alt="VGG16流程图"></p><p>参数很多很大！</p><p>size一直除2，channel一直乘2</p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>随着网络层数增加，会出现退化degradation现象，loss饱和后反而增加了   </p><blockquote><p>当网络退化时，浅层网络能够达到比深层网络更好的训练效果，这时如果我们把低层的特征传到高层，那么效果应该至少不比浅层的网络效果差，或者说如果一个VGG-100网络在第98层使用的是和VGG-16第14层一模一样的特征，那么VGG-100的效果应该会和VGG-16的效果相同。所以，我们可以在VGG-100的98层和14层之间添加一条直接映射（Identity Mapping）来达到此效果。</p><p>从信息论的角度讲，由于DPI（数据处理不等式）的存在，在前向传输的过程中，随着层数的加深，Feature Map包含的图像信息会逐层减少，而ResNet的直接映射的加入，保证了 <img src="https://www.zhihu.com/equation?tex=l+1" alt="[公式]"> 层的网络一定比 <img src="https://www.zhihu.com/equation?tex=l" alt="[公式]"> 层包含更多的图像信息。</p></blockquote><p>$x_{l+1}=x_l+F(x_l,W_l)$</p><img src="/backbone/image-20210227125454086.png" alt="残差块" style="zoom:67%;"><p><img src="/backbone/image-20210227151941237.png" alt="结构表"></p><p><img src="/backbone/image-20210227152012707.png" alt="building block"></p><p>左边的叫Basic block，右边的叫Bottleneck block</p><img src="/backbone/15074510-faee46ef496b76bf.webp" alt="网络结构图"><p>这张图tql太厉害了！！！</p><img src="/backbone/image-20210319221135225.png" alt="resnet50结构" style="zoom:67%;"><p>ResNet由一个输入端、四个后续阶段和输出层组成。</p><p><strong>输入端:</strong></p><p>减少宽和高为原来的1/4，并增加通道大小为64.</p><p><strong>四个后续阶段Stage1、Stage2、Stage3、Stage4:</strong></p><p>从stage2开始，先是一个下采样块，然后剩下的是残差块。每个bottleneck最后一个卷积的输出通道都是前两个的四倍。</p><p>人们可以在每个阶段改变剩余残差块的数量，以获得不同的ResNet模型，如ResNet-50和ResNet-152，其中的数字表示网络中卷积层的数量。</p><img src="/backbone/image-20210319222556310.png" alt="tweak" style="zoom:80%;"><h4 id="ResNet-B"><a href="#ResNet-B" class="headerlink" title="ResNet-B"></a>ResNet-B</h4><p>修改了下采样顺序。<strong>将下采样操作放到非1 × 1卷积层。1 × 1卷积层等价于对输入feature maps沿着channel维度做加权求和，因此设置stride为2会导致丢失3/4的特征信息。对于3 × 3的卷积层，设置stride为2不会丢失特征信息</strong></p><h4 id="ResNet-C"><a href="#ResNet-C" class="headerlink" title="ResNet-C"></a>ResNet-C</h4><p>1个7x7的卷积核参数量是1个3x3卷积核的5.4倍。所以用三个3x3卷积核替代一个7x7卷积核显然更好。</p><h4 id="ResNet-D"><a href="#ResNet-D" class="headerlink" title="ResNet-D"></a>ResNet-D</h4><p>受ResNet-B的启发，我们注意到下采样块路径B中的1 × 1卷积也忽略了3/4的输入特征映射。从经验上看，我们发现在卷积前添加一个stride=2的2×2平均池化层，将1x1卷积stride改为1，在实际应用中效果很好，对计算成本的影响很小。</p><h2 id="InceptionV3"><a href="#InceptionV3" class="headerlink" title="InceptionV3"></a>InceptionV3</h2><h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java核心基础</title>
      <link href="java-he-xin-ji-chu/"/>
      <url>java-he-xin-ji-chu/</url>
      
        <content type="html"><![CDATA[<h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p>处理器：Intel(R) Core(TM)i5-7300HQ CPU @ 2.50GHz 2.50 GHz</p><p>Core 酷睿   i5-7300 i5 i7 指不同类型  第一个7指第7代  HQ指标压 2.5GHZ相当于<strong>每秒2.5G个脉冲</strong>，时钟速度越快，在给定时间内执行指令越多</p><p>系统类型：64位系统，基于x64的处理器</p><p>64位系统，指处理器能同时处理64位    x64：x86架构的64位扩展</p><h4 id="持久化存储"><a href="#持久化存储" class="headerlink" title="持久化存储"></a>持久化存储</h4><p>内存中的数据一断电就没了，所以要存储到磁盘中</p><h4 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h4><p><a href="https://www.cnblogs.com/maskwolf/p/9972982.html">基本数据类型和引用数据类型</a></p>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VQA</title>
      <link href="vqa/"/>
      <url>vqa/</url>
      
        <content type="html"><![CDATA[<h2 id="Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering"><a href="#Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering" class="headerlink" title="Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"></a>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</h2><p><a href="https://cloud.tencent.com/developer/article/1471855">一篇博客，写的不错</a></p><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>18CVPR 2017 VQA Challenge冠军</p><p>image captioning:图像描述 VQA:视觉问答</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>提出了一种结合自底向上和自顶向下的注意机制，使注意能够在物体和其他显著图像区域的水平上计算。</p><p>自下而上的机制(基于更快的R-CNN)提出图像区域，每个区域都有一个相关的特征向量，而自上而下的机制决定特征权重。</p><p>评价标准：CIDEr   SPICE   BLEU-4</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>可以使用预训练的自底向上的注意力特征而不是CNN特征</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>图像与语言理解相结合-计算机视觉和自然语言处理结合。这需要细粒度的视觉处理，或者甚至需要多个步骤的推理来生成高质量的输出。因此，视觉注意机制在图像字幕和VQA中被广泛采用，其通过学习关注图像中突出的区域来提高性能。</p><p>在人类的视觉系统中，注意力可以由当前任务所决定的自上而下的信号(如寻找某物)自愿地集中起来，也可以由与意想不到的、新奇的或显著的刺激相关的自下而上的信号自动地集中起来。在本文中，我们采用类似的术语，将由非视觉或任务特定背景驱动的注意机制称为“自上而下”，而纯粹的前馈注意机制称为“自下而上”。</p><p>如图1， 自顶向下的注意力机制决定不同特征的贡献度，一般的CNN提取特征都是差不多相同的方格块，而自下向上的注意力机制选择了需要注意的区域，从而有很多大小不一的物体级别的方格。自顶向下得到显著图像区域，每个区域是一组特征向量，由Faster R-CNN实现；自顶向下机制利用特定任务上下文预测图像区域的注意分布。然后计算所关注的特征向量作为图像特征在所有区域的加权平均</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>确定区域的数量和区域的位置很重要。之前显著区域关注的比较少，从概念上讲，优势应该类似于在ImageNet上训练前的视觉表示，并利用显著更大的跨领域知识。</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>自顶向下只使用了最简单的one-pass一遍过模型，而没有使用类似堆叠、多头或者双向等复杂的注意力模型。</p><h4 id="Bottom-Up-Attention-Model"><a href="#Bottom-Up-Attention-Model" class="headerlink" title="Bottom-Up Attention Model"></a>Bottom-Up Attention Model</h4><p>使用 Faster R-CNN，表达区域没有使用之前空间图像特征V的定义，而是使用了bounding box.</p>]]></content>
      
      
      <categories>
          
          <category> VQA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark学习</title>
      <link href="spark-xue-xi/"/>
      <url>spark-xue-xi/</url>
      
        <content type="html"><![CDATA[<p><a href="http://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html">spark scala api</a></p><p><a href="https://www.scala-lang.org/api/current/">scala api</a></p><p><a href="https://www.runoob.com/scala/scala-tutorial.html">菜鸟教程scala</a></p><p><a href="https://my.oschina.net/joymufeng/blog/863823">scala下划线的使用</a></p><p><a href="https://www.cnblogs.com/wjunge/p/10043079.html">scala=&gt;的使用</a></p><p><a href="https://blog.csdn.net/wz_TXWY/article/details/100860862">scala方法调用方式</a> 其中注意无括号调用法，如果没有参数，可以省略括号</p><p><a href="https://blog.csdn.net/u010256841/article/details/53467905">scala中为什么不建议使用return</a>  简而言之就是更符合函数式编程，且不影响类型推断等</p><p><a href="https://blog.csdn.net/xianpanjia4616/article/details/81143146">scala中的:: , +:, :+, :::, +++, 等操作的含义</a></p><p><a href="https://www.jianshu.com/p/6eeaa4511068">讲解了很多scala集合操作，可以来这查找</a></p><p><code>collect()</code>返回一个包含此RDD中所有元素的数组。</p><p><img src="/spark-xue-xi/image-20201212132112158.png" alt="collect函数"></p><pre class=" language-scala"><code class="language-scala"><span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span><span class="token punctuation">{</span>SparkConf<span class="token punctuation">,</span> SparkContext<span class="token punctuation">}</span><span class="token comment" spellcheck="true">//SparkConf  Spark应用程序的配置。用于将各种Spark参数设置为键值对。</span><span class="token comment" spellcheck="true">//Spark功能的主入口点。SparkContext表示到Spark集群的连接，可用于在该集群上创建RDDs、累加器和广播变量</span><span class="token keyword">val</span> sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"Association Rule Mining Demo"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//new SparkConf()创建了一个从系统属性和类路径加载默认值的SparkConf对象  </span><span class="token comment" spellcheck="true">//setAppName()为你的应用设置一个名字</span><span class="token keyword">val</span> sc <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span><span class="token keyword">val</span> transactions <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span>transactionsFileName<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//textFile(path: String, minPartitions: Int = defaultMinPartitions)</span><span class="token comment" spellcheck="true">//从HDFS、本地文件系统(在所有节点上都可用)或hadoop支持的文件系统URI中读取文本文件，并以字符串的RDD形式返回。</span><span class="token comment" spellcheck="true">//minPartitions:生成的RDD的最小分区数</span><span class="token comment" spellcheck="true">//cache():使用默认的存储级别(MEMORY_ONLY)来支持这个RDD。</span><span class="token keyword">val</span> transactionSize <span class="token operator">=</span> transactions<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//count():返回RDD中的元素数量。</span><span class="token keyword">def</span> toList<span class="token punctuation">(</span>transaction<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span> List<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span> <span class="token keyword">val</span> list <span class="token operator">=</span> transaction<span class="token punctuation">.</span>trim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toList list<span class="token punctuation">}</span><span class="token comment" spellcheck="true">//trim():从指定字符串列的两端修剪指定字符。没带参数就是去掉空格</span><span class="token comment" spellcheck="true">//split():将提供的字符序列围绕','匹配项拆分,返回Array[String]</span><span class="token comment" spellcheck="true">//toList():将Array[String]转换为List[Strings]</span><span class="token keyword">def</span> findSortedCombinations<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">(</span>elements<span class="token operator">:</span> List<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token keyword">implicit</span> B<span class="token operator">:</span> Ordering<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span> <span class="token keyword">val</span> result <span class="token operator">=</span> elements<span class="token punctuation">.</span>sorted<span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token punctuation">.</span>toSet<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">.</span>subsets<span class="token punctuation">.</span>map<span class="token punctuation">(</span>_<span class="token punctuation">.</span>toList<span class="token punctuation">)</span><span class="token punctuation">.</span>toList result<span class="token punctuation">}</span><span class="token comment" spellcheck="true">//sorted(B):根据顺序对这个数组进行排序。是稳定排序</span><span class="token comment" spellcheck="true">//toSet[T]():转换为Set</span><span class="token comment" spellcheck="true">//subsets():遍历该集合所有子集的迭代器。</span><span class="token keyword">def</span> removeOneItem<span class="token punctuation">(</span>list<span class="token operator">:</span> List<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span> List<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>list <span class="token operator">==</span> <span class="token keyword">null</span><span class="token punctuation">)</span> <span class="token operator">||</span> list<span class="token punctuation">.</span>isEmpty<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> list    <span class="token punctuation">}</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">||</span> <span class="token punctuation">(</span>i <span class="token operator">></span> <span class="token punctuation">(</span>list<span class="token punctuation">.</span>size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> list    <span class="token punctuation">}</span>    <span class="token keyword">val</span> cloned <span class="token operator">=</span> list<span class="token punctuation">.</span>take<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">++</span> list<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>    cloned<span class="token punctuation">}</span><span class="token comment" spellcheck="true">//take(i):选择前i个元素</span><span class="token comment" spellcheck="true">//++：连接两个集合</span><span class="token comment" spellcheck="true">//drpo(i+1):选择除前i+1个元素之外的所有元素。</span><span class="token comment" spellcheck="true">//所以每次去掉了1个元素，如果初始数组传入0，就去掉第一个元素</span><span class="token keyword">val</span> itemsets <span class="token operator">=</span> transactions<span class="token punctuation">.</span>map<span class="token punctuation">(</span>toList<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>findSortedCombinations<span class="token punctuation">(</span>_<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>x <span class="token keyword">=></span> x<span class="token punctuation">)</span><span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>size <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>x <span class="token keyword">=></span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1L</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//map():通过对RDD的所有元素应用一个函数来返回一个新的RDD</span><span class="token comment" spellcheck="true">//flatMap(x => x):</span><span class="token comment" spellcheck="true">//filter(_.size > 0):过滤，过滤掉size=0的,也就是说去掉空集</span><span class="token comment" spellcheck="true">//</span><span class="token keyword">val</span> minSup <span class="token operator">=</span> <span class="token number">0.4</span><span class="token keyword">val</span> combined <span class="token operator">=</span> itemsets<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>x <span class="token keyword">=></span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>_2<span class="token punctuation">,</span> x<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>toDouble <span class="token operator">/</span> transactionSize<span class="token punctuation">.</span>toDouble<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_2 <span class="token operator">>=</span> minSup<span class="token punctuation">)</span><span class="token punctuation">.</span>cache<span class="token comment" spellcheck="true">//reduceByKey(_ + _):Key值相同的Value值相加</span><span class="token comment" spellcheck="true">//(key,(value1,value2))</span><span class="token comment" spellcheck="true">//value2>=minSup的过滤一下</span><span class="token keyword">val</span> subitemsets <span class="token operator">=</span> combined<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>itemset <span class="token keyword">=></span> <span class="token punctuation">{</span>    <span class="token keyword">val</span> list <span class="token operator">=</span> itemset<span class="token punctuation">.</span>_1    <span class="token keyword">val</span> frequency <span class="token operator">=</span> itemset<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_1    <span class="token keyword">val</span> support <span class="token operator">=</span> itemset<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_2    <span class="token keyword">var</span> result <span class="token operator">=</span> List<span class="token punctuation">(</span><span class="token punctuation">(</span>list<span class="token punctuation">,</span> <span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>frequency<span class="token punctuation">,</span> support<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>list<span class="token punctuation">.</span>size <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        result    <span class="token punctuation">}</span>     <span class="token keyword">else</span> <span class="token punctuation">{</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span>i <span class="token keyword">&lt;-</span> <span class="token number">0</span> until list<span class="token punctuation">.</span>size<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">val</span> listX <span class="token operator">=</span> removeOneItem<span class="token punctuation">(</span>list<span class="token punctuation">,</span> i<span class="token punctuation">)</span>            <span class="token keyword">val</span> listY <span class="token operator">=</span> list<span class="token punctuation">.</span>diff<span class="token punctuation">(</span>listX<span class="token punctuation">)</span>            result <span class="token operator">++</span><span class="token operator">=</span> List<span class="token punctuation">(</span><span class="token punctuation">(</span>listX<span class="token punctuation">,</span> <span class="token punctuation">(</span>listY<span class="token punctuation">,</span> <span class="token punctuation">(</span>frequency<span class="token punctuation">,</span> support<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">}</span>        result    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cache<span class="token comment" spellcheck="true">//diff(listX):A-A∩B</span><span class="token comment" spellcheck="true">//所以listY就是那个被去除掉的</span><span class="token keyword">val</span> rules <span class="token operator">=</span> subitemsets<span class="token punctuation">.</span>groupByKey<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//groupByKey():和reduceByKey()好像差不多</span><span class="token keyword">val</span> assocRules <span class="token operator">=</span> rules<span class="token punctuation">.</span>map<span class="token punctuation">(</span>in <span class="token keyword">=></span> <span class="token punctuation">{</span>    <span class="token keyword">val</span> listX <span class="token operator">=</span> in<span class="token punctuation">.</span>_1    <span class="token keyword">val</span> listYLists <span class="token operator">=</span> in<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>toList    <span class="token keyword">val</span> countX <span class="token operator">=</span> listYLists<span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_1<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token keyword">val</span> newListYLists <span class="token operator">=</span> listYLists<span class="token punctuation">.</span>diff<span class="token punctuation">(</span>List<span class="token punctuation">(</span>countX<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>newListYLists<span class="token punctuation">.</span>isEmpty<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">val</span> result <span class="token operator">=</span> List<span class="token punctuation">(</span><span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">,</span> List<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.0D</span><span class="token punctuation">,</span> <span class="token number">0.0D</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        result    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>        <span class="token keyword">val</span> result <span class="token operator">=</span> newListYLists<span class="token punctuation">.</span>map<span class="token punctuation">(</span>t2 <span class="token keyword">=></span> <span class="token punctuation">(</span>listX<span class="token punctuation">,</span> t2<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> t2<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_1<span class="token punctuation">.</span>toDouble<span class="token operator">/</span>countX<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_1<span class="token punctuation">.</span>toDouble<span class="token punctuation">,</span>                             t2<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>_2<span class="token punctuation">)</span><span class="token punctuation">)</span>        result    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//最小置信度</span><span class="token keyword">val</span> minConf <span class="token operator">=</span> <span class="token number">0.7</span><span class="token comment" spellcheck="true">//必须要大于置信度</span><span class="token keyword">val</span> finalResult <span class="token operator">=</span> assocRules<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>x <span class="token keyword">=></span> x<span class="token punctuation">)</span><span class="token punctuation">.</span>filter<span class="token punctuation">(</span>_<span class="token punctuation">.</span>_3 <span class="token operator">>=</span> minConf<span class="token punctuation">)</span><span class="token comment" spellcheck="true">//保存txt文件到输出路径</span>finalResult<span class="token punctuation">.</span>saveAsTextFile<span class="token punctuation">(</span>outputPath<span class="token punctuation">)</span>System<span class="token punctuation">.</span>exit<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程软件trick</title>
      <link href="bian-cheng-ruan-jian-trick/"/>
      <url>bian-cheng-ruan-jian-trick/</url>
      
        <content type="html"><![CDATA[<h2 id="jupyter-notebook"><a href="#jupyter-notebook" class="headerlink" title="jupyter notebook"></a>jupyter notebook</h2><p>jupyter notebook使用：<a href="https://zhuanlan.zhihu.com/p/32320214">https://zhuanlan.zhihu.com/p/32320214</a></p><p>在jupyter notebook中，可直接调用<code>arr.dtype</code> <code>arr.shape</code>显示np相应的属性</p><h4 id="py文件运行"><a href="#py文件运行" class="headerlink" title="py文件运行"></a>py文件运行</h4><p>在jupyter notebook中，运行 %run name.py</p><h3 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h3><p>!cd ..</p><h2 id="pycharm"><a href="#pycharm" class="headerlink" title="pycharm"></a>pycharm</h2><p><a href="https://blog.csdn.net/qq_35473473/article/details/106320708">pycharm配置anaconda环境</a></p><p>注释:Ctrl+/</p><p>取消注释:Ctrl+/</p><p>ctrl+左键就会跳到函数定义处</p><p>把光标放在要查询的对象上，打开视图菜单，quick definition查看对象的定义，quick documentation 快速文档，这个是jet brains自己对python的解释文档，第三个external documentation 外部文档，这个是python 的官方帮助文档，调转到网页帮助文档中。</p><h2 id="spyder"><a href="#spyder" class="headerlink" title="spyder"></a>spyder</h2><h2 id="notepad"><a href="#notepad" class="headerlink" title="notepad++"></a>notepad++</h2><p>编辑-&gt;行操作-&gt;删除空行，可以一键去除空行</p><h2 id="Ecilipse"><a href="#Ecilipse" class="headerlink" title="Ecilipse"></a>Ecilipse</h2><h2 id="Dev-C"><a href="#Dev-C" class="headerlink" title="Dev C++"></a>Dev C++</h2>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据集标注和评价指标</title>
      <link href="shu-ju-ji-biao-zhu/"/>
      <url>shu-ju-ji-biao-zhu/</url>
      
        <content type="html"><![CDATA[<h3 id="数据集标注"><a href="#数据集标注" class="headerlink" title="数据集标注"></a>数据集标注</h3><p><a href="https://baike.baidu.com/reference/735928/e1c09zpIxso79ckYFsufW97lGFgDuI-Mkxj7Qi6HYi-OVg50jiLd_3hXeWnola-o_uuaI6DtXtCZNK-NMJpSJxM">开放数据标注工具浅析</a></p><p>包括：labelme labelImg VIA 精灵标注  LabelHub</p><p>ps：ps标注可以选择使用磁性套索或者其他工具，标注完之后就涂色就可以了，自己设计一种颜色，比如urban street scene现在多此采用cityscapes的涂色方案。然后在使用的时候，把不同的rgb映射到类别空间，也就是灰度空间就可以了。一般，rgb是为了更好的结果类别的展示，训练的时候使用的是灰度图像。</p><h3 id="labelme标注详解"><a href="#labelme标注详解" class="headerlink" title="labelme标注详解"></a>labelme标注详解</h3><p><a href="https://github.com/wkentaro/labelme">Labelme的github地址</a>     <a href="https://blog.csdn.net/u014061630/article/details/88756644">Labelme教程1</a>      <a href="https://blog.csdn.net/fengxin1995/article/details/80511227">Labelme教程2</a></p><h4 id="1下载安装labelme"><a href="#1下载安装labelme" class="headerlink" title="1下载安装labelme"></a>1下载安装labelme</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">##################</span><span class="token comment" spellcheck="true">## for Python 3 ##</span><span class="token comment" spellcheck="true">##################</span><span class="token comment" spellcheck="true">#使用Anaconda创建虚拟环境</span>conda create <span class="token operator">-</span><span class="token operator">-</span>name<span class="token operator">=</span>labelme python<span class="token operator">=</span><span class="token number">3.6</span>source activate labelmepip install pyqt5  <span class="token comment" spellcheck="true"># pyqt5 can be installed via pip on python3</span>pip install labelme</code></pre><h4 id="2打开labelme"><a href="#2打开labelme" class="headerlink" title="2打开labelme"></a>2打开labelme</h4><p>打开anaconda Prompt</p><img src="/shu-ju-ji-biao-zhu/image-20210116145819793.png" alt="image-20210116145819793" style="zoom:80%;"><h4 id="3标注"><a href="#3标注" class="headerlink" title="3标注"></a>3标注</h4><p>使用open或opendir打开一张图片或一个文件夹</p><p>使用create polygons创建多边形标注即可</p><h4 id="4转换"><a href="#4转换" class="headerlink" title="4转换"></a>4转换</h4><p><code>start labelme_json_to_dataset 1.json</code></p><p>py文件在路径:envs\labelme\Lib\site-packages\labelme\cli</p><p><a href="https://blog.csdn.net/qq_39397024/article/details/93487845">labelme打不开，不显示主界面</a>,之前使用扩展屏幕的原因，憨憨的</p><p>labelme_json_to_dataset.exe转换json生成的文件夹为空，原因是json内存储的文件路径的原因！！！！</p><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p><a href="https://blog.csdn.net/qq_28418387/article/details/95662415">F1、IoU等计算方式理解与代码实现</a></p><p>上面这篇文章写得太好了，代码可以拿来直接用</p><p><a href="https://www.cnblogs.com/Trevo/p/11795503.html">评价指标总结和代码实现</a></p><p>最常用的PA和IoU<br>$$<br>PA=\frac{TP+TN}{TP+TN+FP+FN}<br>$$</p><h3 id="SDD-A-Skin-Detection-Dataset-for-Training-and-Assessment-of-Human-Skin-Classifiers"><a href="#SDD-A-Skin-Detection-Dataset-for-Training-and-Assessment-of-Human-Skin-Classifiers" class="headerlink" title="SDD: A Skin Detection Dataset for Training and Assessment of Human Skin Classifiers"></a>SDD: A Skin Detection Dataset for Training and Assessment of Human Skin Classifiers</h3><h4 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h4><p>15年一个皮肤分割数据集</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>超过20000张图像  非常精确：<strong>专业图形工具  三元划分</strong>  与SFA比较 <strong>定性且定量</strong>表明更好</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p><strong>数据库组织良好</strong>：因此使用时只和方法有关，而和数据集无关</p><p><strong>不同光照和成像条件下捕获</strong></p><p>不像许多其他数据集与半自动GT标签，在SDD，<strong>地面真相注释非常精确，感谢专业图形工具的使用和三元划分的想法。</strong></p><p>使用后者，处理皮肤和非皮肤区域边界上的繁琐点会更加方便和容易。</p><p>对于该数据集的评价，由于单直方图方法的沉默特性，无论是定性还是定量都选择了单直方图方法，结果表明使用SDD方法比SFA更有效。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><h4 id="Skin-Classification"><a href="#Skin-Classification" class="headerlink" title="Skin Classification"></a>Skin Classification</h4><p>例如，有些数据库认为人类嘴唇是皮肤的一部分，而有些数据库则忽略了它。</p><p>目前数据库中的地面真值图像往往太不准确(本文将对此进行说明)。</p><p>事实上，数据集有时是手工注释的，但是，由于这项任务既无聊又费时，它可能被漫不经心地完成。</p><p>在其他数据库中，GT图像使用半自动程序生成，以减少注释时间。然而，ground truth图像还不够精确，甚至可能比人工标记的图像更差。</p><p>在一些数据库中，皮肤像素被标记为非皮肤像素，但它们甚至不靠近皮肤区域，在某些情况下，非皮肤像素被标记为皮肤像素，这在一定程度上降低了性能。</p><p>在一些数据库中，皮肤像素被标记为非皮肤像素，在某些情况下，非皮肤像素被标记为皮肤像素，这在一定程度上降低了性能。</p><p>此外，训练集和测试集的数量和多样性对分类器性能的影响也很明显。然而，在一些工作中，这一因素往往被忽略。有很多作品在报告统计结果时没有考虑到照片集的大小和多样性的影响。一些数据集是在特定的成像和照明条件下编译的。这在评估和培训步骤中都可能出现问题。在评估中，如果数据集不够一般化，算法的结果要么太好，要么太差，都是不现实的。在训练中，使用非通用数据库仅根据特定类型的皮肤像素对系统进行优化，会严重影响算法的性能。有些数据库太小，无法用于各种方法的培训。例如，考虑Jones et al.[17]开发的Bayesian方法，在一种情况下，无论皮肤直方图还是非皮肤直方图，LUT (look-up table)的大小都达到了2×2563个细胞。要对该算法进行适当的训练，需要一个比可能的单元格整个大小大几倍的数据集。在某些情况下，即使数据库相对较大，其大多数映像也会重复多次。此外，一些数据集既不能用于公共用途，也不能免费用于非商业学术用途。基于上述问题，本文的目标是编制一个半理想的应用数据集，可用于训练和评估皮肤检测方法</p><h4 id="Previous-Databases"><a href="#Previous-Databases" class="headerlink" title="Previous Databases"></a>Previous Databases</h4><p>一些皮肤检测数据集最初是为人脸检测、手跟踪和人脸识别问题而开发的。最重要的皮肤数据库组是那些专门为皮肤分类器的训练和评估而设计的，而不是其他生物识别应用。</p><p>康柏是一个相对较老的数据库，其图像质量太低，不再值得信赖。新一代的成像设备已经被开发出来并用于商业用途，它们与以前的设备有很大的不同。另一个问题是康柏没有明确的划分，即不同的作者在测试和评估过程中可能使用不同的图像，这将影响性能。半监督标注，非常不准确</p><p>ECU图像保证了背景场景、光照条件和皮肤类型的多样性。照明条件包括室内照明和室外照明;皮肤类型包括白色，棕色，黄色和黑色皮肤。一些作品已经用这个数据库进行了评估。虽然图像的多样性足以用于评估一般的皮肤检测系统，但数据集的大小还不够大。此外，处理注释还存在一个特殊的问题，即所有图像中，特别是那些质量较低或拥挤的图像中的某些区域，注释并不是一项简单的任务。例如，图3中红色箭头表示的点是毛发周围的像素点，或者其他物体往往会带来额外的困难。这实际上是GTs手工标记方案的错误来源</p><p>基于神经网络皮肤分类器的最佳拓扑和阈值，将UCI和SFA进行了比较，发现在评估皮肤检测器[35]时，SFA的准确性略高于UCI。然而，SFA主要包括半护照图像，不适合用于评估目的。另外，SFA中的GT注释也不一致，即有很多图像没有进行准确的标注。此外，在一些图像中，人脸被2-3像素的厚白色边界包围。这将导致不同颜色空间皮肤簇的构建不准确。图4绘制了GTs对应的SFA图像;它们代表了所有的图像。这个数据集是公开的。</p><h4 id="SDD"><a href="#SDD" class="headerlink" title="SDD"></a>SDD</h4><p>据作者所知，它是文献中引入的用于训练和评估皮肤分类器性能的最大数据库。图像是在不同的光照条件下拍摄的，使用不同的成像设备，从世界各地不同肤色的人。一些图像是在线视频和电影的快照，而一些是从流行的人脸识别/跟踪/检测数据集中获取的静态图像[40-43]。SDD中图像的高度多样性使得它可以用于一般系统的训练和评估，即当一种方法被SDD评估时，结果是可靠的，而当一种方法被SDD训练时，它可以达到其最大的潜在性能。</p><p>如前所述,所有图片中有一些像素要么skinness的有一个问题(比如眉毛和眼睛周围地区,嘴唇和鼻子孔,等等)或者是位于皮肤和non-skin的边界,在这两种情况下,很难区分skinness和一些图片,要花很多的时间来注释。将GT图像划分为3个不重叠区域;皮肤像素、非皮肤像素以及在评估和训练步骤中都被考虑的像素(不关心点)</p><p>用的Photoshop CS5</p><p>$$<br>F1=\frac{2\cdot Recall\cdot Precision}{Recall+Precision}<br>$$</p><p>$$<br>IoU=\frac{tp}{tp+fn+fp}<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 肤色分级 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 肤色分级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow学习</title>
      <link href="tensorflow-xue-xi/"/>
      <url>tensorflow-xue-xi/</url>
      
        <content type="html"><![CDATA[<p><a href="https://tensorflow.google.cn/api_docs/python/tf?hl=zh_cn">tensorflowAPI</a></p><p><a href="https://keras-cn.readthedocs.io/en/latest/other/callbacks/">keras教程</a></p><p><a href="https://github.com/aymericdamien/TensorFlow-Examples">https://github.com/aymericdamien/TensorFlow-Examples</a></p><p><a href="https://github.com/instillai/TensorFlow-Course">https://github.com/instillai/TensorFlow-Course</a></p><p><a href="https://github.com/sjchoi86/Tensorflow-101">https://github.com/sjchoi86/Tensorflow-101</a></p><p><a href="https://github.com/terryum/TensorFlow_Exercises">https://github.com/terryum/TensorFlow_Exercises</a></p><p>keras是对tensorflow的再封装，是tensorflow的高级api</p><h4 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h4><p>*表示点乘   矩阵乘tf.matmul</p><p><a href="https://www.jianshu.com/p/30b40b504bae">tf.reduce_sum</a>这个博客太强了，把axis彻彻底底讲明白了   <strong>tensorflow的维度情况是NHWC</strong></p><p>本来维度是(2,3) axis=0 完事就变成了(3)   如果keepdims=True，就变成了(1,3)  </p><p>如果axis是一个元组(1,2,3)，就是先加维度1，加完加2，然后加3，如果没有传axis，默认所有的加起来</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dtype<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#张量数据类型转换</span><span class="token comment" spellcheck="true">#x:要转换的数据</span><span class="token comment" spellcheck="true">#dtype:目标数据类型</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>input<span class="token punctuation">,</span>name<span class="token operator">=</span>None<span class="token punctuation">,</span>out_type<span class="token operator">=</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将矩阵的维度输出为一个维度矩阵</span><span class="token comment" spellcheck="true">#input:张量</span><span class="token comment" spellcheck="true">#name:op的名字，用于tensorboard中</span><span class="token comment" spellcheck="true">#out_type:输出类型</span>tf<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> paddings<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'CONSTANT'</span><span class="token punctuation">,</span> constant_values<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对张量在各个维度进行填充</span><span class="token comment" spellcheck="true">#tensor:待填充的张量</span><span class="token comment" spellcheck="true">#paddings:对哪个维度进行填充，上下左右填充多少,例如[[1,2],[3,4]],上填充一行，下2行，左3列，右4列</span><span class="token comment" spellcheck="true">#mode 填充方式,有’CONSTANT’'REFLECT''SYMMETRIC'</span><span class="token comment" spellcheck="true">#constant_values:constant填充值</span><span class="token comment" spellcheck="true">#name：操作的名字</span>dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>function<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对dataset中的每一个tf.Tensor执行该函数操作</span>tf<span class="token punctuation">.</span>fill<span class="token punctuation">(</span>dims<span class="token punctuation">,</span>value<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#创建一个充满标量值的张量。</span>tf<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#把所有是1的维度都去除，捏一下，挤压一下，把水分挤出来</span></code></pre><h4 id="tf-data-Dataset"><a href="#tf-data-Dataset" class="headerlink" title="tf.data.Dataset"></a>tf.data.Dataset</h4><ol><li>从输入数据创建源数据集。</li><li>应用数据集转换对数据进行预处理。</li><li>遍历数据集并处理元素。</li></ol><p><strong>Source Datasets:</strong></p><p>从列表：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> element <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>  <span class="token keyword">print</span><span class="token punctuation">(</span>element<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tf.Tensor(1, shape=(), dtype=int32)</span><span class="token comment" spellcheck="true">#tf.Tensor(2, shape=(), dtype=int32)</span><span class="token comment" spellcheck="true">#tf.Tensor(3, shape=(), dtype=int32)</span></code></pre><p><strong>as_numpy_iterator</strong></p><p>返回一个迭代器，它将数据集的所有元素转换为numpy。</p><p>使用as_numpy_iterator检查数据集的内容。要查看元素的形状和类型，请直接打印数据集元素，而不是使用as_numpy_iterator</p><p><strong>batch</strong></p><pre class=" language-python"><code class="language-python">batch<span class="token punctuation">(</span>    batch_size<span class="token punctuation">,</span> drop_remainder<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p>将连续数据组合为batch，drop_remainder表示如果最后的数据不够一批，是否删除</p><p>组合完的数据会多一个额外的维度batch_size,如下：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>range<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">for</span> element <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>element<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tf.Tensor([0 1 2], shape=(3,), dtype=int64)</span><span class="token comment" spellcheck="true">#tf.Tensor([3 4 5], shape=(3,), dtype=int64)</span><span class="token comment" spellcheck="true">#tf.Tensor([6 7], shape=(2,), dtype=int64)</span></code></pre><p><strong>padded_batch</strong></p><pre class=" language-python"><code class="language-python">padded_batch<span class="token punctuation">(</span>    batch_size<span class="token punctuation">,</span> padded_shapes<span class="token operator">=</span>None<span class="token punctuation">,</span> padding_values<span class="token operator">=</span>None<span class="token punctuation">,</span> drop_remainder<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p>也是生成batch的，不同的是如果有变长如何填充：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Components of nested elements can be padded independently.</span>elements <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_generator<span class="token punctuation">(</span>    <span class="token keyword">lambda</span><span class="token punctuation">:</span> iter<span class="token punctuation">(</span>elements<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Pad the first component of the tuple to length 4, and the second</span><span class="token comment" spellcheck="true"># component to the smallest size that fits.</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>padded_batch<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>    padded_shapes<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    padding_values<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>list<span class="token punctuation">(</span>dataset<span class="token punctuation">.</span>as_numpy_iterator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#[(array([[ 1,  2,  3, -1],</span><span class="token comment" spellcheck="true">#         [ 4,  5, -1, -1]], dtype=int32),</span><span class="token comment" spellcheck="true">#  array([[ 10, 100],</span><span class="token comment" spellcheck="true">#         [ 11,  12]], dtype=int32))]</span></code></pre><p><strong>map</strong></p><p>Maps <code>map_func</code> across the elements of this dataset.</p><p><strong>prefetch（buffer_size）</strong></p><p>这允许在处理当前元素时准备后面的元素。这通常会提高延迟和吞吐量，但代价是使用额外的内存来存储预先获取的元素。</p><p><strong>cache</strong></p><p><strong>shuffle</strong> </p><p>随机打乱此数据集的元素。</p><p>这个数据集用buffer_size元素填充一个缓冲区，然后从这个缓冲区随机抽取元素，用新元素替换选中的元素。为了实现完美的洗牌，需要一个大于或等于数据集的完整大小的缓冲区。</p><p>例如，如果数据集包含10,000个元素，但buffer_size被设置为1,000，那么shuffle最初将从缓冲区中的前1,000个元素中随机选择一个元素。一旦一个元素被选中，它在缓冲区中的空间将被下一个元素(即1001 -st)替换，保持缓冲区的1,000个元素。</p><p><strong>prefetch</strong></p><h4 id="流水线"><a href="#流水线" class="headerlink" title="流水线"></a>流水线</h4><p><a href="https://blog.csdn.net/jackhh1/article/details/102763999">https://blog.csdn.net/jackhh1/article/details/102763999</a></p><p>加速器执行训练步骤N时，CPU正在为步骤N+1准备数据，这样可以大大减少CPU和GPU的空闲时间</p><h4 id="回调函数"><a href="#回调函数" class="headerlink" title="回调函数"></a>回调函数</h4><p><a href="https://keras-cn.readthedocs.io/en/latest/other/callbacks/">https://keras-cn.readthedocs.io/en/latest/other/callbacks/</a></p><h4 id="tf-io"><a href="#tf-io" class="headerlink" title="tf.io"></a>tf.io</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">process_example_paths</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#先读取再按照对应格式解码</span>            <span class="token keyword">return</span> <span class="token punctuation">{</span><span class="token string">'feature'</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>io<span class="token punctuation">.</span>decode_jpeg<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>io<span class="token punctuation">.</span>read_file<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                    <span class="token string">'label'</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>io<span class="token punctuation">.</span>decode_png<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>io<span class="token punctuation">.</span>read_file<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">#获得如下</span><span class="token comment" spellcheck="true">#tf.Tensor([600 394   3], shape=(3,), dtype=int32)</span><span class="token comment" spellcheck="true">#tf.Tensor([600 394   1], shape=(3,), dtype=int32)</span></code></pre><h3 id="tf-keras"><a href="#tf-keras" class="headerlink" title="tf.keras"></a>tf.keras</h3><h4 id="tf-keras-backend"><a href="#tf-keras-backend" class="headerlink" title="tf.keras.backend"></a>tf.keras.backend</h4><p><a href="https://keras-cn.readthedocs.io/en/latest/backend/">https://keras-cn.readthedocs.io/en/latest/backend/</a></p><p>keras后端API，有Theano和Tensorflow</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#结束当前的TF计算图，并新建一个。有效的避免模型/层的混乱</span></code></pre><h4 id="tf-keras-models"><a href="#tf-keras-models" class="headerlink" title="tf.keras.models"></a>tf.keras.models</h4><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>load_model<span class="token punctuation">(</span>filepath<span class="token punctuation">,</span> custom_objects<span class="token operator">=</span>None<span class="token punctuation">,</span> compile<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> options<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#可选</span><span class="token comment" spellcheck="true">#compile:布尔值，是否在加载后编译模型</span><span class="token comment" spellcheck="true">#options:可选</span></code></pre><p><a href="https://blog.csdn.net/sjtuxx_lee/article/details/80399514">keras保存和加载模型的方法</a></p><h4 id="tf-keras-utils"><a href="#tf-keras-utils" class="headerlink" title="tf.keras.utils"></a>tf.keras.utils</h4><p>tf.keras.utils.plot_model </p><h4 id="tf-keras-Model"><a href="#tf-keras-Model" class="headerlink" title="tf.keras.Model"></a>tf.keras.Model</h4><p><a href="https://keras-cn.readthedocs.io/en/latest/legacy/models/model/">总体介绍看这个</a></p><p>如果你只是载入模型并利用其predict，可以不用进行compile。在Keras中，compile主要完成损失函数和优化器的一些配置，是为训练服务的。predict会在内部进行符号函数的编译工作（通过调用_make_predict_function生成函数）</p><p><strong>fit</strong></p><p>对于batch_size这个参数，因为我们用的tf.data.Dateset，之前已经制定过批了，不用再指定了</p><h4 id="tf-compat"><a href="#tf-compat" class="headerlink" title="tf.compat"></a>tf.compat</h4><p>提供兼容性功能</p><p>tf.compat.v1.reset_default_graph()   #清除默认图形堆栈并重置全局默认图形。</p><p>具体例子看这个博客<a href="https://blog.csdn.net/duanlianvip/article/details/98626111%EF%BC%8C%E6%88%91%E7%9A%84%E7%90%86%E8%A7%A3%E6%9A%82%E6%97%B6%E5%B0%B1%E6%98%AF%E9%87%8D%E6%96%B0%E5%BC%80%E5%A7%8B%EF%BC%8C%E9%98%B2%E6%AD%A2%E5%9C%A8%E4%BB%A5%E5%89%8D%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E7%94%9F%E6%88%90%E6%96%B0%E8%8A%82%E7%82%B9">https://blog.csdn.net/duanlianvip/article/details/98626111，我的理解暂时就是重新开始，防止在以前的基础上生成新节点</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络结构解密</title>
      <link href="wang-luo-jie-gou-jie-mi/"/>
      <url>wang-luo-jie-gou-jie-mi/</url>
      
        <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h3><p><a href="https://zhuanlan.zhihu.com/p/31561570">深度学习基础</a></p><p>representation learning 表示学习</p><h4 id="activation-function"><a href="#activation-function" class="headerlink" title="activation function"></a>activation function</h4><p>激活函数应满足三个性质：</p><ol><li>不会饱和</li><li>零均值</li><li>容易计算</li></ol><p><a href="https://zhuanlan.zhihu.com/p/70821070">谁才是合格的激活函数 </a></p><p><strong>sigmoid的非零均值问题</strong><br>$$<br>\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w_1}=\frac{\partial L}{\partial y} \cdot a_1 \<br>\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial y} \cdot a_2<br>$$<br><strong>sigmoid求导:</strong><br>$$<br>\sigma(x)=\frac{1}{1+e^{-x}}\<br>\sigma’(x)=\sigma(x)[1-\sigma(x)]<br>$$</p><p>a1、a2&gt;0,所以同步变化</p><p>所有的权重只能一起变大或变小，这样模型收敛就需要消耗很多的时间</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201217191234682.png" alt="这时的收敛是这样的"></p><p><strong>Tanh</strong></p><p>Tanh函数是 0 均值的，因此实际应用中 Tanh 会比 sigmoid 更好。但是仍然存在<strong>梯度饱和</strong>与<strong>exp计算</strong>的问题。<br>$$<br>f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\<br>f(x)’=1-[f(x)]^2<br>$$</p><p><strong>ReLU的Dead ReLU问题</strong></p><p>如果某一次权重更新用力过猛，啪的一下，大部分权重都是负的了，下一次前向输出很容易是负的，一激活，就是0，之后就不更新了，就完蛋了</p><p>两个原因：<strong>1初始化太糟糕 2learning rate设太高导致用力过猛</strong></p><p><a href="https://zhuanlan.zhihu.com/p/71882757">不同激活函数的优缺点总结</a></p><p>了解<strong>ELU</strong>和<strong>Maxout</strong>，之前的问题解决了，但计算量和参数量会有损失</p><p><strong>transfer learning 迁移学习</strong> 一般都用ImageNet <strong>1提取高层特征 2微调</strong></p><p><strong>multi-task learning 多任务学习</strong> 低层特征共享，产生分支完成各自的任务</p><p> <strong>end-to-end learning 端到端学习</strong> 一步到位，就是一种表示学习</p><h3 id="计算机视觉四大基本任务"><a href="#计算机视觉四大基本任务" class="headerlink" title="计算机视觉四大基本任务"></a>计算机视觉四大基本任务</h3><p><a href="https://zhuanlan.zhihu.com/p/31727402">计算机视觉四大基本任务</a></p><h3 id="计算机视觉其他应用"><a href="#计算机视觉其他应用" class="headerlink" title="计算机视觉其他应用"></a>计算机视觉其他应用</h3><p><a href="https://zhuanlan.zhihu.com/p/31727405">计算机视觉其他应用</a></p><h2 id="统计学习知识和网络细节"><a href="#统计学习知识和网络细节" class="headerlink" title="统计学习知识和网络细节"></a>统计学习知识和网络细节</h2><h3 id="归一-标准-正则"><a href="#归一-标准-正则" class="headerlink" title="归一/标准/正则"></a>归一/标准/正则</h3><p><a href="https://maristie.com/2018/02/Normalization-Standardization-and-Regularization">Differences between Normalization, Standardization and Regularization</a></p><p><a href="https://www.zhihu.com/question/20455227/answer/370658612">归一化和标准化的作用于区别</a></p><h4 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h4><img src="/wang-luo-jie-gou-jie-mi/image-20210317213036238.png" alt="归一化的作用" style="zoom: 67%;"><h4 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h4><p>特征缩放，在许多学习算法中，标准化是一种<strong>广泛使用的预处理步骤，其目的是将特征缩放到零均值和单位方差:</strong><br>$$<br>x^{’}=\frac{x-u}{\sigma}<br>$$<br>也是特征缩放,但是更<strong>加动态和具有弹性,与整体样本的分布有关,加速收敛</strong></p><p><strong>当整体较为集中时,方差更小,标准化后就会更宽松;如果较为宽松,方差更大,标准化后就会变紧。</strong></p><p>用这些大数据集的均值和方差,代替真实的均值和方差</p><p><strong>coco数据集的均值和方差（三分量顺序是RGB）</strong></p><p><code>mean = [0.471, 0.448, 0.408]</code><br><code>std = [0.234, 0.239, 0.242]</code></p><p><strong>ImageNet数据集的均值和方差（三分量顺序是RGB）</strong></p><p><code>mean = [0.485, 0.456, 0.406]</code><br><code>std = [0.229, 0.224, 0.225]</code></p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>正则化,减少过拟合，损失函数后加正则项</p><p>翻译成中文是归一化，标准化，正则化，前两个是特征缩放，最后一个是减少过拟合</p><p><strong>贝叶斯学派观点来看，正则项是在模型训练过程中引入了某种模型参数</strong>的<a href="https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C">先验</a>分布[后验由数据得出，先验就是理性推断，不拘束于数据]。</p><p><strong>范数</strong>$L_P-norm$</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201125202729072.png" alt="Lp范数"></p><p>p=1，<strong>曼哈顿距离</strong>，p=2，<strong>欧式距离</strong>，p为无穷，<strong>无穷范数或最大范数</strong>。</p><p>做正则项时，称为Lp-正则项。L1-正则项也叫<strong>LASSO</strong>正则项，L2-正则项也叫<strong>Tikhonov或Ringe正则项</strong></p><p><a href="https://liam.page/2017/03/30/L1-and-L2-regularizer/">详解</a></p><p>在这里，我们需要关注的最主要是范数的「非负性」。我们刚才讲，损失函数通常是一个有下确界的函数。而这个性质保证了我们可以对损失函数做最优化求解。如果我们要保证目标函数依然可以做最优化求解，那么我们就必须让正则项也有一个下界。非负性无疑提供了这样的下界，而且它是一个下确界——由齐次性保证[当 c=0 时]</p><p>因此，我们说，范数的性质使得它天然地适合作为机器学习的正则项。而范数需要的向量，则是<strong>机器学习的学习目标——参数向量</strong>。</p><p>L0范数表示向量中非零元素的个数 </p><p>我们考虑这样一种普遍的情况，即：预测目标背后的真是规律，可能只和某几个维度的特征有关；而其它维度的特征，要不然作用非常小，要不然纯粹是噪声。在这种情况下，除了这几个维度的特征对应的参数之外，其它维度的参数应该为零。若不然，则当其它维度的特征存在噪音时，模型的行为会发生预期之外的变化，导致过拟合。引入L0范数</p><p>L0范数不好，又引入L1范数，L1范数也可以在梯度更新时使得参数趋于0.</p><p>L1使得参数稀疏化，L2使得参数稠密的趋近于0</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201125205147790.png" alt="稀疏和稠密的原因"></p><p><strong>提前停止</strong></p><p>提前停止可看做是<strong>时间维度上的正则化</strong>。直觉上，随着迭代次数的增加，如梯度下降这样的训练算法倾向于学习愈加复杂的模型。在实践维度上进行正则化有助于控制模型复杂度，提升泛化能力。在实践中，<strong>提前停止一般是在训练集上进行训练，而后在统计上独立的验证集上进行评估；当模型在验证集上的性能不在提升时，就提前停止训练</strong>。最后，可在测试集上对模型性能做最后测试。</p><p><a href="https://alisure.github.io/2018/04/14/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%93%E6%B7%B7%E6%A6%82%E5%BF%B5%E4%B9%8B%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E4%B8%8E%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9/#:~:text=%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9(Structural%20Risk)%EF%BC%9A,%EF%BC%88%E4%BE%8B%E5%A6%82%E6%AD%A3%E5%88%99%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%89%E3%80%82">期望风险、经验风险、结构风险 简洁</a></p><h3 id="偏差-方差分解"><a href="#偏差-方差分解" class="headerlink" title="偏差-方差分解"></a>偏差-方差分解</h3><p><a href="https://liam.page/2017/03/25/bias-variance-tradeoff/">详解</a></p><p>误差来源有三种：随机误差、偏差和方差</p><p>随机误差是高斯白噪声</p><p>偏差bias描述的是通过学习拟合出来的结果之期望，与真实规律之间的差距，记作$Bias(X)=E[\hat{f}(X)]-f(X)$</p><p>方差variance即是统计学中的定义，描述的是通过学习拟合出来的结果自身的不稳定性，记作$Var(X)=E[(\hat{f}(X)-E[\hat{}f(X)])^2]$</p><p>均方误差可以分解为:$Bias^2+Variance+Random Error$</p><img src="/wang-luo-jie-gou-jie-mi/image-20201125210219206.png" alt="直观图示" style="zoom:67%;"><h3 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h3><p><a href="https://www.jianshu.com/p/903e35e1c95a">期望风险、经验风险、结构风险2深入</a></p><p>损失函数：度量模型一次预测的好坏</p><p>风险函数：度量平均意义下的模型预测好坏</p><h3 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h3><p><a href="https://blog.csdn.net/b1055077005/article/details/100152102">Cross entropy</a></p><p><strong>信息奠基人香农（Shannon）认为“信息是用来消除随机不确定性的东西”。也就是说衡量信息量大小就看这个信息消除不确定性的程度。</strong></p><p>一件事情概率越大，现在发生的不确定性就越小；概率越小，现在要发生的不确定性就越大，发生后消除的不确定性就越多，信息量就越大。一件事情发生后概率就是1，正常的概率是$P(x)$，消除的不确定性是$1-P(x)$</p><p><strong>所以信息量的大小与信息发生的概率成反比。</strong></p><p>$I(x)=-log(P(x))$</p><p><strong>信息熵是信息量的期望，信息源的综合不确定性</strong></p><p><a href="https://www.jianshu.com/p/43318a3dc715">相对熵</a>    <strong>KL散度衡量两个概率分布之间的差异,其实就是两个分布的熵的差值，也就是到底损失了多少信息</strong><br>$$<br>D_{KL}(p||q)=\sum_{i=1}^Np(x_{i})\cdot (logp(x_i)-logq(x_i))\<br>=E[logp(x)-logq(x)]\<br>=\sum_{i=1}^Np(x_{i})\cdot log\frac{p(x_i)}{q(x_i)}\<br>\quad \quad \quad \quad \quad=\sum_{i=1}^Np(x_{i})logp(x_i)-\sum_{i=1}^Np(x_i)logq(x_i))\<br>=-H(x)+cross_entropy<br>$$<br>p是真实分布，q是预测分布,第一行为啥是$p(x_i)$？<strong>后面的是他们的信息量，用它们的概率分布，但实际算期望的时候还是要看真实的概率。</strong>信息量和概率还是要分开看，对每一个条目，算他们信息量的不同，但实际的概率每个x出现的概率还是真实概率</p><p>实际KL散度算的是<strong>预测分布减去真实分布的熵</strong></p><p><strong>交叉熵,第四行和第五行显示了交叉熵，因为实际训练的时候标签确定，所以H(x)是固定的，交叉熵可以替代KL散度</strong></p><h4 id="二元交叉熵解决sigmoid梯度饱和"><a href="#二元交叉熵解决sigmoid梯度饱和" class="headerlink" title="二元交叉熵解决sigmoid梯度饱和"></a>二元交叉熵解决sigmoid梯度饱和</h4><p>以MSELoss为例,分别对w、b链式求导时，结果是包含$\sigma’$,这就会导致梯度饱和，而二元交叉熵只包含$\sigma$,不包含$\sigma’$,所以</p><h4 id="对数损失函数-负对数似然"><a href="#对数损失函数-负对数似然" class="headerlink" title="对数损失函数/负对数似然"></a>对数损失函数/负对数似然</h4><p><a href="https://blog.csdn.net/silver1225/article/details/88914652">负对数似然函数</a></p><p><strong>概率</strong>用于在已知参数的情况下，预测接下来的观测结果；<strong>似然性</strong>用于根据一些观测结果，估计给定模型的参数可能值。数值上相等，意义上不同</p><p>$L(\theta|x)=f_\theta(x)=P_\theta(X=x)=P(X=x|\theta)=P(X=x;\theta)$</p><p>负对数似然:$L(y)=-log(y)$ 负对数似然常和softmax结合使用,$0&lt;=P&lt;=1$</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201223183034910.png" alt="log"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201223183214140.png" alt="a>1,p"></p><p>我们希望得到的概率越大越好，因此概率越接近于1，则函数整体值越接近于0，即使得损失函数取到最小值。</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201125152106078.png" alt="对数损失函数"></p><p>确实是一致的，只要理解了交叉熵损失中的预测概率q(i)=p(y^i|x)</p><p><strong>结构风险本质</strong></p><p>结构风险是对经验风险模型复杂度的惩罚</p><p><strong>结构化风险（正则项）其实是加入了模型参数分布的先验知识</strong>，也就是贝叶斯学派为了将模型往人们期望的地方去发展，继而加入了先验分布，由于是人为的先验，因此也就是一个规则项（这也就是正则项名称的由来）。这样一来，风险函数将进一步考虑了被估计量的先验概率分布</p><p><strong>统计学习方法=模型+策略+算法</strong></p><p>由决策函数表示的模型为非概率模型，由条件概率表示的模型为概率模型。</p><p><strong>当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。</strong></p><p><strong>当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计</strong></p><h3 id="log-sum-exp"><a href="#log-sum-exp" class="headerlink" title="log-sum-exp"></a>log-sum-exp</h3><h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><h3 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h3><p><a href="https://zhuanlan.zhihu.com/p/29895933">指数加权平均</a></p><p>$v_t=\beta v_{t-1} + (1-\beta)\theta_t $</p><p><strong>指数式递减加权   优势</strong>:我们可以看到指数加权平均的求解过程实际上是一个递推的过程，那么这样就会有一个非常大的好处，每当我要求从0到某一时刻（n）的平均值的时候，我并不需要像普通求解平均值的作为，保留所有的时刻值，类和然后除以n。</p><p>而是只需要保留0-(n-1)时刻的平均值和n时刻的温度值即可。也就是每次只需要保留常数值，然后进行运算即可，这对于深度学习中的海量数据来说，是一个很好的<strong>减少内存和空间的做法</strong>。</p><h3 id="带动量的随机梯度下降"><a href="#带动量的随机梯度下降" class="headerlink" title="带动量的随机梯度下降"></a>带动量的随机梯度下降</h3><h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3><p><a href="https://www.cnblogs.com/shine-lee/p/12069176.html">这篇太优质了 ！！！！</a></p><p><strong>感受野是个相对概念，某层feature map上的元素看到前面不同层上的区域范围是不同的，通常在不特殊指定的情况下，感受野指的是看到输入图像上的区域。</strong></p><h4 id="感受野大小"><a href="#感受野大小" class="headerlink" title="感受野大小"></a>感受野大小</h4><p>$$<br>r_l=r_{l-1}+(k_l-1)\cdot j_{l-1}\<br> =r_{l-1}+(k_l-1)\cdot \prod_{i=1}^{l-1} s_i<br>$$</p><p><strong>Layer l 一个元素的感受野rl等价于Layer l−1 上k×k 个感受野的叠加；</strong></p><p><strong>Layer l−1 上连续k 个元素的感受野可以看成是，第1个元素看到的感受野加上剩余k−1步扫过的范围</strong></p><img src="/wang-luo-jie-gou-jie-mi/image-20210312012129435.png" alt="感受野计算示例" style="zoom:67%;"><p>如上图,Conv1的感受野是3,Conv2的感受野是5</p><h3 id="所有的卷积"><a href="#所有的卷积" class="headerlink" title="所有的卷积"></a>所有的卷积</h3><p><a href="https://blog.csdn.net/tsyccnh/article/details/87357447">一篇非常好的教程关于卷积和转置卷积</a></p><p><a href="https://zhuanlan.zhihu.com/p/50573160">腾讯云教程卷积和转置卷积关系</a></p><p><a href="https://www.cnblogs.com/ywheunji/p/11887906.html">卷积核的参数量和计算量</a></p><h4 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h4><p><strong>卷积核</strong></p><ol><li><strong>卷积核默认第三维与输入图片的第三维（通道数）一样，并进行多层卷积，产生一个二维结果</strong></li><li><strong>1个卷积核产生一个二维结果，n个卷积核产生第三维为n的三维结果</strong></li><li><strong>卷积核n的个数就是输出的通道数</strong></li></ol><p>为什么out_channel要大于in_channel?</p><p>Out_channels要能反映图片的多种特征,<strong>每个kernel对不同的特征有不同的敏感度</strong></p><p><strong>大的卷积核尺寸意味着更大的感受野</strong></p><p><a href="https://github.com/vdumoulin/conv_arithmetic">结合github的图演示看</a></p><p><strong>1 通用公式</strong><br>$$<br>o=\frac{i+2p-k}{s}+1 \<br>D2=K(filters_num)<br>$$</p><p><strong>2 没有填充,单位步长</strong><br>$$<br>s=1,p=0,o=i-k+1<br>$$</p><p><strong>3有零填充,单位步长</strong></p><p><strong>3.1same padding</strong></p><p>$$<br>p=0,s=1\<br>o=i-k+1<br>$$<br><strong>3 Half/same padding</strong>         $p=\lfloor \frac{k}{2}\rfloor$<br>$$<br>k=2n+1,n\in N \<br>s=1,p=\lfloor \frac{k}{2}\rfloor=n\<br>o=i<br>$$</p><p><strong>3.2 Full padding</strong>            $p=k-1$<br>$$<br>p=k-1,s=1\<br>o=i+k-1<br>$$<br><strong>4 没有零填充,非单位步长</strong><br>$$<br>p=0,o=\lfloor\frac{i-k}{s}\rfloor+1<br>$$</p><p><strong>5 零填充,非单位步长</strong>     $odd$<br>$$<br>o=\lfloor \frac{i+2p-k}{s}\rfloor+1<br>$$<br><strong>填充方式</strong>有三种：full same  valid,一般为<strong>零填充</strong></p><p>我们平时<strong>所接触的卷积其实是滤波</strong>，不是<strong>真正的数学定义上的卷积</strong>。</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201209205446747.png" alt="步长为2的卷积"></p><p>步长为2的卷积可以<strong>看做步长为1的卷积结果的S=2的采样</strong>，因此可以认为是<strong>下采样</strong>的一种。</p><p>普通卷积的计算看起来是挪动，其实不是挪动，效率太低。<strong>计算机会将卷积核转换成等效的矩阵，将输入转换为向量。通过输入向量和卷积核矩阵的相乘获得输出向量。输出的向量经过整形便可得到我们的二维输出特征。</strong></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201220214743847.png" alt="卷积示例"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201220214800536.png" alt="四个位置补0"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201220214820691.png" alt="拉长，然后就可以矩阵相乘"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201220214911375.png" alt="卷积核所展开的全连接层的权重矩阵"></p><p>在CNN提出之前，我们所提到的<strong>人工神经网络应该多数情况下都是前馈神经网络</strong>，两者区别主要在于<strong>CNN使用了卷积层，而前馈神经网络用的都是全连接层</strong>，而这<strong>两个layer的区别又在于全连接层认为上一层的所有节点下一层都是需要的，通过与权重矩阵相乘层层传递，而卷积层则认为上一层的有些节点下一层其实是不需要的，所以提出了卷积核矩阵的概念</strong>。</p><p>如果卷积核的大小是nxm，那么意味着该卷积核认为上一层节点每次映射到下一层节点都只有nxm个节点是有意义的。到这里，有些初学者会认为<strong>全连接层也可以做到，只要让权重矩阵某些权重赋值为0就可以实现了</strong>，例如假设在计算当前层第2个节点时认为上一层的第1个节点我不需要，那么设置w01=0就可以了。其实没错，<strong>卷积层是可以看做全连接层的一种特例,卷积核矩阵是可以展开为一个稀疏的包含很多0的全连接层的权重矩阵</strong>。</p><p>卷积的两个优势：</p><ol><li>极大的减少参数的个数，如上图，一个4x16,一个3x3</li><li>卷积核关注的是某几个相邻的节点之间的关系，<strong>学习了图片的局部特征，可以说是带有目的性的学习</strong>。这与<strong>全连接层无区别的对待所有节点进行学习有极大的差别</strong>。</li></ol><img src="/wang-luo-jie-gou-jie-mi/image-20201220222854186.png" alt="卷积有利于解决平移不变性" style="zoom:80%;"><h4 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h4><p><a href="https://github.com/vdumoulin/conv_arithmetic">结合github的图演示看</a></p><p><strong>1 p=0,s=1</strong><br>$$<br>p=0,s=1\<br>k’=k,s’=s,p’=k-1\<br>o’=i’+(k-1)<br>$$<br><strong>2 s=1</strong><br>$$<br>s=1\<br>p’=k-p-1\<br>o’=i’+(k-1)-2p<br>$$<br><strong>3 Half/same padding</strong><br>$$<br>k=2n+1,n\in N,s=1,p=\lfloor\frac{k}{2}\rfloor=n\<br>k’=k,s’=s,p’=p\<br>o’=i’<br>$$<br><strong>4 Full padding</strong><br>$$<br>s=1,p=k-1\<br>k’=k,s’=s,p’=0\<br>o’=i’-(k-1)<br>$$<br><strong>5 p=0 ,non-unit strides</strong><br>$$<br>p=0,(i-k)%s=0\<br>k’=k,s’=1,p’=k-1\<br>o’=s(\widetilde{i}’-1)+k\<br>\widetilde{i}’ :adding,s-1,zeros,between,each,input,unit<br>$$<br><strong>6 zero-padding,non-unit strides</strong><br>$$<br>(i+2p-k)%s=0\<br>k’=k,s’=1,p’=k-p-1\<br>o’=s(i’-1)+k-2p<br>$$<br><strong>7 zero-padding,non-unit strides odd</strong><br>$$<br>k’=k,s’=1,p’=k-p-1\<br>a=(i+2p-k)%s\<br>o’=s(i’-1)+a+k-2p<br>$$<br>也叫反卷积/分数步长卷积</p><p>先说一下为什么人们很喜欢叫转置卷积为反卷积或逆卷积。首先举一个例子，将一个4x4的输入通过3x3的卷积核在进行普通卷积（无padding, stride=1），将得到一个2x2的输出。而转置卷积将一个2x2的输入通过同样3x3大小的卷积核将得到一个4x4的输出，看起来似乎是普通卷积的逆过程。就好像是加法的逆过程是减法，乘法的逆过程是除法一样，人们自然而然的认为这两个操作<strong>似乎是一个可逆的过程。但事实上两者并没有什么关系，操作的过程也不是可逆的。</strong></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201221160433547.png" alt="转置乘过来"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201221160511466.png" alt="列向量再转化成卷积核"></p><img src="/wang-luo-jie-gou-jie-mi/image-20201221160546890.png" alt="所有的" style="zoom:67%;"><img src="/wang-luo-jie-gou-jie-mi/image-20201221160601788.png" alt="看起来就像大的在小的上滑动" style="zoom:67%;"><p><img src="/wang-luo-jie-gou-jie-mi/image-20201221160626802.png" alt="如图"></p><p><strong>转置卷积公式</strong></p><h4 id="空洞-扩张卷积"><a href="#空洞-扩张卷积" class="headerlink" title="空洞/扩张卷积"></a>空洞/扩张卷积</h4><p><a href="https://blog.csdn.net/m0_37644085/article/details/89480326">空洞卷积总结</a></p><p><img src="/wang-luo-jie-gou-jie-mi/dilation.gif" alt="dilation"></p><img src="/wang-luo-jie-gou-jie-mi/image-20210311181332266.png" alt="空洞卷积增加感受野" style="zoom:67%;"><p>扩张卷积增加了感受野，而不增加核的大小,<strong>因为中间插入的是空格，需要训练的卷积核参数量是不变的</strong>。</p><p>卷积核大小为k,dilation_rate=d,扩张后的卷积大小如下:<br>$$<br>\hat{k} = k + (k − 1)(d − 1)<br>$$<br>则<br>$$<br>o = \lfloor\frac{i+2p-\hat{k}}{s}\rfloor+1<br>$$<br><strong>空洞卷积的作用</strong></p><ul><li><strong>扩大感受野</strong>：在deep net中为了增加感受野且降低计算量，总要进行降采样(pooling或s2/conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。</li><li><strong>捕获多尺度上下文信息：</strong>空洞卷积有一个参数可以设置dilation rate，具体含义就是在卷积核中填充dilation rate-1个0，因此，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。<strong>多尺度信息在视觉任务中相当重要啊。</strong></li><li>ps: 空洞卷积虽然有这么多优点，但在实际中不好优化，速度会大大折扣。</li></ul><p><strong>空洞卷积gridding问题</strong></p><ul><li><strong>局部信息丢失</strong>：由于空洞卷积的计算方式类似于棋盘格式，某一层得到的卷积结果，来自上一层的独立的集合，没有相互依赖，因此该层的卷积结果之间没有相关性，即局部信息丢失。</li><li><strong>远距离获取的信息没有相关性</strong>：由于空洞卷积稀疏的采样输入信号，使得远距离卷积得到的信息之间没有相关性，影响分类结果。</li></ul><h4 id="1x1卷积-Network-in-Network"><a href="#1x1卷积-Network-in-Network" class="headerlink" title="1x1卷积/Network in Network"></a>1x1卷积/Network in Network</h4><p><a href="https://zhuanlan.zhihu.com/p/35814486">这篇写的好啊，是我太笨了，不带例子就老觉得读不透彻</a></p><p><strong>feature map是卷积核卷出来的，一个卷积核卷出来一个feature，所有feature构成featuremap，feature map数量就是channels</strong></p><p><strong>作用：</strong></p><ol><li><strong>进行卷积核通道数的降维和升维</strong></li><li><strong>实现跨通道的交互和信息整合</strong></li><li><strong>增加非线性特性</strong></li></ol><p><strong>Inception中：</strong></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201222151157720.png" alt="加入1x1"></p><p>原图feature map:28x28x192  1x1 channel:64 3x3 channel:128 5x5 channel:32</p><p>左:$192 × (1×1×64) +192 × (3×3×128) + 192 × (5×5×32) = 387072$ </p><p>右:$192 × (1×1×64) +（192×1×1×96+ 96 × 3×3×128）+（192×1×1×16+16×5×5×32）= 157184$</p><p>对于右边的池化层，原始的不能降channel，然后就会越来越多，这样也可以用1x1降channel</p><p><strong>ResNet：</strong></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201222154809733.png" alt="ResNet"></p><p>假设输入输出都是256 channel,参数真的差太多了！</p><p>左:$3 × 3 ×256×256×2=1179648$</p><p>右:$ 1×1×256×64 + 3×3×64×64 + 1×1×64×256 = 69632$</p><p><strong>全连接层角度：</strong></p><img src="/wang-luo-jie-gou-jie-mi/image-20201222155618108.png" alt="全连接层角度" style="zoom:67%;"><p>原channel 6,之后channel 5,只需要一个1x1x5的卷积核就行，参数很少，FC至少$w_0\cdot h_0 \cdot 6\cdot w_1\cdot h_1\cdot 5$</p><blockquote><p><em>In Convolutional Nets, there is no such thing as “fully-connected layers”. There are only convolution layers with 1x1 convolution kernels and a full connection table-Yann LeCun</em></p></blockquote><h4 id="卷积的反向传播"><a href="#卷积的反向传播" class="headerlink" title="卷积的反向传播"></a>卷积的反向传播</h4><h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>池化一般不涉及零填充<br>$$<br>o=\lfloor\frac{i-k}{s}\rfloor+1<br>$$</p><p><strong>图像中的相邻像素倾向于具有相似的值，因此通常卷积层相邻的输出像素也具有相似的值</strong>。这意味着，卷积层输出中包含的大部分信息都是<strong>冗余</strong>的。</p><p>如果我们使用边缘检测滤波器并在某个位置找到强边缘，那么我们也可能会在距离这个像素1个偏移的位置找到相对较强的边缘。但是它们都一样是边缘，我们并没有找到任何新东西。</p><p>池化层解决了这个问题。这个网络层所做的就是通过<strong>减小输入的大小降低输出值的数量</strong>。</p><p>池化一般通过<strong>简单的最大值、最小值或平均值操作完成</strong>。</p><p><strong>最大池化可以提取特征纹理</strong></p><p><strong>平均池化可以保留背景信息。</strong></p><p><a href="https://firstai.blog.csdn.net/article/details/105299448">综述:最大池化，平均池化，全局最大池化和全局平均池化？区别原来是这样</a></p><h4 id="池化的反向传播"><a href="#池化的反向传播" class="headerlink" title="池化的反向传播"></a>池化的反向传播</h4><p>池化层一般对应在卷积层的后面，属于一对一的关系（它只影响当前深度的一个节点），不与其它卷积核做连接，所以没有权重参数需要学习。所以反向传播的时候，只需对输入参数求导，不需要进行权值更新。</p><p>对于最大池化和平均池化，也有不同:<a href="https://blog.csdn.net/kieven2008/article/details/81603994">教程</a></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20210303003004886.png" alt="最大池化"></p><p><img src="/wang-luo-jie-gou-jie-mi/image-20210303003024968.png" alt="平均池化"></p><h3 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h3><p><a href="https://blog.csdn.net/qq_34919792/article/details/102697817">上采样方法总结</a></p><p>三种方法：</p><ol><li>基于<strong>线性插值</strong>的上采样</li><li>基于<strong>深度学习</strong>的上采样(转置卷积)</li><li><strong>Unpooling</strong>的方法 简单的补0或扩充</li></ol><h4 id="线性插值"><a href="#线性插值" class="headerlink" title="线性插值"></a>线性插值</h4><p><strong>双线性插值？</strong></p><p>应用：</p><ol><li>对数据中的缺失进行合理补偿 </li><li>对数据进行放大或缩小</li><li>其他</li></ol><p><a href="https://www.cnblogs.com/linkr/p/3630902.html">通俗易懂啊</a></p><p><a href="https://www.cnblogs.com/yssongest/p/5303151.html">图像插值的举例说明</a></p><p>根据下式计算目标像素在源图像中的位置：<br>$$<br>srcX=dstX\cdot \frac{srcWidth}{dstWidth}\<br>srcY=dstY\cdot \frac{srcHeight}{dstHeight}<br>$$<br>单线性插值：单变量只有一个x，对y进行插值，线性插值原理是两个点可以确定一条直线，又可以根据x确定y<br>$$<br>\frac{y-y_0}{x-x_0}=\frac{y_1-y_0}{x_1-x_0}\<br>y=\frac{x_1-x}{x_1-x_0}y_0+\frac{x-x_0}{x_1-x_0}y_1\<br>y=k\cdot y0+(1-k)\cdot y_1 \quad \quad<br>$$</p><p>双线性插值：有两个变量x、y，分别在x、y两个方向进行插值</p><p>对于一个目的像素，设置坐标通过反向变换得到的浮点坐标为$(i+u,j+v)$(其中$i、j$均为浮点坐标的整数部分，$u、v$为浮点坐标的小数部分，是取值[0,1)区间的浮点数)，则这个像素得值$f(i+u,j+v)$可由原图像中坐标为$(i,j)(i+1,j)(i,j+1)(i+1,j+1)$所对应的周围四个像素的值决定，即:$f(i+u,j+v) = (1-u)(1-v)f(i,j) + (1-u)vf(i,j+1) + u(1-v)f(i+1,j) + uvf(i+1,j+1)$,<strong>在哪个方向离得近，比如距离是0.3，权重大，是0.7</strong></p><p>现在假如目标图的象素坐标为$(1,1)$，那么反推得到的对应于源图的坐标是$(0.75,0.75)$, 这其实只是一个概念上的虚拟象素,实际在源图中并不存在这样一个象素,那么目标图的象素$（1,1）$的取值不能够由这个虚拟象素来决定，而只能由源图的这四个象素共同决定：$(0,0)(0,1)(1，0)(1,1)$，而由于$(0.75,0.75)$离$(1,1)$要更近一些，那么$(1,1)$所起的决定作用更大一些，这从公式1中的系数$uv=0.75×0.75$就可以体现出来，而$(1,1)$离$(0.75,0.75)$最远，所以$(0,0)$所起的决定作用就要小一些，公式中系数为$(1-u)(1-v)=0.25×0.25$也体现出了这一特点。</p><p><img src="/wang-luo-jie-gou-jie-mi/image-20201120155633758.png" alt="很棒！"></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true">#矩阵运算最简单，所以最重要的是坐标如何对应！</span>a<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.75</span><span class="token punctuation">,</span><span class="token number">0.25</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#注意2和3位置反过来了</span>            <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>c<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.25</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token punctuation">[</span><span class="token number">0.75</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>d<span class="token operator">=</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span>e<span class="token operator">=</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>d<span class="token punctuation">,</span>c<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">>></span><span class="token operator">></span> input <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> inputtensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># align_corners=False</span><span class="token operator">>></span><span class="token operator">></span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.2500</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1.5000</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">2.2500</span><span class="token punctuation">,</span>  <span class="token number">2.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">2.5000</span><span class="token punctuation">,</span>  <span class="token number">2.7500</span><span class="token punctuation">,</span>  <span class="token number">3.2500</span><span class="token punctuation">,</span>  <span class="token number">3.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.2500</span><span class="token punctuation">,</span>  <span class="token number">3.7500</span><span class="token punctuation">,</span>  <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.3333</span><span class="token punctuation">,</span>  <span class="token number">1.6667</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1.6667</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">,</span>  <span class="token number">2.3333</span><span class="token punctuation">,</span>  <span class="token number">2.6667</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">2.3333</span><span class="token punctuation">,</span>  <span class="token number">2.6667</span><span class="token punctuation">,</span>  <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.3333</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.3333</span><span class="token punctuation">,</span>  <span class="token number">3.6667</span><span class="token punctuation">,</span>  <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span></code></pre><p><img src="/wang-luo-jie-gou-jie-mi/image-20201222100731710.png" alt="两种方式"></p><p>第一种，四个角对齐，均匀插值,公式：<br>$$<br>scale = (input_size-1)/(output_size-1)\<br>srcIndex=scale\cdot dstIndex<br>$$<br>第二种，不均匀，出界的只和有原坐标的算，公式：<br>$$<br>scale=input_size/output_size\<br>srcIndex=scale\cdot (dstIndex+0.5)-0.5<br>$$</p><h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><p><strong>PixelShuffle</strong></p><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p><a href="https://www.cnblogs.com/guoyaohua/p/8724433.html">BN教程</a></p><p>激活之前进行BN，增加了两个学习参数scale和shift，再从线性变到非线性</p><p>有轻微的正则化作用，因为使用mint-batch，不同的mini-bath不同，所以会有一些噪音，所以有轻微的正则化效果</p><p>测试时逐样本处理，batch norm从之前训练时的$\mu$和$\sigma$做指数加权平均，方式很多，反正是从训练集得到的</p><h3 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h3><p><a href="https://zhuanlan.zhihu.com/p/35005794">吴育昕 何恺明的GN教程</a></p><ol><li>BN依赖于batch size，batch size较小时，BN效果不好，有些任务往往batch size只有1-2</li><li>训练，验证，测试这三个阶段存在inconsistency。</li></ol><p>Layer Norm和Instance Norm就是Group Norm的特例，归一化避开了batch</p><p>为什么工作？个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。</p><h3 id="CNN的反向传播"><a href="#CNN的反向传播" class="headerlink" title="CNN的反向传播"></a>CNN的反向传播</h3><p>教程:<a href="https://zhuanlan.zhihu.com/p/61898234">https://zhuanlan.zhihu.com/p/61898234</a></p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p><a href="http://kakuguo.ink/2020-03-18-Summary-of-Computer-Vision-Attention/">计算机视觉中的注意力机制</a></p><p><a href="https://zhuanlan.zhihu.com/p/54491016">自然语言处理中的注意力机制</a></p><h2 id="CRF-Loss"><a href="#CRF-Loss" class="headerlink" title="CRF Loss"></a>CRF Loss</h2><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>On Regularized Losses for Weakly-supervised CNN Segmentation Meng</p><p>18ECCV</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>keywords:<a href="https://zh.wikipedia.org/zh-hans/%E6%AD%A3%E5%88%99%E5%8C%96_(%E6%95%B0%E5%AD%A6)#:~:text=%E5%9C%A8%E6%95%B0%E5%AD%A6%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6,%E5%8A%A0%E5%9C%A8%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%BD%93%E4%B8%AD%E3%80%82">Regularization</a>  Semi-supervised Learning CNN Segmentation</p><h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p><a href="https://blog.csdn.net/u014380165/article/details/75142664">https://blog.csdn.net/u014380165/article/details/75142664</a></p><p><a href="https://zhuanlan.zhihu.com/p/43057737">https://zhuanlan.zhihu.com/p/43057737</a></p><h2 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h2><p><a href="https://zhuanlan.zhihu.com/p/36878362">https://zhuanlan.zhihu.com/p/36878362</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人像肤色检测-2</title>
      <link href="ren-xiang-pi-fu-jian-ce-2/"/>
      <url>ren-xiang-pi-fu-jian-ce-2/</url>
      
        <content type="html"><![CDATA[<h3 id="Fair-comparison-of-skin-detection-approaches-on-publicly-available-datasets"><a href="#Fair-comparison-of-skin-detection-approaches-on-publicly-available-datasets" class="headerlink" title="Fair comparison of skin detection approaches on publicly available datasets"></a>Fair comparison of skin detection approaches on publicly available datasets</h3><h4 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h4><p>年份：2020</p><p>期刊：EXPERT SYSTEMS WITH APPLICATIONS  JCR分区Q1</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>提出了一个公平比较方法，使用几个不同的数据集</p><p>主要贡献：</p><ol><li>肤色检测方法详尽介绍和一个公平比较方法</li><li>数据集收集和检查</li><li>一个评估和结合不同皮肤检测方法的框架</li><li>集成</li></ol><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>DeepLabv3 +,根据我们的实验是表现最好的独立方法</p><p>预训练、微调和集成都是有效的</p><p><strong>现在皮肤分割的最大问题就是没有数据集，建议收集一个不同地区的人的大数据集</strong></p><p>皮肤颜色检测器-&gt;颜色恒常性的预处理-&gt;形态学算子的后处理</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>用于区分皮肤和非皮肤像素的有用特征是像素颜色;然而，在不同光照、不同种族和不同采集设备下获得肤色一致性是一项非常具有挑战性的任务。此外，皮肤检测作为其他应用程序的初步步骤，需要计算效率高，不受几何变换、局部遮挡或姿态/面部表情变化的影响，对复杂或伪皮肤背景不敏感，对采集设备的质量具有鲁棒性。</p><p>较近的综述就这三篇  和另外两篇比，这篇主要提供公平比较方法</p><p>现有的最近的深度学习：</p><ul><li>Patch-wise skin segmentation of human body parts via deep neural networks 15 JEI</li><li>Combining Convolutional and Recurrent Neural Networks for Human Skin Detection 17 SPL</li><li>Human Skin Segmentation Using Fully Convolutional Neural Networks 18GCCE</li></ul><h4 id="Skin-detection-approaches"><a href="#Skin-detection-approaches" class="headerlink" title="Skin detection approaches"></a>Skin detection approaches</h4><p>影响因素：</p><ol><li>Human characteristics as ethnicity and age</li><li>Acquisition conditions</li><li>Skin painting</li><li>Complex background</li></ol><p>皮肤检测方法分类：</p><ol><li>考虑是否存在预处理步骤，如色彩校正和照明取消或动态适应，以减少不同获取条件的影响</li><li>考虑用于像素分类的颜色空间 A survey of skin-color modeling and detection methods 07PR                                                         <strong>basic models</strong> (i.e. RGB, normalized RGB), <strong>perceptual models</strong> (i.e. HIS, HSV) <strong>perceptual uniform models</strong> (i.e. CIE-Lab, CIE-Luv) and <strong>orthogonal models</strong> (i.e. YCbCr, YIQ) with the finding that orthogonal models are characterized by a reduced redundancy/correlation among channels, therefore they are the most suited for skin detection</li><li>Examining the problem formulation：图像分割出皮肤存在的区域/基于分隔和分类像素而不考虑邻居/基于像素      基于区域的论文很少，最近的一些卷积神经网络也可以看做基于分割这一类</li><li>执行像素分类的不同方法。1基于规则 2基于使用参数化或非参数化方法的机器学习方法估计颜色分布</li><li>机器学习分类器的不同方法  8种</li></ol><p>最近的两个主方向：</p><ul><li>对于一些应用，背景很容易区分，简单的基于规则的方法就可以，这种往往作为别的复杂任务的一个步骤；然后列举了一系列方法 15年名为SKN的新颜色空间</li><li>神经网络的</li></ul><p>12种基本方法：</p><ul><li>Statistical color models with application to skin detection 2002IJCV   GMM Bayes</li><li>Detector adaptation by maximising agreement between independent data sources 2007CVPR SPL 基于像素</li><li>Cheddad</li><li>Chen</li><li>SA1 SA2 SA3</li><li>DYC</li><li>SegNet</li><li>U-Net</li><li>DeepLab Deeplabv3+  ResNet50预训练 batch size32 学习率0.001 最大epoch30</li></ul><p><strong>重点看TABLE1</strong></p><h4 id="Skin-detection-evaluation-Datasets-and-performance-indicators"><a href="#Skin-detection-evaluation-Datasets-and-performance-indicators" class="headerlink" title="Skin detection evaluation: Datasets and performance indicators"></a>Skin detection evaluation: Datasets and performance indicators</h4><p><strong>重点看TABLE2</strong></p><p>SDD 2015 21000张 精确 未开源</p><p>HGR 2014 1558张  手势图片和皮肤mask  精确 开源</p><p>SFA 2013 1118 张 中等精度 开源</p><p>VDM 2013 285张 精确 人类活动识别+光照广泛 开源</p><p>Pratheepan 2012 78张 精确 简单背景+复杂背景 开源</p><p>Feeval 2009 8991张 不精确质量低但多 开源</p><p>Schmugge 2007 845张 精确 三类 开源</p><p>ECU 2005 4000张 精确 未开源</p><h4 id="A-fair-experimental-comparison"><a href="#A-fair-experimental-comparison" class="headerlink" title="A fair experimental comparison"></a>A fair experimental comparison</h4><p><strong>重点看TABLE3、4</strong></p><h3 id="A-survey-on-skin-detection-in-colored-images"><a href="#A-survey-on-skin-detection-in-colored-images" class="headerlink" title="A survey on skin detection in colored images"></a>A survey on skin detection in colored images</h3><h4 id="Info-1"><a href="#Info-1" class="headerlink" title="Info"></a>Info</h4><p>年份：2018</p><p>期刊：ARTIFICIAL INTELLIGENCE REVIEW JCR分区Q1</p><h3 id="A-Comprehensive-Survey-on-Human-Skin-Detection"><a href="#A-Comprehensive-Survey-on-Human-Skin-Detection" class="headerlink" title="A Comprehensive Survey on Human Skin Detection"></a>A Comprehensive Survey on Human Skin Detection</h3><h4 id="Info-2"><a href="#Info-2" class="headerlink" title="Info"></a>Info</h4><p>2016 IJIGSP</p><h4 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h4><h3 id="Real-time-Segmentation-and-Facial-Skin-Tones-Grading"><a href="#Real-time-Segmentation-and-Facial-Skin-Tones-Grading" class="headerlink" title="Real-time Segmentation and Facial Skin Tones Grading"></a>Real-time Segmentation and Facial Skin Tones Grading</h3><h3 id="Info-3"><a href="#Info-3" class="headerlink" title="Info"></a>Info</h3><p>CVPR</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>毛发和面部皮肤分割方法，DCNN，</p>]]></content>
      
      
      <categories>
          
          <category> 肤色分级 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 肤色分级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semantic Segmentation</title>
      <link href="semantic-segmentation/"/>
      <url>semantic-segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p><a href="https://cloud.tencent.com/developer/article/1589733">https://cloud.tencent.com/developer/article/1589733</a></p><p><a href="https://blog.csdn.net/ShuqiaoS/article/details/87360693">https://blog.csdn.net/ShuqiaoS/article/details/87360693</a></p><h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h2><p>知乎教程：<a href="https://zhuanlan.zhihu.com/p/22976342?utm_source=tuicool&amp;utm_medium=referral">https://zhuanlan.zhihu.com/p/22976342?utm_source=tuicool&amp;utm_medium=referral</a></p><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>15 CVPR 17TPAMI</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ol><li>不含全连接层(fc)的全卷积(fully conv)网络。可适应任意尺寸输入。将现代分类网络<strong>AlexNet、VGGNet、GoogLeNet</strong>改造为FCN，并微调</li><li>增大数据尺寸的反卷积deconv层。能够输出精细的结果</li><li>结合不同深度层结果的跳级(skip)结构。同时确保鲁棒性和精准性</li></ol><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><strong>语义分割面临语义和位置之间固有的紧张关系:全局信息解决what问题，而局部信息解决where问题。</strong><br>深度特征层次将位置和语义编码在一个非线性的local-to-global的金字塔中。在4.2节中，定义了一个skip架构来利用这个结合了深层、粗糙的语义信息和浅层、精细的外观信息的特征。</p><h3 id="FCN-1"><a href="#FCN-1" class="headerlink" title="FCN"></a>FCN</h3><p>卷积网络是建立在平移不变性的基础上的。它们的基本成分(卷积、池化和激活函数)作用于局部输入区域，仅依赖于相对的空间坐标。</p><h4 id="3-1-Adapting-classifiers"><a href="#3-1-Adapting-classifiers" class="headerlink" title="3.1. Adapting classifiers"></a>3.1. Adapting classifiers</h4><img src="/semantic-segmentation/image-20210324164018584.png" alt="转变" style="zoom:80%;"><p>直接生成heatmap,再加损失</p><h2 id="UNet"><a href="#UNet" class="headerlink" title="UNet"></a>UNet</h2><p><a href="https://www.cnblogs.com/PythonLearner/p/14041874.html">UNet详解</a>  最常用….最简单…  一种<strong>U型的网络结构来获取上下文的信息和位置信息</strong></p><p><a href="https://zhuanlan.zhihu.com/p/46251798?edition=yidianzixun&amp;utm_source=yidianzixun&amp;yidian_docid=0KDRTjYB">知乎这篇讲的非常好</a></p><p><strong>前半部分</strong>是<strong>特征提取</strong>部分，<strong>后半部分</strong>是<strong>上采样</strong>,有些文献也把这种结构叫做<strong>编码器-解码器</strong>结构。</p><p>copy and crop：在论文中叫拼接，在UNet有四个拼接操作。如上图所示：有人也叫Skip connect,这一操作的目的是为了<strong>融合特征信息，使深层和浅层的信息融合起来</strong>，在拼接的时候要注意，<strong>不仅图片大小要一致（故要crop,是为了使图片大小一致）</strong>而且<strong>特征的维度（channels）也要一样</strong>，才可以拼接。</p><p><strong>Unet的好处我感觉是：网络层越深得到的特征图，有着更大的视野域，浅层卷积关注纹理特征，深层网络关注本质的那种特征，所以深层浅层特征都是有格子的意义的；另外一点是通过反卷积得到的更大的尺寸的特征图的边缘，是缺少信息的，毕竟每一次下采样提炼特征的同时，也必然会损失一些边缘特征，而失去的特征并不能从上采样中找回，因此通过特征的拼接，来实现边缘特征的一个找回。</strong></p><ol><li><strong>医疗影像的所有特征都很重要，因此低级特征和高级语义特征都很重要，所以U型结构的skip connection结构（特征拼接）更好派上用场</strong></li><li>医学影像数据较少，大网络容易过拟合</li><li><strong>医学影像任务中，往往需要自己设计网络去提取不同的模态特征，因此轻量结构简单的Unet可以有更大的操作空间</strong></li></ol><h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><p>卷积网络的典型用途是分类任务，<strong>其中图像的输出是单个类别标签。</strong> 然而，在许多视觉任务中，<strong>尤其是在生物医学图像处理中，期望的输出应该包括定位，即，应该将类标签分配给每个像素。（也就是分割）</strong></p><p><strong>在U-Net结构中，包括一个捕获上下文信息的收缩路径和一个允许精确定位的对称拓展路径</strong></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>首先要<strong>像素级预测</strong>，其次<strong>训练图像没那么多</strong>，然后<strong>运行要快</strong>，最后<strong>上下文信息和局部位置信息的权衡</strong>。</p><p><a href="https://www.bilibili.com/read/cv8291595">Overlap-tile 重叠-切片</a></p><p>UNet并不是一个完全对称的结构，因为使用的是<strong>不带padding的3x3卷积</strong></p><p><strong>如果想保持尺寸一致该如何做？</strong></p><ol><li>使用插值，插值是不可学习的，会带来一定的误差</li><li>使用转置卷积，会增加参数量，并且模型也不一定能学的很好</li><li>same卷积，即使用padding，但是padding会引入误差，而且模型越深层得到的feature map抽象程度越高，收到padding的影响会呈累积效应 因为填充是0</li><li>Overlap-tile  对称的肯定比0要好 通常需要将图像进行分块的时候才使用</li></ol><p>图像的主要成分是<strong>低频信息</strong>，它形成了<strong>图像的基本灰度等级</strong>，对图像结构的决定作用较小；<strong>中频信息决定了图像的基本结构</strong>，形成了图像的<strong>主要边缘结构</strong>；<strong>高频信息形成了图像的边缘和细节</strong>，是在中频信息上对图像内容的进一步强化。</p><p><a href="https://blog.csdn.net/weixin_38208741/article/details/79823681">图像的高频信息和低频信息</a>简单来说就是，高频就是变化多的细节，低频就是背景板。</p><p>UNet使用<strong>弹性变形</strong>来进行<strong>数据扩充</strong>,这可以使得网络学习到这种<strong>不变性invariance</strong>。</p><p>使用<strong>加权损失</strong>，迫使网络学习<strong>边界像素</strong>。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>由于<strong>没有填充的卷积，输出图像要比输入图像小一个恒定的边界宽度</strong>。</p><p>为了最小化开销并最大限度地利用GPU内存，偏爱大的输入块而不是大的批处理大小，因此将批处理减少到单个图像。</p><p>因此，使用一个高动量(0.99)，以便在当前优化步骤中使用大量之前看到的训练样本来确定更新。</p><h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><p>对于显微镜图像，首先需要<strong>平移和旋转不变性</strong>，以及对<strong>变形和灰度值变化的鲁棒性</strong>。特别是训练样本的<strong>随机变形</strong>似乎是训练带有少量标注图像的分割网络的关键概念。我们在3×3网格上使用<strong>随机位移矢量</strong>生成平滑变形。位移从一个<strong>10像素标准差的高斯分布中采样</strong>。然后使用<strong>双三次插值计算像素位移</strong>。缩路径的最后加入了<strong>Dropout</strong>，隐式的加强了数据增强。</p><h2 id="DeeplabV1"><a href="#DeeplabV1" class="headerlink" title="DeeplabV1"></a>DeeplabV1</h2><h3 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="DeeplabV3"><a href="#DeeplabV3" class="headerlink" title="DeeplabV3+"></a>DeeplabV3+</h2><h3 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h3><h2 id="HDNet"><a href="#HDNet" class="headerlink" title="HDNet"></a>HDNet</h2><h3 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h3><p>目前方法分离了<strong>特征图中点之间的关系</strong>，导致分割结果不连续</p><p>本论文提出混合距离网络，从两个方面来度量距离–位置距离和高维特征距离</p><p>在此基础上，提出了一种位置感知注意力模块，利用位置距离对上下文进行有效采样，生成稀疏混合距离关系。<br>它综合每个点的不同上下文，并生成position-wise的注意值，以紧凑的对象级表示。<br>在训练步骤中，高维特征距离损失也作为在特征空间中压缩类别级表示的辅助损失。</p><p>SOTA in Pascal Context, ADE20K, and COCO Stuff 10K</p><h3 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h3><p>ACNet[6] ICCV 2019 证明卷积核主要集中在中心点上，所以滑动窗口过程会产生每个点的隔离。这种隔离会干扰特征学习，导致分割结果的不连续，称为“<strong>隔离问题</strong>”。同时语义分割网络采用逐像素的交叉熵作为损失函数，对于相邻的像素点的监督是独立的，因此该损失函数无法有效地惩罚不连续的分割结果，这 加剧了分割结果中的孤立问题。</p><p>为了解决像素的孤立问题，许多研究在神经网络结构中加入上下文模块，通过在空间维度捕获像素点的上下文信息，增大像素的感受野，缓解像素孤立的问题。</p><ul><li>DeeplabV1  空洞卷积扩大感受野  <strong>但仍将点与全局场景上下文分隔开</strong></li><li>PSPNet、deeplabv2中进一步提多尺度的金字塔模型获得更大范围、更丰富的上下文信息。                                                                                                      <strong>PSPNet    金字塔池化模块PPM      两个点位于同一个池化窗口，产生相同但不准确的上下文信息  两个点相邻位于不同池化窗口，获得完全不同的上下文信息。</strong></li></ul><img src="/semantic-segmentation/image-20210228030113655.png" alt="PSPNet" style="zoom:80%;"><p>使用不同窗口大小的pooling操作，得到不同尺寸的输出，然后缩放到相同的尺寸，再进行特征融合。</p><p>具体来说，包含4个pooling层次，对于原始特征图，分别通过pooling得到大小为 <img src="https://www.zhihu.com/equation?tex=1%5Ctimes1" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=2%5Ctimes2" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=3%5Ctimes3" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=6%5Ctimes6" alt="[公式]"> 。然后分别使用1×1卷积调整通道数至 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D" alt="[公式]"> （ <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 即为pooling层次数，此时 <img src="https://www.zhihu.com/equation?tex=N=4" alt="[公式]"> ）。然后将这些特征图全部上采样，通过双线性插值完成。再将这些pooling特征图输出，以及原始的特征图（跳跃连接），全部concat起来，作为特征输出，可以由此产生分割的预测结果。</p><p>实现过程中，这些pooling层通过AdaptivePooling层完成，根据输入输出大小计算pooling的窗口大小。其中输出大小为 <img src="https://www.zhihu.com/equation?tex=1%5Ctimes1" alt="[公式]"> 时，其实就是Global Average Pooling。</p><ul><li>Non-Local   local operations就是基于局部区域进行操作，长距离依赖则是图像中非相邻像素点之间的关系。该论文中的<strong>非局部操作是将所有位置对一个位置的特征加权和作为该位置的响应值。使用相同策略，远程和短程背景起相同作用，这种关系表示忽略了一对像素点点之间的位置距离的影响，也失去了对象的整体概念 该方法不能解决对象内部的隔离问题，且导致大量的冗余计算</strong> 。</li></ul><p><img src="/semantic-segmentation/image-20210228004156931.png" alt="Non Local公式"></p><p>上述方法在捕捉上下文信息时丢弃了空间信息，这削弱了它们的分割精度。<strong>HDR 结合位置距离和高维特征距离  位置距离表示特征图上一对点之间的相对位置，而高维特征距离表示两个特征向量之间的相似性</strong>。HDR建模在一定范围内的一个点和它的上下文区域之间的关系，位置距离隐含在区域的范围内。</p><p>其次，提出<strong>位置感知注意模块LAA</strong>，对HDR的不同位置距离进行采样，从而捕获点与其不同上下文之间的关系。</p><p>然后由这些HDRs生成一个基于位置的注意值，以融合不同的关系，补充原有的特征图。</p><p>LAA模块引入<strong>稀疏关系连接来关注高维空间中特征的位置距离</strong>，而不是使用以前的方法[12DANet ,13 CCNet]中<strong>每对特征之间的密集连接，这样消耗的计算量较少</strong>。该算法能够<strong>感知物体在中心位置周围的空间关系，提高了分割结果的连续性，降低了计算复杂度</strong>。利用这一特点，我们的方法在中心对象上的特征分布明显比基于非局部的方法更紧凑</p><p>DANet：基于空洞卷积的FCN添加了两种注意力模块：position attention module和channel attention module</p><p>CCNet:Criss-Cross Attention 以十字形交叉的   对Non Local的一种改进，减少计算量</p><img src="/semantic-segmentation/image-20210228023222192.png" alt="CCNet" style="zoom:67%;"><img src="/semantic-segmentation/image-20210228023250154.png" alt="网络结构图image-20210228023250154" style="zoom:67%;"><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>近年来出现许多用于捕获上下文信息的模块</p><h4 id="Spatial-Aggregation-Module空间聚集模块"><a href="#Spatial-Aggregation-Module空间聚集模块" class="headerlink" title="Spatial Aggregation Module空间聚集模块"></a>Spatial Aggregation Module空间聚集模块</h4><p>Deeplab V1:空洞卷积</p><p>Deeplab V2:<strong>atrous spatial pyramid pooling (ASPP)</strong> 空洞空间金字塔池化 与<strong>SPPNet空间金字塔池化网络</strong>中的SPP模块类似，<strong>在给定的输入上以不同采样率的空洞卷积并行采样，相当于以多个比例捕捉图像的上下文</strong></p><img src="/semantic-segmentation/image-20210228025059270.png" alt="空间金字塔池化层" style="zoom:50%;"><img src="/semantic-segmentation/image-20210228025421137.png" alt="ASPP" style="zoom:50%;"><h4 id="Relation-Module-关系模块"><a href="#Relation-Module-关系模块" class="headerlink" title="Relation Module 关系模块"></a>Relation Module 关系模块</h4><p>CCNet:简化Non Local计算</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>三部分 1 HDR Hybrid distance relation  2 LAA   Location Aware Attention 3 HFD loss</p><p><img src="/semantic-segmentation/image-20210303013337764.png" alt="overview"></p><h4 id="1-HDR-Hybrid-Distance-Relation"><a href="#1-HDR-Hybrid-Distance-Relation" class="headerlink" title="1 HDR: Hybrid Distance Relation"></a>1 HDR: Hybrid Distance Relation</h4><p>目前的方法只考虑上下文信息的捕获，而不考虑特征的连续性，不能很好的解决隔离问题。</p><p>HDR的目的:1 产生连续结果 2 避免冗余计算</p><p>$R_k(i,j)=F(x_{i,j},A_k(x_{i,j}))$</p><p>$x_{i,j}$表示i,j处点的特征向量，$A_k(x_{i,j})$表示range为k的上下文计算得出的特征向量，$F(.,.)$计算两个特征向量之间的高维特征距离</p><p><strong>HDR的本质是两个特征向量之间的特征距离</strong>，每个HDR通过聚合函数$Ak(·)$使用采样后的特征图，使点感知周围区域。</p><p><strong>位置距离是指周围区域的范围。</strong></p><p>所以位置距离通过聚合函数$Ak(·)$嵌入到了HDR中，从而使所提出的HDR既考虑了高维特征距离，又考虑了位置距离。</p><p>随着range k的不同，如下:</p><p><img src="/semantic-segmentation/image-20210302222214915.png" alt="改变k的范围"></p><p>换句话说，<strong>中心点可以感知到在二维平面上由于不同的距离k到物体边界的空间距离。</strong></p><p>此外，HDR在捕捉不同k范围的中心点与其上下文区域之间的关系时，可可视化为稀疏连接；Non Local则是使用孤立的一对点之间的密集连接关系(图5a)。并且k为n和k为n-1时的差值代表kn和kn-1间的上下文，这种带孔的HDR增强了上下文的多样性。</p><p><img src="/semantic-segmentation/image-20210302223254591.png" alt="HDR的稀疏连接和带孔的HDR"></p><p><img src="/semantic-segmentation/image-20210302223538080.png" alt="F距离函数"></p><p>F可以简单实现为余弦距离，只关注了点的分布，忽略了点的特征值的大小，[-1,1]方便后续操作</p><p>为了简化聚合过程的计算，使用<strong>池化操作</strong>作为聚合函数$A_k(.)$,将k专门化为<strong>池化窗口的大小</strong>。因此，特定区域内的距离为<strong>正方形</strong>。</p><p>$Ak(·)$采用<strong>平均池化操作</strong>，考虑k范围内的所有点，对于大对象，无论窗口大小，响应一致。此外，我们还采用了<strong>最大池化操作</strong>，可以<strong>捕捉到显著特征</strong>。为了使每个点获得自己的聚合结果，将聚合函数设为位置级函数，步幅设为1。</p><p>由于k值不同，一串$A_k(.)$函数可以视为局部位置级金字塔。</p><p>和PPM的区别:</p><p>PPM是一个全局金字塔，通过这种下行采样策略，相邻的点可能会得到相同或不同的上下文信息，如下图:</p><p><img src="/semantic-segmentation/image-20210302225644534.png" alt="PPM模块"></p><p>这种全局的方式丢弃了特定位置，产生不精确的上下文信息。</p><p>本论文的HDR是每个点的局部金字塔，所以特征图大小为HxW，就有HxW个金字塔，每个点都有它自己的上下文。</p><p>ASPP[14]是另一个捕捉上下文信息的位置金字塔，因此可以将其表述为与HDRs中提出的聚合相同。但它继承了扩张卷积的缺点，即只对一个感受野中的几个点进行采样，而忽略了整个感受野。而池操作可以考虑整个字段，不会导致这种不连续的效果。</p><h4 id="2-LLA-Location-Aware-Attention"><a href="#2-LLA-Location-Aware-Attention" class="headerlink" title="2 LLA: Location Aware Attention"></a>2 LLA: Location Aware Attention</h4><p>LAA模块利用提出的HDR，获取不同范围的上下文信息，增强对对象大小的感知，增强对象级的连续性。<br>LAA有4个子模块:输入/输出变换、多范围HDRs、HDRs结果的交互、对原始feature map的激励</p><p><img src="/semantic-segmentation/image-20210302234656307.png" alt="LLA模块"></p><p><strong>变换子模块：</strong>$\theta$为1x1卷积，减少了信道数量。$\phi$也是1x1卷积，但是前后信道数目不变。</p><p>**交互子模块:**在每一点融合不同的关系，可以表示为:</p><p>$u_{i,j }= ω([R_{k_1}(i, j), R_{k_2}(i, j), · · · , R_{k_n} (i, j)]),$</p><p>不同的k表示不同的范围 ，ω(·)是一种不同关系的位置非线性融合。</p><p>**激励子模块: **      $v_{i,j }= u_{i,j} × x_{i,j}$</p><p><strong>最终结果:</strong>$y_{i,j} = x_{i,j} + ϕ(v_{i,j})$,可表述为位置方面的增强</p><p><img src="/semantic-segmentation/image-20210303001931862.png" alt="反向传播"></p><p>Channel Attention用的就是DANet中的。</p><p>基于稀疏连接和位置注意，我们的注意模块计算复杂度明显低于Non Local。其次，交互模块和激励模块补充了每个点的特征，增强了位置感知。</p><h4 id="3-HFD-High-dimension-Feature-Distance-Loss"><a href="#3-HFD-High-dimension-Feature-Distance-Loss" class="headerlink" title="3 HFD: High-dimension Feature Distance Loss"></a>3 HFD: High-dimension Feature Distance Loss</h4><img src="/semantic-segmentation/image-20210303005022704.png" alt="HFD loss" style="zoom: 80%;"><p>目前的网络[12DANet,13CCNet,18EMANet]使用头部结构来减少特征图的通道来对每个点进行分类，这是一种从高维特征到预测语义类别的有效概述，而潜在特征的监督仅依赖于梯度通过。<br>为了预测类别内的连续结果和类别间的分化结果，高维特征应保持相同的一致性和差异性。</p><p>HFD Loss:减少类别内点的特征距离的方差，增加类别间的方差</p><p>负对数似然(不就是交叉熵函数吗):</p><p>$L_{H,p} = −E_x[log\widetilde{D}p(x)]$</p><p>x为某一点特征，$\widetilde{D}p(x)$为某一点特征与第q类特征中心的归一化距离。与无监督聚类方法类似，HFD loss方法探索了每个类别的特征中心，并迫使每个点靠近相应的中心。为了得到点与类别中心的精确对应，使用ground truth对结果进行监督，这是与聚类方法的主要区别。</p><p><img src="/semantic-segmentation/image-20210303010956280.png" alt="归一化"></p><p>T维度为CxN，C为特征个数，就是每个点有多少个k，N为类别个数，整体是softmax归一化。</p><p><strong>look-up table：</strong></p><p>查找表T是根据生成的特征图对每个类别进行高维表示。因此，在训练过程中，随着特征图的变化，它会不断更新。</p><p>在更新过程中，采用与批处理归一化层相同的指数移动平均策略，通过小批量逼近实数表示。所以更新不需要梯度，而是依赖于输入，如图8中的黑色虚线，公式为:</p><p><img src="/semantic-segmentation/image-20210303012402882.png" alt="查找表T公式"></p><p>初始中心就是加起来求平均，之后采用移动平均策略。</p><p>提出的HFD Loss使得每个点的高维特征更接近ground truth所表示的特征中心Tp，直接促进了高维特征空间中类别的识别。本文提出的HFD Loss作为辅助Loss监督<strong>头部结构</strong>之前的高维特征图。而传统的交叉熵损失也通过最终的分割结果来监督HDNet。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>数据集:PASCAL Context ADE20K COCO Stuff 10K</p><h4 id="1-Implements-Details"><a href="#1-Implements-Details" class="headerlink" title="1 Implements Details"></a>1 Implements Details</h4><p><img src="/semantic-segmentation/image-20210303021340412.png" alt="全部损失"></p><p><img src="/semantic-segmentation/image-20210303022652697.png" alt="LAA"></p><h2 id="多尺度空洞卷积"><a href="#多尺度空洞卷积" class="headerlink" title="多尺度空洞卷积"></a>多尺度空洞卷积</h2><p><a href="https://blog.csdn.net/zxfhahaha/article/details/102478092">其他人的论文阅读笔记,很赞</a></p><h3 id="Abstract-5"><a href="#Abstract-5" class="headerlink" title="Abstract"></a>Abstract</h3><p>开发了一个新的卷积网络模块，专门用于密集预测。该模块<strong>使用空洞卷积来系统地聚合多尺度上下文信息而不丢失分辨率</strong>。<br>该架构基于<strong>空洞卷积支持感受野的指数扩展，而不会丧失分辨率或覆盖范围</strong>的事实。我们表明，所提出的上下文模块提高了最先进的语义分割系统的准确性。</p><h3 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h3><p>语义分割具有挑战性，因为它需要结合像素级精度和多尺度上下文推理。</p><p>FCN表明<strong>原本用于图像分类的卷积网络架构可以成功地用于密集预测</strong>。这些重新设计的网络在挑战语义分割基准上的表现大大超过了先前的技术水平。由于图像分类和密集预测的结构差异，这引发了新的问题。重新使用的网络的哪些方面是真正必要的，哪些方面在密集操作时降低了准确性?专为密集预测设计的专用模块能否进一步提高准确性?</p><p>现代图像分类网络通过连续的池化和子采样层集成多尺度上下文信息，从而降低分辨率，直到获得全局预测。相反，密集预测需要多尺度上下文推理与全分辨率输出相结合。</p><p>近年来研究了两种方法来解决<strong>多尺度推理和全分辨率密集预测</strong>的矛盾需求。</p><p>一种方法是反复进行上卷积，目的是恢复丢失的分辨率，同时从下采样层进行全局视角。这就留下了一个问题，即是否真的有必要进行严重的中间下采样。</p><p>例如15年ICCV的&lt;&lt;Learning deconvolution network for semantic segmentation&gt;&gt;</p><p><img src="/semantic-segmentation/image-20210310235530599.png" alt="DeConvNet"></p><p>Encoder-Decoder结构 采用了VGG16</p><p><a href="https://www.jianshu.com/p/6c09ecda592b">通俗教程1</a> <a href="https://blog.csdn.net/u010772289/article/details/69526178">详细教程2</a></p><p>产生足够多的候选区域，然后在每个候选区域用网络获得语义分割图(semantic segmentation maps),然后将所有区域的输入组合起来</p><p>另一种方法包括提供多个重新缩放版本的图像作为网络输入，并结合这些多个输入的预测。同样，目前还不清楚是否真的需要对重新缩放后的输入图像进行单独分析</p><p>在这项工作中，我们开发了一个卷积网络模块，在不丢失分辨率或分析重新缩放图像的情况下聚合多尺度上下文信息。该模块可以插入到任何分辨率的现有体系结构中。与从图像分类中继承过来的金字塔形架构不同，提出的上下文模块是专门为密集预测而设计的。该模块基于空洞卷积，支持感受野的指数扩展，而不会丢失分辨率或覆盖范围</p><h3 id="DILATED-CONVOLUTIONS"><a href="#DILATED-CONVOLUTIONS" class="headerlink" title="DILATED CONVOLUTIONS"></a>DILATED CONVOLUTIONS</h3><p>FCN虽然分析了filter dilation但是没有使用它，deeplabV1使用filter dilation简化了FCN的网络结构</p><img src="/semantic-segmentation/image-20210312013030003.png" alt="Figure1" style="zoom:67%;"><p>$F_{i+1}=F_i\cdot_{2^i}k_i,i=0,1,2\cdots$，则获得$(2^{i+2}-1)\cdot (2^{i+2}-1)$的感受野</p><img src="/semantic-segmentation/image-20210312014137893.png" alt="解析" style="zoom: 80%;"><ul><li>a图对应3x3的空洞率为1的卷积，和普通的卷积操作一样，计算量是9个点。对于(a)这个feature map F1而言，F1是由1-dilated convolution 卷积F0得来的，如果不考虑之前层的感受野，那这个卷积核的感受野大小是3x3，也就是F1的每个元素的感受野都是3x3。</li><li>b图对应3x3的空洞率为2的卷积，实际的计算量还是9个点。对于(b)这个feature map而言，(b)是对(a)空洞卷积而来的，卷积核覆盖的区域大小为5x5（图中蓝框），但是这个时候感受野大小并不是5x5，因为a中的元素的感受野就已经为3x3了，覆盖的5x5区域要往外多加（3-1）/2=1个像素，如(b)中红框所示。即1-dilated和2-dilated堆叠起来就能达到7x7的感受野，而普通卷积需要三层3x3的卷积层堆叠才能达到7x7的感受野。</li><li>c图对应3x3的空洞率为4的卷积，实际的计算量还是9个点。同理，对于©而言，卷积核覆盖的(b)区域大小为9x9（图中蓝框），但是由于(b)中的元素的感受野大小为7x7，因此，在这个9x9的区域大小之外还要扩张出（7-1）/2=3个像素，如©中红框所示。即1-dilated、2-dilated、4-dilated堆叠起来就能达到15x15的感受野。</li><li>注意,这个的stride是1</li></ul><h3 id="MULTI-SCALE-CONTEXT-AGGREGATION"><a href="#MULTI-SCALE-CONTEXT-AGGREGATION" class="headerlink" title="MULTI-SCALE CONTEXT AGGREGATION"></a>MULTI-SCALE CONTEXT AGGREGATION</h3><h4 id="network-architecture"><a href="#network-architecture" class="headerlink" title="network architecture"></a>network architecture</h4><p>上下文模块旨在通过聚合多尺度上下文信息来提高密集预测体系结构的性能。该模块接受C特征映射作为输入，并生成C特征映射作为输出。输入和输出具有相同的形式，因此该模块可以插入现有的密集预测体系结构中</p><p><img src="/semantic-segmentation/image-20210312025120461.png" alt="模块"></p><p>从小的局部特征,到大的特征</p><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>卷积网络通常使用随机分布的样本进行初始化。然而，我们发现随机初始化方案对上下文模块并不有效。我们发现一个具有清晰语义的替代初始化更有效:</p><p><img src="/semantic-segmentation/image-20210312031632482.png" alt="初始化公式"></p><p>这种identity初始化设置所有滤波器的值，这样每一层都能将前一层的信息直接传递到下一层。直觉上感到不利于反向传播信息的传递。但实验证明这种担心是多余的。<br>basic 的context module只有64C^2个参数，参数的数量非常少，但实验结果已经表现的非常好了</p><h3 id="Front-End"><a href="#Front-End" class="headerlink" title="Front-End"></a>Front-End</h3>]]></content>
      
      
      <categories>
          
          <category> 语义分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In-camera image processing pipeline</title>
      <link href="in-camera-image-processing-pipeline/"/>
      <url>in-camera-image-processing-pipeline/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在进行之前先把学姐的汇报弄明白</p><p>首先是raw图像格式：<a href="https://zhuanlan.zhihu.com/p/158088019">https://zhuanlan.zhihu.com/p/158088019</a></p><p>ISP大佬的教程:<a href="https://ridiqulous.com/process-raw-data-using-matlab-and-dcraw/#brightnesscorrection">https://ridiqulous.com/process-raw-data-using-matlab-and-dcraw/#brightnesscorrection</a></p><p><strong>ISP pipeline：</strong></p><p><strong>线性处理</strong>-&gt;<strong>处理黑电平和饱和像素</strong>-&gt;<strong>白平衡</strong>，与颜色恒常性的白平衡不一样，仅仅是对三通道乘以增益系数，以补偿因为三种滤波片具有不同光谱灵敏度带来的影响-&gt;<strong>Demosaicking去马赛克/色彩插值</strong>-&gt;<strong>色彩空间转换</strong> 传感器光谱空间到CIEXYZ到sRGB-&gt;<strong>亮度矫正和伽马矫正</strong></p><img src="/in-camera-image-processing-pipeline/image-20201119091331032.png" alt="颜色空间变换的归一化" style="zoom:67%;"><p><strong>亮度矫正与伽马矫正</strong>之前都是线性变换，因为其实都可以乘一个矩阵将它变换回去，而亮度矫正和伽马矫正是非线性的。如果用Lab空间来描述的话，我们好像只关心ab通道的准确，并不关心L通道，L调合适就行了？不同显示器的物理亮度是不一样的，追求亮度的绝对准确复现没有意义，何况最亮的显示器也远无法复现真实世界里的最大亮度。</p><p><strong>伽马矫正</strong>：</p><p>维基百科：<a href="https://zh.wikipedia.org/wiki/%E4%BC%BD%E7%91%AA%E6%A0%A1%E6%AD%A3">https://zh.wikipedia.org/wiki/%E4%BC%BD%E7%91%AA%E6%A0%A1%E6%AD%A3</a></p><p>LearnOpenGL，这篇讲的挺好：<a href="https://learnopengl-cn.readthedocs.io/zh/latest/05%20Advanced%20Lighting/02%20Gamma%20Correction/">https://learnopengl-cn.readthedocs.io/zh/latest/05%20Advanced%20Lighting/02%20Gamma%20Correction/</a></p><p>简书教程：<a href="https://www.jianshu.com/p/321f39b7fa93">https://www.jianshu.com/p/321f39b7fa93</a></p><p><strong>RGB色彩空间是设备相关的？</strong></p><p><strong>wiki百科色彩空间：</strong><a href="https://zh.wikipedia.org/zh-cn/%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%96%93#cite_note-1">https://zh.wikipedia.org/zh-cn/%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%96%93#cite_note-1</a></p><p>RGB的实现方法有<strong>每原色8位或每原色16位</strong>，实际的RAW image可能是12位或14位，但是为了存储将它扩充到16位。</p><p>每台设备（如显示器或打印机）都有自己的色彩空间并只能生成其色域内的颜色。将图像从某台设备移至另一台设备时，因为每台设备按照自己的色彩空间解释 RGB 或 CMYK 值，所以图像颜色可能会发生变化。为了保证图像在不同设备上显示效果一致，必须使用色彩管理</p><p><strong>颜色匹配实验</strong>：<a href="https://zhuanlan.zhihu.com/p/84897327">https://zhuanlan.zhihu.com/p/84897327</a></p><p><strong>设备无关的颜色空间</strong>：<a href="http://www.doho17.cn/News/507.html">http://www.doho17.cn/News/507.html</a></p><p><strong>与设备有关的颜色空间对应的颜色印象如何取决于生成颜色的设备</strong>。例如：在某台计算机显示器上显示的红色与另一台显示器上显示的红色极有可能不同，这是因为每台显示器根据自己的色彩空间解释色的参数。而RGB、CMYK颜色空间都是与设备有关的颜色模型。</p><p><strong>Lab颜色模型</strong>是由CIE（国际照明委员会）制定的一种色彩模式。自然界中任何一点色都可以在Lab空间中表达出来，它的色彩空间比RGB空间还要大。另外，这种模式是以数字化方式来描述人的视觉感应，与设备无关，所以它弥补了RGB和CMYK模式必须依赖于设备色乡特性的不足。</p><p>这个RGB是指什么？就比如每个摄像机根据自己设备的色彩空间，就是设备相关的。sRGB是设备无关的，暂时理解就是指现在很多显示器都是用sRGB，所以是相对设备无关？？？而CIEXYZ则是完完全全把所有的色域都包括了，它是一个参照量，由它在转换到各个依赖于设备的颜色空间</p><p><strong>颜色空间是什么？就是满足我们的设备我们的显示标准的一个颜色配置标准，比如在a颜色空间1是黄色，但是b颜色空间我们设置2是黄色，大概就是这个意思</strong></p>]]></content>
      
      
      <categories>
          
          <category> Color Constancy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 颜色空间 </tag>
            
            <tag> 相机成像 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人像肤色区域检测</title>
      <link href="ren-xiang-fu-se-jian-ce/"/>
      <url>ren-xiang-fu-se-jian-ce/</url>
      
        <content type="html"><![CDATA[<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>肤色区域检测是肤色定级的前提，只有正确的将图像中人体的肤色区域检测出来，才能探究人体皮肤的肤色感光差异以及分布规律，从而选择更合适的颜色空间并做出更好的划分。</p><p>皮肤检测任务方法类别？</p><p>指标较高的方法和模型？</p><p>肤色定级的不同颜色空间为啥如此定义研究的更深一点？–之后</p><p>学习阶段：现有的算法有哪些可以不全面 ？   可以分为几大类？    看综述怎样分类的？ 指标结果贴出来 </p><p>自己试验：一两个几个重要的算法复现了  前沿的？都没有的话复现最先进  拿数据集跑跑   </p><h3 id="Human-Skin-Detection-Using-RGB-HSV-and-YCbCr-Color-Models"><a href="#Human-Skin-Detection-Using-RGB-HSV-and-YCbCr-Color-Models" class="headerlink" title="Human Skin Detection Using RGB, HSV and YCbCr Color Models"></a>Human Skin Detection Using RGB, HSV and YCbCr Color Models</h3><h4 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h4><p>2017年 引用次数102 会议：ICCASP 怎么感觉查不到这个会议？</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>皮肤颜色具有对方向和大小不变性和处理速度快等特点，常用于人体皮肤检测中。提出了一种新的人体皮肤检测算法。识别皮肤像素的三个主要参数是RGB(红、绿、蓝)、HSV(色调、饱和度、值)和YCbCr(亮度、色度)颜色模型。不仅考虑了三种颜色参数的单独范围，而且考虑了交流计数的组合范围，从而提高了识别给定图像皮肤区域的精度。</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>基于阈值的算法，能够处理不同光照条件下的图像，未来应用可能在脸部、手势识别、皮肤病检测等</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>作为预处理步骤，识别关键是皮肤颜色。其他因素如光照条件也会影响结果，所以常常与纹理、边缘特征等线索相结合。</p><p>检测方式是判断单个像素是否位于我们设定的颜色范围，本文是结合RGB、HSV和YCbCr三个颜色空间设定阈值。</p><h4 id="Literature-Review"><a href="#Literature-Review" class="headerlink" title="Literature Review"></a>Literature Review</h4><p>皮肤检测技术可以大致分为<strong>基于像素的技术</strong>和<strong>基于区域的技术</strong>。<strong>pixel-based</strong> <strong>region-based</strong></p><p>在基于像素的皮肤检测中，每个像素根据一定的条件分别被分为皮肤像素和非皮肤像素。基于颜色值的皮肤检测是基于像素的。</p><p>基于区域的皮肤检测技术考虑<strong>像素点的空间关系</strong>.初始皮肤区域不断判断周围皮肤的属性来增大。</p><h4 id="Color-Spaces"><a href="#Color-Spaces" class="headerlink" title="Color Spaces"></a>Color Spaces</h4><p><strong>基于RGB</strong>：RGB,normalized RGB</p><p>规范化RGB就是一个normalized过程$e.g. r=\frac{R}{R+G+B}$</p><p><strong>基于色调Hue-based</strong>：HSI,HSV,HSL</p><p><strong>基于亮度Luminance-based</strong>:YCbCr,YIQ,YUV</p><p>亮度信息存储为单个分量(Y)，而色度信息存储为两个色差分量(Cb和Cr)。Cb表示蓝色分量与参考值之差。Cr表示红色分量与参考值的差值。</p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201119112418920.png" alt="RGB->YCbCr"></p><h4 id="Proposed-Skin-Detection-Algorithm"><a href="#Proposed-Skin-Detection-Algorithm" class="headerlink" title="Proposed Skin Detection Algorithm"></a>Proposed Skin Detection Algorithm</h4><p>ARGB color model:<a href="https://en.wikipedia.org/wiki/RGBA_color_model">https://en.wikipedia.org/wiki/RGBA_color_model</a></p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201119145342738.png" alt="ARGB32位"></p><p>利用右移操作和与0xff按位与，得到每个通道值，然后根据下图的阈值判断：</p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201119145454111.png" alt="阈值"></p><p>这个阈值如何获得的？</p><h4 id="Experiments-Resuts"><a href="#Experiments-Resuts" class="headerlink" title="Experiments Resuts"></a>Experiments Resuts</h4><img src="/ren-xiang-fu-se-jian-ce/image-20201119145605005.png" alt="实验结果" style="zoom:80%;"><img src="/ren-xiang-fu-se-jian-ce/image-20201119145623528.png" alt="精确率和准确率" style="zoom:80%;"><p>还有一点就不贴图了，即这三个颜色空间所获得的结果是差不多的。</p><h3 id="Combining-Convolutional-and-Recurrent-Neural-Networks-for-Human-Skin-Detection"><a href="#Combining-Convolutional-and-Recurrent-Neural-Networks-for-Human-Skin-Detection" class="headerlink" title="Combining Convolutional and Recurrent Neural Networks for Human Skin Detection"></a>Combining Convolutional and Recurrent Neural Networks for Human Skin Detection</h3><h4 id="Info-1"><a href="#Info-1" class="headerlink" title="Info"></a>Info</h4><p>年份：2017  </p><p>引用次数：72</p><p>期刊：IEEE SIGNAL PROCESSING LETTERS  JCR分区Q2</p><h4 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h4><p>高效的传统手工设计的肤色检测算法需要领域专家的广泛工作</p><p>CNN在像素级标记任务中取得了巨大的成功。CNN的架构不足以建模像素与其邻居之间的关系。引进RNN。FCN层捕获一般的局部特征，RNN层建模图像中的语义上下文依赖。在COMPAO和ECU皮肤数据集上验证了方法的有效性，其中RNN层增强了复杂背景下皮肤检测的识别能力。</p><h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>利用RNN层对图像像素间的语义空间依赖进行建模。</p><h4 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h4><p>在许多常用的颜色空间中，皮肤像素和非皮肤像素之间有明显的重叠。例如，背景中的许多物体，如墙壁、木材和布料，可能与不同类型的人类皮肤有着相似的颜色。在不考虑相邻像素的情况下，很难确定单个像素是皮肤还是非皮肤。此外，皮肤检测的性能还受到多种其他因素的影响(光照不均匀、相机特性、受试者种族、年龄、性别等)。<br>最近的研究集中在：</p><p>不同的颜色空间(如RGB [9], YCbCr [10], CIE-XYZ [11], HSV[12],和SKN[13])</p><p>特征提取(如颜色[14],纹理[15],和空间分布[16])</p><p>分类方法(贝叶斯分类器[9],高斯混合模型[17],支持向量机[18],神经网络[19],随机森林[20]等等)。</p><p>基于cnn的架构不擅长建模像素和它们的邻居之间的关系。最近，Zheng等人[27]在CNN的最后一层引入了<strong>条件随机场</strong>来细化粗略的预测。</p><h4 id="Problem-Statement-amp-Method"><a href="#Problem-Statement-amp-Method" class="headerlink" title="Problem Statement &amp; Method"></a>Problem Statement &amp; Method</h4><h4 id="Experiments-and-Analysis"><a href="#Experiments-and-Analysis" class="headerlink" title="Experiments and Analysis"></a>Experiments and Analysis</h4><p>Matlab环境  什么颜色空间仍然是开放问题，但是有人证明性能在某种程度上是独立于颜色空间的</p><p><strong>实验设置</strong></p><ul><li>FCN初始权值：预训练的FCN-8s </li><li>RNN初始权值：正态分布随机数</li><li>最优化方法：带动量SGD，动量0.9</li><li>学习速率：10^-7固定学习速率，<a href="https://blog.csdn.net/qq_17464457/article/details/101846874?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromBaidu-1.not_use_machine_learn_pai&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromBaidu-1.not_use_machine_learn_pai">bias加倍的学习速率</a></li><li>20个epoch per image 55ms</li></ul><p><strong>数据集</strong></p><ul><li>COMPAQ 只使用了4670皮肤图像 &amp; ECU 1000测试 3000训练</li><li>提供了GT skin masks 和 versatile attributes</li><li>COMPAQ结果相对ECU较差，因为数据集质量较差，包含大量低质量图像和半自动的GT</li></ul><p><strong>实验结果</strong></p><ul><li>混淆矩阵</li><li>the receiver operating characteristics <a href="https://www.jianshu.com/p/2ca96fce7e81">ROC</a> TPR和FPR 两个指标相互有点制衡 ROC相比P-R曲线更稳定，样本数量改变后不会振荡 ROC曲线的<strong>绘制</strong>：调整不同阈值设置，每个阈值在ROC空间上产生一个不同点  ‘*‘表示最佳工作点，由斜率Slope得到</li><li>respective area under curve AUC ROC曲线下的面积 物理意义：</li><li>equal error rate 1-EER</li><li>FCN8s+RNN的最终输出层为2通道的softmax，做二分类，设置阈值，超过阈值为皮肤，否则为非皮肤。</li><li>注意，对现有的alogrithm进行比较是困难的，因为它们要么使用不同的评估指标，要么使用非公共数据集，要么使用任意的操作点(或阈值)。</li></ul><img src="/ren-xiang-fu-se-jian-ce/image-20201120174744122.png" alt="Compaq" style="zoom:50%;"><img src="/ren-xiang-fu-se-jian-ce/image-20201120174807952.png" alt="ECU" style="zoom:50%;"><h3 id="Semi-supervised-Skin-Detection-by-Network-with-Mutual-Guidance"><a href="#Semi-supervised-Skin-Detection-by-Network-with-Mutual-Guidance" class="headerlink" title="Semi-supervised Skin Detection by Network with Mutual Guidance"></a>Semi-supervised Skin Detection by Network with Mutual Guidance</h3><h4 id="Info-2"><a href="#Info-2" class="headerlink" title="Info"></a>Info</h4><p>年份：2019</p><p>会议：ICCV</p><p>Megvii Technology 旷视研究院</p><h4 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h4><p>a single human portrait image-&gt;结合人体作为弱语义指导， 考虑到获取大规模的人体标记皮肤数据通常是昂贵和费时的-&gt;半监督学习策略进行皮肤和身体联合检测的dual-task network-&gt;一个共享编码器，分别用于皮肤和身体的两个解码器，两个解码器可以互相引导</p><h4 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>半监督训练，即不需要一个训练样本存在两种GT</p><h4 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h4><p>之前的方法</p><p>Combining haar feature and skin color based classifiers for face detection 2011 ICASSP</p><p>Adaptive learning of an accurate skin-color model 2004 ICAFGR</p><p>尝试在不同的颜色空间中建模皮肤颜色，并在这些空间中训练皮肤分类器</p><p>缺点：严重依赖于肤色的分布，并且没有涉及到语义信息，因此性能有限。</p><p>基于其他检测任务的DNN的改进受限于皮肤数据</p><p>引入人体检测的两个优势：</p><ul><li>为皮肤位置提供先验信息</li><li>检测到皮肤后，可以过滤掉False positive</li></ul><p>皮肤检测也为人体检测提供了信息</p><p><strong>两个检测器的共享编码器会考虑到两个任务的相似性和网络的紧凑性，从输入图像中提取共同的特征图。这种结构使我们在训练皮肤检测网络时不需要增加带注释的训练数据，而只需要增加一个人体面罩数据集，这更容易获得。由于这两个数据集分别包含两种ground truth类型，即一个数据样本中要么有一个目标skin mask，要么有一个body mask，因此我们采用新设计的loss和自定义的训练策略对网络进行半监督的训练。</strong></p><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p><strong>Skin detection and segmentation</strong></p><p>现有的方法可以分为三类:</p><ol><li>在颜色空间上明确定义边界模型/阈值-划分区域定皮肤像素-皮肤和非皮肤像素存在明显重叠</li><li>,应用传统机器学习技术学习种肤色模型-生成/判别模型预测，可能考虑到纹理等局部特征-学习能力有限，精度较低</li><li>利用深度神经网络学习皮肤分割的端到端模型–MLP-&gt;FCN,需要大规模监督-&gt;18CVPR Normalized cut loss for weakly-supervised cnn segmentation 引入条件随机场，使弱监督成为可能-&gt;本文引用额外数据集提高性能</li></ol><p><strong>Multi-task joint learning</strong></p><p>它通常通过在所有任务之间共享隐藏层来应用，同时保留几个特定于任务的输出层作为分支。</p><p>一些多任务网络通常通过共享编码器来学习共同的特征图，从而潜在地同时提高所有任务的性能。</p><h4 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h4><p>**Dataset and Implementation Details **</p><ul><li>10711 RGB images 5000 skin masks 5711 body masks 网络收集 512x512分辨率 </li><li>随机选择470S和475B，作验证集  随即翻转/调整/裁剪-训练数据增强</li><li>Tensorflow NVIDIA GeForce GTX 1080Ti GPU 训练了12个小时</li></ul><p><strong>Comparison with Existing Methods</strong></p><ul><li>2种传统算法 六种NN算法</li><li>Human skin color clustering for face detection  2003 IEEE RGB和HSV空间定阈值</li><li>Statistical color models with application to skin detection 2002 IJCV 学习GMM预测</li><li>缺陷：缺乏高级特征，对复杂环境和光照变化鲁棒性差</li><li>A skin detector based on neural network 2002 ICCCSWSE</li><li>Combining convolutional and recurrent neural networks for human skin detection 2017 SPL</li><li>U- net: Convolutional networks for biomedical image segmentation 2015</li><li>Deep residual learning for image recognition 2016CVPR</li><li>Encoder-decoder with atrous separable convolution for semantic image segmentation 2018ECCV</li></ul><p>We trained the six networks to convergence收敛 with multiple trials实验 with dataset DS, and selected their best results</p><p>For recall, our method ranks only below the GMM method, which has more false alarms so as to suffer from a poor precision. </p><p>定量比较-Table1 Figure6 平衡和不平衡 此文数据集与Pratheepan IoU IoU Top-1 Precision Recall </p><p>几个典型预测结果-Figure5 Figure1 这几个典型带有各种挑战性 传统方法完全失败 其他CNN不稳定 我们的有效且可靠</p><p><strong>Ablation Studies</strong></p><p><a href="https://www.zhihu.com/question/60170398">ablation study</a></p><p>去掉某些特征 模型 算法 对结果会有什么影响，就是控制变量，为了研究你提出的方法是不是有效 </p><p>根据奥卡姆剃刀法则，简单和复杂的方法能达到一样的效果，那么简单的方法更可靠。</p><p><strong>1-Mutual guidance</strong></p><p>Figure4 显示有无互指导的效果 即使没有互指导 也比单任务最好的好 因为共享编码器从身体数据学到了信息</p><p><strong>2-Weakly supervised losses</strong></p><p>Table2Top虽不显著 确实起到了作用</p><p>Figure3 显示了一个例子 不同的弱监督损失的作用</p><p><strong>3-Unbalanced dataset</strong></p><p>Figure1 若使用 不平衡数据集   IoU下降约%6 但仍比其他的高</p><p><strong>4-Backbone networks</strong></p><p>Table2Bottom Mobile-Net 替换UNet IoU相对较低 但互指导起到的作用更大</p><p><strong>5-Training strategy</strong></p><ul><li>Gradient stopping Figure7 </li><li>Initial guidance  给个先验会好一点</li><li>Finetune Figure4 Top <strong>train-from-scratch</strong> and <strong>finetune versions</strong>  有没有预训练</li></ul><h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><p>双任务FCN 输入一个RGB图像I 输出 skin Os 和 body Ob的概率图 两个解码器Ds Db 编码器E输出I的特征图Ei Os和Ei送到Db 反之亦然  网络结构 Figure2a</p><p><strong>1-Network with Mutual Guidance</strong></p><p>Figue2b 将原始网络解耦为没有循环的两个阶段 X表示Stage1 X‘ Stage2 Xk包括Xs和Xb</p><p>I E Dk 相同 Gk和Gk’不同 第一阶段信息很少而第二阶段有Ok作为指引</p><p>Table1 Figure4共享编码器考虑：</p><ol><li>虽然两个任务GT不同，但具有相似的统计信息</li><li>特征图具有共性，如区分人的前景和非人的背景的鲁棒性</li></ol><p><img src="/ren-xiang-fu-se-jian-ce/image-20201122214758315.png" alt="网络结构"></p><p>es eb 第一阶段的指导 一般设为0 应该是一直为0，不会变     E和Dk的结构采用了标准UNet，包括E中的4个下采样 Dk中的4个上采样 </p><p>输入512x512x3  E-&gt;Dk 的特征映射Ei 32x32x1024  另外应用了一个和E相同结构但是通道数只有一半的编码器给Gk，然后将输出与Ei相连，再喂给Dk</p><p>每个FCN层 kernel size 3x3 后面是一个BatchNorm和一个ReLu层</p><p><strong>2-Learning Algorithm</strong></p><p>对于人体检测，由于广泛研究，数据易得 所以对每个数据对，只提供Ms或Mb 很少提供（I，Ms，Mb）所以是一个半监督任务</p><p>这主要有半监督损失和具体训练细节实现</p><p><strong>2.1Semi-supervised loss</strong></p><p>半监督损失由三部分组成，包括：strongly-supervised and weakly-supervised ones</p><p>再有GT的一侧算Cross-entropy loss 另一侧算CRF loss 两个输出集合起来算WCE loss，</p><p><strong>Cross-entropy loss</strong></p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201123092722193.png" alt="Cross-entropy loss"></p><p>where $L_{ce}(x,y)=x\cdot log(y)+(1-x)\cdot log(1-y)$       $l_k$表示一委托个数据样例是否含有GT$M_k$</p><p><strong>CRF loss</strong></p><p>从 On regularized losses for weakly-supervised cnn segmentation 2018ECCV 引入CRFLoss CRF可以使I中颜色相同且相邻的像素在</p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201123093447500.png" alt="CRF loss"></p><p>where  $L_{crf}=S^TWS$  W是I的Affinity Matrix S是平坦化的Ok的列向量   具体看论文</p><p><strong>WCE loss</strong></p><p>输出之间也有限制，皮肤概率高的话，那么身体概率也应该高，也就是两个分布一致，皮肤概率低的话，就没有限制了，所以用皮肤概率做权重，限定WCE loss 对总体的影响</p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201123100039914.png" alt="WCE Loss"></p><p>$L_{wce}(x,y)=x\cdot L_{ce}(x,y)$</p><p><strong>semi-supervised loss</strong></p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201123100808245.png" alt="semi-supervised loss"></p><p>$\lambda_1=0.0001 \quad \lambda_2=0.001 $</p><p><strong>2.2Training details</strong><br><strong>Dual-task joint learning</strong></p><p>奇数次和偶数次分别喂MB和MS，用Lk来指导算loss</p><p><strong>Finetune</strong></p><p><img src="/ren-xiang-fu-se-jian-ce/image-20201124205344966.png" alt="Stage2 Guidance"></p><p>有GT的就直接用真实的指导就行</p><p>两个阶段的指导变化很大，但是他们使用相同的解码器权重,不然G’反向传播，防止Decoder参数对G‘过拟合，对G欠拟合</p><h3 id="SKINNY-A-LIGHTWEIGHT-U-NET-FOR-SKIN-DETECTION-AND-SEGMENTATION"><a href="#SKINNY-A-LIGHTWEIGHT-U-NET-FOR-SKIN-DETECTION-AND-SEGMENTATION" class="headerlink" title="SKINNY: A LIGHTWEIGHT U-NET FOR SKIN DETECTION AND SEGMENTATION"></a>SKINNY: A LIGHTWEIGHT U-NET FOR SKIN DETECTION AND SEGMENTATION</h3><h4 id="Info-3"><a href="#Info-3" class="headerlink" title="Info"></a>Info</h4><p>2020ICIP</p><h4 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h4><p>lightweight轻量的</p><p>全卷积UNet对图像分割很有效</p><p> 空间上下文对皮肤分隔很重要，对UNet很狭窄是什么意思</p><p>扩展了多尺度分析的范围</p><p>在ECU和HGR测试</p><h4 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Skinny受益与几个架构组件,包括inception和dense blocks，从而更好利用上下文 </p><p>inception和dense很关键，并且不影响速度</p><p>未来：增强上下文像素分析思想，使用扩张卷积，保持参数低数量，减少FP</p><h4 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h4><p>由于皮肤外观的低特异性和高方差，该问题仍具有挑战性</p><h4 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h4><p>图一 UNet没有足够上下文，分类错了，Skinny考虑到更广泛背景，分类正确了</p><p>图二 网络结构和两种变体</p><p> <a href="https://blog.csdn.net/u014380165/article/details/75142664">DenseNet</a>      </p><p>注意语义分割和实例分割的区别，语义分割是人都归为一类，实例分割是不同的人标成不同的类 </p><p> <a href="https://zhuanlan.zhihu.com/p/83496100">全景分割</a>将语义分割和实例分割结合在一起，既能分割背景，也能分割实例   </p><p><a href="https://www.sohu.com/a/284185732_129720">全景分割</a>图像中的内容可以按照是否有固定形状分为 things 类别和 stuff 类别，其中，人，车等有固定形状的物体属于 things 类别（可数名词通常属于 things）；天空，草地等没有固定形状的物体属于 stuff 类别（不可数名词属于 stuff）</p><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p><a href="https://www.aiuai.cn/aifarm1159.html">dice-loss</a>  对于二分类问题，GT 分割图是只有 0, 1 两个值的，因此 |X⋂Y| 可以有效的将在 Pred 分割图中未在 GT 分割图中激活的所有像素清零. 对于激活的像素，主要是惩罚低置信度的预测，较高值会得到更好的 Dice 系数.   </p><p>意思就是说主要注重皮肤点预测的怎么样，非皮肤点的预测就不管了</p>]]></content>
      
      
      <categories>
          
          <category> 肤色分级 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 肤色分级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>oj-1</title>
      <link href="codeforces-1/"/>
      <url>codeforces-1/</url>
      
        <content type="html"><![CDATA[<h2 id="1A-Theatre-Square-math"><a href="#1A-Theatre-Square-math" class="headerlink" title="1A Theatre Square-math"></a>1A Theatre Square-math</h2><p>#include&lt;bits/stdc++.h&gt;</p><p>using namespace std</p><p>注意查看变量取值范围,可能数超大，本题就得使用long long定义</p><p>向上取整$ceil()$,向下取整$floor()$,四舍五入到最近整数$round()$,注意返回值是浮点数！</p><p>c++API：<a href="https://devdocs.io/cpp/">https://devdocs.io/cpp/</a></p><pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span><span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">long</span> <span class="token keyword">long</span> <span class="token keyword">int</span> n<span class="token punctuation">,</span>m<span class="token punctuation">,</span>a<span class="token punctuation">;</span>    <span class="token function">scanf</span><span class="token punctuation">(</span><span class="token string">"%lld %lld %lld"</span><span class="token punctuation">,</span><span class="token operator">&amp;</span>n<span class="token punctuation">,</span><span class="token operator">&amp;</span>m<span class="token punctuation">,</span><span class="token operator">&amp;</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">long</span> <span class="token keyword">long</span> <span class="token keyword">int</span> rows<span class="token punctuation">,</span>cols<span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token operator">%</span>a<span class="token punctuation">)</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span>        rows<span class="token operator">=</span>n<span class="token operator">/</span>a<span class="token punctuation">;</span>    <span class="token keyword">else</span>         rows<span class="token operator">=</span>n<span class="token operator">/</span>a<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>m<span class="token operator">%</span>a<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span>        cols<span class="token operator">=</span>m<span class="token operator">/</span>a<span class="token punctuation">;</span>    <span class="token keyword">else</span>         cols<span class="token operator">=</span>m<span class="token operator">/</span>a<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">;</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%lld"</span><span class="token punctuation">,</span>rows<span class="token operator">*</span>cols<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span> </code></pre><h2 id="71A-Way-Too-Long-Words-strings"><a href="#71A-Way-Too-Long-Words-strings" class="headerlink" title="71A Way Too Long Words-strings"></a>71A Way Too Long Words-strings</h2><p>c++: string 建议是在程序中<strong>能使用C++字符串就使用</strong>，除非万不得已<strong>不选用c_string</strong>。</p><p>c++string教程：<a href="https://www.cnblogs.com/c1299401227/p/5370685.html">https://www.cnblogs.com/c1299401227/p/5370685.html</a></p><p><a href="https://blog.csdn.net/qq_42659468/article/details/90381985">https://blog.csdn.net/qq_42659468/article/details/90381985</a></p><p><a href="https://blog.csdn.net/liitdar/article/details/80498634">https://blog.csdn.net/liitdar/article/details/80498634</a></p><p><strong>重定向</strong>：</p><p>//freopen(“in.txt”,”r”,stdin);</p><p>输入输出：</p><p><img src="/codeforces-1/image-20201116202224462.png" alt="输入输出"></p><p>对于这种类型的题，可以输入一个输出一个，用如下方式：</p><p>while(n–){ }</p><p>c语言字符串:  char word[nmax]; scanf(“%s”,word); strlen()</p><p>必须要指明nmax，你就算是定义数字数组也要指明nmax呀，否则就得动态分配。</p><p>c语言字符串教程：<a href="https://www.cnblogs.com/tongye/p/10688941.html">https://www.cnblogs.com/tongye/p/10688941.html</a></p><pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span><span class="token string">&lt;bits/stdc++.h></span></span>using namespace std<span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> n<span class="token punctuation">;</span>    cin<span class="token operator">>></span>n<span class="token punctuation">;</span>    string s<span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        cin<span class="token operator">>></span>s<span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">></span><span class="token number">10</span><span class="token punctuation">)</span>            cout<span class="token operator">&lt;&lt;</span>s<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">&lt;&lt;</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">2</span><span class="token operator">&lt;&lt;</span>s<span class="token punctuation">[</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>        <span class="token keyword">else</span>            cout<span class="token operator">&lt;&lt;</span>s<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//注意换行</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h2 id="118-A-String-Task-string"><a href="#118-A-String-Task-string" class="headerlink" title="118 A String Task-string"></a>118 A String Task-string</h2><p>字符串<strong>大小写转换</strong>：没有直接的转换函数，可以<strong>直接利用ascii码字符加减</strong></p><p>对字符串变换，<strong>可以新设一个字符串，也可逐字符输出，是个黑盒子，它不管你怎么操作的，结果对了就行</strong></p><pre class=" language-c"><code class="language-c"> <span class="token keyword">if</span> <span class="token punctuation">(</span>str<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">==</span><span class="token string">'a'</span><span class="token operator">||</span>str<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">==</span><span class="token string">'e'</span><span class="token operator">||</span>str<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">==</span><span class="token string">'i'</span><span class="token operator">||</span>str<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">==</span><span class="token string">'o'</span><span class="token operator">||</span>str<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">==</span><span class="token string">'u'</span><span class="token operator">||</span>str<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">==</span><span class="token string">'y'</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>   str<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//这里做一个假删除</span> <span class="token punctuation">}</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>str<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>   <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//这一步很重要</span>   <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%c"</span><span class="token punctuation">,</span>str<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span><span class="token macro property">#<span class="token directive keyword">include</span><span class="token string">&lt;bits/stdc++.h></span></span>using namespace std<span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    string s<span class="token punctuation">;</span>    cin<span class="token operator">>></span>s<span class="token punctuation">;</span>    string ans<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">>=</span><span class="token string">'A'</span><span class="token operator">&amp;&amp;</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&lt;=</span><span class="token string">'Z'</span><span class="token punctuation">)</span>            s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">-</span><span class="token string">'A'</span><span class="token operator">+</span><span class="token string">'a'</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token string">'a'</span><span class="token operator">&amp;&amp;</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token string">'o'</span><span class="token operator">&amp;&amp;</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token string">'y'</span><span class="token operator">&amp;&amp;</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token string">'e'</span><span class="token operator">&amp;&amp;</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token string">'u'</span><span class="token operator">&amp;&amp;</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">!=</span><span class="token string">'i'</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            ans<span class="token operator">=</span>ans<span class="token operator">+</span><span class="token string">'.'</span><span class="token operator">+</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>         <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    cout<span class="token operator">&lt;&lt;</span>ans<span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h2 id="78A-Football"><a href="#78A-Football" class="headerlink" title="78A Football"></a>78A Football</h2><p>连续子串等连续问题，可以设一个变量或数组，每一次修改都在前一位的基础上修改</p><pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span><span class="token string">&lt;bits/stdc++.h></span></span>using namespace std<span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    string s<span class="token punctuation">;</span>    cin<span class="token operator">>></span>s<span class="token punctuation">;</span>    <span class="token keyword">int</span> a<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>b<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>flag<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">==</span><span class="token string">'0'</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            a<span class="token operator">++</span><span class="token punctuation">;</span>            b<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">else</span><span class="token punctuation">{</span>            a<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>            b<span class="token operator">++</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token operator">==</span><span class="token number">7</span><span class="token operator">||</span>b<span class="token operator">==</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            flag<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>            cout<span class="token operator">&lt;&lt;</span><span class="token string">"YES"</span><span class="token punctuation">;</span>            <span class="token keyword">break</span><span class="token punctuation">;</span>         <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>flag<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span>        cout<span class="token operator">&lt;&lt;</span><span class="token string">"NO"</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h2 id="230A-Dragons"><a href="#230A-Dragons" class="headerlink" title="230A Dragons-"></a>230A Dragons-</h2><p>c++中sort函数使用方法：<a href="https://www.cnblogs.com/junbaobei/p/10776066.html">https://www.cnblogs.com/junbaobei/p/10776066.html</a></p><p>迭代器：<a href="https://blog.csdn.net/qq_34777600/article/details/80427463">https://blog.csdn.net/qq_34777600/article/details/80427463</a></p><p>sort小总结：<a href="https://blog.csdn.net/yulijuanxmu/article/details/80148417">https://blog.csdn.net/yulijuanxmu/article/details/80148417</a></p><pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span><span class="token string">&lt;bits/stdc++.h></span></span>using namespace std<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//typedef struct node{//这种使用typedef 也可以不用 不一定非要用链表 </span><span class="token comment" spellcheck="true">//    int x;</span><span class="token comment" spellcheck="true">//    int y;</span><span class="token comment" spellcheck="true">//}Dragon; </span><span class="token keyword">struct</span> Dragon<span class="token punctuation">{</span>    <span class="token keyword">int</span> x<span class="token punctuation">;</span>    <span class="token keyword">int</span> y<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//注意分号 </span>bool <span class="token function">cmp</span><span class="token punctuation">(</span>Dragon a<span class="token punctuation">,</span>Dragon b<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> a<span class="token punctuation">.</span>x<span class="token operator">&lt;</span>b<span class="token punctuation">.</span>x<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//前面的小于后面的，排序也一致 </span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> s<span class="token punctuation">,</span>n<span class="token punctuation">;</span>    cin<span class="token operator">>></span>s<span class="token operator">>></span>n<span class="token punctuation">;</span>    Dragon dragons<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        cin<span class="token operator">>></span>dragons<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>x<span class="token operator">>></span>dragons<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">sort</span><span class="token punctuation">(</span>dragons<span class="token punctuation">,</span>dragons<span class="token operator">+</span>n<span class="token punctuation">,</span>cmp<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token operator">></span>dragons<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>x<span class="token punctuation">)</span>            s<span class="token operator">+</span><span class="token operator">=</span>dragons<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>y<span class="token punctuation">;</span>        <span class="token keyword">else</span><span class="token punctuation">{</span>            cout<span class="token operator">&lt;&lt;</span><span class="token string">"NO"</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//得到了想要的可以直接return </span>            <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    cout<span class="token operator">&lt;&lt;</span><span class="token string">"YES"</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h2 id="455A-Boredom-dp"><a href="#455A-Boredom-dp" class="headerlink" title="455A Boredom-dp"></a>455A Boredom-dp</h2><p>动态规划：从新手到专家:<a href="http://hawstein.com/2013/03/26/dp-novice-to-advanced/">http://hawstein.com/2013/03/26/dp-novice-to-advanced/</a></p><p><strong>子串</strong>:串中任意个连续的字符组成的子序列称为该串的子串. </p><p><strong>子序列</strong>:子数列，又称子序列，在数学中，某个序列的子序列是从最初序列通过,去除某些元素但不破坏余下元素的相对位置（在前或在后）而形成的新序列。</p><h2 id="1359B-New-Theatre-Square"><a href="#1359B-New-Theatre-Square" class="headerlink" title="1359B New Theatre Square"></a>1359B New Theatre Square</h2><p>第一次遇见时间超时的问题</p><p><strong>一般1s执行10^8次运算</strong></p><p>按道理来说没事，反正一开始写的代码很多，很乱，其实是个很简单的问题，不知道为啥写的那么乱</p><p>利用一个$y=min(2*x,y)$函数，<strong>将两块砖统一起来了</strong> </p><p><strong>没必要字符矩阵，字符串也是一行一行处理的，最好是string</strong></p><p>挺简单的一道题，不知道为啥做了这么久</p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span><span class="token string">&lt;bits/stdc++.h></span></span><span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> t<span class="token punctuation">;</span>    cin<span class="token operator">>></span>t<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>t<span class="token operator">--</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">int</span> n<span class="token punctuation">,</span>m<span class="token punctuation">,</span>x<span class="token punctuation">,</span>y<span class="token punctuation">;</span>        cin<span class="token operator">>></span>n<span class="token operator">>></span>m<span class="token operator">>></span>x<span class="token operator">>></span>y<span class="token punctuation">;</span>        y<span class="token operator">=</span><span class="token function">min</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> ans<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            string s<span class="token punctuation">;</span>            cin<span class="token operator">>></span>s<span class="token punctuation">;</span>            <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>m<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                <span class="token keyword">if</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">==</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                    <span class="token keyword">if</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token operator">&lt;</span>m <span class="token operator">&amp;</span> s<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">==</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                        ans<span class="token operator">+</span><span class="token operator">=</span>y<span class="token punctuation">;</span>                        j<span class="token operator">++</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//注意这一步，然后一次相当于加了2</span>                    <span class="token punctuation">}</span>                    <span class="token keyword">else</span><span class="token punctuation">{</span>                        ans<span class="token operator">+</span><span class="token operator">=</span>x<span class="token punctuation">;</span>                    <span class="token punctuation">}</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        cout<span class="token operator">&lt;&lt;</span>ans<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>    <span class="token punctuation">}</span>         <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h2 id="1440B-Sum-of-Medians-greedy"><a href="#1440B-Sum-of-Medians-greedy" class="headerlink" title="1440B Sum of Medians-greedy"></a>1440B Sum of Medians-greedy</h2><p>贪心算法，又名贪婪法，是寻找最优解问题的常用方法，这种方法模式一般将求解过程分成若干个步骤，但每个步骤都应用贪心原则，<strong>选取当前状态下最好/最优的选择（局部最有利的选择）</strong>，并以此希望最后堆叠出的结果也是最好/最优的解。{看着这个名字，贪心，贪婪这两字的内在含义最为关键。这就好像一个贪婪的人，他<strong>事事都想要眼前看到最好的那个，看不到长远的东西，也不为最终的结果和将来着想，贪图眼前局部的利益最大化，有点走一步看一步的感觉</strong>。</p><p>对于本题，认真读还是人家的教程讲得好！</p><p><img src="/codeforces-1/image-20201119000826288.png" alt="教程"></p><p><strong>主要是怎么证明他就是最值，我们想象有k个选定的中值，只要每个数前面还有$ceil(n/2)-1$个元素多于他们，那么他们就可以一直往前移动，只要他们往前移动，那么就是非减的。</strong></p><p>本题注意<strong>ans要用long long int存储</strong>，因为正常的int只有$-2^{31}\sim 2^{31}-1$,所以每个数最大是$10^9$,最多有$2\cdot 10^5$个数，显然是超了，必须用long long int，int为<strong>4字节</strong>，<strong>long long int为8字节</strong></p><p><strong>原码反码补码</strong>：</p><p><a href="https://blog.csdn.net/u013760665/article/details/98520702">https://blog.csdn.net/u013760665/article/details/98520702</a></p><p><a href="https://blog.csdn.net/afsvsv/article/details/94553228">https://blog.csdn.net/afsvsv/article/details/94553228</a></p><pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span><span class="token string">&lt;bits/stdc++.h></span></span>using namespace std<span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">int</span> maxn<span class="token operator">=</span><span class="token number">200005</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//数组稍微开大一点就崩溃了，不知道咋回事</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">//freopen("1.txt","r",stdin);</span>    <span class="token keyword">int</span> t<span class="token punctuation">;</span>    cin<span class="token operator">>></span>t<span class="token punctuation">;</span>    <span class="token keyword">int</span> n<span class="token punctuation">,</span>k<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>t<span class="token operator">--</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        cin<span class="token operator">>></span>n<span class="token operator">>></span>k<span class="token punctuation">;</span>        <span class="token keyword">int</span> a<span class="token punctuation">[</span>maxn<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>i<span class="token operator">&lt;=</span>n<span class="token operator">*</span>k<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token comment" spellcheck="true">//可以设立一个更大的数组，然后数组从1开始</span>            cin<span class="token operator">>></span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>          <span class="token comment" spellcheck="true">//从1开始对于本题思维逻辑更清晰 </span>        <span class="token punctuation">}</span>         <span class="token keyword">int</span> median<span class="token operator">=</span><span class="token function">ceil</span><span class="token punctuation">(</span>n<span class="token operator">/</span><span class="token number">2.0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//ceil对于浮点数</span>        <span class="token keyword">int</span> l1<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">//两个指针不一定就是真的指针，只是两个int值用来标定位置</span>        <span class="token keyword">int</span> l2<span class="token operator">=</span>n<span class="token operator">*</span>k<span class="token punctuation">;</span>        <span class="token keyword">long</span> <span class="token keyword">long</span> <span class="token keyword">int</span> ans<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">while</span><span class="token punctuation">(</span>l1<span class="token operator">&lt;=</span>l2<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token comment" spellcheck="true">//两个指针碰头说明找完了</span>            l2<span class="token operator">-</span><span class="token operator">=</span>n<span class="token operator">-</span>median<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//擦去不是说真的把数组删除了，我们之后不看他他对于我们来看就跟删去一样！</span>            ans<span class="token operator">+</span><span class="token operator">=</span>a<span class="token punctuation">[</span>l2<span class="token punctuation">]</span><span class="token punctuation">;</span>            l2<span class="token operator">-</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>            l1<span class="token operator">+</span><span class="token operator">=</span>median<span class="token number">-1</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        cout<span class="token operator">&lt;&lt;</span>ans<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> oj </category>
          
      </categories>
      
      
        <tags>
            
            <tag> codeforces </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器使用</title>
      <link href="fu-wu-qi-shi-yong/"/>
      <url>fu-wu-qi-shi-yong/</url>
      
        <content type="html"><![CDATA[<p>linux教程：<a href="http://c.biancheng.net/view/4017.html">http://c.biancheng.net/view/4017.html</a></p><p><code>unzip -d des src</code>解压到指定目录</p><p><code>df -h</code>查看磁盘空间大小</p><h3 id="linux服务器安装conda？"><a href="#linux服务器安装conda？" class="headerlink" title="linux服务器安装conda？"></a>linux服务器安装conda？</h3><p><a href="https://blog.csdn.net/weixin_40258579/article/details/85001218?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">CSDN</a></p><p><a href="https://docs.anaconda.com/anaconda/install/linux/">官方文档</a></p><p><code>source ~/.bashrc</code>环境变量生效</p><h3 id="安装pytorch"><a href="#安装pytorch" class="headerlink" title="安装pytorch?"></a>安装pytorch?</h3><p><a href="https://blog.csdn.net/xzy5210123/article/details/107237037?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">换源解决速度慢教程</a></p><p>建议安装下面这个版本的pytorch,不然咱们实验室的cuda用不了</p><p><code>pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html</code></p><p><a href="https://discuss.pytorch.org/t/rtx-3000-support/98158">https://discuss.pytorch.org/t/rtx-3000-support/98158</a></p><h3 id="python版本更改？"><a href="#python版本更改？" class="headerlink" title="python版本更改？"></a>python版本更改？</h3><p>还是有大佬的！<a href="https://www.cnblogs.com/chenhuabin/p/10718471.html">https://www.cnblogs.com/chenhuabin/p/10718471.html</a></p><p><img src="/fu-wu-qi-shi-yong/image-20201113190611958.png" alt="这一步是重点"></p><h3 id="Xshell设置"><a href="#Xshell设置" class="headerlink" title="Xshell设置"></a>Xshell设置</h3><p>复制粘贴</p><p><a href="https://www.cnblogs.com/sxdcgaq8080/p/10025759.html">https://www.cnblogs.com/sxdcgaq8080/p/10025759.html</a></p><h4 id="jupyter配置？"><a href="#jupyter配置？" class="headerlink" title="jupyter配置？"></a>jupyter配置？</h4><p>首先，安装jupyter</p><pre class=" language-shell"><code class="language-shell">pip install jupyter  </code></pre><p>生成jupyter配置文件</p><pre class=" language-shell"><code class="language-shell">jupyter notebook --generate-config</code></pre><p>打开ipython，生成密码：</p><pre class=" language-shell"><code class="language-shell"># ipythonIn [1]:  from notebook.auth import passwdIn [2]: passwd()</code></pre><p>会让输入两次密码，输入完成后 复制生成的 秘钥，后面会用到，秘钥带上sha1</p><p>修改配置文件：vi中查找 <a href="https://www.cnblogs.com/actively/p/13023292.html">https://www.cnblogs.com/actively/p/13023292.html</a></p><pre class=" language-shell"><code class="language-shell">vi  /username/.jupyter/jupyter_notebook_config.py    #改几个地方：c.NotebookApp.ip = 'localhost'  #一定要把注释的#号去掉c.NotebookApp.port = 10001                    即对外提供访问的端口c.NotebookApp.open_browser = False            False即启动不打开浏览器c.NotebookApp.password = u'sha1:XXXXX'   这个就是上面生成的秘钥c.NotebookApp.notebook_dir = u'workplace' 即设置jupyter启动后默认的根目录，这个文件夹要自己创建</code></pre><p>然后打开xshell，文件-&gt;当前会话属性-&gt;隧道-&gt;添加 #端口号可以自己设置别的</p><p><img src="/fu-wu-qi-shi-yong/image-20201113093744564.png" alt="隧道属性"></p><p>然后打开jupyter</p><pre class=" language-shell"><code class="language-shell">jupyter notebook</code></pre><p>然后在windows上打开localhost:10001,就能在windows上使用linux的jupyter notebook</p><h3 id="导入Cuda："><a href="#导入Cuda：" class="headerlink" title="导入Cuda："></a>导入Cuda：</h3><p>对于我们实验室，需要在bashrc中导入</p><pre class=" language-shell"><code class="language-shell">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64#export PATH=$PATH:/usr/local/cuda/binexport CUDA_HOME=/usr/local/cuda</code></pre><p>然后<code>source .bashrc</code>，然后可以运行<code>nvcc -V</code>查看版本来检查是否正确</p><h3 id="后台使用jupyter"><a href="#后台使用jupyter" class="headerlink" title="后台使用jupyter"></a>后台使用jupyter</h3><p><a href="https://blog.csdn.net/donaldsy/article/details/96350061">后台运行jupyter教程</a></p><pre class=" language-shell"><code class="language-shell">jupyter notebook & #后台运行，你关掉终端会停止运行nohup jupyter notebook & #后台运行，你关掉终端也会继续运行ps -aux | grep jupyter #查找jupyter命令jobs -l#查看当前有多少在后台运行的命令，加上-l参数可以显示后台运行的pidfg %jobnumber#将后台中的命令调至前台继续运行。%jobnumber是jobs查到的序号，不是pidbg %jobnumber#将一个在后台暂停的命令，变成继续执行。</code></pre><h3 id="opencv安装"><a href="#opencv安装" class="headerlink" title="opencv安装"></a>opencv安装</h3><p><a href="https://www.cnblogs.com/thewaytotheway/p/12847260.html">https://www.cnblogs.com/thewaytotheway/p/12847260.html</a></p><h3 id="tensorflow安装"><a href="#tensorflow安装" class="headerlink" title="tensorflow安装"></a>tensorflow安装</h3><p>pip install tensorflow安装的是CPU版本</p><p>pip install tensorflow-gpu 安装的是gpu版本</p><p><a href="%E8%BF%99%E4%B8%AA%E5%8D%9A%E5%AE%A2%E5%A4%A7%E4%BD%ACtql">https://blog.csdn.net/china_xin1/article/details/109824882</a> 显卡3090只支持CUDA11</p><p>所以要<code>pip install tf-nightly-gpu</code>  尝新版</p><h3 id="查看服务器使用情况"><a href="#查看服务器使用情况" class="headerlink" title="查看服务器使用情况"></a>查看服务器使用情况</h3><p><code>watch -n 5 nvidia-smi </code></p><p>查看GPU使用情况 ,-n 5 指每5s刷新一次</p><p><code>nvcc -V</code></p><p>查看当前cuda版本</p><p><code>top -bn 1 -i -c</code></p><p><a href="https://blog.csdn.net/AlbenXie/article/details/72885951">查看CPU使用情况</a>     </p><p><code>ps aux |grep dch</code></p><pre class=" language-shell"><code class="language-shell">1）ps a 显示现行终端机下的所有程序，包括其他用户的程序。2）ps -A 显示所有程序。3）ps c 列出程序时，显示每个程序真正的指令名称，而不包含路径，参数或常驻服务的标示。4）ps -e 此参数的效果和指定"A"参数相同。5）ps e 列出程序时，显示每个程序所使用的环境变量。6）ps f 用ASCII字符显示树状结构，表达程序间的相互关系。7）ps -H 显示树状结构，表示程序间的相互关系。8）ps -N 显示所有的程序，除了执行ps指令终端机下的程序之外。9）ps s 采用程序信号的格式显示程序状况。10）ps S 列出程序时，包括已中断的子程序资料。11）ps -t <终端机编号> 　指定终端机编号，并列出属于该终端机的程序的状况。12）ps u 　 以用户为主的格式来显示程序状况。13）ps x 　 显示所有程序，不以终端机来区分。14）ps -l 較長,較詳細的顯示該PID的信息</code></pre><p><code>history</code></p><p>查看历史命令</p><h3 id="bashrc"><a href="#bashrc" class="headerlink" title=".bashrc"></a>.bashrc</h3><p><code>cd </code>和<code>cd ~</code>都会返回到用户主目录<code>/home/dch</code> </p><p><code>/home/dch/.bashrc</code> 配置文件，保存个人的一些个性化设置，如命令别名、路径、环境变量等</p><p><code>source .bashrc</code>立刻加载修改后的设置，使之生效。</p><p><code>export</code></p><h3 id="查找文件"><a href="#查找文件" class="headerlink" title="查找文件"></a>查找文件</h3><p><a href="https://www.linuxprobe.com/linux-jar.html">https://www.linuxprobe.com/linux-jar.html</a>   find -name 文件</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matlab</title>
      <link href="matlab/"/>
      <url>matlab/</url>
      
        <content type="html"><![CDATA[<p>教程：<a href="https://www.w3cschool.cn/matlab/">https://www.w3cschool.cn/matlab/</a></p><p>MATLAB Api：<a href="https://www.mathworks.com/help/">https://www.mathworks.com/help/</a></p><h4 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h4><p>双百分号%%在matlab代码中的作用是将代码分块，上下两个%%之间的部分作为一块，在运行代码的时候可以分块运行，查看每一块代码的运行情况。常用于调试程序。</p><p>单百分号%是注释    多行注释是%{   }%  注释快捷键CTRL+R 取消CTRL+T</p><p><strong>matlab数组下标从1开始</strong></p><pre class=" language-matlab"><code class="language-matlab">clear<span class="token comment" spellcheck="true">%清空变量</span>clc<span class="token comment" spellcheck="true">%清空命令行窗口</span><span class="token function">class</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">%显示数据类型</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">%查看各矩阵维度</span><span class="token function">length</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">%返回最大数组维度的长度</span>X <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span>L <span class="token operator">=</span> <span class="token function">length</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token comment" spellcheck="true">%输出9</span><span class="token comment" spellcheck="true">%异常检测</span><span class="token keyword">try</span>    <span class="token punctuation">[</span>C<span class="token punctuation">,</span>scores<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">semanticseg</span><span class="token punctuation">(</span>I<span class="token punctuation">,</span>net<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">catch</span>    <span class="token keyword">continue</span><span class="token punctuation">;</span><span class="token keyword">end</span><span class="token comment" spellcheck="true">%循环</span><span class="token keyword">for</span> k <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span><span class="token function">length</span><span class="token punctuation">(</span>imagepath<span class="token punctuation">)</span>    imagepath<span class="token punctuation">{</span>k<span class="token punctuation">}</span><span class="token operator">=</span><span class="token punctuation">[</span>imagepath<span class="token punctuation">{</span>k<span class="token punctuation">}</span><span class="token punctuation">,</span><span class="token string">'.jpg'</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">end</span></code></pre><p>single 单精度数值数据  double 双精度数值数据 </p><p><strong>Cell Array 元胞数组</strong></p><pre class=" language-matlab"><code class="language-matlab">C <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'one'</span><span class="token punctuation">,</span> <span class="token string">'two'</span><span class="token punctuation">,</span> <span class="token string">'three'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%创建元胞数组</span>     <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">}</span><span class="token punctuation">;</span>emptyCell<span class="token operator">=</span><span class="token function">cell</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">%cell函数创建元胞数组</span>upperLeft<span class="token operator">=</span><span class="token function">C</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">:</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token operator">:</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">%圆括号索引</span>imagepath<span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">}</span><span class="token operator">=</span><span class="token string">'45'</span><span class="token comment" spellcheck="true">%这个只能大括号，圆括号就是类型赋值有问题</span></code></pre><p><strong>categorical 字符数组</strong></p><p>从字符矢量创建分类数组</p><pre class=" language-matlab"><code class="language-matlab">A <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'r'</span> <span class="token string">'b'</span> <span class="token string">'g'</span><span class="token punctuation">;</span> <span class="token string">'g'</span> <span class="token string">'r'</span> <span class="token string">'b'</span><span class="token punctuation">;</span> <span class="token string">'b'</span> <span class="token string">'r'</span> <span class="token string">'g'</span><span class="token punctuation">}</span>B <span class="token operator">=</span> <span class="token function">categorical</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token comment" spellcheck="true">%创建分类数组</span><span class="token function">categories</span><span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token comment" spellcheck="true">%显示B的类别</span><span class="token function">class</span><span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token comment" spellcheck="true">%显示B的数据类型   就是‘categorical’</span></code></pre><h4 id="文件和文件夹"><a href="#文件和文件夹" class="headerlink" title="文件和文件夹"></a>文件和文件夹</h4><pre class=" language-matlab"><code class="language-matlab"><span class="token function">fullfile</span><span class="token punctuation">(</span>filepart1<span class="token punctuation">,</span><span class="token punctuation">...</span><span class="token punctuation">,</span>filepartN<span class="token punctuation">)</span>#从各个部分构建完整文件名 有点类似os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join #还会根据平台改变文件分隔符，创建多个文件路径的话会返回元胞数组f <span class="token operator">=</span> <span class="token function">fullfile</span><span class="token punctuation">(</span><span class="token string">'c:\'</span><span class="token punctuation">,</span><span class="token string">'myfiles'</span><span class="token punctuation">,</span><span class="token string">'matlab'</span><span class="token punctuation">,</span><span class="token punctuation">{</span><span class="token string">'myfile1.m'</span><span class="token punctuation">;</span><span class="token string">'myfile2.m'</span><span class="token punctuation">}</span><span class="token punctuation">)</span>f <span class="token operator">=</span>  <span class="token number">2</span>×<span class="token number">1</span> cell array    <span class="token string">'c:\myfiles\matlab\myfile1.m'</span>    <span class="token string">'c:\myfiles\matlab\myfile2.m'</span>textread<span class="token function">dir</span><span class="token punctuation">(</span><span class="token punctuation">)</span>#获得指定文件夹下的所有子文件夹和文件，并存放在一个文件结构的数组<span class="token punctuation">,</span>各结构体如下#name<span class="token operator">:</span>文件名 date<span class="token operator">:</span>修改日期 bytes<span class="token operator">:</span>文件大小 isdir<span class="token operator">:</span>目录是<span class="token number">1</span>，不是为<span class="token number">0</span> datenum<span class="token operator">:</span>matlab中特定的修改日期 eg<span class="token operator">:</span>imgpath  <span class="token operator">=</span> <span class="token function">dir</span><span class="token punctuation">(</span><span class="token function">fullfile</span><span class="token punctuation">(</span>imgdir<span class="token punctuation">,</span><span class="token string">'*.png'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">% 遍历所有png格式文件</span>I<span class="token operator">=</span><span class="token function">imread</span><span class="token punctuation">(</span><span class="token function">fullfile</span><span class="token punctuation">(</span>imgdir<span class="token punctuation">,</span><span class="token function">imgpath</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%读取第k个</span></code></pre><h4 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h4><p><code>imread()</code>读取图像</p><p>figure创建一个画板 <code>imshow()</code>展示图像</p><p><strong>imageDatastore</strong> </p><p>如果一个图像文件集合中的每个图像可以单独放入内存，但整个集合不一定能放入内存，则可以使用 <code>ImageDatastore</code> 对象来管理。您可以使用 <code>imageDatastore</code> 函数创建 <code>ImageDatastore</code> 对象，指定其属性，然后使用对象函数导入和处理数据。</p><p><code>imds = imageDatastore(location)</code>根据 <code>location</code> 指定的图像数据集合创建一个数据存储 </p><h4 id="深度学习入门之旅"><a href="#深度学习入门之旅" class="headerlink" title="深度学习入门之旅"></a>深度学习入门之旅</h4><p><a href="https://www.pianshen.com/article/71481644395/">https://www.pianshen.com/article/71481644395/</a></p><h4 id="SeriesNetwork"><a href="#SeriesNetwork" class="headerlink" title="SeriesNetwork"></a>SeriesNetwork</h4><p>串联网络是一种用于深度学习的神经网络，层次化排列。它有一个单一的输入层和一个单一的输出层</p><h4 id="DAGNetwork"><a href="#DAGNetwork" class="headerlink" title="DAGNetwork"></a>DAGNetwork</h4><p><a href="https://blog.csdn.net/weixin_43687366/article/details/102557729">DAG入门之旅</a></p><p>DAG网络是用于深度学习的神经网络，其中的层为有向无环图。各层有来自多个层的输入和到多个层的输出。DAGNetwork对象具有单个输入层和单个输出层。</p><p>lgraph.Connections 展示层的连接</p><p>lgraph.Layers 展示层</p><p>figure  plot(lgraph)把网络图形画出来</p>]]></content>
      
      
      <categories>
          
          <category> Matlab </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python学习</title>
      <link href="python-xue-xi/"/>
      <url>python-xue-xi/</url>
      
        <content type="html"><![CDATA[<p>pythonAPI:<a href="https://docs.python.org/zh-cn/3.6/library/index.html">https://docs.python.org/zh-cn/3.6/library/index.html</a></p><p>教程cookbook:<a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/preface.html">https://python3-cookbook.readthedocs.io/zh_CN/latest/preface.html</a></p><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h3><p><strong>print 默认输出</strong>是换行的，如果要实现不换行需要在变量末尾加上 **end=””**：</p><p>True和False可以直接当成0和1来用</p><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token boolean">True</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token boolean">True</span><span class="token operator">>></span><span class="token operator">></span> <span class="token boolean">False</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token boolean">True</span><span class="token operator">>></span><span class="token operator">></span> <span class="token boolean">True</span> <span class="token operator">+</span> <span class="token boolean">False</span> <span class="token operator">+</span> <span class="token number">20</span><span class="token number">21</span></code></pre><p>//:整数除法</p><p>\：可以使用这个换行，运算符后边不会错</p><h4 id="格式化输出，文字和数字等混合输出时使用！"><a href="#格式化输出，文字和数字等混合输出时使用！" class="headerlink" title="格式化输出，文字和数字等混合输出时使用！"></a>格式化输出，文字和数字等混合输出时使用！</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#第一种</span><span class="token string">"Skipping {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>img_path<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#第二种</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[%d, %5d] loss: %.3f'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> running_loss <span class="token operator">/</span> <span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#%5d %5s输出站5个字符宽度，不足补足   %.3f表示保留三位小数</span><span class="token comment" spellcheck="true">#第三种</span>f<span class="token string">'asdf {变量名}'</span></code></pre><h3 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h3><h4 id="文件读取输出"><a href="#文件读取输出" class="headerlink" title="文件读取输出"></a>文件读取输出</h4><p><a href="https://www.cnblogs.com/ymjyqsx/p/6554817.html">python文件读写</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"locs.txt"</span><span class="token punctuation">,</span><span class="token string">"a"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true">#f.write(fname+"\n")</span>    <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true">#每个line都是str</span>        linelist<span class="token operator">=</span>line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#字符串切割</span><span class="token comment" spellcheck="true">#with open as f: 为了保证无论是否出错都能正确地关闭文件</span><span class="token comment" spellcheck="true">#文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的</span><span class="token comment" spellcheck="true">#由于文件读写时都有可能产生IOError，一旦出错，后面的f.close()就不会调用。</span><span class="token comment" spellcheck="true">#所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用try ... finally来实现</span><span class="token keyword">try</span><span class="token punctuation">:</span>    f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'/path/to/file'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">finally</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> f<span class="token punctuation">:</span>        f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#但是每次都这么写实在太繁琐，所以，Python引入了with语句来自动帮我们调用close()方法</span>files <span class="token operator">=</span> file<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#splitlines() 按照行界符('\r', '\r\n', \n'等)分隔，返回一个包含各行作为元素的列表，默认不包含行界符。</span>file<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#一次读取文件的全部内容,把内容读到内存,用一个str对象表示,file.read(size)表示最多读size个字节</span>file<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#每次读取一行；返回的是一个字符串对象，保持当前行的内存</span>file<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#读取所有行然后把它们作为一个字符串列表返回。</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'mytest.txt'</span><span class="token punctuation">,</span><span class="token string">'w'</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#w是覆盖，a是追加</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">+</span><span class="token string">'\n'</span><span class="token punctuation">)</span>   </code></pre><p>一个stackoverflow问题</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> line <span class="token keyword">in</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">)</span><span class="token punctuation">:</span>    do_something<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#为什么经常看到这种写法？为什么不是下面这种</span><span class="token keyword">with</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        do_something<span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#1:with更好</span><span class="token comment" spellcheck="true">#2:for line in f足够,for line in f.readlines()反而冗余,我推测是f所属的类实现了相应__iter__()方法</span><span class="token comment" spellcheck="true">#3:在简短的脚本中，人们经常省略这一步，因为在垃圾回收过程中，当文件对象被回收时，Python会自动关闭文件。然而，尽快关闭文件是一种良好的编程实践，在大型程序中尤其如此。</span></code></pre><h4 id="绝对路径和相对路径"><a href="#绝对路径和相对路径" class="headerlink" title="绝对路径和相对路径"></a>绝对路径和相对路径</h4><p><a href="https://blog.csdn.net/databatman/article/details/49453953">绝对路径和相对路径</a></p><ul><li><h3 id="字符串处理"><a href="#字符串处理" class="headerlink" title="字符串处理"></a>字符串处理</h3></li></ul><p><a href="https://www.cnblogs.com/walo/p/10608436.html">python字符串前加u,r,b,f</a></p><ul><li>u:后面字符串以Unicode格式进行编码,一般用在中文字符串前面,防止因为源码储存格式问题,导致再次使用时出现乱码。</li><li>r:raw,去掉反斜杠的转移机制。</li><li>b:后面字符串是bytes 类型。</li><li>f:以f开头表示在字符串内支持大括号内的python 表达式。</li></ul><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><pre class=" language-python"><code class="language-python">str<span class="token punctuation">.</span>split<span class="token punctuation">(</span>str<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">,</span> num<span class="token operator">=</span>string<span class="token punctuation">.</span>count<span class="token punctuation">(</span>str<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回分割后的字符串列表。</span><span class="token comment" spellcheck="true">#str,分割符，默认为所有的空字符，包括空格、换行(\n)、制表符(\t)等。</span><span class="token comment" spellcheck="true">#num,分割次数。默认为 -1, 即分割所有。</span><span class="token comment" spellcheck="true">#split(",")按逗号分割，最后一条是带换行符的！</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 处理的时候，如果不带参数，默认是清除两边的空白符，例如：\n,\r,\t,' '。</span>str <span class="token operator">=</span> <span class="token string">'123@163.com'</span><span class="token keyword">print</span><span class="token punctuation">(</span>str<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'132'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#@163.com</span>str <span class="token operator">=</span> <span class="token string">'123@163.com'</span><span class="token keyword">print</span><span class="token punctuation">(</span>str<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'23'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#123@163.com</span>S<span class="token punctuation">.</span>rstrip<span class="token punctuation">(</span><span class="token punctuation">[</span>chars<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#用于删除字符串尾部指定的字符，默认字符为所有空字符，包括空格、换行(\n)、制表符(\t)等</span>startwith<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#是否以指定字符串开头</span>str <span class="token operator">=</span> <span class="token string">"hello,i love python"</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"1:"</span><span class="token punctuation">,</span>str<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"h"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"2:"</span><span class="token punctuation">,</span>str<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"l"</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#beg=2,end=10</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"3:"</span><span class="token punctuation">,</span>str<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#空字符</span><span class="token number">1</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token number">2</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token number">3</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token string">'sep'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>元组、列表、字典、字符串<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">></span>str<span class="token comment" spellcheck="true">#返回一个以分隔符sep连接各个元素后生成的字符串,字典只对键进行连接</span>replace<span class="token comment" spellcheck="true">#字符串运算遵循数学法则,单引号和双引号可以混合使用</span><span class="token operator">>></span><span class="token operator">></span><span class="token number">3</span><span class="token operator">*</span><span class="token string">"python"</span><span class="token operator">+</span><span class="token string">'1'</span><span class="token comment" spellcheck="true">#pythonpythonpython1</span><span class="token comment" spellcheck="true">#多个带引号的字符串，解释器会自动拼接</span><span class="token operator">>></span><span class="token operator">></span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token string">'b'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#ab</span></code></pre><h3 id="函数和基本数据结构"><a href="#函数和基本数据结构" class="headerlink" title="函数和基本数据结构"></a>函数和基本数据结构</h3><h4 id="列表list-序列"><a href="#列表list-序列" class="headerlink" title="列表list/序列"></a>列表list/序列</h4><pre class=" language-python"><code class="language-python">list<span class="token punctuation">(</span><span class="token punctuation">)</span> eg<span class="token punctuation">:</span>b<span class="token operator">=</span>list<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回一个新的列表，b和a不共享</span><span class="token comment" spellcheck="true">#a=list("a,b,c,d,e,!")=>a=['a','b','c','d','e']</span>list<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#向列表添加元素</span></code></pre><p><a href="https://blog.csdn.net/zhulove86/article/details/53941555?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3.control">列表推导式</a></p><img src="/python-xue-xi/image-20210120185150919.png" alt="语法" style="zoom: 50%;"><h4 id="字典dict-集合set"><a href="#字典dict-集合set" class="headerlink" title="字典dict/集合set"></a>字典dict/集合set</h4><pre class=" language-python"><code class="language-python">a<span class="token operator">=</span>b<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">"value"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#删除字典中给定键key所对应的值，返回值为被删除的值。</span>a<span class="token operator">=</span>b<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#或b.values()或b.items()</span><span class="token comment" spellcheck="true">#返回值都是view objects,它们提供了关于字典条目的动态视图，这意味着当字典发生变化时，视图会反映这些变化。</span><span class="token comment" spellcheck="true">#支持len(dictview)、iter(dictview)、x in dictview</span><span class="token comment" spellcheck="true">#若想支持map-style,可以a=list(b.keys())转化成列表</span></code></pre><p><a href="https://www.cnblogs.com/gide/p/6370082.html">ordered_dict</a></p><p>模块collections中的子类，使字典的迭代顺序就是插入的顺序</p><p>注:从python3.6开始,普通的字典也是有序的了，即按照插入的顺序迭代。</p><h4 id="enumerate"><a href="#enumerate" class="headerlink" title="enumerate()"></a>enumerate()</h4><p>enumerate() 函数用于将**一个可遍历的数据对象(如列表、元组或字符串)**组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。</p><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> seasons <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Spring'</span><span class="token punctuation">,</span> <span class="token string">'Summer'</span><span class="token punctuation">,</span> <span class="token string">'Fall'</span><span class="token punctuation">,</span> <span class="token string">'Winter'</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> list<span class="token punctuation">(</span>enumerate<span class="token punctuation">(</span>seasons<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'Spring'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'Summer'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'Fall'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'Winter'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> list<span class="token punctuation">(</span>enumerate<span class="token punctuation">(</span>seasons<span class="token punctuation">,</span> start<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># 小标从 1 开始</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'Spring'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'Summer'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'Fall'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'Winter'</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><h4 id="排序函数"><a href="#排序函数" class="headerlink" title="排序函数"></a>排序函数</h4><pre class=" language-python"><code class="language-python">sort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#应用在list上的方法,对已经存在的列表进行操作，无返回值</span>sorted<span class="token punctuation">(</span>iterable<span class="token punctuation">,</span> cmp<span class="token operator">=</span>None<span class="token punctuation">,</span> key<span class="token operator">=</span>None<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#内置函数，可以对所有可迭代的对象进行排序操作，返回副本，原始输入不变</span><span class="token comment" spellcheck="true">#iterable:可迭代对象   </span><span class="token comment" spellcheck="true">#cmp:比较函数，具有两个参数，参数的值都是从可迭代对象中取出，此函数必须遵守的规则为，大于则返回1，小于则返回-1，等于则返回0。</span><span class="token comment" spellcheck="true">#key:指定可迭代对象中的一个元素来进行排序。</span><span class="token comment" spellcheck="true">#reverse:排序规则，True为降序，False为升序</span><span class="token operator">>></span><span class="token operator">></span> L<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> sorted<span class="token punctuation">(</span>L<span class="token punctuation">,</span> cmp<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span>y<span class="token punctuation">:</span>cmp<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>y<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 利用cmp函数</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> sorted<span class="token punctuation">(</span>L<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>               <span class="token comment" spellcheck="true"># 利用key</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'d'</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><h4 id="函数属性"><a href="#函数属性" class="headerlink" title="函数属性"></a>函数属性</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_fun_name</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> func<span class="token punctuation">.</span>__name__<span class="token comment" spellcheck="true">#返回的就是函数的名字</span><span class="token keyword">def</span> <span class="token function">say_hello</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span><span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 看起来没有用处呀</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>get_fun_name<span class="token punctuation">(</span>say_hello<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 那么看下面</span>    haha <span class="token operator">=</span> say_hello    hahaha <span class="token operator">=</span> haha    <span class="token comment" spellcheck="true"># 请问 hahaha 到底是啥？</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>get_fun_name<span class="token punctuation">(</span>hahaha<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#函数名字是最开始的那个实例，其他都是函数的引用。</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    main<span class="token punctuation">(</span><span class="token punctuation">)</span>say_hellosay_hello</code></pre><h2 id="稍微不那么基础的"><a href="#稍微不那么基础的" class="headerlink" title="稍微不那么基础的"></a>稍微不那么基础的</h2><h3 id="类"><a href="#类" class="headerlink" title="类"></a>类</h3><h4 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h4><p><code>dir() eg:dir(nn.Module)</code> dir函数可以返回参数的属性、方法的列表</p><p><code>__init__()</code></p><p>意义等同于类的构造器，作用是创建一个类的实例</p><p><code>__call__()</code></p><p>为了将类的实例对象变为可调用对象,相当于重载<code>()</code>运算符</p><p>Python中的函数是一级对象。这意味着<strong>Python中的函数的引用可以作为输入传递到其他的函数/方法中，并在其中被执行</strong>。<br>而Python中<strong>类的实例（对象）可以被当做函数对待</strong>。也就是说，我们可以将它们作为输入传递到其他的函数/方法中并调用他们，正如我们调用一个正常的函数那样。而类中<code>__call__()</code>函数的意义正在于此。为了将一个类实例当做函数调用，我们需要在类中实现<code>__call__()</code>方法。也就是我们要在类中实现如下方法：<code>def __call__(self, *args)</code>。这个方法接受一定数量的变量作为输入。<br>假设x是X类的一个实例。那么调用<code>x.__call__(1,2)</code>等同于调用<code>x(1,2)</code>。这个实例本身在这里相当于一个函数。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">X</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> range<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>a <span class="token operator">=</span> a        self<span class="token punctuation">.</span>b <span class="token operator">=</span> b        self<span class="token punctuation">.</span>range <span class="token operator">=</span> range    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>a <span class="token operator">=</span> a        self<span class="token punctuation">.</span>b <span class="token operator">=</span> b        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'__call__ with （{}, {}）'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">.</span>a<span class="token punctuation">,</span> self<span class="token punctuation">.</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__del__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> range<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">del</span> self<span class="token punctuation">.</span>a        <span class="token keyword">del</span> self<span class="token punctuation">.</span>b        <span class="token keyword">del</span> self<span class="token punctuation">.</span>range<span class="token operator">>></span><span class="token operator">></span> xInstance <span class="token operator">=</span> X<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> xInstance<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>__call__ <span class="token keyword">with</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></code></pre><p><code>__len__()</code></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ids<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对于一个list,可以直接使用len([1,2,3])</span><span class="token comment" spellcheck="true">#而对于一个对象，里面有很多东西，没法直接使用len,所以要写内置函数__len__(),以后就可以len(对象)</span></code></pre><p><code>__getitem__()</code></p><p>如果在类中定义了<code>__getitem__()</code>方法，那么他的实例对象（假设为P）就可以这样P[key]取值。当实例对象做P[key]运算时，就会调用类中的<code>__getitem__()</code>方法。</p><p>此外，在用 <code>for..in..</code> 迭代对象时，如果对象没有实现 <code>__iter__</code> <code>__next__</code> 迭代器协议，Python的解释器就会去寻找<code>__getitem__</code> 来迭代对象，如果连<code>__getitem__</code> 都没有定义，这解释器就会报对象不是迭代器的错</p><p><code>@staticmethod</code>静态方法和<code>@classmethod</code>类方法</p><p><strong>一般来说，要使用某个类的方法，需要先实例化一个对象再调用方法。</strong><br><strong>而使用@staticmethod或@classmethod，就可以不需要实例化，直接类名.方法名()来调用。</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">foo</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span> <span class="token string">"executing foo(%s)"</span><span class="token operator">%</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">foo</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#self是对实例的绑定,a.foo(x)其实是foo(a,x),对象实例a隐式地作为第一个参数传递</span>        <span class="token keyword">print</span> <span class="token string">"executing foo(%s, %s)"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span>    @classmethod    <span class="token keyword">def</span> <span class="token function">class_foo</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#cls是对类的绑定,和self类似</span>        <span class="token keyword">print</span> <span class="token string">"executing class_foo(%s, %s)"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>cls<span class="token punctuation">,</span> x<span class="token punctuation">)</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">static_foo</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#%self（对象实例）和 cls（类）都不会隐式传递为第一个参数。它们的行为类似于普通函数</span>        <span class="token keyword">print</span> <span class="token string">"executing static_foo(%s)"</span> <span class="token operator">%</span> x    a <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><strong>私有属性和方法</strong></p><p>仅<strong>开头带双下划线__的命名</strong>，用于对象的数据封装，以此命名的属性或者方法为类的私有属性或者私有方法，eg:</p><p><code>def __spam(self):</code></p><p>这就起到了隐藏数据的作用，但是这种实现机制并不是很严格，机制是通过自动”变形”实现的，类中所有以双下划线开头的名称__name都会自动变为<code>_类名__name</code>的新名称。另外这种机制可以阻止继承类重新定义或者更改方法的实现。</p><p>在<strong>类中也可以用单下划线开头来命名属性或者方法</strong>，这只是表示类的定义者<strong>希望这些属性或者方法是”私有的”**，但</strong>实际上并不会起任何作用。**</p><h4 id="装饰器和-符号"><a href="#装饰器和-符号" class="headerlink" title="装饰器和@符号"></a>装饰器和@符号</h4><p>python中的函数可以像普通变量一样当做参数传递给另外一个函数</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">foo</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">bar</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    func<span class="token punctuation">(</span><span class="token punctuation">)</span>bar<span class="token punctuation">(</span>foo<span class="token punctuation">)</span></code></pre><p>具体看这一篇：<a href="https://gohom.win/2015/10/25/pyDecorator/">https://gohom.win/2015/10/25/pyDecorator/</a></p><p>这一篇讲的更清楚<a href="https://foofish.net/python-decorator.html">https://foofish.net/python-decorator.html</a></p><p>简单来说就是复合函数，概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">log</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">wrapper</span><span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kw<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span> <span class="token string">'call %s():'</span> <span class="token operator">%</span> func<span class="token punctuation">.</span>__name__        <span class="token keyword">return</span> func<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kw<span class="token punctuation">)</span>    <span class="token keyword">return</span> wrapper@log<span class="token keyword">def</span> <span class="token function">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span> <span class="token string">'2015-10-26'</span>    <span class="token keyword">return</span> <span class="token string">"done"</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#添加了装饰器之后，现在运行now(),等于运行log(now)把now传递进去了</span></code></pre><p>内置的@property和@*.setter，其实认真看完实例就明白了，就相当于python的get和set方法，关于这两个内置装饰器可以看这个：<a href="https://www.liaoxuefeng.com/wiki/897692888725344/923030547069856">@property和@xxx.setter</a></p><p>看这段代码：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">class</span> <span class="token class-name">D</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataset_dir<span class="token punctuation">:</span> str<span class="token punctuation">,</span> batch_size<span class="token punctuation">:</span> int<span class="token punctuation">,</span>train_dataset<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>dataset_dir <span class="token operator">=</span> dataset_dir        self<span class="token punctuation">.</span>batch_size <span class="token operator">=</span> batch_size        self<span class="token punctuation">.</span>val_dataset <span class="token operator">=</span> None        self<span class="token punctuation">.</span>train_dataset <span class="token operator">=</span> train_dataset<span class="token comment" spellcheck="true">#这里调用的set方法</span>        self<span class="token punctuation">.</span>test_dataset <span class="token operator">=</span> None    <span class="token comment" spellcheck="true">#train_dataset get set 方法</span>    @property    <span class="token keyword">def</span> <span class="token function">train_dataset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token keyword">print</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>__train_dataset<span class="token punctuation">)</span>    @train_dataset<span class="token punctuation">.</span>setter    <span class="token keyword">def</span> <span class="token function">train_dataset</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>__train_dataset <span class="token operator">=</span> dataset <span class="token comment" spellcheck="true">#这里新定义了一个属性</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"!!!!"</span><span class="token punctuation">)</span>d<span class="token operator">=</span>D<span class="token punctuation">(</span><span class="token string">"asdf"</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>d<span class="token punctuation">.</span>train_dataset<span class="token comment" spellcheck="true">#！！！！</span><span class="token comment" spellcheck="true">#2</span></code></pre><h4 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h4><p><a href="https://blog.csdn.net/yilulvxing/article/details/85374142">类的继承,调用父类的属性和方法</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">UNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#继承了nn.Module</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_channels<span class="token punctuation">,</span> n_classes<span class="token punctuation">,</span> bilinear<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#子类的__init__函数</span>        super<span class="token punctuation">(</span>UNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#把父类全部继承，你有想改的，你再自己改！你重写了父类的方法，还想用父类的方法，怎么办？利用super()函数 eg：super().fun()</span><span class="token comment" spellcheck="true">#如果自己也定义了__ __init____ 方法,那么父类的属性是不能直接调用的</span><span class="token comment" spellcheck="true">#可以在 子类的 __init__中调用一下父类的 __init__ 方法,这样就可以调用</span></code></pre><p><a href="https://blog.csdn.net/Windgs_YF/article/details/89026857">python中父类调用子类的属性和方法</a></p><p>python中父类可以调用子类的属性和方法,我怀疑是子类<code>super.__init__()</code>时把子类的self传过去了,父类本身找不到,就找子类的。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Animal</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">a1</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"调用a1"</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>eat<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这一步会调用子类的方法</span>    <span class="token keyword">def</span> <span class="token function">eat</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'kkkkkk'</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Person</span><span class="token punctuation">(</span>Animal<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">p1</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"调用p1"</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>a1<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">eat</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'hhhhhhhh'</span><span class="token punctuation">)</span>p <span class="token operator">=</span> Person<span class="token punctuation">(</span><span class="token punctuation">)</span>p<span class="token punctuation">.</span>p1<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#父类也实现了,子类也实现了,会优先使用子类重写的方法</span><span class="token comment" spellcheck="true">#调用p1</span><span class="token comment" spellcheck="true">#调用a1</span><span class="token comment" spellcheck="true">#hhhhhhhh</span></code></pre><h4 id="super关键字"><a href="#super关键字" class="headerlink" title="super关键字"></a>super关键字</h4><p><a href="https://www.runoob.com/w3cnote/python-super-detail-intro.html">super详解</a></p><pre class=" language-python"><code class="language-python">super<span class="token punctuation">(</span>Class<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#在python2 super把子类的类名和self示例传给父类</span>super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#python3变成了隐式的了</span><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>n <span class="token operator">=</span> <span class="token number">2</span>    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#执行时执行的是super传递过来的self，是子类的实例而不是父类的实例</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'self is {0} @A.add'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n <span class="token operator">+=</span> m<span class="token keyword">class</span> <span class="token class-name">B</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>n <span class="token operator">=</span> <span class="token number">3</span>    <span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'self is {0} @B.add'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">)</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>m<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n <span class="token operator">+=</span> <span class="token number">3</span>b <span class="token operator">=</span> B<span class="token punctuation">(</span><span class="token punctuation">)</span>b<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#输出</span><span class="token comment" spellcheck="true">#self is &lt;__main__.B object at 0x106c49b38> @B.add</span><span class="token comment" spellcheck="true">#self is &lt;__main__.B object at 0x106c49b38> @A.add</span><span class="token comment" spellcheck="true">#8</span></code></pre><h4 id="函数-1"><a href="#函数-1" class="headerlink" title="函数"></a>函数</h4><pre class=" language-python"><code class="language-python">hasattr<span class="token punctuation">(</span>object<span class="token punctuation">,</span>name<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#object:对象 name:字符串,属性名</span><span class="token comment" spellcheck="true">#如果对象有该属性返回true,否则返回false</span>vars<span class="token punctuation">(</span><span class="token punctuation">[</span>object<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#带参数:返回对象object的属性和属性值的字典对象,或者说返回对象的__dict__属性</span><span class="token comment" spellcheck="true">#不带参数:作用同locals(),即以字典对象返回当前位置的全部局部变量</span></code></pre><h3 id="参数和返回值"><a href="#参数和返回值" class="headerlink" title="参数和返回值"></a>参数和返回值</h3><h4 id="return-self"><a href="#return-self" class="headerlink" title="return self"></a>return self</h4><p>其实就是返回自身实例，用于链式调用  <a href="https://blog.csdn.net/jclian91/article/details/81238782">看这篇教程</a></p><h4 id="多个返回值或返回字典"><a href="#多个返回值或返回字典" class="headerlink" title="多个返回值或返回字典"></a>多个返回值或返回字典</h4><p>多个返回值，返回的是一个tuple</p><pre class=" language-python"><code class="language-python"><span class="token keyword">return</span> <span class="token punctuation">{</span><span class="token comment" spellcheck="true">#返回的是一个字典</span>    <span class="token string">'image'</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>img<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">'mask'</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>mask<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre><h4 id="可变长度参数"><a href="#可变长度参数" class="headerlink" title="可变长度参数"></a>可变长度参数</h4><p><code>*args</code>将参数打包成tuple给函数体调用</p><p><code>**kwargs</code>打包关键字参数成dict给函数体调用</p><p>注意点：参数<code>arg</code>、<code>*args</code>、<code>**kwargs</code>三个参数的位置必须是一定的。必须是<code>(arg,*args,**kwargs)</code>这个顺序，否则程序会报错。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">function</span><span class="token punctuation">(</span>arg<span class="token punctuation">,</span><span class="token operator">*</span>args<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>arg<span class="token punctuation">,</span>args<span class="token punctuation">,</span>kwargs<span class="token punctuation">)</span>function<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span>a<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> b<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> c<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#注意传参方式！</span><span class="token comment" spellcheck="true">#6 (7,8,9) {'c':3,'a':1,'b':2}</span>data_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'transform'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'base_size'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>               <span class="token string">'crop_size'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'logger'</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span>               <span class="token string">'scale'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">}</span><span class="token keyword">def</span> <span class="token function">get_segmentation_dataset</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>get_segmentation_dataset<span class="token punctuation">(</span><span class="token string">"1"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> a<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">**</span>data_kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#split=1, a=2, **data_kwargs都是传给**kwargs</span><span class="token comment" spellcheck="true">#注意，元组整体传参前面必须加*，字典前面必须加**</span></code></pre><h4 id="类型注解"><a href="#类型注解" class="headerlink" title="类型注解"></a>类型注解</h4><p><code>def add_to_graph(self, dataset) -&gt; tf.data.Dataset:</code> 箭头的作用是给函数添加注解，用来说明返回值的数据类型</p><p><code>def get_filters_count(level: int, initial_filters: int) -&gt; int:</code>冒号的作用是说参数的数据类型</p><p><a href="https://blog.csdn.net/jeffery0207/article/details/93734942">Typing模块介绍</a></p><p>在实际使用中， <code>Any, Union, Tuple, List, Sequence, Mapping, Callable, TypeVar,Optional, Generic</code>等的使用频率比较高，其中<code>Union、Optional、Sequence、Mapping</code>非常有用，注意掌握。</p><p>Union</p><p>即并集，所以<code>Union[X, Y]</code> 意思是要么X类型、要么Y类型</p><p>Optional</p><p><code>Optional[X]</code>与<code>Union[X, None]</code>，即它默认允许None类型</p><p>Sequence</p><p>即序列，需要注意的是，<code>List</code>一般用来标注返回值；<code>Sequence、Iterable</code>用来标注参数类型</p><p>Mapping</p><p>即字典，需要注意的是，<code>Dict</code>一般用来标注返回值；<code>Mapping</code>用来标注参数类型</p><p>Any<br>Any与任何类型兼容</p><p>类型检查函数</p><p><code>isinstance(object,classinfo)</code></p><p>判断一个变量object是否是classinfo类型的。与type区别:type不会考虑子类是一种父类类型，不考虑继承关系;instance认为是，考虑继承关系。</p><h3 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h3><p><a href="https://www.cnblogs.com/zhaopanpan/p/8577045.html">好的博客教程</a></p><p>python解释器检测到错误，触发异常（也允许程序员自己触发异常）。程序员编写特定的代码，专门用来捕捉这个异常（这段代码与程序逻辑无关，与异常处理有关）。如果捕捉成功则进入另外一个处理分支，执行你为其定制的逻辑，使程序不会崩溃，这就是异常处理。</p><p><strong>程序运行中的异常可以分为两类：语法错误和逻辑错误。首先，我们必须知道，语法错误跟异常处理无关，所以我们在处理异常之前，必须避免语法上的错误。</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#AttributeError 试图访问一个对象没有的树形，比如foo.x，但是foo没有属性x</span><span class="token comment" spellcheck="true">#IOError 输入/输出异常；基本上是无法打开文件</span><span class="token comment" spellcheck="true">#ImportError 无法引入模块或包；基本上是路径问题或名称错误</span><span class="token comment" spellcheck="true">#IndentationError 语法错误（的子类） ；代码没有正确对齐</span><span class="token comment" spellcheck="true">#IndexError 下标索引超出序列边界，比如当x只有三个元素，却试图访问x[5]</span><span class="token comment" spellcheck="true">#KeyError 试图访问字典里不存在的键</span><span class="token comment" spellcheck="true">#KeyboardInterrupt Ctrl+C被按下</span><span class="token comment" spellcheck="true">#NameError 使用一个还未被赋予对象的变量</span><span class="token comment" spellcheck="true">#SyntaxError Python代码非法，代码不能编译(个人认为这是语法错误，写错了）</span><span class="token comment" spellcheck="true">#TypeError 传入对象类型与要求的不符合</span><span class="token comment" spellcheck="true">#UnboundLocalError 试图访问一个还未被设置的局部变量，基本上是由于另有一个同名的全局变量，导致你以为正在访问它</span><span class="token comment" spellcheck="true">#ValueError 传入一个调用者不期望的值，即使值的类型是正确的</span><span class="token comment" spellcheck="true">#SystemExit 解释器请求退出</span><span class="token comment" spellcheck="true">#Exception 万能异常，捕获任意异常</span></code></pre><h4 id="try-except"><a href="#try-except" class="headerlink" title="try except"></a>try except</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">try</span><span class="token punctuation">:</span>     被检测的代码块<span class="token keyword">except</span> 异常类型：     <span class="token keyword">try</span>中一旦检测到异常，就执行这个位置的逻辑eg：    <span class="token keyword">try</span><span class="token punctuation">:</span>        train_net<span class="token punctuation">(</span>net<span class="token operator">=</span>net<span class="token punctuation">,</span>                  epochs<span class="token operator">=</span>args<span class="token punctuation">.</span>epochs<span class="token punctuation">,</span>                  batch_size<span class="token operator">=</span>args<span class="token punctuation">.</span>batchsize<span class="token punctuation">,</span>                  lr<span class="token operator">=</span>args<span class="token punctuation">.</span>lr<span class="token punctuation">,</span>                  device<span class="token operator">=</span>device<span class="token punctuation">,</span>                  img_scale<span class="token operator">=</span>args<span class="token punctuation">.</span>scale<span class="token punctuation">,</span>                  val_percent<span class="token operator">=</span>args<span class="token punctuation">.</span>val <span class="token operator">/</span> <span class="token number">100</span><span class="token punctuation">)</span>    <span class="token keyword">except</span> KeyboardInterrupt<span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'INTERRUPTED.pth'</span><span class="token punctuation">)</span>        logging<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'Saved interrupt'</span><span class="token punctuation">)</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            sys<span class="token punctuation">.</span>exit<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">except</span> SystemExit<span class="token punctuation">:</span>            os<span class="token punctuation">.</span>_exit<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></code></pre><p><code>try finally</code></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">dealwith_file</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">try</span><span class="token punctuation">:</span>        f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'file'</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">:</span>            int<span class="token punctuation">(</span>line<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">True</span>    <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">False</span>    <span class="token keyword">finally</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''不管try语句中的代码是否报错,都会执行finally分支中的代码'''</span>        <span class="token triple-quoted-string string">'''去完成一些连接操作的收尾工作'''</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'finally 被执行了'</span><span class="token punctuation">)</span>        f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>ret <span class="token operator">=</span> dealwith_file<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>ret<span class="token punctuation">)</span></code></pre><h4 id="assert"><a href="#assert" class="headerlink" title="assert"></a>assert</h4><p>Python assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。</p><p><code>assert newW &gt; 0 and newH &gt; 0, 'Scale is too small'</code></p><h4 id="raise"><a href="#raise" class="headerlink" title="raise"></a>raise</h4><p><a href="https://blog.csdn.net/sinat_38682860/article/details/98469803">NotImplemented详解</a></p><p><code>raise NotImplemented</code></p><p>NotImplemented是Python在内置命名空间中的六个常数之一。其他有False、True、None、Ellipsis 和 <strong>debug</strong>。和 Ellipsis很像，NotImplemented]能被重新赋值（覆盖）。对它赋值，甚至改变属性名称， 不会产生 SyntaxError。所以它不是一个真正的“真”常数。当然，我们应该永远不改变它。</p><p>NotImplemented 是个特殊值，它能被二元特殊方法返回（比如<code>__eq__()、__lt__() 、__add__() 、__rsub__() </code>等），表明某个类型没有像其他类型那样实现这些操作。同样，它或许会被原地处理（in place）的二元特殊方法返回（比如<code>__imul__()、__iand__()</code>等）。还有，它的实际值为True：</p><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> bool<span class="token punctuation">(</span>NotImplemented<span class="token punctuation">)</span><span class="token boolean">True</span><span class="token operator">>></span><span class="token operator">></span> b1 <span class="token operator">==</span> a1Could <span class="token operator">not</span> compare B against the other <span class="token keyword">class</span><span class="token class-name">Comparing</span> an A <span class="token keyword">with</span> a B<span class="token boolean">True</span></code></pre><p>这就是返回了NotImplemented的所做的。NotImplemented告诉运行时，应该让其他对象来完成某个操作。在表达b1 == a1中，<code>b1.__eq__(a1)</code>返回了NotImplemented，这说明Python试着用<code>a1.__eq__(b1)</code>。由于a1足够可以返回True，因此这个表达可以成功。如果A中的<code>__eq__()</code>也返回NotImplemented，那么运行时会退化到使用内置的比较行为，即比较对象的标识符（在CPython中，是对象在内存中的地址）。</p><p>注意：如果在调用<code>b1.__eq__(a1)</code>时抛出NotImpementedError，而不进行处理，就会中断代码的执行。而NotImplemented无法抛出，仅仅是用来进一步测试是否有其他方法可供调用。</p><h2 id="各种包"><a href="#各种包" class="headerlink" title="各种包"></a>各种包</h2><h3 id="模块与包"><a href="#模块与包" class="headerlink" title="模块与包"></a>模块与包</h3><p><a href="https://www.jianshu.com/p/95afe2c3d526">模块与包的区别</a></p><p>简而言之单个py问价就是一个模块，讲几个py文件合起来就是一个包，但是python没法识别，所以需要添加一个<code>__init__.py</code>来告诉解释器</p><pre class=" language-python"><code class="language-python">packet<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>__init__<span class="token punctuation">.</span>py<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>a<span class="token punctuation">.</span>py<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>b<span class="token punctuation">.</span>py<span class="token keyword">import</span> packet<span class="token punctuation">.</span>a <span class="token comment" spellcheck="true">#用.</span><span class="token keyword">import</span> packet <span class="token comment" spellcheck="true">#导入的是__inti__.py里面导入的</span></code></pre><p>事实上，当我们向文件导入某个模块时，<strong>导入的是该模块中那些名称不以下划线（单下划线“_”或者双下划线“__”）开头的变量、函数和类</strong>。因此，如果我们不想模块文件中的某个成员被引入到其它文件中使用，可以在其名称前添加下划线。</p><p><code>__all__</code></p><p>除此之外，还可以借助模块提供的<code> __all__</code> 变量，该变量的值是一个列表，存储的是当前模块中一些成员（变量、函数或者类）的名称。通过在模块文件中设置 <code>__all__</code> 变量，当其它文件以<code>from 模块名 import *</code>的形式(仅此种形式)导入该模块时，该文件中只能使用 <code>__all__</code> 列表中指定的成员。</p><h3 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a>tqdm</h3><p><a href="https://www.cnblogs.com/wanghui-garcia/p/11514579.html">tqdm详细教程</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm<span class="token keyword">for</span> fname <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>input_dir<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#fname就是os.listdir(input_dir)的每一项</span><span class="token comment" spellcheck="true">#tqdm(range(i))可以使用trange(i)替换</span></code></pre><p>另一种调用方式：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> tqdm<span class="token punctuation">(</span>total<span class="token operator">=</span>n_train<span class="token punctuation">,</span> desc<span class="token operator">=</span>f<span class="token string">'Epoch {epoch + 1}/{epochs}'</span><span class="token punctuation">,</span> unit<span class="token operator">=</span><span class="token string">'img'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#使用with的好处是不用手动关闭pbar</span></code></pre><p>文档</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">tqdm</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#装饰一个迭代器对象，返回一个表现得就像原来可迭代的迭代器；但是在每次值被请求时就打印一个动态的更新进度条</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> iterable<span class="token operator">=</span>None<span class="token punctuation">,</span> desc<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#进度条的前缀</span>               total<span class="token operator">=</span>None<span class="token punctuation">,</span>               leave<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#默认为True，即在迭代的最后保持进度条的所有踪迹，简单来说就是会把进度条的最终形态保留下来。</span>               <span class="token comment" spellcheck="true">#否则最后进度条消失</span>               file<span class="token operator">=</span>None<span class="token punctuation">,</span> ncols<span class="token operator">=</span>None<span class="token punctuation">,</span> mininterval<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>               maxinterval<span class="token operator">=</span><span class="token number">10.0</span><span class="token punctuation">,</span> miniters<span class="token operator">=</span>None<span class="token punctuation">,</span> ascii<span class="token operator">=</span>None<span class="token punctuation">,</span> disable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>               unit<span class="token operator">=</span><span class="token string">'it'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#用来定义每个迭代单元的字符串。默认为"it"，表示每个迭代；在下载或解压时，设为"B"，代表每个“块”。</span><span class="token comment" spellcheck="true">#显示速度时,是1s/it还是1s/B,在时间后面</span>               unit_scale<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> dynamic_ncols<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>               smoothing<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> bar_format<span class="token operator">=</span>None<span class="token punctuation">,</span> initial<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> position<span class="token operator">=</span>None<span class="token punctuation">,</span>               postfix<span class="token operator">=</span>None<span class="token punctuation">,</span> unit_divisor<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>pbar<span class="token punctuation">.</span>set_description<span class="token punctuation">(</span><span class="token string">'Epoch %d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#与前面tqdm构造时的desc相同</span><span class="token keyword">def</span> <span class="token function">set_postfix</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ordered_dict<span class="token operator">=</span>None<span class="token punctuation">,</span> refresh<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#设置/修改后缀,在速度后面</span><span class="token comment" spellcheck="true">#ordered_dict是传入有序字典,后边的**kwargs则是对ordered_dict的覆盖和补充。输出为a=1这种形式。</span><span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#手动更新进度条，对流streams有用，比如读文件</span><span class="token comment" spellcheck="true">#n:int, optional 添加到迭代内部计数器的增长数[default:1]</span></code></pre><h3 id="os"><a href="#os" class="headerlink" title="os"></a>os</h3><p>os模块提供了一个统一的操作系统接口函数，os模块能在不同操作系统平台如nt，posix中的特定函数间自动切换，从而实现跨平台操作。</p><p><a href="https://www.jianshu.com/p/86f88b3d7efd">简书教程</a></p><pre class=" language-python"><code class="language-python">a<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">"datasets"</span><span class="token punctuation">,</span> <span class="token string">"lifesat"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#连接两个路径</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#datasets\lifesat\</span>os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#递归的创建文件目录</span><span class="token comment" spellcheck="true">#exist_ok:是否在目录存在时触发异常。</span><span class="token comment" spellcheck="true">#如果exist_ok为False（默认值），则在目标目录已存在的情况下触发FileExistsError异常；</span><span class="token comment" spellcheck="true">#如果exist_ok为True，则在目标目录已存在的情况下不会触发FileExistsError异常。</span>os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#获取指定目录下的所有文件和目录 包括隐藏文件</span><span class="token comment" spellcheck="true">#eg:a/b.txt,listdir(a),输出b.txt,没有/之前的</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#分离文件名与扩展名 eg：</span>path_01<span class="token operator">=</span><span class="token string">'E:\STH\Foobar2000\install.log'</span>path_02<span class="token operator">=</span><span class="token string">'E:\STH\Foobar2000'</span>res_01<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span>path_01<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回的是tuple</span>res_02<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>splitext<span class="token punctuation">(</span>path_02<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>root_01<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>root_02<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#('E:\\STH\\Foobar2000\\install', '.log')</span><span class="token comment" spellcheck="true">#('E:\\STH\\Foobar2000', '')</span><span class="token comment" spellcheck="true">#返回文件名和路径名</span>a <span class="token operator">=</span> <span class="token string">"D:\\class_datas\\master\\semanticseg\\Pytorch-UNet\\train.py"</span><span class="token keyword">print</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>basename<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#train.py</span><span class="token comment" spellcheck="true">#D:\class_datas\master\semanticseg\Pytorch-UNet</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#environ是一个字符串所对应环境的映像对象。举个例子来说，environ['HOME']就代表了当前这个用户的主目录。</span><span class="token comment" spellcheck="true">#CUDA——VISIBLE</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>expanduser<span class="token punctuation">(</span>root<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#主要的功能在于把路径中的~转化为user目录，一般使用在Linux系统，代码中设置了某些路径的环境变量的时候。</span></code></pre><h3 id="shutil"><a href="#shutil" class="headerlink" title="shutil"></a>shutil</h3><p><a href="https://liujiangblog.com/course/python/61">https://liujiangblog.com/course/python/61</a></p><p><strong>shutil模块是对os模块的补充，主要针对文件的拷贝、删除、移动、压缩和解压操作。</strong></p><h3 id="sys"><a href="#sys" class="headerlink" title="sys"></a>sys</h3><p><a href="https://www.liujiangblog.com/course/python/54">教程</a></p><p><strong>sys模块主要是针对与Python解释器相关的变量和方法，不是主机操作系统。</strong></p><p>sys.argv</p><h3 id="argparse"><a href="#argparse" class="headerlink" title="argparse"></a>argparse</h3><p>python自带的命令行参数解析包，可以用来方便地读取命令行参数，当你的代码需要频繁地修改参数的时候，使用这个工具可以将参数和代码分离开来，让你的代码更简洁，适用范围更广。它解析sys.argv中的参数。</p><p><a href="https://geek-docs.com/python/python-tutorial/python-argparse.html">教程</a></p><p><a href="https://vra.github.io/2017/12/02/argparse-usage/">一篇比较好的个人博客 add_argument里的各个参数在这看</a></p><h4 id="可选参数、必需参数和位置参数"><a href="#可选参数、必需参数和位置参数" class="headerlink" title="可选参数、必需参数和位置参数"></a>可选参数、必需参数和位置参数</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#位置参数</span><span class="token keyword">import</span> argparseparser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'age'</span><span class="token punctuation">)</span>args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'{args.name} is {args.age} years old'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#$ positional_arg.py Peter 23</span><span class="token comment" spellcheck="true">#Peter is 23 years old</span><span class="token comment" spellcheck="true">#可选参数</span><span class="token keyword">import</span> argparse<span class="token comment" spellcheck="true"># help flag provides flag help</span><span class="token comment" spellcheck="true"># store_true actions stores argument as True</span>parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-o'</span><span class="token punctuation">,</span> <span class="token string">'--output'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">"shows output"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#具有两个选项的参数,short -o和long --output,如果设置为store_true，则将参数存储为True。</span><span class="token comment" spellcheck="true">#参数一旦存在，则action将其设置为true，如果加上default时，未设置就是default起作用，否则就是action的相反。</span>args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> args<span class="token punctuation">.</span>output<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#存在该参数，则显示一些输出。</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"This is some output"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#可选参数推断规则 --foo-bar -> foo_bar -x -> x</span><span class="token comment" spellcheck="true">#可将可选参数变为必需参数</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--name'</span><span class="token punctuation">,</span> required<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置required为True</span></code></pre><h4 id="add-argument的各种参数"><a href="#add-argument的各种参数" class="headerlink" title="add_argument的各种参数"></a>add_argument的各种参数</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#metavar参数可以让命令的帮助信息更好看一些！</span><span class="token comment" spellcheck="true">#除此之外，对于有nargs参数的命令行参数，可以用metavar来设置每一个具体的参数的名称：</span>parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>prog<span class="token operator">=</span><span class="token string">'PROG'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#prog代替了test.py</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-x'</span><span class="token punctuation">,</span> nargs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--foo'</span><span class="token punctuation">,</span> nargs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'bar'</span><span class="token punctuation">,</span> <span class="token string">'baz'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>print_help<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这个可以输出帮助信息</span><span class="token comment" spellcheck="true">#输出如下</span>usage<span class="token punctuation">:</span> PROG <span class="token punctuation">[</span><span class="token operator">-</span>h<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span>x X X<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>foo bar baz<span class="token punctuation">]</span>optional arguments<span class="token punctuation">:</span> <span class="token operator">-</span>h<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>help     show this help message <span class="token operator">and</span> exit <span class="token operator">-</span>x X X <span class="token operator">-</span><span class="token operator">-</span>foo bar baz<span class="token operator">>></span><span class="token operator">></span>python train<span class="token punctuation">.</span>py <span class="token operator">-</span>h<span class="token comment" spellcheck="true">#可以显示提示信息,和上面的parser.print_help()一样</span>parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">'Train the UNet on images and target masks'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#usage下面加了一行描述信息</span>                      formatter_class<span class="token operator">=</span>argparse<span class="token punctuation">.</span>ArgumentDefaultsHelpFormatter<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#提示信息格式</span><span class="token comment" spellcheck="true">#创建一个解析器</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-l'</span><span class="token punctuation">,</span> <span class="token string">'--learning-rate'</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'LR'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>float<span class="token punctuation">,</span> nargs<span class="token operator">=</span><span class="token string">'?'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span>                        help<span class="token operator">=</span><span class="token string">'Learning rate'</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">'lr'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#添加一个新参数</span><span class="token comment" spellcheck="true">#argparse默认的变量名是--或-后面的字符串</span><span class="token comment" spellcheck="true">#但是你也可以通过dest=xxx来设置参数的变量名，然后在代码中用args.xxx来获取参数的值。</span><span class="token comment" spellcheck="true">#nargs=x</span><span class="token comment" spellcheck="true">#x的值  含义</span><span class="token comment" spellcheck="true">#N   参数的绝对个数（例如：3）</span><span class="token comment" spellcheck="true">#'?'   0或1个参数</span><span class="token comment" spellcheck="true">#'*'   0或所有参数</span><span class="token comment" spellcheck="true">#'+'   所有，并且至少一个参数</span>arg<span class="token operator">=</span>parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>arg<span class="token punctuation">.</span>dataset<span class="token operator">=</span><span class="token comment" spellcheck="true">#某个值</span><span class="token comment" spellcheck="true">#如果dataset之前没定义,那么就是给arg添加了一个新属性dataset</span></code></pre><h4 id="修改args来不用命令行使用"><a href="#修改args来不用命令行使用" class="headerlink" title="修改args来不用命令行使用"></a>修改args来不用命令行使用</h4><p><a href="https://stackoom.com/question/3RVtX/%E8%B0%83%E8%AF%95%E6%97%B6%E6%A8%A1%E6%8B%9Fargparse%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%BE%93%E5%85%A5">修改args来不用命令行使用</a></p><p><img src="/python-xue-xi/image-20201116220928895.png" alt="首选方式"></p><h3 id="logging"><a href="#logging" class="headerlink" title="logging"></a>logging</h3><p>logging模块定义的函数和类为应用程序和库的开发实现了一个灵活的事件日志系统。</p><p>logging模块提供了两种记录日志的方式：</p><ul><li>第一种方式是使用logging提供的模块级别的函数</li><li>第二种方式是使用Logging日志系统的四大组件</li></ul><p>其实，logging所提供的模块级别的日志记录函数也是对logging日志系统相关类的封装而已。</p><p><a href="https://blog.csdn.net/pansaky/article/details/82685663">logging用法详解：组件的办法</a></p><p><a href="https://www.cnblogs.com/yyds/p/6901864.html">日志logging模块教程</a></p><p>logging模块与log4j的机制是一样的，只是具体的实现细节不同。模块提供logger，handler，filter，formatter。</p><ul><li><p>logger：提供日志接口，供应用代码使用。logger最常用的操作有两类：配置和发送日志消息。可以通过logging.getLogger(name)获取logger对象，如果不指定name则返回root对象，多次使用相同的name调用getLogger方法返回同一个logger对象。</p></li><li><p>handler：将日志记录（log record）发送到合适的目的地（destination），比如文件，socket等。一个logger对象可以通过addHandler方法添加0到多个handler，每个handler又可以定义不同日志级别，以实现日志分级过滤显示。用于将日志记录发送到指定的目的位置</p></li><li><p>filter：提供一种优雅的方式决定一个日志记录是否发送到handler。提供更细粒度的日志过滤功能，用于决定哪些日志记录将会被输出（其它的日志记录将会被忽略）</p></li><li><p>formatter：指定日志记录输出的具体格式。formatter的构造方法需要两个参数：消息的格式字符串和日期字符串，这两个参数都是可选的。</p><p>logging模块提供的模块级别的那些函数实际上也是通过这几个组件的相关实现类来记录日志的，只是在创建这些类的实例时设置了一些默认值。</p></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#四大组件的方法</span>logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#日志对象，logging模块中最基础的对象</span>logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置日志级别</span>BASIC_FORMAT <span class="token operator">=</span> <span class="token string">"%(asctime)s: %(message)s"</span>DATE_FORMAT <span class="token operator">=</span> <span class="token string">'%Y-%m-%d %H:%M:%S'</span>formatter <span class="token operator">=</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span>BASIC_FORMAT<span class="token punctuation">,</span> DATE_FORMAT<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#日志记录输出的具体格式</span>chlr <span class="token operator">=</span> logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#就好比windows的console，打印在CMD</span>chlr<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置chlr handler的格式</span>chlr<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span><span class="token string">'INFO'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置chlr handler的级别,INFO级别及以上输出</span>fhlr <span class="token operator">=</span> logging<span class="token punctuation">.</span>FileHandler<span class="token punctuation">(</span>osp<span class="token punctuation">.</span>join<span class="token punctuation">(</span>final_log_path<span class="token punctuation">,</span> log_file<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#文件日志，打印在文件里</span>fhlr<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>formatter<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置格式</span><span class="token comment" spellcheck="true">#文件没有setLevel,我怀疑就是使用logger的Level</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>chlr<span class="token punctuation">)</span>logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>fhlr<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将相应的handler添加到logger上</span><span class="token keyword">return</span> logger<span class="token comment" spellcheck="true">#之后使用就是eg:logger.debug(message)</span><span class="token comment" spellcheck="true">#模块级别</span>logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">,</span> format<span class="token operator">=</span><span class="token string">'%(levelname)s:%(name)s:%(message)s'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#basicConfig:对root logger进行一次性配置</span><span class="token comment" spellcheck="true">#level:指定日志器的日志级别,>=INFO级别的日志才会输出 </span><span class="token comment" spellcheck="true">#format:指定日志格式字符串，即指定日志输出时所包含的字段信息以及它们的顺序。</span><span class="token comment" spellcheck="true"># eg WARNING:root:This is a warning log.</span><span class="token comment" spellcheck="true">#如果不设置basicConfig默认级别为Warning</span><span class="token comment" spellcheck="true">#filename:指定使用指定的文件名而不是StreamHandler创建FileHandler。</span><span class="token comment" spellcheck="true">#filemode:指定打开文件的模式，如果指定了filename（如果文件模式未指定，则默认为'a'）。</span><span class="token comment" spellcheck="true">#datefmt:使用指定的日期/时间格式。</span><span class="token comment" spellcheck="true">#handlers:如果指定，这应该是一个已经创建的处理程序的迭代器添加到根记录器。任何尚未设置格式化程序的处理程序都将被分配在此函数中创建的默认格式化程序。</span>logging<span class="token punctuation">.</span>info<span class="token punctuation">(</span>f<span class="token string">'Using device {device}'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#创建一条严重级别为INFO的日志记录,也可这么写logging.log(logging.INFO, "This is a info log.")</span></code></pre><h4 id="格式字符串字段"><a href="#格式字符串字段" class="headerlink" title="格式字符串字段"></a>格式字符串字段</h4><pre class=" language-python"><code class="language-python"><span class="token operator">%</span><span class="token punctuation">(</span>levelname<span class="token punctuation">)</span>s<span class="token comment" spellcheck="true">#该日志记录的文字形式的日志级别（'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'）</span><span class="token operator">%</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span>s<span class="token comment" spellcheck="true">#所使用的日志器名称，默认是'root'，因为默认使用的是 rootLogger</span><span class="token operator">%</span><span class="token punctuation">(</span>message<span class="token punctuation">)</span>s<span class="token comment" spellcheck="true">#日志记录的文本内容，通过 msg % args计算得到的</span><span class="token operator">%</span><span class="token punctuation">(</span>asctime<span class="token punctuation">)</span>s<span class="token comment" spellcheck="true">#字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒</span></code></pre><h3 id="glob"><a href="#glob" class="headerlink" title="glob"></a>glob</h3><p><a href="https://blog.csdn.net/gufenchen/article/details/90723418">教程</a></p><p>glob是python自带的一个操作文件的相关模块，由于模块功能比较少，所以很容易掌握。用它可以查找符合特定规则的文件路径名。使用该模块查找文件，只需要用到： <code>*</code>  <code>?</code>  <code>[]</code>三个匹配符;</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#”*”匹配0个或多个字符；</span><span class="token comment" spellcheck="true">#”?”匹配单个字符；</span><span class="token comment" spellcheck="true">#”[]”匹配指定范围内的字符，如：[0-9]匹配数字。</span>glob<span class="token punctuation">.</span>glob<span class="token comment" spellcheck="true">#返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径。</span><span class="token comment" spellcheck="true">#python的glob模块可以对文件夹下所有文件进行遍历，并保存为一个list列表</span>mask_file <span class="token operator">=</span> glob<span class="token punctuation">.</span>glob<span class="token punctuation">(</span>self<span class="token punctuation">.</span>masks_dir <span class="token operator">+</span> idx <span class="token operator">+</span> self<span class="token punctuation">.</span>mask_suffix <span class="token operator">+</span> <span class="token string">'.*'</span><span class="token punctuation">)</span></code></pre><h3 id="time"><a href="#time" class="headerlink" title="time"></a>time</h3><p><a href="https://www.cnblogs.com/pal-duan/p/10568829.html">time模块常用方法</a></p><pre class=" language-python"><code class="language-python">time<span class="token punctuation">.</span>strftime<span class="token punctuation">(</span>format<span class="token punctuation">[</span><span class="token punctuation">,</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将struct_time类型的时间转换为format参数指定格式的字符串。</span><span class="token comment" spellcheck="true">#format:指定转换时间的字符串格式。</span><span class="token comment" spellcheck="true">#t:struct_time类型的时间，如果不填默认为当前时间（即time.localtime()返回的时间）</span></code></pre><h3 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h3><h4 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h4><pre class=" language-python"><code class="language-python">Image<span class="token punctuation">.</span>crop<span class="token punctuation">(</span>left<span class="token punctuation">,</span> up<span class="token punctuation">,</span> right<span class="token punctuation">,</span> below<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#切四刀获得一个新的小块</span><span class="token comment" spellcheck="true">#left:与左边界的距离</span><span class="token comment" spellcheck="true">#up:与上边界的距离</span><span class="token comment" spellcheck="true">#right:还是与左边界的距离</span><span class="token comment" spellcheck="true">#below:还是与上边界的距离</span>img <span class="token operator">=</span> img<span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span>ow<span class="token punctuation">,</span> oh<span class="token punctuation">)</span><span class="token punctuation">,</span> Image<span class="token punctuation">.</span>BILINEAR<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#Image.NEAREST:最近邻插值</span><span class="token comment" spellcheck="true">#Image.BILINEAR:双线性插值</span><span class="token comment" spellcheck="true">#Image.BICUBIC:双三次插值</span><span class="token comment" spellcheck="true">#Image.ANTIALIAS:面积插值</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>rotate<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#45°旋转</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>FLIP_LEFT_RIGHT<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#水平翻转</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>FLIP_TOP_BOTTOM<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#垂直翻转</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>ROTATE_90<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 90</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>ROTATE_180<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#180°顺时针翻转</span>out <span class="token operator">=</span> im<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>Image<span class="token punctuation">.</span>ROTATE_270<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#270°顺时针翻转</span></code></pre><h4 id="ImageOps"><a href="#ImageOps" class="headerlink" title="ImageOps"></a>ImageOps</h4><p><a href="https://blog.csdn.net/icamera0/article/details/50785776">ImageOps模块介绍</a></p><p>PIL的一些图像处理操作</p><pre class=" language-python"><code class="language-python">ImageOps<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>image<span class="token punctuation">,</span> border<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> fill<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>⇒ image<span class="token comment" spellcheck="true">#eg:ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)</span><span class="token comment" spellcheck="true">#按照变量border的四元组，在图像的左、上、右、下四个边，使用给定的颜色填充相应的行和列。</span></code></pre><h3 id="random"><a href="#random" class="headerlink" title="random"></a>random</h3><p><a href="https://blog.csdn.net/qq_42849332/article/details/81516356?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.control">random新手必看</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> random<span class="token keyword">print</span><span class="token punctuation">(</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#随机生成[0,1.0)的浮点数</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#用于生成一个指定范围内的整数。其中参数a是下限，参数b是上限，生成的随机数n: a &lt;= n &lt;= b(闭区间)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch学习</title>
      <link href="pytorch-xue-xi/"/>
      <url>pytorch-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><p>handbook：<a href="https://github.com/zergtant/pytorch-handbook">https://github.com/zergtant/pytorch-handbook</a></p><p>pytorch API:<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><p>类似例程：<a href="https://github.com/yunjey/pytorch-tutorial">https://github.com/yunjey/pytorch-tutorial</a></p><p>Docs：<a href="https://github.com/fendouai/PyTorchDocs">https://github.com/fendouai/PyTorchDocs</a></p><h3 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h3><p>torch运用就和np一样</p><p>一个简单的网络最基本的步骤就是<strong>预处理，前向，损失，反向，更新</strong></p><h4 id="torch-tensor"><a href="#torch-tensor" class="headerlink" title="torch.tensor"></a>torch.tensor</h4><p>torch.tensor(3.14)这是标量   torch.tensor([3.14])这是向量，判断是几维张量主要是<strong>看有几个中括号</strong></p><p><img src="/pytorch-xue-xi/image-20201113101903090.png" alt="标量、向量和矩阵"></p><p>不是基本数据类型如int，float，string等，而是<strong>引用数据类型</strong></p><p>是在类中封装好的。所以肯定相应操作比如运算符等人家已经给你重载了，所以不用想的太多</p><p>两个tensor<strong>相加如果是同维度</strong>的话，就直接<strong>对应元素相加</strong></p><h4 id="pytorch通道顺序"><a href="#pytorch通道顺序" class="headerlink" title="pytorch通道顺序"></a>pytorch通道顺序</h4><p><strong>NCHW</strong></p><h4 id="基本函数"><a href="#基本函数" class="headerlink" title="基本函数"></a>基本函数</h4><pre class=" language-python"><code class="language-python">x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回一个新的与原张量数据相同但形状不同的张量,-1是指从其他维度推断！</span>y<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#"_"结尾的函数,会用结果替换原变量</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>   eg<span class="token punctuation">:</span>x<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回这个张量的值作为一个标准的Python数。这只适用于只有一个元素的张量。不可微操作</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span> eg<span class="token punctuation">:</span>a<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回张量作为一个(嵌套的)列表。对于标量，返回一个标准的Python数字，就像item()一样。如果需要，张量会首先自动移动到CPU。</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  a<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true">#返回维度 eg:torch.Size([4, 4])</span>numpy_a<span class="token operator">=</span>a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tensor转numpy</span>torch_a<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>numpy_a<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#numpy转tensor</span><span class="token comment" spellcheck="true">#Tensor和numpy对象共享内存，转换很快，但这也意味着，如果其中一个变了，另一个也会变</span>x<span class="token punctuation">.</span>type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果没有提供dtype返回类型，否则将该对象强制转换为指定的类型,并返回该对象。</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#禁止梯度计算的上下文管理器，当您确定不会调用张量.backward()时，禁用梯度计算对于推断是很有用的。</span><span class="token comment" spellcheck="true">#它将减少计算的内存消耗，否则需要require_grad =True。</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span> → torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>Generator<span class="token comment" spellcheck="true">#Sets the seed for generating random numbers. Returns a torch.Generator object.</span>torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#eg:输入shape为(A·1·B·1·C·1·D),输入张量的shape就是(A·B·C·D)</span><span class="token comment" spellcheck="true">#如果指定维度的话，那只对该维度去1。注意：返回的张量与输入张量共享存储空间，因此改变一个张量的内容将改变另一个张量的内容。</span><span class="token comment" spellcheck="true">#另外如果对批次batch为1也去掉的话，可能会引发错误。</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 增加一个1维度</span>troch<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#_, predicted = torch.max(outputs, 1)</span><span class="token comment" spellcheck="true">#outputs是数据Tensor，1表示求第一维度上的最大值</span><span class="token comment" spellcheck="true">#_是不要了  torch.max（）的返回值分两部分，分别是values和indices</span>torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>input<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token punctuation">(</span>Tensor<span class="token punctuation">,</span> LongTensor<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 返回一个命名元组(values, indices)，其中values是给定维度dim中输入张量的每一行的最大值。indices是找到的每个最大值(argmax)的索引位置。</span><span class="token comment" spellcheck="true"># 返回一个命名元组(values,indices)</span><span class="token comment" spellcheck="true"># 其中values是给定维度dim中输入张量的每一行的最大值</span><span class="token comment" spellcheck="true"># indices是找到的每个最大值(argmax)的索引位置。</span><span class="token comment" spellcheck="true"># 如果keepdim为True，则输出张量与输入张量的大小相同，除了dim维度的大小为1。</span><span class="token comment" spellcheck="true"># 否则，dim被压缩(参见torch.squeeze())，导致输出张量比输入少1维。</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#rand从(0,1)的均匀分布中随机抽样</span><span class="token comment" spellcheck="true">#randn从标准正态分布随机抽样</span><span class="token comment" spellcheck="true">#torch.normal(mean,std) 正态分布随机抽样</span><span class="token comment" spellcheck="true">#torch.linspace()线性间距向量  </span><span class="token comment" spellcheck="true">#torch.ones()初始化为1   torch.zeros()初始化为0  torch.eye()初始化为单位矩阵</span>torch<span class="token punctuation">.</span>complex<span class="token punctuation">(</span>real<span class="token punctuation">,</span> imag<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensor<span class="token comment" spellcheck="true">#real为实部，imag为虚部，real和imag必须位数相同，如果real和imag同为float32那么生成的complex就为complex64。</span><span class="token operator">>></span><span class="token operator">></span> real <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> imag <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> z <span class="token operator">=</span> torch<span class="token punctuation">.</span>complex<span class="token punctuation">(</span>real<span class="token punctuation">,</span> imag<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> ztensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">+</span><span class="token number">3.j</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token operator">+</span><span class="token number">4.j</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> z<span class="token punctuation">.</span>dtypetorch<span class="token punctuation">.</span>complex64torch<span class="token punctuation">.</span>__version__<span class="token comment" spellcheck="true">#查看torch版本</span>torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#最小</span>              high<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#最大 </span>              size<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#维度 </span>              <span class="token operator">*</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>None<span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">,</span> dtype<span class="token operator">=</span>None<span class="token punctuation">,</span> layout<span class="token operator">=</span>torch<span class="token punctuation">.</span>strided<span class="token punctuation">,</span> device<span class="token operator">=</span>None<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> → Tensor <span class="token comment" spellcheck="true">#均匀分布取样</span>torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>input<span class="token punctuation">,</span> mat2<span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensor<span class="token comment" spellcheck="true"># 对一个batch的矩阵进行矩阵乘积,(bxnxm)x(bxmxp)=(bxnxp)</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>permut<span class="token punctuation">(</span><span class="token operator">*</span>dims<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">></span>Tensor<span class="token comment" spellcheck="true"># 返回维度交换后的原始张量的视图。</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>other<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">></span>Tensor<span class="token comment" spellcheck="true"># 和其它张量具有相同的维度。就从现有的值复制扩充。</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>tensors<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">*</span><span class="token punctuation">,</span>out<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将一个序列里的张量拼接在一起，按维数拼接</span>torch<span class="token punctuation">.</span>clone<span class="token punctuation">(</span>input<span class="token punctuation">,</span><span class="token operator">*</span><span class="token punctuation">,</span> memory_format<span class="token operator">=</span>torch<span class="token punctuation">.</span>preserve_format<span class="token punctuation">)</span> → Tensor<span class="token comment" spellcheck="true"># return a copy of input</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>input<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensor</code></pre><p>torch.manual_seed() :<a href="https://blog.csdn.net/X_singing/article/details/104447052?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">https://blog.csdn.net/X_singing/article/details/104447052?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><p>为了保证能够复现，初始随机梯度是固定的</p><h4 id="求导和网络相关知识"><a href="#求导和网络相关知识" class="headerlink" title="求导和网络相关知识"></a>求导和网络相关知识</h4><p>grad属性保存梯度值，grad_fn保存梯度函数</p><p>nn.functional函数的特点是不具有可学习的参数，<code>net.parameters()​</code>返回网络可学习的参数</p><p>forward函数的输入和输出都是Tensor ,在反向传播前，先要将所有参数的梯度清零,如果不清0，计算得到的梯度值会进行累加</p><p><strong>torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。</strong></p><h3 id="使用GPU"><a href="#使用GPU" class="headerlink" title="使用GPU"></a>使用GPU</h3><p><a href="https://blog.csdn.net/u014380165/article/details/77340765">CUDA\cuDNN是什么</a></p><p>CUDA:NVIDIA推出的用于自家GPU的<strong>并行计算</strong>框架，也就是说CUDA只能在NVIDIA的GPU上运行，<strong>而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。</strong></p><p><strong>在 CUDA 的架构下，一个程序分为两个部份：host 端和 device 端。Host 端是指在 CPU 上执行的部份，而 device 端则是在显示芯片上执行的部份。Device 端的程序又称为 “kernel”。通常 host 端程序会将数据准备好后，复制到显卡的内存中，再由显示芯片执行 device 端程序，完成后再由 host 端程序将结果从显卡的内存中取回。</strong></p><p>cuDNN:是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。</p><pre class=" language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这一步是设置我们使用的GPU</span><span class="token comment" spellcheck="true"># 确认我们的电脑支持CUDA，然后显示CUDA信息：</span><span class="token keyword">print</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#然后这些方法将递归遍历所有模块并将模块的参数和缓冲区 转换成CUDA张量：</span>net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#记住：inputs, targets 和 images 也要转换。</span>inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></code></pre><h3 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h3><p><code>torch.save</code></p><p>将对象保存到磁盘文件中。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>obj<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#被保存的对象</span>           f<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>str<span class="token punctuation">,</span> os<span class="token punctuation">.</span>PathLike<span class="token punctuation">,</span> BinaryIO<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#</span>           pickle_module<span class="token operator">=</span><span class="token operator">&lt;</span>module <span class="token string">'pickle'</span> <span class="token keyword">from</span> <span class="token string">'/opt/conda/lib/python3.6/pickle.py'</span><span class="token operator">></span><span class="token punctuation">,</span>           pickle_protocol<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> _use_new_zipfile_serialization<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> → None<span class="token comment" spellcheck="true">#常见的PyTorch约定是使用.pt文件扩展名保存张量。</span><span class="token comment" spellcheck="true"># Save to file</span><span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span></code></pre><p><code>torch.load</code></p><p>从文件中加载用<code>torch.save()</code>保存的对象。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#文件路径</span>           map_location<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#加载到的位置</span>           pickle_module<span class="token operator">=</span><span class="token operator">&lt;</span>module <span class="token string">'pickle'</span> <span class="token keyword">from</span> <span class="token string">'/opt/conda/lib/python3.6/pickle.py'</span><span class="token operator">></span><span class="token punctuation">,</span>           <span class="token operator">**</span>pickle_load_args<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#eg:torch.load(args.load, map_location=device)</span></code></pre><p>保存和加载</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 只保存模型参数</span><span class="token comment" spellcheck="true"># 保存</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'\parameter.pkl'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 加载</span>model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'\parameter.pkl'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 保存完整模型</span><span class="token comment" spellcheck="true"># 保存</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'\model.pkl'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 加载</span>model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'\model.pkl'</span><span class="token punctuation">)</span></code></pre><h3 id="torch-autograd"><a href="#torch-autograd" class="headerlink" title="torch.autograd"></a>torch.autograd</h3><p>torch.autograd提供了实现任意标量值函数的自动微分的类和函数。它只需要对现有代码进行最小的更改——只需要声明张量s，对于这些张量，计算梯度时应带有requires_grad=True关键字。到目前为止，我们只支持浮点张量类型(half、float、double和bfloat16)和复数张量类型(cfloat、cdouble)的自动求导。</p><h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p>Variable API已被弃用:在使用autograd时，不再需要Variable。Autograd自动支持将requires_grad设置为True的张量。下面是一些变化的快速指南:</p><p><code>Variable(tensor)</code>和<code>Variable(tensor, requires_grad)</code>仍然按预期工作，但它们返回的是张量而不是变量。</p><p><code>var.data</code>和<code>tensor.data</code>是一样的。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tensor([[-0.4404]], requires_grad=True)</span><span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#tensor([[-0.4404]])</span></code></pre><p>像<code>var.backward()</code>、<code>var.detach()</code>、<code>var.register_hook()</code>这样的方法现在可以在具有相同方法名的张量上工作。</p><p>此外,现在可以使用工厂方法创建requires_grad=True的张量,如<code>torch.randn()</code>、<code>torch.zeros()</code>、<code>torch.ones()</code>和其他类似如下的方法:</p><p><code>autograd_tensor =torch.randn((2, 3, 4),requires_grad=True)</code></p><p>具体来说，在pytorch中的Variable就是一个存放会变化值的地理位置，里面的值会不停发生变化，就像一个装鸡蛋的篮子，鸡蛋数会不断发生变化。那谁是里面的鸡蛋呢，自然就是pytorch中的tensor了。（也就是说，<strong>pytorch都是有tensor计算的，而tensor里面的参数都是Variable的形式</strong>）。如果用Variable计算的话，那返回的也是一个同类型的Variable。</p><p>也就是说现在requires_grad=True的tensor就相当于以前的Variable,也就是进行反向传播的变量。</p><p><code>detach()</code></p><p>返回一个与当前图分离的新张量。结果将永远不需要梯度。</p><h3 id="torch-cuda"><a href="#torch-cuda" class="headerlink" title="torch.cuda"></a>torch.cuda</h3><p>``torch.cuda.device_count() → int`返回可用的GPU数量</p><h3 id="torch-cuda-amp"><a href="#torch-cuda-amp" class="headerlink" title="torch.cuda.amp"></a>torch.cuda.amp</h3><p><a href="https://blog.csdn.net/l7H9JA4/article/details/114324414">Pytorch自动混合精度教程</a></p><p>Automatic mixed precision package自动混合精度包</p><p>torch.cuda.amp提供了方便的混合精度方法，在某些操作中需要使用torch.float32 (float)数据类型而有些操作使用torch.float16(half)。<br>有些操作，比如线性层和卷积，在float16中要快得多。<br>其他操作，比如减少操作，通常需要float32的动态范围。<br>混合精度尝试将每个op匹配到其适当的数据类型。</p><p>一般来说，自动混合精度训练同时使用<code>torch.cuda.amp.autocast</code>和<code>torch.cuda.amp.GradScaler</code>,当然如果需要也可以单独使用。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 用户使用混合精度训练基本操作：</span><span class="token comment" spellcheck="true"># amp依赖Tensor core架构，所以model参数必须是cuda tensor类型</span>model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># GradScaler对象用来自动做梯度缩放</span>scaler <span class="token operator">=</span> GradScaler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> epochs<span class="token punctuation">:</span>    <span class="token keyword">for</span> input<span class="token punctuation">,</span> target <span class="token keyword">in</span> data<span class="token punctuation">:</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 在autocast enable 区域运行forward</span>        <span class="token keyword">with</span> autocast<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># model做一个FP16的副本，forward</span>            output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>            loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 用scaler，scale loss(FP16)，backward得到scaled的梯度(FP16)</span>        scaler<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># scaler 更新参数，会先自动unscale梯度</span>        <span class="token comment" spellcheck="true"># 如果有nan或inf，自动跳过</span>        scaler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># scaler factor更新</span>        scaler<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="torch-backends"><a href="#torch-backends" class="headerlink" title="torch.backends"></a>torch.backends</h3><p>torch.backends控制PyTorch支持的各种后端的行为。这些后端包括:</p><ul><li><code>torch.backends.cuda</code></li><li><code>torch.backends.cudnn</code></li><li><code>torch.backends.mkl</code></li><li><code>torch.backends.mkldnn</code></li><li><code>torch.backends.openmp</code></li></ul><h4 id="torch-cudnn"><a href="#torch-cudnn" class="headerlink" title="torch.cudnn"></a>torch.cudnn</h4><p><a href="https://blog.csdn.net/byron123456sfsfsfa/article/details/96003317">torch.backends.cudnn.benchmark详解</a></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>benchmark<span class="token comment" spellcheck="true">#一个bool值，如果为真，将导致cuDNN对多个卷积算法进行基准测试并选择最快的。</span><span class="token comment" spellcheck="true">#耗费一些预处理时间，选择最好的卷积算法，大大减少之后的训练时间，网络结构不能变，输入输出不能变等</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic<span class="token comment" spellcheck="true">#如果该bool为真，则导致cuDNN只使用确定性卷积算法。参见torch.is_deterministic()和torch.set_deterministic()。</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>enabled<span class="token comment" spellcheck="true">#一个控制是否启用cuDNN的bool值。</span></code></pre><h3 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h3><p>网络结构图的基本构建模块<code>import torch.nn as nn</code></p><p>卷积层和线性层在__init__里面，而激活和池化在forward函数里面。</p><p>torch.nn 只支持小批量输入。整个 torch.nn 包都只支持小批量样本，而不支持单个样本。 例如，nn.Conv2d 接受一个4维的张量， 每一维分别是sSamples * nChannels * Height * Width（样本数x通道数x高x宽）。 如果你有单个样本，<strong>只需使用 <code>input.unsqueeze(0) </code>来添加其它的维数</strong></p><h4 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h4><p><strong>nn.Module</strong></p><p>所有神经网络模块的基类。你的模型也应该子类化这个类。</p><p>模块还可以包含其他模块，允许将它们嵌套在树结构中。</p><p><a href="https://www.cnblogs.com/wupiao/articles/13287061.html">Variables training 和 train()  eval()</a></p><pre class=" language-python"><code class="language-python">net<span class="token punctuation">.</span>training <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment" spellcheck="true"># 布尔值表示该模块是处于训练模式training mode还是评估模式evaluation mode。注意，对module的设置仅仅影响本层，子module不受影响</span>net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 将本层及子层的training设定为True,使用BatchNormalizetion()和Dropout()</span>net<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 将本层及子层的training设定为False,不使用BatchNormalization()和Dropout()</span></code></pre><p><a href="https://www.cnblogs.com/marsggbo/p/12075244.html">buffers()和parameters()的区别</a></p><p><code>buffers()</code></p><p>指那些不需要参与反向传播的参数,反向传播不需要被optimizer更新</p><pre class=" language-python"><code class="language-python">buffers<span class="token punctuation">(</span>recurse<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> → Iterator<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> buf <span class="token keyword">in</span> model<span class="token punctuation">.</span>buffers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token operator">>></span><span class="token operator">></span>     <span class="token keyword">print</span><span class="token punctuation">(</span>type<span class="token punctuation">(</span>buf<span class="token punctuation">)</span><span class="token punctuation">,</span> buf<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">></span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">></span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span> 1L<span class="token punctuation">,</span> 5L<span class="token punctuation">,</span> 5L<span class="token punctuation">)</span></code></pre><p><code>parameters()</code></p><p>是<code>nn.parameter.Paramter</code>，也就是组成Module的参数。例如一个<code>nn.Linear</code>通常由<code>weight</code>和<code>bias</code>参数组成。它的特点是默认<code>requires_grad=True</code>,也就是说训练过程中需要反向传播的，反向传播需要被optimizer更新的。</p><pre class=" language-python"><code class="language-python">parameters<span class="token punctuation">(</span>recurse<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">></span> Iterator<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parameter<span class="token punctuation">.</span>Parameter<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#recurse (bool)如果为True，则生成此模块和所有子模块的参数。否则，只生成此模块的直接成员参数。</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token operator">>></span><span class="token operator">></span>     <span class="token keyword">print</span><span class="token punctuation">(</span>type<span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">,</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#输出的是一个w,一个b！别忘了b！</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">></span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">></span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span> 1L<span class="token punctuation">,</span> 5L<span class="token punctuation">,</span> 5L<span class="token punctuation">)</span></code></pre><p><code>named_parameters()</code></p><p>返回模块参数的迭代器，生成参数名称和参数本身</p><pre class=" language-python"><code class="language-python">named_parameters<span class="token punctuation">(</span>prefix<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">''</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#作为所有参数名称的前缀</span>                 recurse<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>→ Iterator<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>str<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span>                <span class="token comment" spellcheck="true">#如果为真，则生成该模块和所有子模块的参数。否则，只会产生作为该模块直接成员的参数。</span><span class="token keyword">for</span> name<span class="token punctuation">,</span>parameters <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#可同时返回名字和参数</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span><span class="token string">':'</span><span class="token punctuation">,</span>parameters<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#conv1.weight : torch.Size([6, 1, 3, 3])</span><span class="token comment" spellcheck="true">#conv1.bias : torch.Size([6])</span><span class="token comment" spellcheck="true">#fc1.weight : torch.Size([10, 1350])</span><span class="token comment" spellcheck="true">#fc1.bias : torch.Size([10])                </span></code></pre><p><code>modules()</code></p><p>返回一个网络所有模块的迭代器</p><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> l <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>l<span class="token punctuation">,</span> l<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> idx<span class="token punctuation">,</span> m <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>net<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>idx<span class="token punctuation">,</span> <span class="token string">'->'</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">></span> Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">></span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><p><a href="https://zhuanlan.zhihu.com/p/98563721">state_dcit和load_state_dict源码详解</a></p><p><code>state_dict()</code></p><p>返回一个字典，其中包含模块的整个状态,存储了网络结构的名字和对应的参数。parameters和buffers(如运行平均值)都包括在内。键是对应的parameter和buffer名称。</p><pre class=" language-python"><code class="language-python">state_dict<span class="token punctuation">(</span>destination<span class="token operator">=</span>None<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> keep_vars<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">></span>dict<span class="token operator">>></span><span class="token operator">></span> module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'bias'</span><span class="token punctuation">,</span> <span class="token string">'weight'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># torch.nn.modules.module.py</span><span class="token keyword">class</span> <span class="token class-name">Module</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">state_dict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> destination<span class="token operator">=</span>None<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> keep_vars<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> destination <span class="token keyword">is</span> None<span class="token punctuation">:</span>            destination <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>            destination<span class="token punctuation">.</span>_metadata <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>        destination<span class="token punctuation">.</span>_metadata<span class="token punctuation">[</span>prefix<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> local_metadata <span class="token operator">=</span> dict<span class="token punctuation">(</span>version<span class="token operator">=</span>self<span class="token punctuation">.</span>_version<span class="token punctuation">)</span>        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>_parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> param <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                destination<span class="token punctuation">[</span>prefix <span class="token operator">+</span> name<span class="token punctuation">]</span> <span class="token operator">=</span> param <span class="token keyword">if</span> keep_vars <span class="token keyword">else</span> param<span class="token punctuation">.</span>data        <span class="token keyword">for</span> name<span class="token punctuation">,</span> buf <span class="token keyword">in</span> self<span class="token punctuation">.</span>_buffers<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> buf <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                destination<span class="token punctuation">[</span>prefix <span class="token operator">+</span> name<span class="token punctuation">]</span> <span class="token operator">=</span> buf <span class="token keyword">if</span> keep_vars <span class="token keyword">else</span> buf<span class="token punctuation">.</span>data        <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> module <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span>destination<span class="token punctuation">,</span> prefix <span class="token operator">+</span> name <span class="token operator">+</span> <span class="token string">'.'</span><span class="token punctuation">,</span> keep_vars<span class="token operator">=</span>keep_vars<span class="token punctuation">)</span>        <span class="token keyword">for</span> hook <span class="token keyword">in</span> self<span class="token punctuation">.</span>_state_dict_hooks<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            hook_result <span class="token operator">=</span> hook<span class="token punctuation">(</span>self<span class="token punctuation">,</span> destination<span class="token punctuation">,</span> prefix<span class="token punctuation">,</span> local_metadata<span class="token punctuation">)</span>            <span class="token keyword">if</span> hook_result <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                destination <span class="token operator">=</span> hook_result        <span class="token keyword">return</span> destination</code></pre><p>通过<code>_modules</code>递归所有子模块,再通过<code>_parameters</code>和<code>_buffers</code>获得所有parameters和buffers,主意之前的parameters()等函数也是利用他们获取相应的值。而<code>_state_dict_hooks</code>就是在读取state_dict是希望执行的操作,一般为空，所以不做考虑。另外有一点需要注意的是，在读取<code>Module</code>时采用的递归的读取方式，并且名字间使用<code>.</code>做分割，以方便后面<code>load_state_dict</code>读取参数。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>my_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 参数直接作为模型类成员变量</span>        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'my_buffer'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 参数注册为 buffer</span>        self<span class="token punctuation">.</span>my_param <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>f3 <span class="token operator">=</span> self<span class="token punctuation">.</span>fc    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> xmodel <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'my_param'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.3052</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'my_buffer'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5583</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'fc.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6322</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0255</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4747</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0530</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'conv.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.3346</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.2962</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'conv.bias'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5205</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'fc2.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4949</span><span class="token punctuation">,</span>  <span class="token number">0.2815</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.3006</span><span class="token punctuation">,</span>  <span class="token number">0.0768</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'f3.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6322</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0255</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4747</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0530</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><code>load_state_dict()</code></p><p>将参数和缓冲区从state_dict复制到这个模块及其子模块中。</p><pre class=" language-python"><code class="language-python">load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span>str<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#传入一个state_dict</span>                strict<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#state_dict就是你之前保存的模型参数序列，而_load_from_state_dict中的local_state表示你的代码中定义的模型的结构。</span><span class="token comment" spellcheck="true">#判断上面参数拷贝过程中是否有unexpected_keys或者missing_keys,如果有就报错，代码不能继续执行。当然，如果strict=False，则会忽略这些细节。</span><span class="token comment" spellcheck="true">#missing_keys is a list of str containing the missing keys</span><span class="token comment" spellcheck="true">#unexpected_keys is a list of str containing the unexpected keys</span></code></pre><p><code>cuda</code>与<code>to</code></p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#在这没有区别</span>    <span class="token comment" spellcheck="true"># allocates a tensor on GPU 1</span>    a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>cuda<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># transfers a tensor from CPU to GPU 1</span>    b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># a.device and b.device are device(type='cuda', index=1)</span>    <span class="token comment" spellcheck="true"># You can also use ``Tensor.to`` to transfer a tensor:</span>    b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>cuda<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># b.device and b2.device are device(type='cuda', index=1)</span><span class="token comment" spellcheck="true"># .to(device)可以指定CPU或者GPU</span><span class="token comment" spellcheck="true"># 单GPU或者CPU</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果是多GPU</span><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>  model <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model，device_ids<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># .cuda()只能指定GPU</span><span class="token comment" spellcheck="true">#指定某个GPU</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICE'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'1'</span>model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果是多GPU</span>os<span class="token punctuation">.</span>environment<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0,1,2,3'</span>device_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span>net  <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dataparallel<span class="token punctuation">(</span>net<span class="token punctuation">,</span> device_ids <span class="token operator">=</span>device_ids<span class="token punctuation">)</span>net  <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dataparallel<span class="token punctuation">(</span>net<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 默认使用所有的device_ids </span>net <span class="token operator">=</span> net<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><strong>nn.sequential</strong></p><p>顺序容器。模块将按照它们在构造函数中传递的顺序被添加到它。另外，也可以传入模块的有序dict。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Example of using Sequential</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>          nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Example of using Sequential with OrderedDict</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>          <span class="token punctuation">(</span><span class="token string">'conv1'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          <span class="token punctuation">(</span><span class="token string">'relu1'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          <span class="token punctuation">(</span><span class="token string">'conv2'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          <span class="token punctuation">(</span><span class="token string">'relu2'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a>线性层</h4><p><code>nn.Linear</code>全连接层</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token punctuation">:</span> int<span class="token punctuation">,</span> out_features<span class="token punctuation">:</span> int<span class="token punctuation">,</span> bias<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#在全连接层之前通过view函数将其改为一维向量</span></code></pre><h4 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h4><p><code>nn.Dropout</code></p><p>在训练过程中,使用伯努利分布样本，以概率p随机地将输入张量中的一些元素置零。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#元素置0的概率</span>                   inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#</span><span class="token comment" spellcheck="true"># 对所有元素中每个元素按照概率0.5置为0，对点执行</span></code></pre><p><code>nn.Dropout2d</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>                   inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 对每个通道按照概率0.5置为0，对平面执行</span></code></pre><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p><code>nn.Conv2d()</code>卷积核是二维的</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">:</span> int<span class="token punctuation">,</span>                 out_channels<span class="token punctuation">:</span> int<span class="token punctuation">,</span>                kernel_size<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                 stride<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>                 dilation<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                 groups<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                 bias<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#是否要添加偏置参数作为可学习参数的一个，默认为True。</span>                padding_mode<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'zeros'</span><span class="token punctuation">)</span></code></pre><p>教程：<a href="https://www.jianshu.com/p/45a26d278473">https://www.jianshu.com/p/45a26d278473</a></p><p>接受$(N,C_{in},H,W)$,输出$(N,C_{out},H_{out},W_{out}$)</p><p>卷积核的规模就是kernel_size x input_channel x output_channel</p><p>$out(N_i,C_{out_j})=bias(C_{out_j})+∑<em>{k=0}^{C</em>{in-1}}weight(C_{out_j},k)⋆input(N_i,k)$</p><p><strong>Variables</strong></p><p>~Conv2d.weight(Tensor):维度为<code>(out_channels,in_channels/groups,kernel_size[0],kernel_size[1])</code>，权重值取样于$u(-\sqrt k,\sqrt k)$,$k=$</p><p>~Conv2d.bias(Tensor):维度为<code>(out_channels)</code>，值取样于$u(-\sqrt k,\sqrt k)$,$k=$</p><p><code>nn.ConvTranspose2d</code></p><p>在由几个输入平面组成的输入图像上应用一个二维转置卷积运算符。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">:</span> int<span class="token punctuation">,</span>                         out_channels<span class="token punctuation">:</span> int<span class="token punctuation">,</span>                         kernel_size<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                         stride<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                         padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>                         output_padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>                         groups<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                         dilation<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                         padding_mode<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'zeros'</span><span class="token punctuation">)</span></code></pre><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>Pytorch<strong>池化操作的步长默认与池化卷积核的大小一样</strong>，<strong>池化一般不考虑overlap</strong></p><p><code>nn.MaxPool2d()</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                    stride<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#default value是kernel_size</span>                   padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>                    dilation<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                    return_indices<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助</span>                   ceil_mode<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果true，向上取整而不是floor向下取整</span></code></pre><p><code>nn.AvgPool2d()</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span>                   stride<span class="token operator">=</span>None<span class="token punctuation">,</span>                   padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                    ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#when True, will use ceil instead of floor to compute the output shape</span>                   count_include_pad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                   divisor_override<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><h4 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h4><p><code>nn.BatchNorm2d()</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#Channel数</span>                    eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#为数值稳定性而加在分母上的值。</span>                    momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#指数加权平均的参数</span>                    affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#是否有可学习参数</span>                    track_running_stats<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这一层不用到测试</span></code></pre><p>``</p><h4 id="非线性激活函数"><a href="#非线性激活函数" class="headerlink" title="非线性激活函数"></a>非线性激活函数</h4><p><code>nn.ReLU()</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#inplace-选择是否进行覆盖运算 x=x+1 还是 y=x+1 x=y 节省内存</span></code></pre><p><code>nn.Softmax(dim=None)</code></p><p>对指定维度应用Softmax</p><p><code>nn.Softmax2d()</code></p><p>输入为(N,C,H,W),输出为(N,C,H,W) 就是你想的那样</p><h4 id="距离函数"><a href="#距离函数" class="headerlink" title="距离函数"></a>距离函数</h4><p><code>nn.CosineSimilarity</code><br>$$<br>similarity=\frac{x_1\cdot x_2}{max(||x_1||_2\cdot||x_2||_2,eps)}<br>$$</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CosineSimilarity<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 计算余弦相似性的维度</span>                          eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 避免除0的小数</span></code></pre><p>Input1:(*,D,*),D是要计算的维度</p><p>Input2:(*,D,*)</p><p>Output:(*,*)</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><a href="https://www.cnblogs.com/wanghui-garcia/p/10862733.html">常见损失函数总结,讲得好啊</a></p><p><code>nn.BCELoss</code></p><p>计算二元交叉熵</p><p>$l(x,y)=L={l_1,\cdots,l_N}^T,l_n=-w_n[y_n\cdot logx_n+(1-y_n)\cdot log(1-x_n)]$</p><p>我们的解决方案是BCELoss将其对数函数输出固定为大于或等于-100。这样，我们总是可以有一个有限的损失值和一个线性的反向方法。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span>weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#手动给的权重</span>                 size_average<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#</span>                 reduce<span class="token operator">=</span>None<span class="token punctuation">,</span>                 reduction<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'mean'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#指定要应用于输出的reduction操作:' none ' | 'mean' | ' sum '。none输出向量,其他输出标量</span><span class="token comment" spellcheck="true">#“none”:表示不进行任何reduction，“mean”:输出的和除以输出中的元素数，即求平均值，“sum”:输出求和。</span><span class="token comment" spellcheck="true">#注意:size_average和reduce正在被弃用，与此同时，指定这两个arg中的任何一个都将覆盖reduction参数。默认值:“mean”</span></code></pre><p><code>nn.BCEWithLogitsLoss</code></p><p>这种损失结合了Sigmoid层和BCELoss在一个类里。这个版本通过将操作合并到一个层比使用一个简单的Sigmoid后面跟着一个BCELoss在数值上更稳定，我们利用<strong>log-sum-exp</strong>技巧的数值稳定性。</p><p>$l(x,y)=L={l_1,\cdots,l_N}^T,l_n=-w_n[y_n\cdot log\sigma(x_n)+(1-y_n)\cdot log(1-\sigma(x_n))]$</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span>weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                           size_average<span class="token operator">=</span>None<span class="token punctuation">,</span>                           reduce<span class="token operator">=</span>None<span class="token punctuation">,</span>                           reduction<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'mean'</span><span class="token punctuation">,</span>                           pos_weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#正值例子的权重，必须是有着与分类数目相同的长度的向量.可以通过增加正值示例的权重来权衡召回率和准确性。</span></code></pre><p><code>nn.NLLLoss</code></p><p>负对数似然损失。用C类来训练分类问题是有用的。</p><p>如果提供了可选参数weight，它应该是一个一维张量，为每一类赋权。当你有一个不平衡的训练集时，这是特别有用的。</p><p>$l(x,y)=L={l_1,\cdots,l_N}^T,l_n=-w_{y_nx_{n,y_n}},w_c=weight[c]\cdot1{c\ne ignore_index}$只选择第n个数据的实际yn类别作为loss</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                 <span class="token comment" spellcheck="true">#一个手动标给每个类别的权重，如果给定，必须是一个C大小张量,否则默认所有的权重全是1</span>                 size_average<span class="token operator">=</span>None<span class="token punctuation">,</span>                 ignore_index<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#指定一个被忽略的目标值，该目标值不影响输入梯度。</span>                 <span class="token comment" spellcheck="true">#当size_average为真时，对非忽略目标的损失进行平均。</span>                 reduce<span class="token operator">=</span>None<span class="token punctuation">,</span>                 reduction<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'mean'</span><span class="token punctuation">)</span></code></pre><p>输入:(N,C), C代表类别的数量；或者在计算高维损失函数例子中输入大小为(N,C,d1,d2,…,dK)，k&gt;=1</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#低维示例:</span>m <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># input is of size N x C = 3 x 5</span>input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>inputtensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8676</span><span class="token punctuation">,</span>  <span class="token number">1.5017</span><span class="token punctuation">,</span>  <span class="token number">0.2963</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.9431</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0929</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.3540</span><span class="token punctuation">,</span>  <span class="token number">1.0994</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1085</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4001</span><span class="token punctuation">,</span>  <span class="token number">0.0102</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.3653</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3828</span><span class="token punctuation">,</span>  <span class="token number">0.6257</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4996</span><span class="token punctuation">,</span>  <span class="token number">0.1928</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>m<span class="token punctuation">(</span>input<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.8899</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5205</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.7259</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.9653</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.1152</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#1->0.5205</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.5082</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7628</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.9707</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.2623</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8520</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#0->1.5082</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6841</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4323</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4237</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4.5490</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8566</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#4->1.8566</span>       grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>LogSoftmaxBackward<span class="token operator">></span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#each element in target has to have 0 &lt;= value &lt; C</span>target <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>output <span class="token operator">=</span> loss<span class="token punctuation">(</span>m<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>outputtensor<span class="token punctuation">(</span><span class="token number">1.2951</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">></span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#高维示例:就是逐像素返回呗,对每一个像素来说都是一个低维示例</span><span class="token comment" spellcheck="true"># 2D loss example (used, for example, with image inputs)</span>N<span class="token punctuation">,</span> C <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span><span class="token number">4</span>loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># input is of size N x channel x height x width</span>data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> C<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#输出为5*4*8*8</span>m <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># each element in target has to have 0 &lt;= value &lt; C</span>target <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>N<span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span><span class="token punctuation">.</span>random_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#target.size()=target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)</span>output <span class="token operator">=</span> loss<span class="token punctuation">(</span>m<span class="token punctuation">(</span>conv<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>outputtensor<span class="token punctuation">(</span><span class="token number">1.5501</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLoss2DBackward<span class="token operator">></span><span class="token punctuation">)</span></code></pre><p><code>nn.CrossEntropyLoss</code></p><p>将<code>nn.LogSoftmax()</code>和<code>nn.NLLLoss()</code>方法结合到一个类中</p><p>当用C类训练分类问题时，它是有用的。如果提供了，可选的参数weight权重应该是一个一维张量，为每个类分配权重。当你有一个不平衡的训练集时，这是特别有用的。</p><p>损失函数:<br>$$<br>log(x,class)=-log(\frac{exp(x[class])}{\sum_jexp(x[j])})=-x[class]+log(\sum_jexp(x[j]))<br>$$<br>加上weight:<br>$$<br>log(x,class)=weight<a href="-x%5Bclass%5D+log(%5Csum_jexp(x%5Bj%5D))">class</a><br>$$<br>x选的只是class那个！</p><h4 id="视觉层"><a href="#视觉层" class="headerlink" title="视觉层"></a>视觉层</h4><p><code>nn.Upsample</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>size<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#指定目标输出的大小</span>                  scale_factor<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#输出为输入的倍数，和size只能指定一个</span>                  mode<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">'nearest'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#上采样算法，包括最近邻、线性、双线性、双三次、三线性插值算法</span>                  align_corners<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>bool<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#如果为True，输入的角像素将与输出张量对齐，</span><span class="token comment" spellcheck="true">#因此将保存下来这些像素的值。仅当使用的算法为'linear', 'bilinear'or 'trilinear'时可以使用。默认设置为False</span><span class="token comment" spellcheck="true">#语义分割设置为true</span></code></pre><h4 id="Utilities"><a href="#Utilities" class="headerlink" title="Utilities"></a>Utilities</h4><p>来自<code>torch.nn.utils</code>模块</p><p><code>nn.utils.clip_grad_value_</code></p><p><a href="https://zhuanlan.zhihu.com/p/99953668">深度炼丹之梯度裁剪</a></p><p>backward之后,step之前</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_value_<span class="token punctuation">(</span>parameters<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>Iterable<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                 <span class="token comment" spellcheck="true">#将梯度归一化的张量的可迭代或单个张量 net.parameters()</span>                                clip_value<span class="token punctuation">:</span> float<span class="token punctuation">)</span> → None<span class="token comment" spellcheck="true">#梯度被裁剪到[-clip_value,clip_value]</span>eg<span class="token punctuation">:</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_value_<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span></code></pre><h4 id="自定义模块"><a href="#自定义模块" class="headerlink" title="自定义模块"></a>自定义模块</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DoubleConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true">#要继承</span>    <span class="token triple-quoted-string string">"""(convolution => [BN] => ReLU) * 2"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> mid_channels<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#父类__init__</span>        <span class="token keyword">if</span> <span class="token operator">not</span> mid_channels<span class="token punctuation">:</span>            mid_channels <span class="token operator">=</span> out_channels        self<span class="token punctuation">.</span>double_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> mid_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>mid_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mid_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment" spellcheck="true">#要定义一个forward函数</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>double_conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#返回自身</span></code></pre><h4 id="forward函数"><a href="#forward函数" class="headerlink" title="forward函数"></a>forward函数</h4><p>定义每次调用时执行的计算。应该被所有子类重写。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#主要是使用了__call__特殊方法,使得forward被自动调用。</span><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> init_age<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'我年龄是:'</span><span class="token punctuation">,</span>init_age<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>age <span class="token operator">=</span> init_age    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> added_age<span class="token punctuation">)</span><span class="token punctuation">:</span>        res <span class="token operator">=</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>added_age<span class="token punctuation">)</span>        <span class="token keyword">return</span> res    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'forward 函数被调用了'</span><span class="token punctuation">)</span>                <span class="token keyword">return</span> input_ <span class="token operator">+</span> self<span class="token punctuation">.</span>age<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'对象初始化。。。。'</span><span class="token punctuation">)</span>a <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#初始化</span>input_param <span class="token operator">=</span> a<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#__call__起作用</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"我现在的年龄是："</span><span class="token punctuation">,</span> input_param<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#对象初始化。。。。</span><span class="token comment" spellcheck="true">#我年龄是: 10</span><span class="token comment" spellcheck="true">#forward 函数被调用了</span><span class="token comment" spellcheck="true">#我现在的年龄是： 12</span></code></pre><p>关于 <code>__call__</code> 方法，不得不先提到一个概念，就是*可调用对象callable，我们平时自定义的函数、内置函数和类都属于可调用对象，但凡是可以把一对括号<code>()</code>应用到某个对象身上都可称之为可调用对象，判断对象是否为可调用对象可以用函数 <code>callable</code>。如果在类中实现了 <code>__call__</code> 方法，那么实例对象也将成为一个可调用对象。</p><p>利用这种特性，可以实现基于类的装饰器。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Counter</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> func<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>func <span class="token operator">=</span> func        self<span class="token punctuation">.</span>count <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>count <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>func<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>@Counter<span class="token keyword">def</span> <span class="token function">foo</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    foo<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>foo<span class="token punctuation">.</span>count<span class="token punctuation">)</span></code></pre><p><a href="https://blog.csdn.net/u012436149/article/details/70145598">pytorch结构介绍</a></p><p>forward函数使用的具体流程:</p><ol><li>调用module的<code>call</code>方法</li><li><code>module</code>的<code>call</code>里面调用<code>module</code>的<code>forward</code>方法</li><li><code>forward</code>里面如果碰到<code>Module</code>的子类，回到第1步，如果碰到的是<code>Function</code>的子类，继续往下</li><li>调用<code>Function</code>的<code>call</code>方法</li><li><code>Function</code>的<code>call</code>方法调用了Function的<code>forward</code>方法。</li><li><code>Function</code>的<code>forward</code>返回值</li><li><code>module</code>的<code>forward</code>返回值</li><li>在<code>module</code>的<code>call</code>进行<code>forward_hook</code>操作，然后返回值。</li></ol><h3 id="torch-nn-init"><a href="#torch-nn-init" class="headerlink" title="torch.nn.init"></a>torch.nn.init</h3><p>初始化方式一:</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#从正态分布N(mean,std^2)中取值填充张量</span><span class="token comment" spellcheck="true">#eg:w=torch.empty(3,5)</span><span class="token comment" spellcheck="true">#nn.init.normal_(w)</span></code></pre><p>初始化方式二:</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>data<span class="token comment" spellcheck="true">#都是对Variable中的tensor进行处理，但是不进行梯度计算和被进行梯度跟踪</span>w<span class="token operator">=</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span>w<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>mean<span class="token punctuation">,</span>std<span class="token punctuation">)</span>fill_<span class="token punctuation">(</span>value<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">></span>Tensor<span class="token comment" spellcheck="true">#用指定值填充张量</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">></span>Tensor<span class="token comment" spellcheck="true">#用0填充张量</span></code></pre><h3 id="torch-nn-functioal"><a href="#torch-nn-functioal" class="headerlink" title="torch.nn.functioal"></a>torch.nn.functioal</h3><p><a href="https://www.zhihu.com/question/66782101">nn与nn.functional的区别</a></p><p><code>upsample</code></p><p>将输入上采样到给定的大小或给定的scale_factor</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>upsample<span class="token punctuation">(</span>input<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#输入张量</span>                             size<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#输出大小</span>                             scale_factor<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#变换因子</span>                             mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#模式</span>                             align_corners<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p><code>avg_pool2d</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span>input<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#操作的Tensor</span>                               kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>None<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> count_include_pad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> divisor_override<span class="token operator">=</span>None<span class="token punctuation">)</span> → Tensor</code></pre><p><code>max_pool2d</code></p><h3 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h3><p><a href="https://zhuanlan.zhihu.com/p/94019888">optim教程</a></p><p>是一个实现各种优化算法的包。已经支持了最常用的方法，接口也足够通用，因此将来还可以轻松地集成更复杂的方法。</p><p>要使用torch.optim要先创建一个optim 对象，这个对象会一直保持当前状态或根据计算的梯度更新参数。创建optim对象时，要给它一个包含模型参数的的可迭代对象(所有的都应该是 Variable)，然后指定learning rate,weight decay等参数.</p><pre class=" language-python"><code class="language-python">optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span>var1<span class="token punctuation">,</span> var2<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">)</span></code></pre><p>注意：由于要把模型参数传给 optim ，所以如果要使用GPU时，要在 把模型参数传给 optim之前写 model().cuda()，因为调用 .cuda() 前后不是一个参数对象，在此optimize期间，要保证 optimized parameters 在同一位置，不要 .cpu()或 .cuda() 乱用，注意下顺序 <strong>！！！有待验证！！！</strong></p><p>也支持为每个参数单独设置选项。若想这么做，不要直接传入Variable的iterable，而是传入dict的iterable。这种方法在对每层分别指定learning rate时很有用:</p><pre class=" language-python"><code class="language-python">optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span>                <span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>base<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">,</span>                <span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>classifier<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">}</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span></code></pre><p>上面这样写，表示 model.base 的 lr 是 1e-2，model.classifier 的 lr 是 1e-3，momentum=0.9 同时用于这两个参数</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Optimizer</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> defaults<span class="token punctuation">)</span><span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_log_api_usage_once<span class="token punctuation">(</span><span class="token string">"python.optimizer"</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>defaults <span class="token operator">=</span> defaults        self<span class="token punctuation">.</span>_hook_for_profile<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>params<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">"params argument given to the optimizer should be "</span>                            <span class="token string">"an iterable of Tensors or dicts, but got "</span> <span class="token operator">+</span>                            torch<span class="token punctuation">.</span>typename<span class="token punctuation">(</span>params<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>state <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>dict<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>param_groups <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># 在源码中有一个参数param_groups来存储params</span>        param_groups <span class="token operator">=</span> list<span class="token punctuation">(</span>params<span class="token punctuation">)</span></code></pre><h4 id="torch-optim-Optimizer"><a href="#torch-optim-Optimizer" class="headerlink" title="torch.optim.Optimizer"></a>torch.optim.Optimizer</h4><p>所有优化器的基类。</p><p><code>zero_grad(set_to_none: bool = False)</code></p><p><a href="https://www.jb51.net/article/189433.htm">zero_grad教程</a></p><p>设置被优化的张量的梯度为0,显然，我们进行下一次batch梯度计算的时候，前一个batch的梯度计算结果，没有保留的必要了。所以在下一次梯度更新的时候，先使用optimizer.zero_grad把梯度信息设置为0。</p><p>唯一一个参数意思是不设为0而设置为None</p><p>optimizer执行的两种方式:<code>optimizer.step()</code>和``optimizer.step(closure)`，</p><p>所有的optim 都实现了前一种方法，第一种方法会更新所有参数，这是大多数 optim 都支持的方法，只要损失反向传播后就可以调用此函数:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> input<span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>关于第二种：optimizer.step(closure) 一些优化算法，例如 Conjugate Gradient 和 LBFGS 需要重复多次计算，因此你需要传入一个 closure 去允许它们重新计算你的模型。这个closure 应当清空梯度，计算损失，然后返回</p><pre class=" language-python"><code class="language-python"><span class="token keyword">for</span> input<span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">closure</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span>closure<span class="token punctuation">)</span></code></pre><h4 id="torch-optim-RMSprop"><a href="#torch-optim-RMSprop" class="headerlink" title="torch.optim.RMSprop"></a>torch.optim.RMSprop</h4><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>RMSprop<span class="token punctuation">(</span>params<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#用于优化的参数iterable或定义参数组的dicts</span>                    lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>                    alpha<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">,</span>                    eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span>                    weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#权重衰减</span>                    momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#动量</span>                    centered<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><h4 id="torch-optim-SGD"><a href="#torch-optim-SGD" class="headerlink" title="torch.optim.SGD"></a>torch.optim.SGD</h4><p>实现随机梯度下降(可选动量)。</p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 待优化参数的可迭代对象或者是定义了参数组的dict</span>                lr<span class="token operator">=</span><span class="token operator">&lt;</span>required parameter<span class="token operator">></span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#学习率</span>                momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#动量</span>                dampening<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#</span>                weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#权重衰减</span>                nesterov<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#</span><span class="token operator">>></span><span class="token operator">></span> optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> loss_fn<span class="token punctuation">(</span>model<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="torch-utils-tensorboard"><a href="#torch-utils-tensorboard" class="headerlink" title="torch.utils.tensorboard"></a>torch.utils.tensorboard</h3><p>一旦你安装了TensorBoard，这些工具可以让你将PyTorch模型和指标记录到一个目录中，以便在TensorBoard UI中可视化。标量、图像、直方图、图形和嵌入可视化都支持PyTorch模型和张量以及Caffe2 网络和blobs。</p><p><a href="https://blog.csdn.net/bigbennyguo/article/details/87956434">tensorboardX介绍</a></p><h4 id="SummaryWriter"><a href="#SummaryWriter" class="headerlink" title="SummaryWriter"></a>SummaryWriter</h4><p><code>CLASS torch.utils.tensorboard.writer.SummaryWriter</code></p><p>直接将条目写入log_dir中的事件文件中，以供TensorBoard使用。</p><p>SummaryWriter类提供了一个高级API，用于在<strong>给定目录中创建事件文件并向其添加摘要和事件</strong>。该类异步更新文件内容。这<strong>允许训练程序调用方法直接从训练循环向文件添加数据，而不会减慢训练速度。</strong></p><p><code>__init__</code>创建一个SummaryWriter，将事件和摘要写入事件文件中。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter__init__<span class="token punctuation">(</span>log_dir<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#保存目录位置，默认值是'run/CURRENT_DATETIME_HOSTNAME',所以每次运行后都会更改，日期时间肯定会变</span>         comment<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#注解添加到默认log_dir的后缀，若log_dir被指定，则此参数不起作用。</span>         purge_step<span class="token operator">=</span>None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#</span>         max_queue<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#在一个'add'调用强制刷新磁盘之前，挂起事件和汇总的队列的大小。默认为10项。</span>         flush_secs<span class="token operator">=</span><span class="token number">120</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#将挂起事件和摘要刷新到磁盘的频率(以秒为单位)。默认每120s一次</span>         filename_suffix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#添加到log_dir目录中所有事件文件名的后缀。</span></code></pre><p>SummaryWriter类是您记录TensorBoard使用和可视化数据的主要入口。例如:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torchvision<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms<span class="token comment" spellcheck="true"># Writer will output to ./runs/ directory by default</span>writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token punctuation">)</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>trainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">'mnist_train'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet50<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Have ResNet model take in grayscale rather than RGB</span>model<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>images<span class="token punctuation">,</span> labels <span class="token operator">=</span> next<span class="token punctuation">(</span>iter<span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span><span class="token punctuation">)</span>grid <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">)</span>writer<span class="token punctuation">.</span>add_image<span class="token punctuation">(</span><span class="token string">'images'</span><span class="token punctuation">,</span> grid<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>writer<span class="token punctuation">.</span>add_graph<span class="token punctuation">(</span>model<span class="token punctuation">,</span> images<span class="token punctuation">)</span>writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这可以通过TensorBoard进行可视化，TensorBoard安装和运行:</span>pip install tensorboardtensorboard <span class="token operator">-</span><span class="token operator">-</span>logdir<span class="token operator">=</span><span class="token comment" spellcheck="true">#包含记录文件的文件夹路径</span><span class="token comment" spellcheck="true">#记录文件名类似如下:events.out.tfevents.1616293286.vision806-desktop</span></code></pre><h3 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h3><p>PyTorch数据加载工具的核心是torch.utils.data.DataLoader类。它表示一个数据集上的Python可迭代对象。</p><p>pytorch输入数据PipeLine一般遵循一个“三步走”的策略，一般pytorch 的数据加载到模型的操作顺序是这样的：<br>① 创建一个 Dataset 对象。必须实现<code>__len__()</code>、<code>__getitem__()</code>这两个方法，这里面会用到transform对数据集进行扩充。<br>② 创建一个 DataLoader 对象。它是对DataSet对象进行迭代的，一般不需要事先里面的其他方法了。<br>③ 循环遍历这个 DataLoader 对象。将img, label加载到模型中进行训练</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> MyDataset<span class="token punctuation">(</span><span class="token punctuation">)</span>           <span class="token comment" spellcheck="true"># 第一步：构造Dataset对象</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第二步：通过DataLoader来构造迭代对象</span>num_epoches <span class="token operator">=</span> <span class="token number">100</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_epoches<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># 第三步：逐步迭代数据</span><span class="token operator">&amp;</span>nbsp<span class="token punctuation">;</span><span class="token operator">&amp;</span>nbsp<span class="token punctuation">;</span><span class="token operator">&amp;</span>nbsp<span class="token punctuation">;</span> <span class="token keyword">for</span> img<span class="token punctuation">,</span> label <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 训练代码</span></code></pre><h4 id="Map-style-datasets"><a href="#Map-style-datasets" class="headerlink" title="Map-style datasets"></a><strong>Map-style datasets</strong></h4><p>map风格的数据集是一个实现<code>__getitem__()</code>和<code>__len__()</code>协议的数据集，并表示从(可能非整数)索引/键到数据样本的映射。</p><p>例如，当使用dataset[idx]访问这样的数据集时，可以从磁盘上的文件夹中读取idx-th映像及其对应的标签。</p><p>查看Dataset了解更多细节。</p><h4 id="Iterable-style-datasets"><a href="#Iterable-style-datasets" class="headerlink" title="Iterable-style datasets"></a>Iterable-style datasets</h4><p>iterable风格的数据集是IterableDataset的一个子类的实例，它实现了<code>__iter__()</code>协议，并表示数据样本上的一个iterable。这种类型的数据集特别适合于这样的情况:随机读取代价很高，甚至不太可能，而且批大小取决于所获取的数据。</p><p>例如，当调用iter(dataset)时，这样的数据集可以返回从数据库、远程服务器甚至实时生成的日志读取的数据流。</p><p>查看IterableDataset了解更多细节。</p><h4 id="Data-Loading-Order-and-Sampler"><a href="#Data-Loading-Order-and-Sampler" class="headerlink" title="Data Loading Order and Sampler"></a>Data Loading Order and Sampler</h4><h4 id="Memory-Pinning"><a href="#Memory-Pinning" class="headerlink" title="Memory Pinning"></a>Memory Pinning</h4><p>pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。</p><p>主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。</p><p>而显卡中的显存全部是锁页内存！</p><p>当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。</p><h4 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a>torch.utils.data.DataLoader</h4><p>数据加载程序。<strong>组合数据集dataset和采样器sampler</strong>,并提供给定数据集上的迭代。</p><p>在训练模型时使用到此函数，用来把训练数据分成多个小组，此函数每次抛出一组数据。直至把所有的数据都抛出。就是做一个数据的初始化。</p><p>DataLoader支持map-style和iterable-style的数据集，具有单进程或多进程加载、自定义加载顺序和可选的自动批处理(整理)和内存固定.</p><p><strong>DataLoader本质是一个可迭代对象，所以:</strong></p><ol><li>可以使用<code>for inputs, labels in dataloaders</code>进行可迭代对象的访问</li><li>先使用iter对dataloader进行第一步包装，使用<code>iter(dataloader)</code>返回的是一个迭代器，然后就可以可以使用next访问了。Dataloader的<code>__iter__()</code>根据num_workers的数量返回单线程或多线程的迭代器</li><li>我们一般不需要再自己去实现DataLoader的方法了，只需要在构造函数中指定相应的参数即可，比如常见的batch_size，shuffle等等参数。所以使用DataLoader十分简洁方便。</li><li>DataLoader实际上一个较为高层的封装类，它的功能都是通过更底层的<code>_DataLoader</code>来完成的，但是<code>_DataLoader</code>类较为低层，这里就不再展开叙述了。DataLoaderIter就是<code>_DataLoaderIter</code>的一个框架, 用来传给<code>_DataLoaderIter</code> 一堆参数, 并把自己装进DataLoaderIter 里。</li></ol><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>Dataset<span class="token punctuation">[</span>T_co<span class="token punctuation">]</span><span class="token punctuation">,</span>                            batch_size<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>int<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                            shuffle<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>                    <span class="token comment" spellcheck="true">#表示每一个epoch之后是否对样本进行随机打乱,所有的先打乱,再取batch.具体解析见下文</span>                            sampler<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>Sampler<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                            <span class="token comment" spellcheck="true">#自定义从数据集中抽取样本的策略，如果指定这个参数，那么shuffle必须为False。</span>                            batch_sampler<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>Sampler<span class="token punctuation">[</span>Sequence<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                            num_workers<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#要使用多少子进程装载数据。0表示数据将在主进程中加载。</span>                            collate_fn<span class="token punctuation">:</span> Callable<span class="token punctuation">[</span>List<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">,</span> Any<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                            pin_memory<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#是否锁页内存</span>                            drop_last<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#如果数据集大小不能被批大小整除，则设置为True可删除最后一个不完整的批处理。</span><span class="token comment" spellcheck="true">#如果为False，并且dataset的大小不能被batch-size整除，那么最后一批将变小。(默认值:False)</span>                            timeout<span class="token punctuation">:</span> float <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>           <span class="token comment" spellcheck="true">#timeout (numeric, optional): 如果是正数，表明等待从worker进程中收集一个batch等待的时间，若超出设定的时间还没有收集到，那就不收集这个内容了。这个numeric应总是大于等于0。默认为0</span>                            worker_init_fn<span class="token punctuation">:</span> Callable<span class="token punctuation">[</span>int<span class="token punctuation">,</span> None<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                            multiprocessing_context<span class="token operator">=</span>None<span class="token punctuation">,</span>                            generator<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span>                            prefetch_factor<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>                            persistent_workers<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""    批训练，把数据变成一小批一小批数据进行训练。    DataLoader就是用来包装所使用的数据，每次抛出一批数据"""</span><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> DataBATCH_SIZE <span class="token operator">=</span> <span class="token number">5</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 把数据放在数据库中</span>torch_dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>    <span class="token comment" spellcheck="true"># 从数据库中每次抽出batch size个样本</span>    dataset<span class="token operator">=</span>torch_dataset<span class="token punctuation">,</span>    batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span>    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">show_batch</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># training</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"steop:{}, batch_x:{}, batch_y:{}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>step<span class="token punctuation">,</span> batch_x<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    show_batch<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#loader在这里就是迭代器,在for里面一旦取用自动更新</span><span class="token comment" spellcheck="true">#for data in testloader: 这里的data就是一个batch的数据,是一个tuple,既有train_X,也有train_Y.</span></code></pre><p><a href="https://blog.csdn.net/qq_27825451/article/details/96130126">pytorch数据预处理三剑客之Dataset，DataLoader，Transform</a></p><p>上面这篇讲的非常好</p><p><strong>为什么机器学习需要打乱数据?</strong></p><p>防止数据按一定规律排列,这样神经网络学习时会把这种规律当做一种特征学习,从而过拟合。这样做使得数据更接近于真实分布。</p><p>比如随机梯度下降,就比较依赖于数据的随机程度,如果不对数据进行打乱处理,可能异常值集中在数据某一块,会对算法收敛拟合造成干扰。</p><h4 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h4><p>表示一个Dataset的抽象类。</p><p><strong>所有表示从键到数据样本映射的数据集都应该子类化它</strong>。所有子类都应该覆盖<code>__getitem__()</code>，从而支持获取一个给定键的数据样本。子类也可以选择性地覆盖<code>__len__()</code>，许多Sampler实现和DataLoader默认选项都希望它返回数据集的大小。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#dataset的抽象父类定义如下</span><span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> T_co<span class="token punctuation">:</span>        <span class="token keyword">raise</span> NotImplementedError<span class="token keyword">def</span> <span class="token function">__add__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">:</span> <span class="token string">'Dataset[T_co]'</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token string">'ConcatDataset[T_co]'</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> ConcatDataset<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">,</span> other<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><code>__init__(self)</code>:主要是数据的获取，比如从某个文件中获取</p><p><code>__len__(self)</code>:整个数据集的长度</p><p><code>__getitem__(self,index)</code>:这个是最重要的，一般情况下它会包含以下几个业务需要处理</p><ol><li>比如如果我们需要在读取数据的同时对图像进行增强的话，当然，图像增强的方法可以使用Pytorch内置的图像增强方式，也可以使用自定义或者其他的图像增强库,这个很灵活。</li><li>在Pytorch中得到的图像必须是tensor，也就是说我们必须要将数据格式转化成pytorch的tensor格式才行。</li></ol><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># coding: utf-8</span><span class="token keyword">import</span> os<span class="token keyword">import</span> torch<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image<span class="token keyword">import</span> cv2<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToTensor<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms<span class="token keyword">import</span> random<span class="token keyword">class</span> <span class="token class-name">LaneDataSet</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataset<span class="token punctuation">,</span> transform<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        param：            detaset: 实际上就是tusimple数据集的三个文本文件train.txt、val.txt、test.txt三者的文件路径            transform: 决定是否进行变换,它其实是一个函数或者是几个函数的组合        构造三个列表，存储每一张图片的文件路径                  '''</span>        self<span class="token punctuation">.</span>_gt_img_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>_gt_label_binary_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>_gt_label_instance_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform        <span class="token keyword">with</span> open<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> file<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 打开其实是那个 training下面的那个train.txt 文件</span>            <span class="token keyword">for</span> _info <span class="token keyword">in</span> file<span class="token punctuation">:</span>                info_tmp <span class="token operator">=</span> _info<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info_tmp<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info_tmp<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info_tmp<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_shuffle<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">_shuffle</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 将gt_image、binary_image、instance_image三者所对应的图片路径组合起来，再进行随机打乱</span>    c <span class="token operator">=</span> list<span class="token punctuation">(</span>zip<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">)</span><span class="token punctuation">)</span>    random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>c<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_instance_list <span class="token operator">=</span> zip<span class="token punctuation">(</span><span class="token operator">*</span>c<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">assert</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">)</span> \               <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 读取图片</span>    img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>IMREAD_COLOR<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#真实图片 (720,1280,3)</span>    label_instance_img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>IMREAD_UNCHANGED<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># instance图片 （720,1280）</span>    label_binary_img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>IMREAD_GRAYSCALE<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#binary图片 （720,1280)</span>    <span class="token comment" spellcheck="true"># optional transformations,裁剪成（256,512）</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">:</span>        img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        label_binary_img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>label_binary_img<span class="token punctuation">)</span>        label_instance_img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>label_instance_img<span class="token punctuation">)</span>    img <span class="token operator">=</span> img<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#（3,720,1280） 这里都没有问题</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>img<span class="token punctuation">,</span> label_binary_img<span class="token punctuation">,</span> label_instance_img<span class="token punctuation">)</span>  </code></pre><h4 id="torch-utils-data-TensorDataset-tensors"><a href="#torch-utils-data-TensorDataset-tensors" class="headerlink" title="torch.utils.data.TensorDataset(*tensors)"></a>torch.utils.data.TensorDataset(*tensors)</h4><p>TensorDataset 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能。该类通过每一个 tensor 的第一个维度进行索引。因此，该类中的 tensor 第一维度必须相等。</p><p>举个例子,六张图片,六个label,维度分别是(6,H,W,C)和(6,)</p><h4 id="torch-utils-data-Concat-datasets"><a href="#torch-utils-data-Concat-datasets" class="headerlink" title="torch.utils.data.Concat(datasets)"></a>torch.utils.data.Concat(datasets)</h4><p>连接多个数据集产生一个新的数据集,该类用于组装不同的现有数据集。</p><p>datasets:(<em>sequence</em>) – List of datasets to be concatenated</p><h4 id="torch-utils-data-Subset"><a href="#torch-utils-data-Subset" class="headerlink" title="torch.utils.data.Subset"></a>torch.utils.data.Subset</h4><p><code>torch.utils.data.random_split</code></p><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>random_split<span class="token punctuation">(</span>dataset<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>Dataset<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#要被划分的数据集</span>                              lengths<span class="token punctuation">:</span> Sequence<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#要生成的切片长度</span>                              generator<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>Generator<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">&lt;</span>torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>Generator object<span class="token operator">></span><span class="token punctuation">)</span> → List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>Subset<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#用于随机排列的生成器。</span><span class="token comment" spellcheck="true">#eg:random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))</span><span class="token comment" spellcheck="true">#eg:train, val = random_split(dataset, [n_train, n_val])</span></code></pre><h4 id="torch-utils-data-Sampler"><a href="#torch-utils-data-Sampler" class="headerlink" title="torch.utils.data.Sampler"></a>torch.utils.data.Sampler</h4><p>所有采样器的基类。</p><p>每个采样器子类都必须提供一个<code>__iter__()</code>方法，提供一种遍历数据集元素索引的方法，以及一个<code>__len__()</code>方法，该方法返回返回的迭代器的长度。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#pytorch采样器</span><span class="token keyword">class</span> <span class="token class-name">Sampler</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">class</span> <span class="token class-name">SequentialSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">RandomSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">SubsetRandomSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">WeightedRandomSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">BatchSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#Dataloader中的采样器</span><span class="token keyword">if</span> sampler <span class="token keyword">is</span> None<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># give default samplers</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>_dataset_kind <span class="token operator">==</span> _DatasetKind<span class="token punctuation">.</span>Iterable<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># See NOTE [ Custom Samplers and IterableDataset ]</span>        sampler <span class="token operator">=</span> _InfiniteConstantSampler<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># map-style</span>        <span class="token keyword">if</span> shuffle<span class="token punctuation">:</span>            sampler <span class="token operator">=</span> RandomSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            sampler <span class="token operator">=</span> SequentialSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span></code></pre><h3 id="加速Pytorch训练"><a href="#加速Pytorch训练" class="headerlink" title="加速Pytorch训练"></a>加速Pytorch训练</h3><p><a href="https://zhuanlan.zhihu.com/p/97190313">加速pytorch训练</a></p><h4 id="prefetch-generator"><a href="#prefetch-generator" class="headerlink" title="prefetch_generator"></a>prefetch_generator</h4><p>使用 prefetch_generator库在后台加载下一batch的数据，原本Pytorch默认的DataLoader会创建一些worker线程来预读取新的数据，但是除非这些线程的数据全部都被清空，这些线程才会读下一批数据。使用prefetch_generator，我们可以保证线程不会等待，每个线程都总有至少一个数据在加载。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#使用</span><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token keyword">from</span> prefetch_generator <span class="token keyword">import</span> BackgroundGenerator<span class="token keyword">class</span> <span class="token class-name">DataLoaderX</span><span class="token punctuation">(</span>DataLoader<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> BackgroundGenerator<span class="token punctuation">(</span>super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__iter__<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#然后用DataLoaderX替换原本的DataLoader</span></code></pre><h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><p><a href="https://www.cnblogs.com/houjun/p/10406640.html">torchvision简介</a></p><p>该库是Pytorch项目的一部分。安装pytorch时，torchvision独立于torch。torchvision包由流行的数据集（torchvision.datasets）、模型架构(torchvision.models)和用于计算机视觉的常见图像转换组成t(torchvision.transforms)。</p><h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionmnist <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">"path/to/mnist/"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transforms<span class="token punctuation">,</span> target_transform<span class="token operator">=</span>None<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token punctuation">:</span> str<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#存放training.pt和test.pt的root directory</span>                           train<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#从training.pt创建数据集,否则从test.pt</span>                           transform<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>Callable<span class="token punctuation">,</span> NoneType<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span><span class="token comment" spellcheck="true">#转换操作</span>                           target_transform<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>Callable<span class="token punctuation">,</span> NoneType<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                           download<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span> → None<span class="token comment" spellcheck="true">#下载到本地并存放到root directory</span></code></pre><h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchvisionvgg16 <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#pretrained=True加载别人预训练好的模型,否则就是权重随机初始化的模型</span></code></pre><h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p><a href="https://blog.csdn.net/qq_38410428/article/details/94719553">transforms的二十二个方法</a></p><p><a href="https://blog.csdn.net/qq_27825451/article/details/97145592?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control">第三篇-pytorch数据预处理三剑客</a></p><p>变换是常见的图像变换。它们可以使用<code>Compose</code>链接在一起。此外，还有<code>torchvision.transforms.functional</code>模块。<strong>Functional transforms可以对转换进行细粒度控制</strong>。如果您必须构建更复杂的转换管道（例如，在分割任务的情况下），这将非常有用。</p><p>所有的转换都接受<strong>PIL Image，Tensor Image或batch of tensor Image</strong>作为输入。Tensor Image是一个具有(C, H, W)形状的张量，其中C是通道，H和W是图像的高度和宽度。Batch of Tensor Images是(B, C, H, W) 形状的张量，其中B是一个Batch中图像的个数。对Batch of Tensor Image应用确定或随机变换，就能对这批图像的所有图像进行相同的变换。</p><p>注意事项：</p><p>（1）transfroms中的数据增强操作针对的是pillow的Image图像格式，而我们很多时候在使用opencv读进去的又是ndarray格式，所以需要第一步先将ndarray转化成Image格式，即<code>transforms.ToPILImage()</code>.</p><p>（2）但是我们后需要的数据又是需要ndarray格式或者是tensor格式，故而有需要将Image转换回来，即<code>transforms.ToTensor()</code>。</p><p>自从v0.8.0以来，所有的<strong>随机转换都使用torch默认的随机生成器来采样随机参数</strong>。这是一个破坏后项兼容的更改，后向兼容是指向低版本兼容，用户应该设置随机状态如下:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Previous versions</span><span class="token comment" spellcheck="true"># import random</span><span class="token comment" spellcheck="true"># random.seed(12)</span><span class="token comment" spellcheck="true"># Now</span><span class="token keyword">import</span> torchtorch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">17</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#请记住，pytorch随机数生成器和Python随机数生成器的相同种子不会产生相同的结果。</span></code></pre><p><code>torchvision.transforms.Compose(transforms)</code><br>将几个变换组合在一起</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Compose</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transforms<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>transforms <span class="token operator">=</span> transforms    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#self.transform(img)实际上是一个函数调用形式,果然实现了__call__()s</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>transforms<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#从这可以看出,传入的是一个容器,列表就可以</span>            img <span class="token operator">=</span> t<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token keyword">return</span> img <span class="token comment" spellcheck="true">#返回的直接就是img,注意,处理的是单张img,返回的也是单张img</span>    <span class="token keyword">def</span> <span class="token function">__repr__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        format_string <span class="token operator">=</span> self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__ <span class="token operator">+</span> <span class="token string">'('</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>transforms<span class="token punctuation">:</span>            format_string <span class="token operator">+=</span> <span class="token string">'\n'</span>            format_string <span class="token operator">+=</span> <span class="token string">'    {0}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>t<span class="token punctuation">)</span>        format_string <span class="token operator">+=</span> <span class="token string">'\n)'</span>        <span class="token keyword">return</span> format_string<span class="token operator">>></span><span class="token operator">></span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">>></span><span class="token operator">></span>     transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token operator">>></span><span class="token operator">></span>     transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h4 id="裁剪-Crop"><a href="#裁剪-Crop" class="headerlink" title="裁剪(Crop)"></a>裁剪(Crop)</h4><h4 id="翻转和旋转-Flip-and-Rotation"><a href="#翻转和旋转-Flip-and-Rotation" class="headerlink" title="翻转和旋转(Flip and Rotation)"></a>翻转和旋转(Flip and Rotation)</h4><h4 id="图像变换-resize"><a href="#图像变换-resize" class="headerlink" title="图像变换(resize)"></a>图像变换(resize)</h4><pre class=" language-python"><code class="language-python">torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token comment" spellcheck="true">#将PIL Image或者 ndarray 转换为tensor，并且归一化至[0-1] 注意事项：归一化至[0-1]是直接除以255，若自己的ndarray数据尺度有变化，则需要自行修改。</span>torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token punctuation">,</span> std<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#用平均值和标准偏差归一化张量图像。给定mean：(M1,…,Mn)和std：(S1,…,Sn)对于n通道，此变换将标准化输入的每个通道，torch.*Tensor即 input[channel] = (input[channel] - mean[channel]) / std[channel]</span><span class="token comment" spellcheck="true">#mean(sequence)-每个通道的均值序列。</span><span class="token comment" spellcheck="true">#std(sequence)-每个通道的标准偏差序列。</span></code></pre><h3 id><a href="#" class="headerlink" title></a></h3><p>对transforms操作，使数据增强更灵活</p><p>Normalize参数解惑<a href="https://blog.csdn.net/xys430381_1/article/details/85724668">https://blog.csdn.net/xys430381_1/article/details/85724668</a></p><h4 id="functional"><a href="#functional" class="headerlink" title="functional"></a>functional</h4><p><code>torchvision.transforms.functional.adjust_gamma(img: torch.Tensor, gamma: float, gain: float = 1) → torch.Tensor</code></p><p>对图片进行gamma校正<br>$$<br>I_{out}=255\cdot gain \cdot (\frac{I_{in}}{255})^y<br>$$<br>img:PIL Image或Tensor </p><h3 id="torchvision-utils"><a href="#torchvision-utils" class="headerlink" title="torchvision.utils"></a>torchvision.utils</h3><h4 id="torchvision-utils-make-grid"><a href="#torchvision-utils-make-grid" class="headerlink" title="torchvision.utils.make_grid()"></a>torchvision.utils.make_grid()</h4><p>把若干图像拼成一副图像来显示<a href="https://blog.csdn.net/u012343179/article/details/83007296">https://blog.csdn.net/u012343179/article/details/83007296</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>numpy、pandas、matplotlib学习</title>
      <link href="numpy-he-pandas-xue-xi/"/>
      <url>numpy-he-pandas-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><p>菜鸟教程：<a href="https://www.runoob.com/numpy/numpy-ndarray-object.html">https://www.runoob.com/numpy/numpy-ndarray-object.html</a></p><p>API:<a href="https://docs.scipy.org/doc/numpy-1.9.0/genindex.html">https://docs.scipy.org/doc/numpy-1.9.0/genindex.html</a></p><p>Python<code>list</code>只能与整数相乘，在这种情况下，将<code>list</code>重复的元素：</p><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span></code></pre><p>如果要进行矢量运算，请<code>numpy.ndarray</code>改用：</p><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token operator">>></span><span class="token operator">></span> ar <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> ar <span class="token operator">*</span> <span class="token number">3</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#numpy数组索引</span>pre<span class="token operator">=</span>pre<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#不用三个中括号，只用一个，里面只用一个逗号</span></code></pre><p><a href="https://zhuanlan.zhihu.com/p/60365398">numpy广播机制</a></p><h4 id="数组生成"><a href="#数组生成" class="headerlink" title="数组生成"></a>数组生成</h4><p>np.linspace()和np.arrange()一个是样本数量，一个步长</p><h4 id="axis详解"><a href="#axis详解" class="headerlink" title="axis详解"></a>axis详解</h4><p><a href="https://www.jianshu.com/p/30b40b504bae">虽然是tf的教程，但是类似</a></p><ol><li>0维，又称0维张量，数字，标量：1   <code>()</code></li><li>1维，又称1维张量，数组，vector：[1, 2, 3]  <code>(3)</code></li><li>2维，又称2维张量，矩阵，二维数组：[[1,2], [3,4]] <code>(2,2)</code></li><li>3维，又称3维张量，立方（cube），三维数组：[[[5,6], [7,8]]]  <code>(1,2,2)</code> ,越往里axis越大，axis是<code>(0,1,2)</code>每个3维的有一个2维的，每个2维的有2个1维的，每个1维的有2个标量,数字7的坐标是[0,1,0]</li><li>n维：你应该get到点了吧~</li></ol><p>本来是2x3x4,<code>tf.reduce_sum(tensor, axis=0)</code>，加完了就变成了3x4</p><h4 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h4><pre class=" language-python"><code class="language-python">np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>list<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将列表转化为数组</span>np<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#求标准差</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#矩阵乘</span>np<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#element wise</span>a<span class="token operator">*</span>b<span class="token comment" spellcheck="true">#element wise</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>condition<span class="token punctuation">[</span><span class="token punctuation">,</span>x<span class="token punctuation">,</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回根据条件从x或y中选择的元素。条件为真，返回x，否则返回y。</span><span class="token comment" spellcheck="true">#&lt;&lt;&lt;a</span><span class="token comment" spellcheck="true">#array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span><span class="token comment" spellcheck="true">#&lt;&lt;&lt;np.where(a &lt; 5, a, 10*a)</span><span class="token comment" spellcheck="true">#array([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#显示np数组中的unique值，还有返回index值等功能</span>np<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#向上取整 </span>np<span class="token punctuation">.</span>floor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#向下取整</span>np<span class="token punctuation">.</span>round<span class="token punctuation">(</span>a<span class="token punctuation">,</span>decimals<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>out<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将数组四舍五入到指定的小数上</span>np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>a<span class="token punctuation">,</span>axis<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">></span>ndarray    eg<span class="token punctuation">:</span>img_nd <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>img_nd<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#img_nd原先是二维的(0,1),扩张完变(0,1,2)</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>a<span class="token punctuation">,</span>axes<span class="token operator">=</span>None<span class="token punctuation">)</span> eg<span class="token punctuation">:</span>img_nd<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> 或 np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>img_nd<span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#(a,b,c)->(c,a,b)</span>ndarray<span class="token punctuation">.</span>max<span class="token punctuation">(</span>axis<span class="token operator">=</span>None<span class="token punctuation">,</span> out<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#返回指定维度的最大值,没指定的话就是所有的最大值</span>np<span class="token punctuation">.</span>newaxis<span class="token comment" spellcheck="true">#作用是增加一个维度</span><span class="token comment" spellcheck="true">#a=np.array([1,2,3,4,5])</span><span class="token comment" spellcheck="true">#aa=a[:,np.newaxis]</span><span class="token comment" spellcheck="true">#print(aa.shape)  (5,1) 现有5个,5个里每个再有1个</span><span class="token comment" spellcheck="true">#print (aa)      [[1],[2],[3],[4],[5]] </span>np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>a1<span class="token punctuation">,</span> a2<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将数组序列拼接在一起</span><span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#(2,2)</span><span class="token operator">>></span><span class="token operator">></span> b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#(1,2)</span><span class="token operator">>></span><span class="token operator">></span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#拼接完->(3,2)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>numpy<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span>a<span class="token punctuation">,</span> order<span class="token operator">=</span><span class="token string">'C'</span><span class="token punctuation">)</span>ndarray<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>order<span class="token operator">=</span><span class="token string">'C'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#都是将多维数组降为一维，区别为:</span><span class="token comment" spellcheck="true">#ndarray.flatten(order='C')返回拷贝，对拷贝的修改不会影响原数组</span><span class="token comment" spellcheck="true">#numpy.ravel(a, order='C')返回视图，对视图的修改会影响原数组</span><span class="token comment" spellcheck="true">#order:使用这个索引顺序读取a的元素。有'C' 'F' 'A' 'K'</span><span class="token comment" spellcheck="true">#'C'意味着以行主、C风格的顺序索引元素，最后一个轴索引变化最快，回到第一个轴索引变化最慢。eg:[0,0,0],[0,0,1],[0,0,2],[0,1,0],[0,1,1],...</span><span class="token comment" spellcheck="true">#'F'表示以fortran风格的列主顺序索引元素，第一个索引变化最快，最后一个索引变化最慢。注意，' C '和' F '选项不考虑底层数组的内存布局，只参考轴索引的顺序。eg:[0,0,0],[1,0,0],[2,0,0],[0,1,0],[1,1,0],[2,1,0],...</span><span class="token comment" spellcheck="true">#'A'表示如果a在内存中是Fortran连续的，则以类似Fortran的索引顺序读取元素，否则以类似c的顺序读取</span><span class="token comment" spellcheck="true">#'K'的意思是按照元素在内存中出现的顺序读取它们，除非在步数为负时反转数据。默认情况下，使用'C'索引顺序。</span></code></pre><h2 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h2><p>教程：<a href="https://www.yiibai.com/pandas/python_pandas_environment_setup.html">https://www.yiibai.com/pandas/python_pandas_environment_setup.html</a></p><p>API:<a href="https://pandas.pydata.org/pandas-docs/stable/reference/index.html">https://pandas.pydata.org/pandas-docs/stable/reference/index.html</a></p><p>将dataframe一行转化为int，astype</p><pre><code>top, right, bottom, left = row[0:].astype(int)</code></pre><p>获取dataframe的行数和列数</p><pre class=" language-python"><code class="language-python">df<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">//</span>行数df<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">//</span>列数</code></pre><h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>易百教程：<a href="https://www.yiibai.com/matplotlib">https://www.yiibai.com/matplotlib</a></p><p>API：<a href="https://matplotlib.org/3.3.2/api/index.html">https://matplotlib.org/3.3.2/api/index.html</a></p><p>假如目前只有一个figure1，你再绘图是在它上面不断覆盖的</p><p>分为plt绘图与axes绘图两种方式</p><h4 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h4><p><a href="https://www.fontke.com/tool/rgb/800000/">根据RGB  CMYK  HSL  HSV  XYZ 值来查询颜色</a></p><p><code>plt.figure(1)</code>建立一个新的展示图像的画板</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>image <span class="token keyword">as</span> imgplt<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltx <span class="token operator">=</span> imgplt<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'label.png'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">300</span><span class="token punctuation">:</span><span class="token number">350</span><span class="token punctuation">,</span><span class="token number">150</span><span class="token punctuation">:</span><span class="token number">200</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#matplotlib显示图像，注意x只用一个[]，不同维度用逗号分割！</span><span class="token comment" spellcheck="true">#imread的返回值是图片数据data，数据类型是class:`numpy.array`。这个图片数据data的维度如下：</span><span class="token comment" spellcheck="true">#- (M, N) 对于灰度级图片</span><span class="token comment" spellcheck="true">#- (M, N, 3) 对于RGB彩色图片.</span><span class="token comment" spellcheck="true">#- (M, N, 4) 对于RGBA彩色图片</span></code></pre><p><code>plt.imsave(文件名,X,format='png')</code>图片的保存</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np   <span class="token comment" spellcheck="true">#可以这样用PIL读取处理图片,然后最后再转成ndarray</span><span class="token keyword">from</span> PIL <span class="token keyword">import</span> Imageimg <span class="token operator">=</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span><span class="token string">'label.png'</span><span class="token punctuation">)</span>img2 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>img<span class="token punctuation">)</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>img2<span class="token punctuation">)</span></code></pre><p>PIL读取的图片不能输出，用<code>np.array()</code>转化会变成单通道的灰度图</p><p>三通道RGBnumpy读取显示是0-1，而不是0-255，这个是归一化的RGB！！正好和概率二分类对应！</p><p><strong>Snipaste竟然可以实时获取颜色值！！！如下：</strong></p><p><img src="/numpy-he-pandas-xue-xi/image-20201207093513840.png" alt="采集颜色值"></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> numpy </tag>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dp专题</title>
      <link href="dp-zhuan-ti/"/>
      <url>dp-zhuan-ti/</url>
      
        <content type="html"><![CDATA[<h2 id="九章教程dp"><a href="#九章教程dp" class="headerlink" title="九章教程dp"></a>九章教程dp</h2><p><a href="https://www.bilibili.com/video/BV1xb411e7ww">https://www.bilibili.com/video/BV1xb411e7ww</a></p><p>可以解决三种类型的问题：1计数 2最值 3存在性</p><p>解动态规划需要开一个数组，一维、二维、三维，它代表什么</p><p>两个意识：<strong>最后一步</strong>  <strong>子问题</strong></p><p><img src="/dp-zhuan-ti/image-20201118000510306.png" alt="状态"></p><p><img src="/dp-zhuan-ti/image-20201118000813791.png" alt="转移"></p><h3 id="与递归的区别？"><a href="#与递归的区别？" class="headerlink" title="与递归的区别？"></a>与递归的区别？</h3>]]></content>
      
      
      <categories>
          
          <category> oj </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>托福笔记</title>
      <link href="tuo-fu-bi-ji/"/>
      <url>tuo-fu-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="曲根词汇"><a href="#曲根词汇" class="headerlink" title="曲根词汇"></a>曲根词汇</h2><h4 id="阅读记忆"><a href="#阅读记忆" class="headerlink" title="阅读记忆"></a>阅读记忆</h4><p>经济学人 时代 卫报</p><p>生词圈出，反复研读句子。勾画词组，了解替换，熟词辟意。</p><h4 id="词根词缀"><a href="#词根词缀" class="headerlink" title="词根词缀"></a>词根词缀</h4><p>prefix </p><ol><li>肯定，否定</li><li>方向</li><li>数字</li></ol><p>suffix</p><ol><li>词性</li></ol><p>root-根的逆推/想到熟词</p><ol><li>意义</li></ol><h4 id="词源-Etymology"><a href="#词源-Etymology" class="headerlink" title="词源 Etymology"></a>词源 Etymology</h4><p><strong>看读音，看拼写，看意思</strong></p><p>元音和元音字母组合之间可以替换：<strong>a,e,i,o,u,y</strong></p><p>辅音之间：<strong>p\b,t\d,k\g\c\qu\x,f\v,s\x\z\th</strong></p><p>形近字母的互换：<strong>u/v/w(特征是去掉元音),m/n（m/n可以省略）</strong></p><p>字母<strong>g、h</strong>的脱落（不发音）</p><p>造新词往往在<strong>单词结尾加轻辅音</strong></p><p>固定转换：s/t/d， p/b/ph/f/v        amorphous：form的倒写 </p><h4 id="字母组合（单音节，无词根词缀，-大体意思）"><a href="#字母组合（单音节，无词根词缀，-大体意思）" class="headerlink" title="字母组合（单音节，无词根词缀， 大体意思）"></a>字母组合（单音节，无词根词缀， 大体意思）</h4><p>sp表示发出，散开，产生</p><p>scr、cr多和手上的动作有关（注意：s在造词的时候无意义，只起到加强语气的作用）</p><p>词根词源字典：<a href="http://www.etymon.cn/index.html">http://www.etymon.cn/index.html</a></p><h4 id="联想法记单词"><a href="#联想法记单词" class="headerlink" title="联想法记单词"></a>联想法记单词</h4><p>单音节词汇 —-形近词  终点记不一样的地方</p><p>多音节词汇 —拆词，拆成认识的  词根词缀   拼音   熟词  与熟词形近的部分 等</p><p>​    </p><p>im/in: 1 into 2 not</p><p>vis: to see</p><p>it: to go</p><p>fact: to make</p><p>dis:1 not 2 apart</p><p>re:1 again 2 back</p><p>aneous:整体的形容词后缀</p><p>ade:名词后缀</p><p>er：n或v后缀，…的人</p><p>le:如果是动词，那么le就是动词后缀，往往表示动作的反复行为</p><p>ate:动词后缀</p><p>uous/ious/ous：形容词后缀</p><p>ish：形容词后缀，像$\cdots$一样的</p><p><strong>词根arch,archy= government,to rule统治</strong></p><p> ——词根arch 来自希腊语的arkhos，一般构成名词，亦可以当词根讲 意为government,to rule。arch 还有chief,first,old的含义。它们属于一对同源异形根，在派生词中，arch 常指统治的人物，作 ruler 统治者讲；而 archy 常指统治这一行为、方式，作rule 管理/管辖/统治或 government 政体讲。同义词根有来自希腊语的cracy/crat 和来自拉丁语的reg。</p><p>acc/app/ass/att，a+辅音双写表动作的加强</p><p>an/a：not</p><p><strong>后缀-ence,-ency的含义、词源和例词</strong></p><p>汉：来源于拉丁语及法语的名词后缀-ence(-ency)的用法与<a href="http://www.etymon.cn/yingyucizhui/yingyuhouzhui/205.html">-ance</a>(-ancy)基本相同。它们加在动词或动词词根后，意为the act or fact of ～ing或者the quality or condition of ～ing,即表示行为或该行为的性质状态等。这些名词往往有与之对应的以-ent结尾的形容词。</p><p> -ence,-ency与形容词后缀-ent相对应（如 difference-different；urgency-urgent），表示性质、状态、行为，后缀-ence和-ency义同，有些英语单词具有-ence和-ency两种形式（如 innocence = innocency ; persistency = persistence）</p><p>bene：词根 good male：词根evil</p><p>dict: to say</p><p>sion/dion: 名词后缀</p><p>vol:will 意愿</p>]]></content>
      
      
      <categories>
          
          <category> 英语 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tofel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>颜色恒常性之Bayesiancolorconstancy</title>
      <link href="yan-se-heng-chang-xing-zhi-bayesiandecisiontheory/"/>
      <url>yan-se-heng-chang-xing-zhi-bayesiandecisiontheory/</url>
      
        <content type="html"><![CDATA[<h2 id="论文思路"><a href="#论文思路" class="headerlink" title="论文思路"></a>论文思路</h2><p>论文：&lt;&lt;Bayesian color constancy &gt;&gt;</p><h3 id="Title-amp-amp-Abstract"><a href="#Title-amp-amp-Abstract" class="headerlink" title="Title&amp;&amp;Abstract"></a>Title&amp;&amp;Abstract</h3><p>MSE:</p><p><a href="https://blog.csdn.net/qq_36512295/article/details/86526799">https://blog.csdn.net/qq_36512295/article/details/86526799</a></p><p>MMSE:</p><p><a href="https://blog.csdn.net/tanghonghanhaoli/article/details/82751690?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522160395601819724842903308%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=160395601819724842903308&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v28-28-82751690.first_rank_ecpm_v3_pc_rank_v2&amp;utm_term=MMSE&amp;spm=1018.2118.3001.4187">https://blog.csdn.net/tanghonghanhaoli/article/details/82751690?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522160395601819724842903308%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=160395601819724842903308&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v28-28-82751690.first_rank_ecpm_v3_pc_rank_v2&amp;utm_term=MMSE&amp;spm=1018.2118.3001.4187</a></p><blockquote><p>the Maximum Local Mass Estimate</p></blockquote><p><strong>什么是局部最大质量估计？</strong></p><h3 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h3><p><strong>Figure1</strong></p><blockquote><p>We assume that each surface is <strong>flat and matte</strong>, so that it may be characterized by a single spectral reflectance function</p></blockquote><p><strong>为什么要flat and matte?</strong></p><p>平坦是保证处处一致，无光泽不光滑是保证是漫反射吧应该？</p><blockquote><p>The spectral power distri- bution of the light reaching the observer from each surface is given as the <strong>wavelength-by-wavelength product</strong> of the illuminant spectral power distribution and the surface reflectance function.</p></blockquote><p><strong>什么是wavelength-by-wavelength product?</strong></p><p>二者直接相乘</p><p><strong>Figure2</strong></p><p>Fig2（a）:</p><p>对于简单的乘积例子，给定高斯噪声时，后验分布的图像，最优解在岭处</p><p>Fig2（b）：</p><p>横截面表明，即使在岭处都有最大的固定值，一些局部区域也会含有不同的概率质量</p><hr><p><strong>什么是parametre vector？参数是什么意思？</strong></p><p>Y=ax+b Y是因变量，x是自变量，a,b就是参数</p><p><strong>什么是rendering equation？</strong></p><p><a href="https://zhuanlan.zhihu.com/p/52497510">https://zhuanlan.zhihu.com/p/52497510</a></p><p><strong>什么是Gaussian observation noise？高斯噪声？</strong></p><p>高斯噪声百度百科：<a href="https://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0/8587563?fr=aladdin">https://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0/8587563?fr=aladdin</a></p><p>为什么深度学习去噪都采用高斯白噪声？<a href="https://www.zhihu.com/question/67938028">https://www.zhihu.com/question/67938028</a></p><p>高斯白噪声解释：<a href="https://blog.csdn.net/szlcw1/article/details/41758711">https://blog.csdn.net/szlcw1/article/details/41758711</a></p><p><strong>Figure3</strong></p><hr><p><strong>delta loss function损失函数？</strong></p><p>常见损失函数：<a href="https://blog.csdn.net/perfect1t/article/details/88199179">https://blog.csdn.net/perfect1t/article/details/88199179</a></p><p>应该就是指单峰函数，0-1函数$\delta(\widetilde{x}-x)=0\quad if(\widetilde{x}=x)$ 相同的是预判正确的，所以是没有损失的</p><p><strong>期望损失？</strong></p><p><a href="https://blog.csdn.net/hx14301009/article/details/79870851">https://blog.csdn.net/hx14301009/article/details/79870851</a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="A-Why-Color-Constancy-is-Difficult"><a href="#A-Why-Color-Constancy-is-Difficult" class="headerlink" title="A. Why Color Constancy is Difficult"></a>A. Why Color Constancy is Difficult</h4><p>1.Problem Statement</p><blockquote><p>The entries of sj specify the fraction of incident light reflected in Nl evenly spaced wavelength bands throughout the visible spectrum</p></blockquote><p><strong>是不是错了，应该是reflected in</strong> $N_j$<strong>?这句话怎么理解?</strong></p><p>应该意思是$N_l$个均匀排列波长带</p><p>2.Why It Is Difficult</p><blockquote><p>It is underdetermined and it is nonlinear.</p></blockquote><p><strong>什么是欠定的？</strong></p><p><a href="http://blog.sina.com.cn/s/blog_531bb7630100xx6c.html">http://blog.sina.com.cn/s/blog_531bb7630100xx6c.html</a></p><blockquote><p>If we have data from N image locations (say, 10) and assume one illuminant, then we have NNr measurements (e.g., 10 x 3 = 30) available to estimate Nl(N + 1) scene parameters [e.g., 31 x (10 + 1) = 341].</p></blockquote><p><strong>这句话怎么理解？</strong></p><blockquote><p>To address the underdeterminancy of color constancy, previous investigators have described spectral functions by using low-dimensional linear models</p></blockquote><p>什么是低维线性模型？</p><p><a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1604305021&amp;ver=2681&amp;signature=FecbuKdwyqukrDBQO*pJ3q2jZFxLcCWxabeUx7eeSsOp9MNNUxijWNnlaNGRWguX2sl69suc3xXZInrRwvy-CsE1AVD*Vr3NvyjihI-8QmMzO04JeBJpKBXi75iy89*z&amp;new=1">https://mp.weixin.qq.com/s?src=11&amp;timestamp=1604305021&amp;ver=2681&amp;signature=FecbuKdwyqukrDBQO<em>pJ3q2jZFxLcCWxabeUx7eeSsOp9MNNUxijWNnlaNGRWguX2sl69suc3xXZInrRwvy-CsE1AVD</em>Vr3NvyjihI-8QmMzO04JeBJpKBXi75iy89*z&amp;new=1</a></p><blockquote><p>The columns of $B_e$ are the basis functions of the linear model, since the matrix product $B_ew_e$ expresses a weighted sum of these columns.</p></blockquote><p><strong>什么是基函数？</strong></p><p><a href="https://www.jianshu.com/p/5cc427f0df33">https://www.jianshu.com/p/5cc427f0df33</a></p><blockquote><p>If we assume that a population of spectra lie within an $N_m$-dimensional linear model, then we can parameterize the spectra by specifying the model weights.</p></blockquote><p><strong>这句话如何理解？</strong></p>]]></content>
      
      
      <categories>
          
          <category> Color Constancy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CC/AWB </tag>
            
            <tag> Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>随想与感悟</title>
      <link href="sui-xiang-yu-gan-wu/"/>
      <url>sui-xiang-yu-gan-wu/</url>
      
        <content type="html"><![CDATA[<h2 id="2020-10-20"><a href="#2020-10-20" class="headerlink" title="2020.10.20"></a>2020.10.20</h2><ol><li>ACCV 大概在六七月份</li><li>写论文博客时，要分两个过程，自己读主要写每个细节，按照论文的流程走下来；和别人讲、真正理解是要按自己的思路把整个流程理一遍</li><li><img src="/sui-xiang-yu-gan-wu/image-20201021103706939.png" alt="IJCAI"></li><li><img src="/sui-xiang-yu-gan-wu/image-20201021103921695.png" alt="ICCV"></li></ol><h2 id="2020-10-30"><a href="#2020-10-30" class="headerlink" title="2020.10.30"></a>2020.10.30</h2><ol><li>DCT、GAN、自监督</li><li>学习宇翔学长如何讲</li><li>时间和效率都要保证 太堕落了</li><li>下次组会前要把三篇贝叶斯看完加代码整完</li></ol><h2 id="2020-11-19"><a href="#2020-11-19" class="headerlink" title="2020.11.19"></a>2020.11.19</h2><ol><li>还是要做ppt，相当于再捋一次思路，锻炼提炼总结能力。</li></ol><h2 id="2020-12-16"><a href="#2020-12-16" class="headerlink" title="2020.12.16"></a>2020.12.16</h2><p>能不能自动跟新网络结构，自己学习到什么地步了，觉得哪不好了，就下次迭代就会添加一个新的网络结构</p><p>现在这些文章，要么多尺度，要么注意力，其实就是在不同尺度上跳舞，记住更多的信息，然后到时候投票选出来呗~</p><p>特征该如何保存，学习到的特征有没有更好的保存与表示的方法，人的大脑是如何保存的？学习与记忆是不是分开的？</p><h2 id="2021-3-12"><a href="#2021-3-12" class="headerlink" title="2021.3.12"></a>2021.3.12</h2><p>看代码,最好按模块看，这样很快，实际要改的时候，再去细致的看，能跑通就先跑通一遍</p>]]></content>
      
      
      <categories>
          
          <category> 奇奇怪怪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 奇怪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>conda使用</title>
      <link href="conda-shi-yong/"/>
      <url>conda-shi-yong/</url>
      
        <content type="html"><![CDATA[<p>pip安装包应该是安装到了Lib/site-packages</p><p>菜鸟教程pip使用介绍：<a href="https://www.runoob.com/w3cnote/python-pip-install-usage.html">https://www.runoob.com/w3cnote/python-pip-install-usage.html</a></p><p>windows用命令行运行python文件</p><p>利用anaconda prompt ，然后 python  *.py</p>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> conda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>奇奇怪怪不成体系问题合集</title>
      <link href="qi-qi-guai-guai-bu-cheng-ti-xi-wen-ti-he-ji/"/>
      <url>qi-qi-guai-guai-bu-cheng-ti-xi-wen-ti-he-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="HP打印机状态需要注意、打印显示用户干预如何处理？"><a href="#HP打印机状态需要注意、打印显示用户干预如何处理？" class="headerlink" title="HP打印机状态需要注意、打印显示用户干预如何处理？"></a>HP打印机状态需要注意、打印显示用户干预如何处理？</h2><p>和打印机连接在<strong>同一Wifi并已经添加了打印机</strong>的情况下，<strong>右键打印机</strong>-&gt;<strong>属性</strong>-&gt;<strong>Web服务</strong>，获得<strong>打印机IP</strong>，然后<strong>右键打印机</strong>-&gt;<strong>打印机属性</strong>-&gt;<strong>端口</strong>-&gt;<strong>添加端口</strong>-&gt;<strong>Standard TCP/IP Port</strong>-&gt;<strong>输入你的打印机的IP</strong>-&gt;<strong>填写随便一个端口名</strong>-&gt;<strong>应用</strong>-&gt;<strong>问题解决</strong>！</p><h2 id="文件扩展名？什么是-mat文件？"><a href="#文件扩展名？什么是-mat文件？" class="headerlink" title="文件扩展名？什么是.mat文件？"></a>文件扩展名？什么是.mat文件？</h2><p>文件扩展名查询：<a href="https://www.reviversoft.com/zh-cn/file-extensions/mat">https://www.reviversoft.com/zh-cn/file-extensions/mat</a></p><h2 id="Mathtype破解问题"><a href="#Mathtype破解问题" class="headerlink" title="Mathtype破解问题"></a>Mathtype破解问题</h2><p><a href="https://blog.csdn.net/weixin_43115631/article/details/110067650">首先是这篇博客</a></p><p><a href="https://blog.csdn.net/qq_40750329/article/details/102858972">其次是自己的CSDN</a></p><h2 id="IDM老是弹出更新界面？"><a href="#IDM老是弹出更新界面？" class="headerlink" title="IDM老是弹出更新界面？"></a>IDM老是弹出更新界面？</h2><p><a href="https://www.cnblogs.com/jingtaoxin/p/13773077.html">最新的破解</a></p>]]></content>
      
      
      <categories>
          
          <category> 奇奇怪怪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 奇怪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>肤色分级与系统</title>
      <link href="fu-se-fen-ji-yu-xi-tong/"/>
      <url>fu-se-fen-ji-yu-xi-tong/</url>
      
        <content type="html"><![CDATA[<h2 id="论文阅读-lt-lt-Brief-overview-of-PANTONE-SkinTone-Guide-chart-in-CIEL-a-b-color-space-gt-gt"><a href="#论文阅读-lt-lt-Brief-overview-of-PANTONE-SkinTone-Guide-chart-in-CIEL-a-b-color-space-gt-gt" class="headerlink" title="论文阅读-<<Brief overview of PANTONE SkinTone Guide chart in CIEL*a*b* color space>>"></a>论文阅读-&lt;&lt;Brief overview of PANTONE SkinTone Guide chart in CIEL*a*b* color space&gt;&gt;</h2><h3 id="Title-amp-Keywords-amp-Abstract-amp-Conclusion"><a href="#Title-amp-Keywords-amp-Abstract-amp-Conclusion" class="headerlink" title="Title&amp;Keywords&amp;Abstract&amp;Conclusion"></a>Title&amp;Keywords&amp;Abstract&amp;Conclusion</h3><h4 id="CIEL-a-b-color-space"><a href="#CIEL-a-b-color-space" class="headerlink" title="CIEL*a*b* color space?"></a>CIEL*a*b* color space?</h4><p><strong>颜色开发培训讲义</strong>：<a href="https://www.zhihu.com/column/cxqingzong-color">https://www.zhihu.com/column/cxqingzong-color</a>     <strong>这篇不能更赞</strong></p><p><strong>可见光谱:</strong></p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014213348777.png" alt="可见光谱"></p><p>我们所说的<strong>颜色主要分两种</strong>：</p><blockquote><p><strong>光源色（light source color）</strong>：来自发光体的颜色。如太阳，灯泡，led灯，等等。</p><p><strong>表面色（surface color</strong>）：不是来自发光体的物体色。物体本身不发光，但能看到物体的颜色，是因为这些物体能对来自于其他发光体的光的选择性的吸收和反射。</p></blockquote><p>颜色也可以简单分为两大类：</p><blockquote><p><strong>非彩色</strong>：黑白灰</p><p><strong>彩色</strong>：红黄蓝绿等</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014210822647.png" alt="彩色与非彩色"></p><p><strong>颜色感知的三要素，光源，物体和观察者，缺一不可，缺少一个要素，我们看不到颜色，或者其中一个要素发生改变，我们看到的颜色都会不一样。</strong></p><p><strong>1光源</strong></p><p><strong>不同光源性质是不一样的</strong>，有些光源会亮一点白一点，如太阳，或者就是太阳光，一天内不同时间段的太阳光也会有很大差异，导致在这些不同的光源下看相同一个颜色都会有很大差异.</p><p>所以我们在<strong>颜色开发或者颜色沟通交流的时候，会指定一个标准光源</strong>，这样能确保我们双方看颜色条件的一致性。我们比对颜色使用的光源都是有标准规定的光源。通常在下图所示的对色灯箱里看颜色。灯箱里面装有不同的常用光源。</p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014215437189.png" alt="对色灯塔及常见光源" style="zoom: 67%;"><blockquote><p><strong>Light sources：发光体（照明体）</strong>。泛指能发出光（可见光）的物体，如太阳，蜡烛，灯泡等。但是有些发光体发出的光是变化的不稳定的。例如太阳光，就算在同一天光照辐射都是不一样的，是变化的，更何况在不同的天气，不同的季节。所以很难用这些不稳定的光源来进行对颜色的描述和交流。</p><p><strong>lluminants：光源</strong>。是一个可以定量描述的发光体。是国际照明委员会 CIE（Commission Internationale de L’Eclairage）为了对颜色的评估和计算而定义了不同类型的，能用数学表（相对能量和波长)表示的标准光源。</p></blockquote><p><strong>色温Color temperature是照明光学中用于定义光源颜色的一个物理量。光源的色温是以光源发光时所显现的颜色与一个绝对黑体被高温燃烧时所显现的颜色相一致时的燃烧温度来定义的，它的单位是绝对温度Kelvin开尔文【K】。是为了量化光源色彩的一个物理量</strong></p><p>开尔文与摄氏度的转换关系如下：</p><p><strong>K(开尔文）=273.15+T(摄氏度）</strong></p><p>K值越高，显现的颜色就愈趋向于白蓝色；K值越低，显现的颜色就愈趋向于黄红色。</p><p>开尔文认为，假定纯黑体，能够将落在其上的所有热量吸收，而没有损失，同时又能够将热量生成的能量全部以“光”的形式释放出来的话，它产生辐射最大强度的波长随温度变化而变化。</p><p><strong>显色指数color rendering index (CRI)</strong> :<strong>与标准的参考光源相比较，一个光源对物体颜色外貌所产生的效果</strong>。换句话说，是<strong>一个光源与标准光源（例如日光）相比较下在颜色辨认方面的一种测量方式</strong>。CRI是一种得到普遍认可的度量标准，也是目前评价与报告<strong>光源显色性</strong>的惟一途径。</p><p><strong>Ra</strong> <strong>=</strong> <strong>物体在某一光源照射下所显现的颜色 ÷ 物体本身所具有的颜色</strong>。</p><p>Ra表示某光源的显色指数。Ra愈接近100%，<strong>表明在该光源照射下，物体所显现的颜色与物体本身所具有的颜色的差异就愈小</strong>。</p><p><strong>标准光源</strong>的光谱要求如下：</p><p>（1）光源的<strong>色温必须是5000K-6500K</strong>，在这种光源色温下观察颜色的效果基本类似于中国大部分地区上午8点至10点，下午3点至5点的自然光下的观察效果。</p><p>（2）光源的<strong>显色指数Ra&gt;90</strong></p><p>光源的性质，可以通过<strong>光谱功率分布曲线（SPD）</strong>来描述。不同的光源有着不同的光谱功率分布曲线。光谱功率分布（SPD）的意思就是光源发出可见光的不同光谱波长（400nm~700nm）的功率是不同的。功率可以理解成强度的大小。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015094547985.png" alt="常用光源的光谱功率分布曲线"></p><p><strong>2物体</strong></p><blockquote><p><strong>物体关于颜色的性质是对不同波长的电磁波的选择性吸收</strong>，所以我们用<strong>光谱反射率曲线</strong>来表达物体的这种性质。<strong>红色绿色蓝色</strong>的光谱反射率曲线的<strong>最大特征</strong>是，它<strong>有明显的波峰</strong>。<strong>波峰所在的位置的电磁波波长代表着这个物体的颜色</strong>。</p><p>但是<strong>黑白灰</strong>就不一样。物体之所以能够呈现出<strong>白色</strong>，是因为这个物体对<strong>不同波长的电磁波几乎都不吸收</strong>，所以都被反射出来。<strong>黑色</strong>跟白色刚好相反，黑色物体<strong>几乎完全吸收所有波长的电磁波</strong>，所以从黑色的光谱反射率曲线来看，所有波长的光谱的反射率都很低很低。</p></blockquote><p><strong>3观察者人眼：</strong></p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015095247048.png" alt="人眼观察黄光" style="zoom:67%;"><p>类似人眼三种视锥细胞对不同波长的光的响应，研究人员也得到一个标准观察者的三刺激值（x，y，z），作为测色仪辨别颜色的视锥细胞。通过这三个参数xyz，来描述一个颜色，也就是后面将要介绍的<strong>CIE-XYZ颜色空间</strong>。</p><p><strong>CIE-XYZ颜色空间：</strong></p><p>我们将<strong>光源</strong>、<strong>物体</strong>和<strong>观察者</strong>这三要素的性质相乘，也就是光源的光谱功率分布曲线乘以物体的光谱反射率曲线乘以标准观察者，得到三个参数<strong>X，Y，Z</strong>（都是大写字母），不同的颜色，有着不同XYZ值。</p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100147722.png" alt="CIEXYZ" style="zoom:80%;"><p>按照下图里的公式算出<strong>x</strong>（小写X），<strong>y</strong>（小写Y）。<strong>xyz值（小写）代表着XYZ（大写）的占比</strong>，这样三个参数缩减到两个参数，<strong>两个参数形成一个平面的二维颜色空间，也就是CIE-XYZ颜色空间</strong>。CIE XYZ颜色空间具有不均匀性。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100255681.png" alt="CIE-XYZ颜色空间"></p><p><strong>缺点：不容易对颜色差异的大小进行判定</strong>，<strong>无法非常直观的判定这个颜色</strong>就是我需要的颜色，不知道这个颜色跟我需要的颜色的差异的大小。</p><p>我们把<strong>人眼感觉不出的色彩差别量（变化范围）叫做颜色的宽容量</strong>。颜色的宽容量反映在<strong>CIExy色度图上即为两个色度点之间的距离</strong>。因为，每种颜色在色度图上是一个点，但<strong>对人的视感觉来说，当这种颜色的色度坐标位置变化很小时，人眼仍认为它是原来的颜色，感觉不出它的变化。</strong>所以，对视感觉效果来说，<strong>在这个变化的距离（或范围）以内的色彩差别量，在视觉效果上是等效的。</strong>对色彩复制和其它颜色工业部门来说这种位于人眼宽容量范围之内的色彩差别量是允许存在的。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015220402922.png" alt="不同标准色度点的颜色宽容量"></p><p><strong>CIE-L*a*b*颜色空间：</strong></p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201016174528078.png" alt="介绍"></p><p>跟之前介绍的孟塞尔颜色体系的颜色空间是一样，是三维空间中立体的球形。空间中有三个维度，形成三个互相垂直的轴，分别是：</p><ul><li>L*轴：从上到下；<strong>表示明度</strong>，范围由0到100，表示颜色从深（黑）到浅（白）。</li><li>a*轴：从左到右；<strong>表示红绿</strong>，数值变化由正到负，表示颜色从红（正）到绿（负）。a值越大颜色越红，a值越小颜色越绿。</li><li>b*轴：从里到外。<strong>表示黄蓝</strong>，数值变化由正到负，表示颜色从黄（正）到蓝（负）。b值越大颜色越黄，b值越小颜色越蓝。</li><li><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100941807.png" alt="CIE-L*a*b*"></li></ul><p><strong>两个颜色之间的差异大小</strong>。引入一个概念——<strong>色差△E</strong>。<strong>两个颜色的差异大小，就是这两个颜色的在颜色空间上两个点的距离</strong>。色差的计算公式如下：</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015100802146.png" alt="色差的计算公式"></p><p><strong>同色异谱：</strong></p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019221820491.png" alt="同色异谱"></p><blockquote><p>有时候我们看两个物体的颜色，在某种场景下，比如上图左边的两个物体在室外太阳光底线看起来颜色的一样的，但是一旦我们拿到室内，如上图右边，在荧光灯管底线发现，其实这两个物体的颜色是相差非常大的。这就是同色异谱现象，<strong>同色异谱也叫做条件对色</strong>，顾名思义，这两个颜色只有符合一定观察条件下颜色才能相等，实际上这两个颜色并非完全一样。</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019222411178.png" alt="同色异谱"></p><blockquote><p><strong>同色异谱（条件等色）的根源在于两物体的光谱反射率曲线不同</strong>，也就是说有不同的颜色色粉配方。同色异谱中的<strong>“谱”指的就是光谱反射率曲线</strong>。如下图，就是上面两个颜色色卡的光谱反射率曲线，可以看到两者的差异是非常大的。也可以看到两个光谱反射率曲线的交叉点很多。</p><p>《颜色技术原理》中提到过史泰鲁斯 （stiles）和 维 泽 斯 基（ wyszecki）发现两个同色异谱的颜色的光谱反射曲线在可见光谱波段 （400~700nm） 内， 至少在三个不同波长上必须具有相同的反射率。也就是两者的光谱反射率曲线至少要有三个交叉点 。</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019222914891.png" alt="不同的光谱反射率曲线"></p><p>常见颜色空间介绍：<a href="https://blog.csdn.net/JiangHui1211/article/details/84592774?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">https://blog.csdn.net/JiangHui1211/article/details/84592774?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><p><strong>RGB颜色空间</strong>:</p><p><strong>任意色光F</strong>都可以用RGB<strong>三种颜色不同分量的相加混合而成</strong>：**F=r[R]+g[G]+b[B]**。</p><p><strong>一般我们读取图片获得的三维矩阵是RGB空间</strong></p><p><strong>色度学规则</strong>：<br>　　(1)通过<strong>R,G,B这三种颜色能产生任何颜色</strong>，并且<strong>这三种颜色混合后产生的颜色是唯一</strong>的。<br>　　(2)如果<strong>两个颜色相等，这三个颜色分量再乘以或者除以相同的数，得到的颜色仍然相等</strong>。<br>　　(3)<strong>混合色的亮度等于每种颜色亮度的和</strong>。</p><p><strong>RGB颜色空间</strong>的<strong>均匀性非常差，且两种颜色之间的知觉差异色差不能表示为该颜色空间中两点间的距离</strong>，但是<strong>利用线性或非线性变换</strong>，则<strong>可以从RGB颜色空间推导出其他的颜色特征空间</strong>。</p><p><strong>CMYK模式：</strong></p><p>俗称<strong>四色打印模式</strong>，是最佳的打印模式。因为在实际应用中，青色、洋红色和黄色很难叠加形成真正的黑色，最多不过是褐色而已。因此才引入了K——黑色。黑色的作用是强化暗调，加深暗部色彩。</p><p><strong>HSV颜色空间：</strong></p><p>感觉和孟塞尔颜色体系很像。**HSV即色相(Hue)、饱和度(Saturation)、明度(Value)，又称HSB(B即Brightness)**。</p><p>RGB和CMYK<strong>面向硬件</strong>，可用于<strong>图片编码</strong>；HSV<strong>面向用户</strong>，可用于<strong>图片编辑软件</strong>。</p><p><strong>sRGB色彩空间：</strong></p><p>standard Red Green Blue，<strong>标准红绿蓝色彩空间</strong>是惠普与微软于1996年一起开发的用于<strong>显示器、打印机以及因特网的一种标准RGB</strong>色彩空间。这种标准得到了W3C、Exif、英特尔、Pantone、Corel以及其它许多业界厂商的支持。</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015222536629.png" alt="sRGB色域"></p><p>维基百科：<a href="https://zh.wikipedia.org/wiki/Lab%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4">https://zh.wikipedia.org/wiki/Lab%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4</a></p><p>在<strong>RGB</strong>或<strong>CMYK</strong>值与<strong>L*a*b*</strong> 之间没有转换的简单公式，因为<strong>RGB和CMYK色彩空间是设备依赖的</strong>。RGB或CMYK值<strong>首先必须被变换到特定绝对色彩空间中，比如sRGB或Adobe RGB</strong>。这种调整将是设备依赖的，但是<strong>变换的结果数据是设备无关的</strong>，允许把数据变换成<strong>CIE 1931色彩空间</strong>并接着变换成<strong>L*a*b*</strong>。</p><h4 id="Hue-Angle色相角"><a href="#Hue-Angle色相角" class="headerlink" title="Hue Angle色相角?"></a>Hue Angle色相角?</h4><blockquote><p>孟塞尔颜色体系：</p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201014214736980.png" alt="image-20201014214736980" style="zoom:50%;"><p><strong>1）色相/色调/Hue</strong>，对于色相我们比较熟悉的是这个色环，色环上不同位置代表不同色相。色相的排列顺序是按照可见光波长从低到高，逆时针分布。这是色相环的概念。把一周均分成五5种主色互相调和成五种中间色，相邻的两个位置之间再均分10份，共100份</p><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201015221249004.png" alt="色相带"></p><p><strong>2）明度/Value/Lightness</strong>，很容易理解，就是一个颜色中含有白和黑的比例：白越多黑越少，这个颜色的明度就越高。</p><p><strong>3）色度chroma</strong>。是一个颜色里面含有这个色相的浓度。<strong>很多人容易把饱和度和明度的概念混淆。是因为他们不理解饱和度和明度在色彩空间中的位置。明度在色彩空间中的位置是从顶部到底部，明度从高到低。而饱和度在色彩空间中的位置是从里到外，饱和度从低到高。</strong></p></blockquote><h4 id="色度-色域？"><a href="#色度-色域？" class="headerlink" title="色度?色域？"></a>色度?色域？</h4><p>研究颜色测量的学科叫做<strong>色度学</strong>，色度学的任务就是用数量化来表征色觉特性。色度”中的“度”是度量的意思。 类似于长度，高度等等概念。度量长度或高度使用的工具是尺子，而度量颜色的工具就是<strong>颜色感知三要素</strong>。</p><p><strong>色域</strong>是对一种颜色进行编码的方法，也指一个技术系统能够产生的颜色的总合。在计算机图形处理中，色域是<strong>颜色的某个完全的子集</strong>。颜色子集最常见的应用是用来精确地代表一种给定的情况。例如一个给定的色彩空间或是某个输出装置的呈色范围。</p><h3 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h3><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170452039.png" alt="Table1" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170512027.png" alt="Table2" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170533519.png" alt="Figure1" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170555412.png" alt="Figure2" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019170613588.png" alt="Figure3" style="zoom:50%;"><blockquote><p>lightness and chroma:亮度和色度</p><p>yellow and red categories:Hue色相</p></blockquote><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019215839363.png" alt="观察者"></p><blockquote><p>用于色彩排列和分类的这种三维系统已经融入目前广泛使用的<strong>色彩空间模型、色差公式和色容差系统</strong>。</p></blockquote><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019223925779.png" alt="转换" style="zoom:67%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019224245507.png" alt="从色彩到色彩测量" style="zoom:50%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019224402876.png" alt="从色彩到色彩测量" style="zoom:50%;"><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><blockquote><p><strong>Applications</strong>:</p><ol><li>diagnosis and treatments of cutaneous disorders </li><li>matching our body color to get maxillofacial soft tissue prostheses</li><li>face detection and recognition</li><li>cosmetics</li></ol><p>CIEL*a*b* is a <strong>device-independent color space</strong> which we used in our study.</p></blockquote><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><blockquote><p>110 colors numbered from 1Y01 SP to 4R15 SP</p><p> the first number <strong>indicates the chroma, varies from 1 to 5</strong> which it is the highest</p><p> the rigid represents <strong>yellowness (Y)</strong> or <strong>redness (R) as hue</strong> and </p><p><strong>two last numbers show the lightness, varies from 1 to 15</strong> which it is the darkest. These samples are sorted in <strong>decreasing order</strong> of lightness (5).</p></blockquote><h2 id="小实验"><a href="#小实验" class="headerlink" title="小实验"></a>小实验</h2><blockquote><p>MATLAB Api：<a href="https://www.mathworks.com/help/">https://www.mathworks.com/help/</a></p></blockquote><pre class=" language-matlab"><code class="language-matlab"><span class="token comment" spellcheck="true">% shows example of illuminant estimation based on Grey-World, Shades of</span><span class="token comment" spellcheck="true">% Gray, max-RGB, and Grey-Edge algorithm</span><span class="token comment" spellcheck="true">%example images</span>input_im<span class="token operator">=</span><span class="token function">double</span><span class="token punctuation">(</span><span class="token function">imread</span><span class="token punctuation">(</span><span class="token string">'test_3.jpg'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>input_im<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">title</span><span class="token punctuation">(</span><span class="token string">'input image'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% %Grey-World</span><span class="token comment" spellcheck="true">% [wR,wG,wB,out]=general_cc(input_im,0,1,0);</span><span class="token comment" spellcheck="true">% figure(2);</span><span class="token comment" spellcheck="true">% imshow(uint8(out));</span><span class="token comment" spellcheck="true">% title(</span><span class="token string">'Grey-World'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%max-RGB</span><span class="token comment" spellcheck="true">% [wR,wG,wB,out]=general_cc(input_im,0,-1,0);</span><span class="token comment" spellcheck="true">% figure(3);imshow(uint8(out));</span><span class="token comment" spellcheck="true">% title(</span><span class="token string">'max-RGB'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% Shades of Grey</span>mink_norm<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">% any number between 1 and infinity</span><span class="token punctuation">[</span>wR<span class="token punctuation">,</span>wG<span class="token punctuation">,</span>wB<span class="token punctuation">,</span>out<span class="token punctuation">]</span><span class="token operator">=</span><span class="token function">general_cc</span><span class="token punctuation">(</span>input_im<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>mink_norm<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">title</span><span class="token punctuation">(</span><span class="token string">'Shades of Grey'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% Grey-Edge</span><span class="token comment" spellcheck="true">% mink_norm=5;    % any number between 1 and infinity</span><span class="token comment" spellcheck="true">% sigma=2;        % sigma </span><span class="token comment" spellcheck="true">% diff_order=1;   % differentiation order (1 or 2)</span><span class="token comment" spellcheck="true">% [wR,wG,wB,out]=general_cc(input_im,diff_order,mink_norm,sigma);</span><span class="token comment" spellcheck="true">% figure(5);imshow(uint8(out));</span><span class="token comment" spellcheck="true">% title(</span><span class="token string">'Grey-Edge'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% 截取皮肤区域</span><span class="token comment" spellcheck="true">% out=out(375:480,185:290,:);</span><span class="token comment" spellcheck="true">% out=out(360:420,360:420,:);</span>out<span class="token operator">=</span><span class="token function">out</span><span class="token punctuation">(</span><span class="token number">165</span><span class="token operator">:</span><span class="token number">240</span><span class="token punctuation">,</span><span class="token number">230</span><span class="token operator">:</span><span class="token number">320</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% 展示截取皮肤区域</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% convert to lab</span>labI <span class="token operator">=</span> <span class="token function">rgb2lab</span><span class="token punctuation">(</span><span class="token function">uint8</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">%seperate l,a,b</span><span class="token comment" spellcheck="true">%matlab的下标从1开始</span>l <span class="token operator">=</span> <span class="token function">labI</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>a <span class="token operator">=</span> <span class="token function">labI</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>b <span class="token operator">=</span> <span class="token function">labI</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">% % 显示各维度直方图</span><span class="token comment" spellcheck="true">% figure(4);</span><span class="token comment" spellcheck="true">% hist(l);</span><span class="token comment" spellcheck="true">% figure(5);</span><span class="token comment" spellcheck="true">% hist(a);</span><span class="token comment" spellcheck="true">% figure(6);</span><span class="token comment" spellcheck="true">% hist(b);</span><span class="token comment" spellcheck="true">%网格曲面图</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">subplot</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">meshc</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>l<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">subplot</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">meshz</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>l<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><img src="/fu-se-fen-ji-yu-xi-tong/result_0.png" alt="result_0" style="zoom: 50%;"><img src="/fu-se-fen-ji-yu-xi-tong/result_1.png" alt="result_1" style="zoom: 50%;"><p><img src="/fu-se-fen-ji-yu-xi-tong/result_3.png" alt="result_3" style="zoom: 67%;"><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019214729213.png" alt="image-20201019214729213"></p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019214825960.png" alt="三维图" style="zoom:80%;"><h2 id="接下来打算要做"><a href="#接下来打算要做" class="headerlink" title="接下来打算要做"></a>接下来打算要做</h2><p><img src="/fu-se-fen-ji-yu-xi-tong/image-20201019155029016.png" alt="计划流程图"></p><h2 id="Pantone-Skintone-Review"><a href="#Pantone-Skintone-Review" class="headerlink" title="Pantone Skintone Review"></a>Pantone Skintone Review</h2><p>彩通肤色指南[PANTONE SkinTone Guide]是根据科学测量各种人类皮肤类型中数干种实际肤色而建立。这个色库为再现实体肤色而配制，是人类肤色的完整视觉参考，适用于与肤色相关的任何市场。</p><p>首个匹配和再现逼真肤色的科学指南，适用于各个行业。1000多种人体皮肤测量值，收集于不同年龄和种族的参与者。为获得一致的样品精确度．采用几种高端X—Rite分光光度计和小巧的手持PANTONE CAPSURE分光光度计来测量皮肤样本。根据这些测量值，Pantone<br>建立了精确的皮肤色彩空间．并创建了彩通肤色色库(PANTONE SkinToneLibrary)．它确定了1 10种再现性最强的色彩。<br><strong>现有的主要问题是，这110种肤色是如何确定的？</strong></p><p><strong>这也是我们需要做的！</strong></p>]]></content>
      
      
      <categories>
          
          <category> 肤色分级 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 肤色分级 </tag>
            
            <tag> 颜色空间 </tag>
            
            <tag> MATLAB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人脸识别与肤色分类</title>
      <link href="ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/"/>
      <url>ren-lian-shi-bie-yu-fu-se-fen-lei-ji-dlib-ku/</url>
      
        <content type="html"><![CDATA[<h2 id="莫名其妙被另一个文件覆盖了？？？文件丢失？？为什么呀我吐了！"><a href="#莫名其妙被另一个文件覆盖了？？？文件丢失？？为什么呀我吐了！" class="headerlink" title="莫名其妙被另一个文件覆盖了？？？文件丢失？？为什么呀我吐了！"></a>莫名其妙被另一个文件覆盖了？？？文件丢失？？为什么呀我吐了！</h2>]]></content>
      
      
      <categories>
          
          <category> face_recognition </category>
          
      </categories>
      
      
        <tags>
            
            <tag> face_recognition </tag>
            
            <tag> tkinter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献图书资源搜索与使用管理</title>
      <link href="wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/"/>
      <url>wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/</url>
      
        <content type="html"><![CDATA[<h2 id="文献引用信息有误"><a href="#文献引用信息有误" class="headerlink" title="文献引用信息有误"></a>文献引用信息有误</h2><blockquote><p>进入Mendeley网站，搜索出错的文献，然后“+Add to library”</p></blockquote><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201012160759194.png" alt="image-20201012160759194" style="zoom:67%;"><blockquote><p>回到Mendeley sync一下，出现该项，但是此时无法打开，如下图将你的论文添加：</p></blockquote><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201012160634578.png" alt="image-20201012160634578"></p><h2 id="SCI一区、二区、影响因子？"><a href="#SCI一区、二区、影响因子？" class="headerlink" title="SCI一区、二区、影响因子？"></a>SCI一区、二区、影响因子？</h2><p>一般SCI论文分四个区，一区都是国际顶级期刊，二区次之，三区和四区是一般的SCI期刊，有两种，一种是web of science的JCR（journal citation report）分区，在web of science 搜索论文下面会有按钮显示期刊影响力。另一种是中科院分区，请在高校或中科院内登录<a href="http://www.fenqubiao.com/">http://www.fenqubiao.com</a>。</p><p>影响因子（英文：Impact Factor），简称IF，是汤森路透（Thomson Reuters）出品的期刊引证报告（Journal Citation Reports，JCR）中的一项数据。 即某期刊前两年发表的论文在该报告年份（JCR year）中被引用总次数除以该期刊在这两年内发表的论文总数。这是一个国际上通行的期刊评价指标。影响因子现已成为国际上通用的期刊评价指标，它不仅是一种测度期刊有用性和显示度的指标，而且也是测度期刊的学术水平，乃至论文质量的重要指标。影响因子是一个相对统计量</p><h2 id="知名会议和期刊"><a href="#知名会议和期刊" class="headerlink" title="知名会议和期刊"></a>知名会议和期刊</h2><p>查找会议论文：<a href="https://blog.csdn.net/qq_35091353/article/details/107209512">https://blog.csdn.net/qq_35091353/article/details/107209512</a></p><p><strong>ICCV</strong>:International Conference on Computer Vision</p><p><strong>CVPR</strong>:International Conference on Computer Vision and Pattern Recognition</p><p><strong>ECCV</strong>:Europeon Conference on Computer Vision</p><p><strong>TPAMI</strong>:IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</p><p><strong>TIP</strong>:IEEE TRANSACTIONS ON IMAGE PROCESSING</p><p><strong>TSP</strong>: IEEE TRANSACTIONS ON SIGNAL PROCESSING</p><p><strong>ICLR：International Conference on Learning Representations</strong>   顶级会议，大佬背书</p><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201119155123687.png" alt="不完整列表"></p><h2 id="图书搜索"><a href="#图书搜索" class="headerlink" title="图书搜索"></a>图书搜索</h2><blockquote><p><strong>虫部落快搜</strong>：<a href="https://search.chongbuluo.com/">https://search.chongbuluo.com/</a></p><p>集合了很多搜索，包括<strong>鸠摩搜书</strong></p><p><strong>Z-Library：</strong><a href="https://1lib.net/">https://1lib.net/</a>   <strong>真的是宝藏</strong></p><p>Library Genesis：<a href="https://libgen.lc/">https://libgen.lc/</a></p></blockquote><h2 id="如何快速有效的读论文？"><a href="#如何快速有效的读论文？" class="headerlink" title="如何快速有效的读论文？"></a>如何快速有效的读论文？</h2><p><strong>进入新方向先看综述！！！</strong></p><h3 id="误区？"><a href="#误区？" class="headerlink" title="误区？"></a>误区？</h3><p><strong>从头到尾恨不得嚼透每个单词</strong>，读完后束之高阁让记忆随风飘摇，<strong>无选择的精读</strong></p><p>读论文和学英语区分开！！！读论文不是来学英语！<strong>那些在阅读中遇到的生单词（学术词汇除外）真的会对通篇的理解形成严重障碍吗？</strong></p><p><strong>只看不记</strong></p><h3 id="如何读一篇论文？"><a href="#如何读一篇论文？" class="headerlink" title="如何读一篇论文？"></a>如何读一篇论文？</h3><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201103091823835.png" alt="论文结构" style="zoom:67%;"><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201103092224607.png" alt="是否值得读"></p><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201103092352775.png" alt="如何读"></p><p><img src="/wen-xian-tu-shu-zi-yuan-sou-suo-yu-shi-yong-guan-li/image-20201103092602961.png" alt="如何读"> 很多人对于做笔记到底写什么各执一词，这里我觉得每个人在科研的不同阶段对于文章的关注点可能不尽相同，所以很难一言以蔽之。</p><p>比如<strong>初涉科研的小白</strong>，文献阅读能力和论文写作能力比较欠缺，那么可以在<strong>笔记中[用一句话（英文）概括实验、结果、讨论章节中的每一段内容，组成一个阅读笔记]。</strong>这样既可以锻炼英语书写表达能力，也可以逼迫自己[在理解的基础上进行一定量的输出，这是一个加深理解和记忆的过程」。<br>对于<strong>阅读科研文献比较熟练，有一些科研工作经历的人来说</strong>，这个笔记的内容可能是<strong>文中某个新的实验方法、异于其他研究的实验条件、阅读时自己的新想法等等</strong>。精读文献并认真做笔记并不代表读者对于这篇文章的消化过程就此终止，我个人觉得优秀的科研论文、大牛的研究著作依然是常读常新，每位从事科研学习和工作的人在不同的时期都能从中汲取养分。</p><h3 id="总结汇报"><a href="#总结汇报" class="headerlink" title="总结汇报"></a>总结汇报</h3><ul><li>基本信息(标题、作者、作者单位、发表期刊/会议、发表时间)</li><li>核心问题</li><li>主要思路/创新</li><li>存在问题与改进思路</li></ul><h3 id="技巧？"><a href="#技巧？" class="headerlink" title="技巧？"></a>技巧？</h3><p>review论文</p><p>与论文相应的PPT、博客、视频、课程、代码等</p><p>有代码的文章重点看！有代码的话, 一方面便于自己将一些核心思想或者基础知识理解透彻; 另一方面, 将来写文章,做对比试验也方便</p><p>英文论文中，<strong>每段话第一句一般都是主旨句</strong>，剩下内容都是围绕第一句展开（自己写论文时也可以这样，先写一段中心句，下面内容围绕它展开）</p><h2 id="要数据和源码"><a href="#要数据和源码" class="headerlink" title="要数据和源码"></a>要数据和源码</h2><blockquote><p>Xiaodan学姐您好！</p><p>冒昧打扰您，目前我就读于北京邮电大学，研究生一年级，此前一直对于图像美学和图像质量评价非常感兴趣。</p><p>有幸拜读过您的最新的文章 “Beyond Vision: A Multimodal Recurrent Attention Convolutional Neural Network for Unified Image Aesthetic Prediction Tasks”，其工作将多种方法巧妙运用，行文风格清晰，解决了我之前自学时的很多疑问，让我颇为受用，并且诞生出了以您这篇文章为基础进行后续学习和研究的想法。</p><p>因此在开学前的这几个月时间中，我也着手努力复现您这篇文章的架构，并寄希望于复现后，进一步研究学姐在总结部分提出的几个设想，但是因为个人知识的欠缺，在复现结构时遇到了很多困难，难以解决，我身边的同学和老师因为鲜有涉及这方面的研究，也不能给我提供相应的帮助。</p><p>综合上述两点，我想咨询学姐是否方便将源码提供给我这个后辈学习和研究，我必遵守相应的科研道德，以及您的相关要求合理使用这份宝贵的资源。如有搅扰，望您海涵。</p><p>最后，祝学姐工作顺利，科研顺心，生活美满！</p><p>耕耘前辈您好！</p><p>冒昧打扰您，我是就读于北京邮电大学的一名研究生，此前一直对于图像美学和图像质量评价非常感兴趣。</p><p>有幸拜读过您19年的文章 “Theme Aware Aesthetic Distribution Prediction with Full Resolution Photos”，您的工作利用巧妙的方法解决了美学图像多尺寸的输入问题，且行文风格清晰，让我颇为受用。</p><p>在研究生的初期，自己所在项目组一直在开展医学影像相关的工作，但是我个人对这一方面不感兴趣，在与导师交流后，同意我以计算机摄影学和图像美学评价为毕设课题。在看到您的文章过后，文章中的算法我较为熟悉，因此诞生出了以您这篇文章为基础进行后续学习和研究的想法。</p><p>在今年疫情的半年内，我也着手努力复现您这篇文章的架构，但是因为个人知识的欠缺，在复现结构时遇到了很多困难，难以解决，我身边的同学和老师因为鲜有涉及这方面的研究，也不能给我提供相应的帮助。</p><p>综合上述两点，我想咨询前辈是否方便将源码提供给我这个后辈学习和研究，我必遵守相应的科研道德，以及您的相关要求合理引用您的工作，倍加珍惜这份宝贵的资源。如有搅扰，望您海涵。</p><p>最后，祝前辈工作顺利，科研顺心，生活美满！</p></blockquote><h2 id="导出参考文献"><a href="#导出参考文献" class="headerlink" title="导出参考文献"></a>导出参考文献</h2><p>参考该博客：<a href="https://blog.csdn.net/xlcaoyi/article/details/90511973">https://blog.csdn.net/xlcaoyi/article/details/90511973</a></p>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mendeley </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Typora使用问题</title>
      <link href="typora-shi-yong-wen-ti/"/>
      <url>typora-shi-yong-wen-ti/</url>
      
        <content type="html"><![CDATA[<h2 id="无序列表如何退出？"><a href="#无序列表如何退出？" class="headerlink" title="无序列表如何退出？"></a>无序列表如何退出？</h2><p>左上角的选项很迷，可以使用源代码模式退出，还可以按一下退格两下enter退出</p><h2 id="公式块编辑常见命令？"><a href="#公式块编辑常见命令？" class="headerlink" title="公式块编辑常见命令？"></a>公式块编辑常见命令？</h2><blockquote><p>版权声明：本文为博主原创文章，遵循<a href="http://creativecommons.org/licenses/by-sa/4.0/"> CC 4.0 BY-SA </a>版权协议，转载请附上原文出处链接和本声明。</p><p>本文链接：<a href="https://blog.csdn.net/mingzhuo_126/article/details/82722455">https://blog.csdn.net/mingzhuo_126/article/details/82722455</a> </p></blockquote><p><a href="https://blog.csdn.net/u013914471/article/details/82973812">https://blog.csdn.net/u013914471/article/details/82973812</a></p><p>$\in$ 属于 $\notin$ 不属于  ${ }$ 打大括号 $\sim$打波浪线 $\partial$ 偏导<br>$$<br>change\line<br>$$</p><p><img src="/typora-shi-yong-wen-ti/image-20201125205741314.png" alt="数学模式重音符"></p><p>$\lfloor x \rfloor$ 向下取整  $\lceil x \rceil$向上取整  $\begin{cases} \end{cases}$分段函数大花括号  $\varepsilon$ </p><h2 id="emoji表情md编写？"><a href="#emoji表情md编写？" class="headerlink" title="emoji表情md编写？"></a>emoji表情md编写？</h2><p>参考大神博客：<a href="https://sunhwee.com/posts/a927e90e.html">https://sunhwee.com/posts/a927e90e.html</a></p><h3 id="Markdown设置？"><a href="#Markdown设置？" class="headerlink" title="Markdown设置？"></a>Markdown设置？</h3><p>可以在偏好设置的Markdown处设置内联公示、代码行号、公式序号</p><h2 id="括号问题"><a href="#括号问题" class="headerlink" title="括号问题"></a>括号问题</h2><p>只有中文括号可以显示在博客中</p><h2 id="莫名其妙的文件被另一个文件覆盖了？"><a href="#莫名其妙的文件被另一个文件覆盖了？" class="headerlink" title="莫名其妙的文件被另一个文件覆盖了？"></a>莫名其妙的文件被另一个文件覆盖了？</h2>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>颜色恒常性之&lt;&lt;A Multi-Hypothesis Approach to Color Constancy&gt;&gt;</title>
      <link href="yan-se-heng-chang-xing-zhi-mhcc/"/>
      <url>yan-se-heng-chang-xing-zhi-mhcc/</url>
      
        <content type="html"><![CDATA[<h2 id="论文思路"><a href="#论文思路" class="headerlink" title="论文思路"></a>论文思路</h2><p>论文：&lt;&lt;A Multi-Hypothesis Approach to Color Constancy&gt;&gt;</p><h3 id="Title-amp-Abstract-amp-Conclusion"><a href="#Title-amp-Abstract-amp-Conclusion" class="headerlink" title="Title&amp;Abstract&amp;Conclusion"></a>Title&amp;Abstract&amp;Conclusion</h3><blockquote><p>Multi-Hypothesis？</p></blockquote><p><strong>多假设都有什么假设？</strong></p><ol><li>Under the prevalent assumption that the scene is illuminated by a single or dominant light source, the observed pixels of an image are typically modelled using the physical model of Lambertian image formation captured under a trichromatic photosensor:</li><li>we assume that the color of the light and the surface reflectance are independent.</li><li>the function modelling the prior also depends on factors such as the environment (indoor / outdoor), the time of day, ISO etc. However, the size of currently available datasets prevent us from modelling more complex proxies.</li></ol><blockquote><p><strong>Our likelihood estimator</strong> learns to answer <strong>a camera-agnostic question</strong> and thus enables <strong>effective multi-camera training</strong> by disentangling illuminant estimation from the supervised learning task.</p><p>learning from image samples that were <strong>captured by multiple cameras</strong></p></blockquote><p><strong>相机无关和多相机图片训练到底是如何实现的？</strong></p><p>只是这个似然估计器与相机无关，多个相机获得多个数据集，对每个数据集利用KMeans找出候选光源，然后都喂入网络，实现了多相机图片训练</p><h3 id="Figure"><a href="#Figure" class="headerlink" title="Figure"></a>Figure</h3><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201019104708991.png" alt="Figure1"></p><blockquote><p><strong><u>（d）use an illuminant candidate set per camera</u></strong>. <strong><u>[ r/g ,b/g ]</u></strong></p></blockquote><p><strong>每个摄像机获得一个候选集吗？最后是如何训练的？对于每个摄像机的候选集，是如何选取划分的？</strong></p><p>每个摄像机有自己的一个照片集，对这里的图片进行分类，每个摄像机获得一个候选集。训练应该就是将这些候选集都喂入。</p><p><strong>[r/g,b/g]这个图如何读?</strong></p><p>图上一个点应该是代表一个光源，<strong>为了将三维降成二维？</strong>，显示了光源的分布</p><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201018164327078.png" alt="pipeline"></p><p><strong>如果现在有一张待还原的照片，如何还原，都要生成n个候选光源吗？怎么生成？</strong></p><p>n个候选光源已经生成好了，现在训练网络是需要不同光源的权重配比不同！所以到时候对于待还原照片，还是相同的过程，用每个候选光源修正图片，然后放入网络，得到权重。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>$$<br>\rho_{k}(X)=\int_{\Omega}E(\lambda)S(\lambda,X)C_{k}(\lambda)d\lambda\quad\quad\quad k\in{R,G,B}<br>$$</p><p><strong>为什么要积分？</strong></p><p>积分是因为比如绿色，打个比方是755~760这个频段的波长共同作用生成的，所以需要进行积分。<br>$$<br>\rho_{k}^E=\int_{\Omega}E(\lambda)C_{k}(\lambda)d\lambda\quad\quad\quad k\in{R,G,B}<br>$$</p><p>The goal of computational CC then becomes estimation of the <u><strong>global illumination color</strong></u>$\rho_k^E$？</p><p><strong>为什么变成了这个形式？</strong></p><p>对于物体成像的颜色，$S(\lambda,X)$表示物体本身的影响，$\rho_k^E$就表示光源的影响。后边我们说的光源就是指$\rho_k^E$整体。</p><blockquote><p>due to the <strong>ill-posed</strong> nature of the problem, <strong>multiple illuminant solutions are often possible with varying probability</strong>.</p></blockquote><p><strong>什么是ill-posed ？</strong></p><p>适定问题是指定解满足下面三个要求的问题：① 解是存在的；② 解是唯一的；③ 解连续依赖于定解条件，即解是稳定的。这三个要求中，只要有一个不满足，则称之为不适定问题</p><blockquote><p>avoid <u><strong>distribution shift</strong></u> and <u><strong>resulting domain gap problems</strong></u> [1, 41, 22], associated with camera specific training, and propose a well-founded strategy to leverage multiple data.</p></blockquote><p><strong>什么是distributin shift&amp;domain gap？</strong></p><p><strong>distribution shift</strong>: <a href="https://zh.d2l.ai/">https://zh.d2l.ai/</a> </p><p><strong>domain gap problem</strong>:<a href="https://zhuanlan.zhihu.com/p/195704051">https://zhuanlan.zhihu.com/p/195704051</a></p><blockquote><p>Principled combination of datasets is of high value for learning based color constancy given the typically small nature of individual color constancy datasets (on the order of only hundreds of images).</p></blockquote><p><strong>这句话在说啥？</strong></p><blockquote><p>We provide <strong><u>a training-free model adaptation strategy for new cameras</u></strong>.</p></blockquote><p><strong>加入一个新的摄像机，如何改进模型？</strong></p><p>新加入一个摄像机，只要这个摄像机的候选光源已知了，就可以直接拿这个网络训练了，所以不需要再重新训练或微调。</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="Bayesian-framework"><a href="#Bayesian-framework" class="headerlink" title="Bayesian framework"></a>Bayesian framework</h4><blockquote><p> They <strong>model the prior of the illuminant and the surface reflectance as a <u>truncated multivariate normal distribution</u> on the weights of a linear model</strong></p></blockquote><p><strong>什么是truncated multivariate normal distribution on the weights of a linear model?</strong></p><p>截断正态分布：指限制变量x取值范围(scope)的一种分布。例如，限制x取值在0到50之间，即{0&lt;x&lt;50}。</p><blockquote><p>Bayesian works [44, 23], <strong>discretise the illuminant space</strong> and <strong>model the surface reflectance priors</strong> by <u><strong>learning real world histogram frequencies</strong></u>;</p></blockquote><p>通过学习真实世界的直方图频率，来离散化光源空间和对表面反射率进行先验建模。<strong>可以查看它如何学习真实世界的直方图频率应用到肤色定级。</strong></p><blockquote><p>in [44] the prior is modelled as a <strong><u>uniform distribution over a subset of illuminants</u></strong> while [23] uses the <strong><u>empirical distribution of the training illuminants</u></strong>.</p></blockquote><p><strong>对于光源概率44和23有两种想法：直接建模成均匀分布和利用训练光源的经验分布。</strong></p><p>经验分布函数：<a href="https://zh.wikipedia.org/zh-hans/%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0#:~:text=%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%EF%BC%88%E8%8B%B1%E8%AA%9E%EF%BC%9Aempirical,%E6%A0%B7%E6%9C%AC%E6%89%80%E5%8D%A0%E7%9A%84%E6%AF%94%E4%BE%8B%E3%80%82">https://zh.wikipedia.org/zh-hans/%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0#:~:text=%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%EF%BC%88%E8%8B%B1%E8%AA%9E%EF%BC%9Aempirical,%E6%A0%B7%E6%9C%AC%E6%89%80%E5%8D%A0%E7%9A%84%E6%AF%94%E4%BE%8B%E3%80%82</a></p><h4 id="Fully-supervised-methods"><a href="#Fully-supervised-methods" class="headerlink" title="Fully supervised methods"></a>Fully supervised methods</h4><blockquote><p>frame color constancy as a classification problem：CCC and FCCC using a color space that identifies image re-illumination with a histogram shift. </p></blockquote><p><strong>CCC和FCCC待看</strong></p><h4 id="Multi-device-training"><a href="#Multi-device-training" class="headerlink" title="Multi-device training"></a>Multi-device training</h4><blockquote><p> [37] affords <strong>fast adaptation to previously unseen cameras</strong>, and robustness to changes in capture device by leveraging annotated samples across different cameras and datasets in a <strong><u>meta-learning</u></strong> framework</p></blockquote><p><strong>meta-learning?</strong></p><blockquote><p>A recent approach [8], makes an assumption that sRGB images collected from the web are well white balanced, therefore, they apply <strong><u>a simple de-gamma correction</u></strong> to approximate an <strong><u>inverse tone mapping</u></strong> and then find achromatic pixels with a CNN to predict the illuminant. </p></blockquote><p><strong>de-gamma correction？inverse tone mapping？</strong></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><blockquote><p>Let y = (yr, yg, yb) be a pixel from an input image Y in <strong><u>linear RGB space</u></strong>. </p></blockquote><p><strong>线性RGB空间？</strong></p><p><a href="https://www.cnblogs.com/guanzz/p/7416821.html">https://www.cnblogs.com/guanzz/p/7416821.html</a></p><p>gamma校正将把线性颜色空间转变为非线性空间</p><blockquote><p>We model the global illumination, Eq. (2), with the <strong><u>standard linear model</u></strong> [51] such that each pixel y is the product of the surface reflectance r = (rr, rg, rb) and a global illuminant ? = (?r, ?g, ?b) shared by all pixels such that</p></blockquote><p><strong>标准线性模型？</strong></p><p>可能就是三个函数相乘得到一个线性模型？</p><blockquote><p>we propose to frame the CC problem with a <strong><u>probabilistic generative model</u></strong> with unknown surface re- flectances and illuminant</p></blockquote><p><strong>概率生成模型？</strong></p><p><strong>公式推导</strong></p><p>$$<br>P(l|Y)=\frac{P(Y|l)P(l)}{P(Y)}<br>$$</p><p>$$<br>P(Y|l)=\int_rP(Y|l,R=r)P(R=r)dr<br>$$</p><p>公式(4)利用了全概率公式<br>$$<br>\int_rP(Y|l,R=r)P(R=r)dr=P(R=diag(l)^{-1}Y)<br>$$<br>公式(5),由于$y_k=r_k\cdot l_k\quad\quad k\in R,G,B$ 所以当且仅当$R=diag(l)^{-1}$时，才能生成Y,所以此时$P(Y|l,R=diag(l)^{-1})=1$,$P(Y|l,R=else)=0$,所以只剩下一项$P(R=diag(l)^{-1}Y)$</p><blockquote><p>We highlight that learned affine transformation parameters are training <strong>camera-dependent and provide further discussion</strong> on camera agnostic considerations in Section</p></blockquote><p><strong>为什么这个参数是摄像机依赖的？</strong></p><p>因为$B_l$是光源的先验估计，由公式二，全局光源由光源功率和接收函数决定。所以是摄像机依赖的。</p><blockquote><p>In order to estimate the illuminant  l*, we optimise the quadratic cost (minimum MSE Bayesian estimator), minimised by the mean of the posterior distribution:<br>$$<br>l^*=\int_l l\cdot P(l|Y)dl<br>$$</p></blockquote><p><strong>为什么是这个公式？</strong></p><p>我们现在获得了n个光源$l_0、l_1\cdots l_n$和n个概率$p_0、p_1\cdots p_n$,我们如何确定最优光源$l^*$?该论文就是简单使MSE最小，当$l^*$是期望时MSE最小，如果你忘了为啥了可以列个二次函数求导！</p><blockquote><p>We require <strong>a differentiable method</strong> in order to train our model end-to-end, and therefore the use of <strong>a simple Maximum a Posteriori （MAP）inference strategy is not possible</strong>. Therefore to estimate the illuminant l*, we use the minimum mean square error Bayesian estimator, which is minimised by the posterior mean of l (c.f. Eq. (6))”</p></blockquote><p><strong>为什么MAP不行？</strong></p><p>因为反向传播我们是需要求导的，而如果用极大后验估计求$l^*$，似然是用网络得到的，是没有办法求导的；所以我们需要采取一个办法他不需要对网络那一块求导就能得到$l^*$，所以使用最简单的方法-使MSE最小，$l^*$就是各个候选光源的期望。<br>$$<br>l^*=\sum_{i=1}^n l_i\cdot softmax(log(P(l_i|Y)))\<br>=\frac{1}{\sum e^{log(P(l_i|Y))}}\sum_{i=1}^nl_i\cdot e^{log(P(l_i|Y))}\<br>=\frac{1}{\sum P(l_i|Y)}\sum_{i=1}^nl_i\cdot P(l_i|Y)<br>$$</p><blockquote><p>The resulting vector $l^*$ is l2-normalised.</p></blockquote><p><strong>l2-normalised？</strong></p><p><a href="https://blog.cweihang.io/ml/trick/l2_normalize">https://blog.cweihang.io/ml/trick/l2_normalize</a></p><p>？？？</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><strong>Gehler-Shi</strong> dataset存在非一致真实值的情况 2个摄像机，分别为Canon 1D和Canon 5D 室内室外组合 佳能RAW格式保存，并提供了tiff格式，还提供了颜色检查板的坐标 因为自带程序包含非线性处理，所以使用Dcraw转换为tiff格式，并且只对RGGB的两个G取了平均，没有进行去马赛克，12位</p><p>NUS 8个摄像机</p><p>Cube+  Canon550D 主要室外</p><p>NUS Shi均为3折 用之前工作提供的划分 Cube+没提供，所以用所有的图像训练，用比赛数据集测试，还跟人家的比赛结果比了比</p><p>NUS加了个多摄像机模式 自己弄了个划分</p><p><strong>Trimean？</strong></p><p>三均值<br>$$<br>TM=\frac{Q_1+2Q_2+Q_3}{4}<br>$$<br>Q1,Q3为数据的两个四分位点，Q2为中位数</p><h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3><p><strong>1x1 Conv?</strong></p><p>也叫Network in Network,添加了一个非线性运算，可用于压缩信道或增加信道</p><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201027160655612.png" alt="吴恩达课程"></p><blockquote><p>Towards reproducibility, and fair comparison, our suppplementary material provides the cross validation splits, used in the main paper, for multi-device training</p></blockquote><p><strong>Cross validation？</strong></p><p>交叉验证：<a href="https://zhuanlan.zhihu.com/p/24825503">https://zhuanlan.zhihu.com/p/24825503</a></p><h2 id="个人思路"><a href="#个人思路" class="headerlink" title="个人思路"></a>个人思路</h2><h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a><strong>创新点</strong></h3><ol><li><p>提供了最佳光源的多个可能性</p></li><li><p>采用分类的方法而不是回归</p></li><li><p>设计的网络是摄像机无关的，可以使用多设备数据集进行训练，对于新型设备的泛化性比较好</p></li></ol><h3 id="多假设"><a href="#多假设" class="headerlink" title="多假设"></a>多假设</h3><ol><li>Under the prevalent assumption that the scene is illuminated by a single or dominant light source, the observed pixels of an image are typically modelled using the physical model of Lambertian image formation captured under a trichromatic photosensor:</li><li>we assume that the color of the light and the surface reflectance are independent.</li><li>the function modelling the prior also depends on factors such as the environment (indoor / outdoor), the time of day, ISO etc. However, the size of currently available datasets prevent us from modelling more complex proxies.</li></ol><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029093444705.png" alt="朗伯特模型"></p><p>对此公式可以进行简化，原式$=S(X)\int_\Omega E(\lambda)C_k(\lambda)d\lambda\quad k\in {R,G,B}$</p><p><strong>公式简化的两种解释</strong></p><p>1：相机R、G、B光谱敏感函数是<strong>狄拉克δ函数</strong>，就是说，每个相机的光敏R、G、B三通道每个只能感应波长的一个值</p><p>2：RGB的能感知的光谱构成可见光的一个划分,$Sup(Rc)$支撑集表示Rc能感知的光谱。对于每个支撑集，<strong>假设反射率函数与波长无关</strong></p><p>我们对该公式用如下形式表示：$y_k=r_k\cdot l_k \quad k\in{R,G,B}$ </p><p>已知一个参数$y_k$,即<strong>我们已经知道的照片</strong>，求两个参数$r_k,l_k$,分别为<strong>物体对成像的影响</strong>和<strong>光照对成像的影响</strong>。</p><p>已知一个参数，求两个参数，约束过少。</p><p><strong>琅伯特模型经典假设：</strong></p><p>  1：<strong>固定相机拍摄的固定场景物体颜色的改变只能由改变光照实现</strong></p><p>  2：<strong>固有物体反射率图像可以通过过滤光照颜色来实现</strong></p><p>过滤光照颜色即除$l_k$,即$r_k=\frac{y_k}{l_k}$,只要获得$r_k$再乘以标准光照，就能获得白平衡图像。所以我们需要做的就是<strong>估计光照</strong>$l_k$</p><p>曾经困扰过我的是$l_k=E\cdot C_k$，<strong>可以变换乘法的位置吗？</strong>后来明白，<strong>我们实际进行颜色还原时，是对每一个像素点进行处理，那么每一个E和C都是一个标量！所以自然可以变换位置</strong></p><p>之前的<strong>回归方法</strong>，是利用网络直接学习$l_k$,这样提供一个点估计，但是<strong>颜色还原问题本身具有不适定性</strong>，可能有多个$l_k$符合条件,每个$l_k$的概率不同。</p><p>所以作者想的是对于图像数据集利用K_means对光源进行聚类，获得的聚类中心点就是候选的光源，也就是多个可能性，解决了上面回归方法的单个点估计的考量。具体做法见下图:</p><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029101605001.png" alt="网络架构"><br>$$<br>P(l|Y)=\frac{P(Y|l)P(l)}{P(Y)}<br>$$</p><p>$$<br>P(Y|l)=\int_rP(Y|l,R=r)P(R=r)dr=P(R=diag(l)^{-1}Y)<br>$$</p><p>第一步变换应用了<strong>全概率公式</strong></p><p><strong>第二步变换</strong>由于$y_k=r_k\cdot l_k\quad\quad k\in R,G,B$ 所以当且仅当$R=diag(l)^{-1}$时，才能生成Y,所以此时$P(Y|l,R=diag(l)^{-1})=1$,$P(Y|l,R=else)=0$,所以只剩下一项$P(R=diag(l)^{-1}Y)$</p><p>所以我们这个<strong>CNN网络</strong>为$f^W$,则$log(P(Y|l))=log(P(R=diag(l)^{-1}Y))=f^W(diag(l)^{-1}Y)$，即每个候选光源是场景光源的概率。</p><p>另外，在实际场景中，<strong>不同候选光源出现的概率</strong>是不同的，即$P(l)$不同,基于此我们添加了两个参数$G_l、B_l$，分别为增益系数和$log(P(l))$<br>$$<br>log(P(l|Y))=Gl\cdot log(P(Y|l))+B_l<br>$$<br>而引入这两个参数会带来问题！</p><p>本来我们的网络是<strong>摄像机无关</strong>的,因为没有$log(P(l))$ ,为什么说摄像机无关呢？</p><p>多个相机获得多个数据集，对每个数据集利用KMeans找出候选光源，然后都喂入网络，实现了多相机图片训练。假设我们现在引入了 一个新的摄像机，并获得一个该摄像机的数据集，我们要做的就是对该摄像机进行K-Means，然后测试时，对于一张图片 ，我们现在修正图片需要做的是之前的加上新的候选光源一起修正分别得到概率就行，不需要重新训练或微调。</p><p>而引入$log(P(l))$,之前我们的公式表明，<strong>全局光源由光源功率和接收函数决定</strong>，这个时候就必然引入了摄像机关联。</p><p>所以如果要多设备训练的话，就不引入这两个参数，这样虽<strong>然降低了灵活性，少了两个学习参数，但是现在可用的数据集变多了，大数据集弥补了</strong>。</p><p>我们现在获得了n个光源$l_0、l_1\cdots l_n$和n个概率$p_0、p_1\cdots p_n$,我们如何确定最优光源$l^*$?</p><p>首先<strong>简单的MAP是不行的</strong>，因为反向传播我们是需要求导的，而如果用极大后验估计求$l^*$，似然是用网络得到的，是没有办法求导的；</p><p>所以我<strong>们需要采取一个办法他不需要对网络那一块求导就能得到</strong>$l^*$，该论文就是利用简单的线性组合获得$l^*$，使MSE最小，当$l^*$是期望时MSE最小，如果你忘了为啥了可以列个二次函数求导！<br>$$<br>l^*=\sum_{i=1}^n l_i\cdot softmax(log(P(l_i|Y)))\<br>=\frac{1}{\sum e^{log(P(l_i|Y))}}\sum_{i=1}^nl_i\cdot e^{log(P(l_i|Y))}\<br>=\frac{1}{\sum P(l_i|Y)}\sum_{i=1}^nl_i\cdot P(l_i|Y)<br>$$<br>上式<strong>使用softmax是使概率归一。</strong></p><p><strong>CNN结构：</strong></p><p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029112002665.png" alt="Architecture"></p><h3 id="失败案例"><a href="#失败案例" class="headerlink" title="失败案例"></a>失败案例</h3><p><strong>1 GT在光源分布外</strong></p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029112126449.png" alt="failure-1" style="zoom:50%;"><p>因为是线性内插，分布外的点得不到</p><p><strong>2 打破了单一光源假设</strong></p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029113004477.png" alt="failure-2" style="zoom:67%;"><p><strong>3 问题的不适定本性</strong></p><img src="/yan-se-heng-chang-xing-zhi-mhcc/image-20201029113057076.png" alt="failure-3" style="zoom: 80%;"><p>学到了看起来非常可信的白平衡图片，认为石头是白的，其实石头是黄的</p>]]></content>
      
      
      <categories>
          
          <category> Color Constancy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CC/AWB </tag>
            
            <tag> Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo-git-github博客搭建</title>
      <link href="hexo-git-github-bo-ke-da-jian/"/>
      <url>hexo-git-github-bo-ke-da-jian/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考：洪卫的博客教程<a href="https://sunhwee.com/posts/6e8839eb.html">https://sunhwee.com/posts/6e8839eb.html</a></p><p>参考：hexo-theme-matery主题<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md#%E9%85%8D%E7%BD%AE%E5%9F%BA%E6%9C%AC%E8%8F%9C%E5%8D%95%E5%AF%BC%E8%88%AA%E7%9A%84%E5%90%8D%E7%A7%B0%E8%B7%AF%E5%BE%84url%E5%92%8C%E5%9B%BE%E6%A0%87icon">https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md#%E9%85%8D%E7%BD%AE%E5%9F%BA%E6%9C%AC%E8%8F%9C%E5%8D%95%E5%AF%BC%E8%88%AA%E7%9A%84%E5%90%8D%E7%A7%B0%E8%B7%AF%E5%BE%84url%E5%92%8C%E5%9B%BE%E6%A0%87icon</a></p><p>参考：过客～励む的博客<a href="https://yafine-blog.cn/posts/4ab2.html">https://yafine-blog.cn/posts/4ab2.html</a></p><p>参考:大佬的artitalk的教程<a href="https://zhangxiaocai.cn/posts/7404e01a.html">https://zhangxiaocai.cn/posts/7404e01a.html</a></p></blockquote><h2 id="按流程搭建遇到的问题："><a href="#按流程搭建遇到的问题：" class="headerlink" title="按流程搭建遇到的问题："></a>按流程搭建遇到的问题：</h2><h3 id="搭建hexo博客时，到了最后一步，hexo-s后只出现代码，而不是首页？"><a href="#搭建hexo博客时，到了最后一步，hexo-s后只出现代码，而不是首页？" class="headerlink" title="搭建hexo博客时，到了最后一步，hexo s后只出现代码，而不是首页？"></a>搭建hexo博客时，到了最后一步，hexo s后只出现代码，而不是首页？</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012224004113.png" alt="错误代码"></p><p><strong>在npm install安装依赖时出现了错误</strong></p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012224051241.png" alt="错误为第10行"></p><p>仔细查看错误信息，我们不难发现是ejs出现了问题。我们可以先执行以下代码后再继续后续操作。</p><pre><code>npm install ejs@2.7.4 --ignore-scripts</code></pre><p><strong><em>注意：之后所有的Bash命令都在最后一个MyBlog文件夹下操作，也就是你之前安装hexo那个文件夹！</em></strong></p><h3 id="什么是github-io？"><a href="#什么是github-io？" class="headerlink" title="什么是github.io？"></a>什么是github.io？</h3><blockquote><p>官网的一句话来形容 Websites for you and your projects</p></blockquote><h3 id="购买个人域名之后打开失败？"><a href="#购买个人域名之后打开失败？" class="headerlink" title="购买个人域名之后打开失败？"></a>购买个人域名之后打开失败？</h3><blockquote><p>极有可能是你未设置域名解析！</p></blockquote><h3 id="写文章发布文章不生成文件夹及图片无法显示？"><a href="#写文章发布文章不生成文件夹及图片无法显示？" class="headerlink" title="写文章发布文章不生成文件夹及图片无法显示？"></a>写文章发布文章不生成文件夹及图片无法显示？</h3><h4 id="不生成文件夹？"><a href="#不生成文件夹？" class="headerlink" title="不生成文件夹？"></a>不生成文件夹？</h4><p>首先，新建博客一定要用hexo new post命令，不然很多信息识别不出来</p><p>然后将_config.yml文件中的post asset folder设置为true，之后会出现文件夹</p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012224940500.png" alt="设置为true"></p><h4 id="图片不显示？"><a href="#图片不显示？" class="headerlink" title="图片不显示？"></a>图片不显示？</h4><p>首先下载依赖</p><pre><code>npm install hexo-asset-image --save</code></pre><p>然后对于typora编辑，偏好设置为：</p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012225346785.png" alt="image-20201012225346785" style="zoom: 80%;"><p>然后图片编写时，使用相对路径，例如：</p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201012225527582.png" alt="例子"></p><p><strong><em>另外注意：千万不要错误使用转义符’\‘!!!!</em></strong></p><h3 id="数学公式块无法正常显示？"><a href="#数学公式块无法正常显示？" class="headerlink" title="数学公式块无法正常显示？"></a>数学公式块无法正常显示？</h3><blockquote><p>后面配置了主题就可以了！</p><p>但是注意：<strong>数学公式中如果出现了连续两个{，中间一定要加空格！</strong></p></blockquote><h3 id="菜单导航配置在哪？"><a href="#菜单导航配置在哪？" class="headerlink" title="菜单导航配置在哪？"></a>菜单导航配置在哪？</h3><blockquote><p>菜单导航配置在themes/hexo-theme-matery/__config.yml</p></blockquote><h2 id="什么是TOC"><a href="#什么是TOC" class="headerlink" title="什么是TOC?"></a>什么是TOC?</h2><h3 id="什么是RSS订阅？"><a href="#什么是RSS订阅？" class="headerlink" title="什么是RSS订阅？"></a>什么是RSS订阅？</h3><blockquote><p>​        RSS也称为RSS订阅或RSS提要，博客和新闻网站的一个常见做法是联合其内容。Web联合是指来自网站的内容可供其他站点或远程应用程序使用。Web联合的最常用方法是使用称为<strong>ReallySimpleSyndication</strong>的协议。RSS是一种协议，允许网站将其内容或其部分内容提供给其他网站或应用程序。</p></blockquote><h3 id="DaoVoice"><a href="#DaoVoice" class="headerlink" title="DaoVoice?"></a>DaoVoice?</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013144940429.png" alt="设置"></p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013144951426.png" alt="设置"></p><h3 id="新建文章模板修改失败？"><a href="#新建文章模板修改失败？" class="headerlink" title="新建文章模板修改失败？"></a>新建文章模板修改失败？</h3><blockquote><p>莫名其妙post.md上下都变成了两个—，奇怪</p></blockquote><h3 id="修改页脚"><a href="#修改页脚" class="headerlink" title="修改页脚?"></a>修改页脚?</h3><h3 id="修改社交链接？"><a href="#修改社交链接？" class="headerlink" title="修改社交链接？"></a>修改社交链接？</h3><h3 id="不蒜子？不蒜子访问量和人数无法区分问题？"><a href="#不蒜子？不蒜子访问量和人数无法区分问题？" class="headerlink" title="不蒜子？不蒜子访问量和人数无法区分问题？"></a>不蒜子？不蒜子访问量和人数无法区分问题？</h3><blockquote><p>是一个极简网页计数器</p></blockquote><h3 id="添加动漫人物"><a href="#添加动漫人物" class="headerlink" title="添加动漫人物?"></a>添加动漫人物?</h3><blockquote><p>由于一直要安包，不敢继续弄了</p></blockquote><h3 id="gitalk-error-not-found？"><a href="#gitalk-error-not-found？" class="headerlink" title="gitalk error not found？"></a>gitalk error not found？</h3><blockquote><p><del>既有可能是yml的设置错误！</del></p><p><del><strong>owner和admin都填写github用户名</strong>，<strong>repo填我们的博客github仓库名</strong></del></p><p>问题终于解决！</p><img src="/hexo-git-github-bo-ke-da-jian/image-20201018215249728.png" alt="config" style="zoom:80%;"><img src="/hexo-git-github-bo-ke-da-jian/image-20201018215408171.png" alt="outh app" style="zoom: 50%;"></blockquote><h3 id="yml、yaml格式不正确？"><a href="#yml、yaml格式不正确？" class="headerlink" title="yml、yaml格式不正确？"></a>yml、yaml格式不正确？</h3><blockquote><p>使用这个在线校验器校验：<a href="http://www.bejson.com/validators/yaml_editor/">http://www.bejson.com/validators/yaml_editor/</a></p></blockquote><p>教程：<a href="https://www.runoob.com/w3cnote/yaml-intro.html">https://www.runoob.com/w3cnote/yaml-intro.html</a></p><p>缩进不允许使用tab，只允许空格</p><h3 id="gitalk未找到相关issues？"><a href="#gitalk未找到相关issues？" class="headerlink" title="gitalk未找到相关issues？"></a>gitalk未找到相关issues？</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013183732541.png" alt="URL"></p><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013183843079.png" alt="回调函数"></p><blockquote><p>一定要<strong>使用https而不是http</strong></p><p>还是不行？？？</p><p>失败！</p></blockquote><h3 id="网站根目录在哪里？"><a href="#网站根目录在哪里？" class="headerlink" title="网站根目录在哪里？"></a>网站根目录在哪里？</h3><p><img src="/hexo-git-github-bo-ke-da-jian/image-20201013200031772.png" alt="网站根目录"></p><h3 id="如何删除文章："><a href="#如何删除文章：" class="headerlink" title="如何删除文章："></a>如何删除文章：</h3><p>先hexo clean，然后在直接删除，如果不hexo clean的话，还是会再生成。</p><h2 id="git学习"><a href="#git学习" class="headerlink" title="git学习:"></a>git学习:</h2><blockquote><p>廖雪峰git教程 <a href="https://www.liaoxuefeng.com/wiki/896043488029600">https://www.liaoxuefeng.com/wiki/896043488029600</a></p></blockquote><p>git add先将文件放到车里，git commit把一车的东西运往其他城市。</p><p>Git的<code>commit id</code>不是1，2，3……递增的数字，而是一个SHA1计算出来的一个非常大的数字，用十六进制表示。为什么<code>commit id</code>需要用这么一大串数字表示呢？因为Git是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。</p><h2 id="js、ejs学习："><a href="#js、ejs学习：" class="headerlink" title="js、ejs学习："></a>js、ejs学习：</h2><blockquote><p>js:<a href="https://www.runoob.com/js/js-tutorial.html">https://www.runoob.com/js/js-tutorial.html</a></p></blockquote><p>JavaScript 是<strong>脚本语言，浏览器会在读取代码时，逐行地执行脚本代码</strong>。而对于传统编程来说，会在执行前对所有代码进行编译。</p><p>对象最好使用**.**来调用，防止方法调用失败！</p><h2 id="博客编写规范"><a href="#博客编写规范" class="headerlink" title="博客编写规范"></a>博客编写规范</h2><p><strong>不做内容的搬运工！</strong>而是</p><ol><li><strong>论文：记录遇到的问题，只针对问题进行解答</strong></li><li><strong>汇报：对于组会汇报内容进行详细编写</strong></li><li><strong>相应知识点：除了贴教程，最好加一两句自己的心得体会！</strong></li></ol><p>复制的话先清除样式，不然容易出问题</p><h2 id="图片loading不显示？"><a href="#图片loading不显示？" class="headerlink" title="图片loading不显示？"></a>图片loading不显示？</h2><p>gulp加速关掉图片压缩！！！！最好不使用gulp压缩，最好使用hexo-neat</p><h2 id="网页代码显示莫名其妙格式错误？"><a href="#网页代码显示莫名其妙格式错误？" class="headerlink" title="网页代码显示莫名其妙格式错误？"></a>网页代码显示莫名其妙格式错误？</h2><p>语言指定成c++就会出错，指定成c没事，所以还是指定成c，如果还是不行的话可以不指定语言，那样就没有高亮</p>]]></content>
      
      
      <categories>
          
          <category> 博客搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> git </tag>
            
            <tag> ejs </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
