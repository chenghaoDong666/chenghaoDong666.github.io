<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="pytorch学习, 大模型,llm,搜推算法,多模态,cv">
    <meta name="description" content="计算机视觉与图像处理，大模型算法，搜索算法，多模态算法以及其他相关的技术博客">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>pytorch学习 | 月源</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
	<link rel="stylesheet" type="text/css" href="/libs/artitalk/artitalk.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="月源" type="application/atom+xml">
</head>





    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: #FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: #49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: #2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>


<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
	
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
					<div>
						
						<img src="./medias/loading.gif" data-original="/medias/logo.png" class="logo-img" alt="LOGO">
						
						<span class="logo-span">月源</span>
					</div>
				</a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/artitalk" class="waves-effect waves-light">
      
      <i class="fas fa-heartbeat" style="zoom: 0.6;"></i>
      
      <span>说说</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="./medias/loading.gif" data-original="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">月源</div>
        <div class="logo-desc">
            
            计算机视觉与图像处理，大模型算法，搜索算法，多模态算法以及其他相关的技术博客
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/artitalk" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-heartbeat"></i>
			
			ArtiTalk
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/chenghaoDong666" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>See Me in github
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/chenghaoDong666" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="See Me in github" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

	
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('密码提示：帅不帅？')).toString(CryptoJS.enc.Hex)) {
                alert('你不够帅，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/28.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">pytorch学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/pytorch/">
                                <span class="chip bg-color">pytorch</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/python/" class="post-category">
                                python
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2020-11-02
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    30.3k
                </div>
                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><p>handbook：<a target="_blank" rel="noopener" href="https://github.com/zergtant/pytorch-handbook">https://github.com/zergtant/pytorch-handbook</a></p>
<p>pytorch API:<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>
<p>类似例程：<a target="_blank" rel="noopener" href="https://github.com/yunjey/pytorch-tutorial">https://github.com/yunjey/pytorch-tutorial</a></p>
<p>Docs：<a target="_blank" rel="noopener" href="https://github.com/fendouai/PyTorchDocs">https://github.com/fendouai/PyTorchDocs</a></p>
<h3 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h3><p>torch运用就和np一样</p>
<p>一个简单的网络最基本的步骤就是<strong>预处理，前向，损失，反向，更新</strong></p>
<h4 id="torch-tensor"><a href="#torch-tensor" class="headerlink" title="torch.tensor"></a>torch.tensor</h4><p>torch.tensor(3.14)这是标量   torch.tensor([3.14])这是向量，判断是几维张量主要是<strong>看有几个中括号</strong></p>
<p><img src="./medias/loading.gif" data-original="/pytorch-xue-xi/image-20201113101903090.png" alt="标量、向量和矩阵"></p>
<p>不是基本数据类型如int，float，string等，而是<strong>引用数据类型</strong></p>
<p>是在类中封装好的。所以肯定相应操作比如运算符等人家已经给你重载了，所以不用想的太多</p>
<p>两个tensor<strong>相加如果是同维度</strong>的话，就直接<strong>对应元素相加</strong></p>
<h4 id="pytorch通道顺序及索引"><a href="#pytorch通道顺序及索引" class="headerlink" title="pytorch通道顺序及索引"></a>pytorch通道顺序及索引</h4><p><strong>NCHW</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Jason66661010/p/13592020.html">很棒的索引教程</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#基本索引</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment">#torch.Size([3,28,28])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment">#torch.Size([28,28])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#tensor(0.8082)</span>
<span class="token comment">#连续选取</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment">#torch.Size([2,3,28,28])</span>
<span class="token comment">#由于是两张图片，所以第一维变为2</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment">#torch.Size(2,1,28,28)</span>
<span class="token comment"># ...作用</span>
<span class="token comment"># …代替了切片操作中前面所有的:， 即a[:, :, None] 和a[…, None]等价</span>
<span class="token comment"># None作用</span>
<span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span>–<span class="token operator">&gt;</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>–<span class="token operator">&gt;</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> b<span class="token punctuation">]</span> <span class="token comment"># None的作用就相当于在对应维度增加了一个维度</span>
<span class="token comment"># pos[..., (1, 0)] y, x -&gt; x, y</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="基本函数"><a href="#基本函数" class="headerlink" title="基本函数"></a>基本函数</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span>
<span class="token comment">#返回一个新的与原张量数据相同但形状不同的张量,-1是指从其他维度推断！</span>
y<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token comment">#"_"结尾的函数,会用结果替换原变量</span>
torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>   eg<span class="token punctuation">:</span>x<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#返回这个张量的值作为一个标准的Python数。这只适用于只有一个元素的张量。不可微操作</span>
torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span> eg<span class="token punctuation">:</span>a<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span> a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#返回张量作为一个(嵌套的)列表。对于标量，返回一个标准的Python数字，就像item()一样。如果需要，张量会首先自动移动到CPU。</span>
a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  a<span class="token punctuation">.</span>shape
<span class="token comment">#返回维度 eg:torch.Size([4, 4])</span>
numpy_a<span class="token operator">=</span>a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#tensor转numpy</span>
torch_a<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>numpy_a<span class="token punctuation">)</span><span class="token comment">#numpy转tensor</span>
<span class="token comment">#Tensor和numpy对象共享内存，转换很快，但这也意味着，如果其中一个变了，另一个也会变</span>
x<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span>
<span class="token comment">#如果没有提供dtype返回类型，否则将该对象强制转换为指定的类型,并返回该对象。</span>
torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token comment"># 返回自然对数的新张量</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token comment">#禁止梯度计算的上下文管理器，当您确定不会调用张量.backward()时，禁用梯度计算对于推断是很有用的。</span>
<span class="token comment">#它将减少计算的内存消耗，否则需要require_grad =True。</span>
torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#eg:输入shape为(A·1·B·1·C·1·D),输入张量的shape就是(A·B·C·D)</span>
<span class="token comment">#如果指定维度的话，那只对该维度去1。注意：返回的张量与输入张量共享存储空间，因此改变一个张量的内容将改变另一个张量的内容。</span>
<span class="token comment">#另外如果对批次batch为1也去掉的话，可能会引发错误。</span>
torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 增加一个1维度</span>
troch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#_, predicted = torch.max(outputs, 1)</span>
<span class="token comment">#outputs是数据Tensor，1表示求第一维度上的最大值</span>
<span class="token comment">#_是不要了  torch.max（）的返回值分两部分，分别是values和indices</span>
torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> dim<span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>Tensor<span class="token punctuation">,</span> LongTensor<span class="token punctuation">)</span>
<span class="token comment"># 返回一个命名元组(values, indices)，其中values是给定维度dim中输入张量的每一行的最大值。indices是找到的每个最大值(argmax)的索引位置。</span>
<span class="token comment"># 返回一个命名元组(values,indices)</span>
<span class="token comment"># 其中values是给定维度dim中输入张量的每一行的最大值</span>
<span class="token comment"># indices是找到的每个最大值(argmax)的索引位置。</span>
<span class="token comment"># 如果keepdim为True，则输出张量与输入张量的大小相同，除了dim维度的大小为1。</span>
<span class="token comment"># 否则，dim被压缩(参见torch.squeeze())，导致输出张量比输入少1维。</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">import</span> torch
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a<span class="token operator">=</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b<span class="token operator">=</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> indices<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b_max <span class="token operator">=</span> torch<span class="token punctuation">.</span>take_along_dim<span class="token punctuation">(</span>b<span class="token punctuation">,</span>indices<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a<span class="token operator">=</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> a
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0163</span><span class="token punctuation">,</span> <span class="token number">0.0711</span><span class="token punctuation">,</span> <span class="token number">0.5564</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.4507</span><span class="token punctuation">,</span> <span class="token number">0.8675</span><span class="token punctuation">,</span> <span class="token number">0.5974</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b<span class="token operator">=</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> b
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.7542</span><span class="token punctuation">,</span> <span class="token number">0.1793</span><span class="token punctuation">,</span> <span class="token number">0.5399</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.2292</span><span class="token punctuation">,</span> <span class="token number">0.5329</span><span class="token punctuation">,</span> <span class="token number">0.2084</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> indices<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>take_along_dim<span class="token punctuation">(</span>b<span class="token punctuation">,</span>indices<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.5399</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.5329</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#rand从(0,1)的均匀分布中随机抽样</span>
torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#randn从标准正态分布随机抽样</span>
<span class="token comment">#torch.normal(mean,std) 正态分布随机抽样</span>
<span class="token comment">#torch.linspace()线性间距向量  </span>
<span class="token comment">#torch.ones()初始化为1   torch.zeros()初始化为0  torch.eye()初始化为单位矩阵</span>
torch<span class="token punctuation">.</span><span class="token builtin">complex</span><span class="token punctuation">(</span>real<span class="token punctuation">,</span> imag<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span> → Tensor
<span class="token comment">#real为实部，imag为虚部，real和imag必须位数相同，如果real和imag同为float32那么生成的complex就为complex64。</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> real <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> imag <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> z <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">complex</span><span class="token punctuation">(</span>real<span class="token punctuation">,</span> imag<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> z
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token operator">+</span><span class="token number">3.j</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token operator">+</span><span class="token number">4.j</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> z<span class="token punctuation">.</span>dtype
torch<span class="token punctuation">.</span>complex64
torch<span class="token punctuation">.</span>__version__
<span class="token comment">#查看torch版本</span>
torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment">#最小</span>
              high<span class="token punctuation">,</span><span class="token comment">#最大 </span>
              size<span class="token punctuation">,</span><span class="token comment">#维度 </span>
              <span class="token operator">*</span><span class="token punctuation">,</span> generator<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> layout<span class="token operator">=</span>torch<span class="token punctuation">.</span>strided<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> → Tensor <span class="token comment">#均匀分布取样</span>
torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> mat2<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span> Tensor
<span class="token comment">#矩阵乘</span>
torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> mat2<span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span> → Tensor
<span class="token comment"># 对一个batch的矩阵进行矩阵乘积,(bxnxm)x(bxmxp)=(bxnxp)</span>
torch<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> dims<span class="token punctuation">)</span> → Tensor <span class="token comment"># 调整通道顺序</span>
torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>other<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>Tensor
<span class="token comment"># 和其它张量具有相同的维度。就从现有的值复制扩充。</span>
torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>tensors<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">*</span><span class="token punctuation">,</span>out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 将一个序列里的张量拼接在一起，按维数拼接</span>
torch<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span><span class="token operator">*</span><span class="token punctuation">,</span> memory_format<span class="token operator">=</span>torch<span class="token punctuation">.</span>preserve_format<span class="token punctuation">)</span> → Tensor
<span class="token comment"># return a copy of input</span>
torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> dim<span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span> → Tensor

loss<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment">#这是一个列表,和tensor不通用</span>
b <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment">#a维度是(4,4,4)  b维度是(4,)</span>
b <span class="token operator">=</span> <span class="token punctuation">[</span>a<span class="token punctuation">]</span> <span class="token comment">#a维度是(4,4,4)  b维度是(1 ,) #上面的会把batch信息保留</span>

torch<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span><span class="token builtin">int</span>
<span class="token comment"># 返回input tensor中元素的总数,eg:</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>numel<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token comment"># 120</span>
torhch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>nelement<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span><span class="token builtin">int</span>
<span class="token comment"># numel()的别名</span>
torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> <span class="token punctuation">[</span>split<span class="token punctuation">,</span> split<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token punctuation">)</span>
<span class="token comment">#</span>
torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> t <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 使变平</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>t<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>t<span class="token punctuation">,</span> start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Tensor<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> value<span class="token punctuation">)</span>
<span class="token comment"># 将tensor中和mask为1的位置相对应的替换为value</span>
mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>mask<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
a<span class="token punctuation">.</span>data<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>mask<span class="token punctuation">.</span>byte<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1053</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0352</span><span class="token punctuation">,</span>  <span class="token number">1.4759</span><span class="token punctuation">,</span>  <span class="token number">0.8849</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7233</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0529</span><span class="token punctuation">,</span>  <span class="token number">0.6663</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1082</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7243</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0364</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0657</span><span class="token punctuation">,</span>  <span class="token number">0.8359</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">1.4160</span><span class="token punctuation">,</span>  <span class="token number">1.1594</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.4163</span><span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">,</span>    <span class="token operator">-</span>inf<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> shifts<span class="token punctuation">,</span> dims<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span> → Tensor
<span class="token comment"># shifts:元素移位的维数,如果该参数是一个元组（例如shifts=(x,y)）</span>
<span class="token comment"># dims必须是一个相同大小的元组（例如dims=(a,b)），相当于在第a维度移x位，在b维度移y位</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>x<span class="token punctuation">,</span> shifts<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dims<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
           <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
           <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
           <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> steps<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>  <span class="token number">3.0000</span><span class="token punctuation">,</span>   <span class="token number">4.7500</span><span class="token punctuation">,</span>   <span class="token number">6.5000</span><span class="token punctuation">,</span>   <span class="token number">8.2500</span><span class="token punctuation">,</span>  <span class="token number">10.0000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># (end - start)/(steps - 1)</span>
<span class="token comment"># 设置梯度计算为开或关的上下文管理器,可以用作上下文管理器或一个函数,这个上下文管理器是线程本地的,他不会影响其他线程。</span>
torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span>mode<span class="token punctuation">)</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span>is_train<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>   y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> y<span class="token punctuation">.</span>requires_grad
<span class="token boolean">False</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>set_grad_enabled<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> y<span class="token punctuation">.</span>requires_grad
<span class="token boolean">True</span>
<span class="token comment"># 沿给定的dim计算张量输入中非零值的数量。 如果没有指定dim，则计算张量中的所有非零值。 </span>
torch<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span> → Tensor
<span class="token comment"># 将input中的所有元素限制在[min,max]范围内,操作定义如下</span>
<span class="token comment">#      | min, if x_i &lt; min</span>
<span class="token comment">#y_i = | x_i, if min &lt;= x_i &lt;= max</span>
<span class="token comment">#      | max, if x_i &gt; max</span>
<span class="token comment"># 和np.where用法相同</span>
torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>a<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">,</span>a<span class="token punctuation">,</span><span class="token number">5</span><span class="token operator">*</span>a<span class="token punctuation">)</span>
<span class="token comment"># https://zhuanlan.zhihu.com/p/352877584</span>
torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 返回排序后的值所对应原a的下标，即torch.sort()返回的indices</span>
torch<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#可以理解为填充或修改:https://blog.csdn.net/weixin_45547563/article/details/105311543 </span>
scatter_<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> dim<span class="token punctuation">,</span> index<span class="token punctuation">,</span> src<span class="token punctuation">)</span>
<span class="token comment"># 网格函数</span>
torch<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span><span class="token operator">*</span>tensors<span class="token punctuation">)</span>
<span class="token comment">#tensors: 两个一维向量，如果是0维，当作1维处理</span>
<span class="token comment"># 返回：两个矩阵</span>
<span class="token comment"># 第一个矩阵行相同，列是第一个向量的各个元素</span>
<span class="token comment"># 第二个矩阵列相同，行是第二个向量的各个元素</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
grid_x<span class="token punctuation">,</span> grid_y <span class="token operator">=</span> torch<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
grid_x:  
tensor([[1, 1, 1, 1],
        [2, 2, 2, 2],
        [3, 3, 3, 3]])
grid_y:  
tensor([[4, 5, 6, 7],
        [4, 5, 6, 7],
        [4, 5, 6, 7]])
'''</span>
<span class="token comment"># torch.stack 不会在现有的维度上加</span>
A <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        	  <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">33</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        	  <span class="token punctuation">[</span><span class="token number">44</span><span class="token punctuation">,</span> <span class="token number">55</span><span class="token punctuation">,</span> <span class="token number">66</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  <span class="token punctuation">[</span><span class="token number">77</span><span class="token punctuation">,</span> <span class="token number">88</span><span class="token punctuation">,</span><span class="token number">99</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
result1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">)</span><span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment"># torch.Size([2, 3, 3])</span>
<span class="token comment"># torch.chunk(tensor,chunk数,维度)</span>
a<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="求导和网络相关知识"><a href="#求导和网络相关知识" class="headerlink" title="求导和网络相关知识"></a>求导和网络相关知识</h4><p>grad属性保存梯度值，grad_fn保存梯度函数</p>
<p>nn.functional函数的特点是不具有可学习的参数，<code>net.parameters()​</code>返回网络可学习的参数</p>
<p>forward函数的输入和输出都是Tensor ,在反向传播前，先要将所有参数的梯度清零,如果不清0，计算得到的梯度值会进行累加</p>
<p><strong>torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。</strong></p>
<h4 id="经典报错"><a href="#经典报错" class="headerlink" title="经典报错"></a>经典报错</h4><p><strong>一:int和torch</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">count_parameters</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    total_params <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> m<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        total_params <span class="token operator">+=</span> torch<span class="token punctuation">.</span>DoubleTensor<span class="token punctuation">(</span><span class="token punctuation">[</span>p<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"m.total_params"</span><span class="token punctuation">,</span> m<span class="token punctuation">.</span>total_params<span class="token punctuation">,</span> <span class="token string">"total_params"</span><span class="token punctuation">,</span> total_params<span class="token punctuation">)</span>
    m<span class="token punctuation">.</span>total_params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> total_params<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>m.total_params[0] += total_params</code>,如果<code>m.parameters</code>为空,那么<code>total_params</code>就是0,类型为int,<code>m.total_params[0]</code>的类型为<code>torch.Size([])</code>,相当于一个空tensor,要是<code>m.total_params=total_params</code>,就相当于把int赋值给tensor,触发TypeError</p>
<p><strong>二：RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
w1<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
w2<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
 
d <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w1<span class="token punctuation">)</span>
f <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>d<span class="token punctuation">,</span> w2<span class="token punctuation">)</span>
d<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment"># 因为这句, 代码报错了 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</span>
 
f<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>因为f的求导需要用到d，但是这个时候d已经被改变了，所以报错</p>
<p>这种报错会显示在这一行<code>batch_loss.backward()</code>,多用<code>b=...a...</code>之类的形式，少用<code>a=...a...</code>之类的形式</p>
<p>似乎对于tensor变量的整体运算覆盖原变量，比如<code>x=2*x</code>不会导致以上问题；但是逐元素操作覆盖原位置元素就会引起这个问题，举例也是这个情况，在循环中，每次访问<code>feature[i,:,:]</code>，如果写成<code>feature[i,:,:]=...feature[i,:,:]...</code>，这设计的是python的变量赋值规则了</p>
<p><strong>三：view size is not compatible with input tensor‘s size and stride</strong></p>
<p>tensor不是contiguous连续引起的错误,查看<code>targets.is_contiguous()</code>为False</p>
<p>两种解决办法：1）按照提示使用reshape代替；2）将变量先转为contiguous ，再进行view:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">targets<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>targets<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token operator">*</span>targets<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1774900">如何判断张量是否连续?</a></p>
<p>nD 张量底层实现是使用一块连续内存的一维数组，由于 PyTorch 底层实现是 C 语言 (C/C++ 使用行优先的存储方式)，所以 PyTorch 中的 nD 张量也按照行优先的顺序进行存储的。</p>
<p>下图为一个形状为$(2×3)$的2D张量，为了方便将其命名为$A$。</p>
<p><img src="./medias/loading.gif" data-original="/pytorch-xue-xi/image-20210519165512549.png" alt="A"></p>
<p>张量 $A$ 在内存中实际以一维数组的形式进行存储，并且使用<strong>行优先的顺序进行存储</strong>,其中一维数组的形式存储比较好理解,而行优先指的就是存储顺序按照张量$A$的行依次存储。 张量$A$在内存中的实际存储形式如下所示。</p>
<p><img src="./medias/loading.gif" data-original="/pytorch-xue-xi/image-20210519165623798.png" alt="A"></p>
<p>张量$A$通常称为存储的逻辑结构，而实际存储的一维数组形式称为存储的物理结构。</p>
<ul>
<li>如果元素在存储的逻辑结构上相邻，在存储的物理结构中也相邻，则称为连续存储的张量；</li>
<li>如果元素在存储的逻辑结构上相邻，但是在存储的物理结构中不相邻，则称为不连续存储的张量；</li>
</ul>
<p><strong>交换维度的操作能够将连续存储的张量转变成不连续存储的张量。</strong></p>
<p>nD 张量，对于任意一个维度$i(i=0,…,n−1,i≠n−1)$都满足下面的等式则说明 nD 张量连续，不满足则说明 nD 张量不连续。<br>$$<br>stride[i]=stride[i+1]\times size[i+1]<br>$$<br>$stride[i]$表示逻辑结构中第 $i$个维度上相邻的元素在物理结构中间隔的元素个数.</p>
<p>$size[i]$表示逻辑结构中第$i$个维度的元素个数。</p>
<p>对于$A$,$stride[0]=3,stride[1]=1,size[1]=3$,所以是连续的</p>
<p>假设将$A$转置得到$A^T$,如下图：</p>
<img src="./medias/loading.gif" data-original="/pytorch-xue-xi/image-20210519171403494.png" alt="image-20210519171403494" style="zoom:67%;">

<p><strong>在 PyTorch 中交换维度的操作并没有改变其实际的存储，换句话说，交换维度后的张量与原始张量共享同一块内存</strong>，因此交换维度后的张量 AT 底层存储和原始张量 A 都是相同的一维数组。</p>
<p>对于$A^T$,$stride[0]=1,stride[1]=3,size[1]=2$,不连续</p>
<p><code>view</code>只能用于数据连续存储的张量，而<code>reshape</code>则不需要考虑张量中的数据是否连续存储</p>
<p>原始张量的视图简单来说就是和原始张量共享数据，因此如果改变使用 view 方法返回的新张量，原始张量也会发生相对应的改变。</p>
<p>reshape 方法可能返回的是原始张量的视图或者拷贝，当处理连续存储的张量 reshape 返回的是原始张量的视图，而当处理不连续存储的张量 reshape 返回的是原始张量的拷贝</p>
<p><strong>四:Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same</strong></p>
<p>整理数据集的时候,使用了np.array,然后默认保存成float64,但是pytorch中默认是float32。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 解决办法,dtype="float32"</span>
image_n <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>image_n<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">255.0</span>
image_d <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>image_d<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">255.0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p><strong>五:RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</strong></p>
<p>比如整体网络是G+S,有一个G_loss,有一个S_loss,必须<code>loss=G_loss+S_loss,loss.backward()</code>,</p>
<p>而不能<code>G_loss.backward(),S_loss.backward()</code>,因为为了节省空间,<code>G_loss.backward()</code>时已经把G的中间结果删除了</p>
<blockquote>
<p>To reduce memory usage, during the <code>.backward()</code> call, all the intermediary results are deleted when they are not needed anymore. Hence if you try to call <code>.backward()</code> again, the intermediary results don’t exist and the backward pass cannot be performed (and you get the error you see).You can call <code>.backward(retain_graph=True)</code> to make a backward pass that will not delete intermediary results, and so you will be able to call <code>.backward()</code> again. All but the last call to backward should have the <code>retain_graph=True</code> option.</p>
</blockquote>
<p><strong>六:TypeError: can’t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.</strong></p>
<p>所有在CPU上的<code>Tensor</code>（除了<code>CharTensor</code>）都支持与NumPy数组相互转换。</p>
<p>此外上面提到还有一个常用的方法就是直接用<code>torch.tensor()</code>将NumPy数组转换成<code>Tensor</code>，需要注意的是该方法总是会进行数据拷贝，返回的<code>Tensor</code>和原来的数据不再共享内存。</p>
<p><strong>RuntimeError: Can’t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.</strong></p>
<p>requires_grad = True的也不能转换成numpy</p>
<p><strong>七:错误归一化</strong></p>
<p>対生成的图片,错误的使用了归一化</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 比如输出图片,只要确保图片在0-1之间就好</span>
temp1<span class="token operator">=</span>np<span class="token punctuation">.</span>clip<span class="token punctuation">(</span>temp1<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
temp2<span class="token operator">=</span>np<span class="token punctuation">.</span>clip<span class="token punctuation">(</span>temp2<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># 如果进行归一化,如下图</span>
temp1<span class="token operator">=</span>temp1<span class="token operator">/</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>temp1<span class="token punctuation">)</span><span class="token operator">-</span>np<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>temp1<span class="token punctuation">)</span><span class="token punctuation">)</span>
temp2<span class="token operator">=</span>temp2<span class="token operator">/</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>temp2<span class="token punctuation">)</span><span class="token operator">-</span>np<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>temp2<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 相当于对图片进行了放缩</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>想象一下 本来图片的像素是0.5 1 放缩完之后就成了1 2了 怪不得那么亮~</p>
<h4 id="经典示例"><a href="#经典示例" class="headerlink" title="经典示例"></a>经典示例</h4><p><strong>1.resnet.conv1</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> layers <span class="token operator">==</span> <span class="token number">50</span><span class="token punctuation">:</span>
	resnet <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet50<span class="token punctuation">(</span>pretrained<span class="token operator">=</span>pretrained<span class="token punctuation">)</span>
<span class="token keyword">elif</span> layers <span class="token operator">==</span> <span class="token number">101</span><span class="token punctuation">:</span>
	resnet <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet101<span class="token punctuation">(</span>pretrained<span class="token operator">=</span>pretrained<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
	resnet <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet152<span class="token punctuation">(</span>pretrained<span class="token operator">=</span>pretrained<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>layer0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>resnet<span class="token punctuation">.</span>conv1<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>bn1<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>relu<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>conv2<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>bn2<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>relu<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>conv3<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>bn3<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>relu<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>maxpool<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>layer1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer4 <span class="token operator">=</span> resnet<span class="token punctuation">.</span>layer1<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>layer2<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>layer3<span class="token punctuation">,</span> resnet<span class="token punctuation">.</span>layer4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>resnet是你定义的一个类,那么类中的属性如<code>resnet.conv1</code>自然也可以调用啊。</p>
<h3 id="使用GPU"><a href="#使用GPU" class="headerlink" title="使用GPU"></a>使用GPU</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014380165/article/details/77340765">CUDA\cuDNN是什么</a></p>
<p>CUDA:NVIDIA推出的用于自家GPU的<strong>并行计算</strong>框架，也就是说CUDA只能在NVIDIA的GPU上运行，<strong>而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。</strong></p>
<p><strong>在 CUDA 的架构下，一个程序分为两个部份：host 端和 device 端。Host 端是指在 CPU 上执行的部份，而 device 端则是在显示芯片上执行的部份。Device 端的程序又称为 “kernel”。通常 host 端程序会将数据准备好后，复制到显卡的内存中，再由显示芯片执行 device 端程序，完成后再由 host 端程序将结果从显卡的内存中取回。</strong></p>
<p>cuDNN:是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span class="token comment">#这一步是设置我们使用的GPU</span>
<span class="token comment"># 确认我们的电脑支持CUDA，然后显示CUDA信息：</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token comment">#然后这些方法将递归遍历所有模块并将模块的参数和缓冲区 转换成CUDA张量：</span>
net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token comment">#记住：inputs, targets 和 images 也要转换。</span>
inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="多GPU"><a href="#多GPU" class="headerlink" title="多GPU"></a>多GPU</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86441879">教程</a></p>
<h3 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h3><p><strong>ResNet的调用</strong></p>
<p><img src="./medias/loading.gif" data-original="/pytorch-xue-xi/image-20210709100522321.png" alt="resnet预训练模型下载"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> pretrained<span class="token punctuation">:</span>
<span class="token comment"># model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))</span>
	model_path <span class="token operator">=</span> <span class="token string">'./initmodel/resnet152_v2.pth'</span>
	model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_path<span class="token punctuation">)</span><span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>torch.save</code></p>
<p>将对象从内存中保存到磁盘文件中。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>obj<span class="token punctuation">,</span><span class="token comment">#被保存的对象</span>
		   f<span class="token punctuation">:</span> Union<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>PathLike<span class="token punctuation">,</span> BinaryIO<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment">#</span>
           pickle_module<span class="token operator">=</span><span class="token operator">&lt;</span>module <span class="token string">'pickle'</span> <span class="token keyword">from</span> <span class="token string">'/opt/conda/lib/python3.6/pickle.py'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>
           pickle_protocol<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> _use_new_zipfile_serialization<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> → <span class="token boolean">None</span>
<span class="token comment">#常见的PyTorch约定是使用.pt文件扩展名保存张量。</span>
<span class="token comment"># Save to file</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>torch.load</code></p>
<p>加载用<code>torch.save()</code>保存的磁盘文件到内存中。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">,</span><span class="token comment">#文件路径</span>
		   map_location<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#加载到的位置</span>
		   pickle_module<span class="token operator">=</span><span class="token operator">&lt;</span>module <span class="token string">'pickle'</span> <span class="token keyword">from</span> <span class="token string">'/opt/conda/lib/python3.6/pickle.py'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span>
           <span class="token operator">**</span>pickle_load_args<span class="token punctuation">)</span>
<span class="token comment">#eg:torch.load(args.load, map_location=device)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>保存和加载,<code>torch.save()</code>将参数由内存或显存保存到硬盘,<code>torch.load()</code>再由硬盘加载到内存或显存,</p>
<p>然后再调用<code>model.load_state_dict()</code>将参数加载到模型中</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 只保存模型参数</span>
<span class="token comment"># 保存</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'\parameter.pkl'</span><span class="token punctuation">)</span>
<span class="token comment"># 加载</span>
model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'\parameter.pkl'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 保存完整模型</span>
<span class="token comment"># 保存</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'\model.pkl'</span><span class="token punctuation">)</span>
<span class="token comment"># 加载</span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'\model.pkl'</span><span class="token punctuation">)</span>
<span class="token comment"># 但是这样直接加载有时候会出问题，比如分割,本来的模型是2类输出,但是假设我们现在要新训练一个模型,同时利用之前模型的预训练信息，但是这个时候最后的那个prediction部分模型大小就不匹配了</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="./medias/loading.gif" data-original="/pytorch-xue-xi/image-20211009203504314.png" alt="image-20211009203504314" style="zoom:80%;">

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/133250753">Pytorch断点续接</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 模型参数的加载 优化器参数的加载 epoch的恢复</span>
checkpoint <span class="token operator">=</span> <span class="token punctuation">{</span>
        <span class="token string">"net"</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">'optimizer'</span><span class="token punctuation">:</span>optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"epoch"</span><span class="token punctuation">:</span> epoch
    <span class="token punctuation">}</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span><span class="token string">"./models/checkpoint"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span><span class="token string">"./models/checkpoint"</span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> <span class="token string">'./models/checkpoint/ckpt_best_%s.pth'</span> <span class="token operator">%</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    
<span class="token keyword">if</span> RESUME<span class="token punctuation">:</span>
    path_checkpoint <span class="token operator">=</span> <span class="token string">"./models/checkpoint/ckpt_best_1.pth"</span>  <span class="token comment"># 断点路径</span>
    checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>path_checkpoint<span class="token punctuation">)</span>  <span class="token comment"># 加载断点</span>

    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'net'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 加载模型可学习参数</span>

    optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 加载优化器参数</span>
    start_epoch <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span>  <span class="token comment"># 设置开始的epoch</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>学习率的调节会用到epoch</p>
<h3 id="torch-Tensor"><a href="#torch-Tensor" class="headerlink" title="torch.Tensor"></a>torch.Tensor</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># tensor类型转换 如long int 转换成torch.float 直接利用type函数</span>
mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>mask<span class="token operator">&lt;</span>threshold<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
<span class="token comment"># .cuda non_blocking经常与DataLoader的pin_memory搭配使用</span>
src_input <span class="token operator">=</span> src_input<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment"># 1. x = x.cuda(non_blocking=True)</span>
<span class="token comment"># 2. 进行一些和x无关的操作</span>
<span class="token comment"># 3. 执行和x有关的操作</span>
<span class="token comment"># 在non_blocking=true下，1不会阻塞2，1和2并行。这样将数据从CPU移动到GPU的时候，它是异步的。在它传输的时候，CPU还可以干其他的事情（不依赖于数据的事情）</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="Tensor-Attributes"><a href="#Tensor-Attributes" class="headerlink" title="Tensor Attributes"></a>Tensor Attributes</h3><p>Each <code>torch.Tensor</code> has a <code>torch.dtype</code> <code>torch.device</code>, and <code>torch.layout</code>.</p>
<p><strong><code>torch.dtype</code></strong></p>
<p><code>os.environ['CUDA_VISIBLE_DEVICES'] = '0'</code></p>
<p><code>CUDA_VISIBLE_DEVICES</code> is the mask used by CUDA to determine what devices it exposes to the user program, which is pytorch in this case. There is no way pytorch can know about that reliably.</p>
<p>Within you application, gpu numbers will always start at 0 and grow up from there.<br>When you use <code>CUDA_VISIBLE_DEVICES</code>, you hide some devices so they won’t be numbered.<br>If you have 4 gpus: 0, 1, 2, 3.<br>And run CUDA_VISIBLE_DEVICES=1,2 python some_code.py. Then the device that you will see within python are device 0, 1. Using device 0 in your code will use device 1 from global numering. Using device 1 in your code will use 2 outside.<br>So in your case if you always set <code>CUDA_VISIBLE_DEVICES</code> to a single device, in your code, the device id will always be 0, that is expected. Unfortunately, there is no way to know what is the global numbering.</p>
<p>A <code>torch.dtype</code>is an object that represents the data type of a <code>torch.Tensor</code>.Pytorch has different 12 types.</p>
<p><strong><code>torch.device</code></strong></p>
<p>A <code>torch.device</code> is an object representing the device on which a <code>torch.Tensor</code> is or will be allocated.</p>
<p>The <code>torch.device</code> contains a device type (<code>'cpu'</code> or <code>'cuda'</code>) and optional device ordinal for the device type. If the device ordinal is not present, this object will always represent the current device for the device type, even after <code>torch.cuda.set_device()</code> is called; e.g., a <code>torch.Tensor</code> constructed with device <code>'cuda'</code> is equivalent to <code>'cuda:X'</code> where X is the result of <code>torch.cuda.current_device()</code>.</p>
<p>A <code>torch.Tensor</code>’s device can be accessed via the <code>Tensor.device</code> property.</p>
<p>A <code>torch.device</code> can be constructed via a string or via a string and device ordinal</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># via a string </span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>
device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>  <span class="token comment"># current cuda device</span>
device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>
<span class="token comment"># via a string device ordinal</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cpu'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment"># torch.cuda.set_device(device)</span>
<span class="token comment"># Sets the current device. 不鼓励使用此函数。在大多数情况下，最好使用 CUDA_VISIBLE_DEVICES 环境变量。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>The <code>torch.device</code> argument in functions can generally be substituted with a string. This allows for fast prototyping of code.</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># Example of a function that takes in a torch.device</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> cuda1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>cuda1<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># You can substitute the torch.device with a string</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches <code>Tensor.get_device()</code>, which returns an ordinal for cuda tensors and is not supported for cpu tensors.</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># legacy</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p><strong><code>torch.layout</code></strong></p>
<p>in beta</p>
<h3 id="torch-autograd"><a href="#torch-autograd" class="headerlink" title="torch.autograd"></a>torch.autograd</h3><p>torch.autograd提供了实现任意标量值函数的自动微分的类和函数。它只需要对现有代码进行最小的更改——只需要声明张量s，对于这些张量，计算梯度时应带有requires_grad=True关键字。到目前为止，我们只支持浮点张量类型(half、float、double和bfloat16)和复数张量类型(cfloat、cdouble)的自动求导。</p>
<h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p>Variable API已被弃用:在使用autograd时，不再需要Variable。Autograd自动支持将requires_grad设置为True的张量。下面是一些变化的快速指南:</p>
<p><code>Variable(tensor)</code>和<code>Variable(tensor, requires_grad)</code>仍然按预期工作，但它们返回的是张量而不是变量。</p>
<p><code>var.data</code>和<code>tensor.data</code>是一样的。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>
<span class="token comment">#tensor([[-0.4404]], requires_grad=True)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
<span class="token comment">#tensor([[-0.4404]])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>像<code>var.backward()</code>、<code>var.detach()</code>、<code>var.register_hook()</code>这样的方法现在可以在具有相同方法名的张量上工作。</p>
<p>此外,现在可以使用工厂方法创建requires_grad=True的张量,如<code>torch.randn()</code>、<code>torch.zeros()</code>、<code>torch.ones()</code>和其他类似如下的方法:</p>
<p><code>autograd_tensor =torch.randn((2, 3, 4),requires_grad=True)</code></p>
<p>具体来说，在pytorch中的Variable就是一个存放会变化值的地理位置，里面的值会不停发生变化，就像一个装鸡蛋的篮子，鸡蛋数会不断发生变化。那谁是里面的鸡蛋呢，自然就是pytorch中的tensor了。（也就是说，<strong>pytorch都是有tensor计算的，而tensor里面的参数都是Variable的形式</strong>）。如果用Variable计算的话，那返回的也是一个同类型的Variable。</p>
<p>也就是说现在requires_grad=True的tensor就相当于以前的Variable,也就是进行反向传播的变量。</p>
<p><code>detach()  data()  detach_()</code></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_33913332/article/details/93300411">教程</a></p>
<p>返回一个新的<code>Variable</code>，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个<code>Variable</code>永远不需要计算其梯度，不具有grad。**即使之后重新将它的requires_grad置为true,它也不会具有梯度grad.**这样我们就会继续使用这个新的<code>Variable</code>进行计算，后面当我们进行反向传播时，到该调用detach()的<code>Variable</code>就会停止，不能再继续向前进行传播</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># detach与data的区别</span>
<span class="token comment"># 相同 1.都和x共享同一块数据 2.都和x的计算历史无关 3.requires_grad = False</span>
<span class="token comment"># 不同</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
out <span class="token operator">=</span> a<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> out<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> out<span class="token punctuation">.</span>data<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 修改b、c均会同时改变a</span>
<span class="token comment"># 不修改b,out可以反向传播；修改b,out反向传播会报错</span>
<span class="token comment"># 修改c,out可以反向传播,但会生成错误的梯度值。</span>
<span class="token comment"># 所以out.data()在某些情况下不安全</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>上面内容实现的原理是:In-place 正确性检查(In-place操作是指函数最后带”_”的操作)</p>
<p>所有的<code>Variable</code>都会记录用在他们身上的 <code>in-place operations</code>。如果<code>pytorch</code>检测到<code>variable</code>在一个<code>Function</code>中已经被保存用来<code>backward</code>，但是之后它又被<code>in-place operations</code>修改。当这种情况发生时，在<code>backward</code>的时候，<code>pytorch</code>就会报错。这种机制保证了，如果你用了<code>in-place operations</code>，但是在<code>backward</code>过程中没有报错，那么梯度的计算就是正确的。</p>
<p><code>pred_fake = self.netD(fake_AB.detach()) </code># 固定了G,detach隔断了返现传播流</p>
<p><code>detach()</code>会截断反向传播</p>
<h4 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27783097">知乎教程</a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/tsq292978891/article/details/79364140">CSDN教程</a></p>
<p><strong>对Function的直观理解</strong></p>
<ul>
<li>的</li>
</ul>
<p>虽然pytorch可以自动求导，但是有时候一些操作是不可导的，这时候你需要自定义求导方式。也就是所谓的 <code>Extending torch.autograd</code></p>
<p><strong>Function与Module的差异与应用场景</strong></p>
<ul>
<li><strong>Function一般只定义一个操作，因为其无法保存参数，因此适用于激活函数、pooling</strong>等操作;<strong>Module是保存了参数，因此适合于定义一层，如线性层，卷积层，也适用于定义一个网络</strong></li>
<li>Function需要定义三个方法：<code>__init__</code>, forward, backward(需要自己写求导公式);Module：只需定义<code>__init__</code>和forward，而backward的计算由自动求导机制构成</li>
<li>可以不严谨的认为，Module是由一系列Function组成，因此其在forward的过程中，Function和Variable组成了计算图，在backward时，只需调用Function的backward就得到结果，因此Module不需要再定义backward。</li>
<li>Module不仅包括了Function，<strong>还包括了对应的参数，以及其他函数与变量，这是Function所不具备的</strong></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#属性（成员变量）</span>
<span class="token comment">#saved_tensors: 传给forward()的参数，在backward()中会用到。</span>
<span class="token comment">#needs_input_grad:长度为 :attr:num_inputs的bool元组，表示输出是否需要梯度。可以用于优化反向过程的缓存。</span>
<span class="token comment">#num_inputs: 传给函数 :func:forward的参数的数量。</span>
<span class="token comment">#num_outputs: 函数 :func:forward返回的值的数目。</span>
<span class="token comment">#requires_grad: 布尔值，表示函数 :func:backward 是否永远不会被调用。</span>

<span class="token comment">#成员函数</span>
<span class="token comment">#forward()</span>
<span class="token comment">#forward()可以有任意多个输入、任意多个输出，但是输入和输出必须是Variable。(官方给的例子中有只传入tensor作为参数的例子)</span>
<span class="token comment">#backward()</span>
<span class="token comment">#backward()的输入和输出的个数就是forward()函数的输出和输入的个数。其中，backward()输入表示关于forward()输出的梯度(计算图中上一节点的梯度)，#backward()的输出表示关于forward()的输入的梯度。在输入不需要梯度时（通过查看needs_input_grad参数）或者不可导时，可以返回None。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h4><p>上下文管理器，为autograd引擎启用异常检测。做了两件事:</p>
<p>Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.。任何生成” nan “值的向后计算都将引发错误。</p>
<p>此模式应仅用于调试，因为不同的测试将降低程序执行速度。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>detect_anomaly
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">with</span> autograd<span class="token punctuation">.</span>detect_anomaly<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     inp <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     out <span class="token operator">=</span> run_fn<span class="token punctuation">(</span>inp<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>set_detect_anomaly<span class="token punctuation">(</span>mode<span class="token punctuation">:</span><span class="token boolean">True</span> <span class="token keyword">or</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token comment"># 上下文管理器，设置自动grad引擎的异常检测开关。</span>
<span class="token comment"># Set_detect_anomaly将根据参数模式启用或禁用自grad异常检测。它可以用作上下文管理器或函数。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>在模型正常训练阶段不建议打开**<code>autograd.detect_anomaly，</code>**会使训练速度大大减慢，以笔者 这里的测试，打开后，原本4个小时的训练被减慢至7.5个小时；打开后可以辅助找到出现Nan值的位置</p>
<p><code>assert torch.isnan(src_n_feat).int().sum() ==0</code> 判断张量中是否有值为nan</p>
<h3 id="torch-cuda"><a href="#torch-cuda" class="headerlink" title="torch.cuda"></a>torch.cuda</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> → <span class="token builtin">int</span> <span class="token comment"># 返回可用的GPU数量</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_device_name<span class="token punctuation">(</span><span class="token punctuation">)</span> → <span class="token builtin">str</span> <span class="token comment"># 返回GPU的名字,如NVIDIA GeForce RTX 3080 Ti </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>



<h4 id="memory-managetment"><a href="#memory-managetment" class="headerlink" title="memory managetment"></a>memory managetment</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/424512257">pytorch显存机制分析</a></p>
<p><code>torch.cuda.memory_reserved(device=None)</code></p>
<h3 id="torch-cuda-amp"><a href="#torch-cuda-amp" class="headerlink" title="torch.cuda.amp"></a>torch.cuda.amp</h3><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/l7H9JA4/article/details/114324414">Pytorch自动混合精度教程</a></p>
<p>Automatic mixed precision package自动混合精度包</p>
<p>torch.cuda.amp提供了方便的混合精度方法，在某些操作中需要使用torch.float32 (float)数据类型而有些操作使用torch.float16(half)。<br>有些操作，比如线性层和卷积，在float16中要快得多。<br>其他操作，比如减少操作，通常需要float32的动态范围。<br>混合精度尝试将每个op匹配到其适当的数据类型。</p>
<p>一般来说，自动混合精度训练同时使用<code>torch.cuda.amp.autocast</code>和<code>torch.cuda.amp.GradScaler</code>,当然如果需要也可以单独使用。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>amp <span class="token keyword">import</span> autocast<span class="token punctuation">,</span> GradScaler
<span class="token comment"># 用户使用混合精度训练基本操作：</span>
<span class="token comment"># amp依赖Tensor core架构，所以model参数必须是cuda tensor类型</span>
model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span class="token comment"># GradScaler对象用来自动做梯度缩放</span>
scaler <span class="token operator">=</span> GradScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> epochs<span class="token punctuation">:</span>
    <span class="token keyword">for</span> <span class="token builtin">input</span><span class="token punctuation">,</span> target <span class="token keyword">in</span> data<span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 在autocast enable 区域运行forward</span>
        <span class="token keyword">with</span> autocast<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># model做一个FP16的副本，forward</span>
            output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
            loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
        <span class="token comment"># 用scaler，scale loss(FP16)，backward得到scaled的梯度(FP16)</span>
        scaler<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># scaler 更新参数，会先自动unscale梯度</span>
        <span class="token comment"># 如果有nan或inf，自动跳过</span>
        scaler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>
        <span class="token comment"># scaler factor更新</span>
        scaler<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="autocast自定义函数"><a href="#autocast自定义函数" class="headerlink" title="autocast自定义函数"></a>autocast自定义函数</h4><p>对于用户自定义的autograd函数，需要用<code>@torch.cuda.amp.custom_fwd</code>装饰forward函数,<code>@torch.cuda.amp.custom_bwd</code>装饰backward函数:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyMM</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token decorator annotation punctuation">@custom_fwd</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
        ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
        <span class="token keyword">return</span> a<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>b<span class="token punctuation">)</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token decorator annotation punctuation">@custom_bwd</span>
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
        a<span class="token punctuation">,</span> b <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors
        <span class="token keyword">return</span> grad<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>b<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>grad<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>调用时再autocat</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">mymm <span class="token operator">=</span> MyMM<span class="token punctuation">.</span><span class="token builtin">apply</span>
 
<span class="token keyword">with</span> autocast<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    output <span class="token operator">=</span> mymm<span class="token punctuation">(</span>input1<span class="token punctuation">,</span> input2<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="torch-backends"><a href="#torch-backends" class="headerlink" title="torch.backends"></a>torch.backends</h3><p>torch.backends控制PyTorch支持的各种后端的行为。这些后端包括:</p>
<ul>
<li><code>torch.backends.cuda</code></li>
<li><code>torch.backends.cudnn</code></li>
<li><code>torch.backends.mkl</code></li>
<li><code>torch.backends.mkldnn</code></li>
<li><code>torch.backends.openmp</code></li>
</ul>
<h4 id="torch-cudnn"><a href="#torch-cudnn" class="headerlink" title="torch.cudnn"></a>torch.cudnn</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/byron123456sfsfsfa/article/details/96003317">torch.backends.cudnn.benchmark详解</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>benchmark
<span class="token comment">#一个bool值，如果为真，将导致cuDNN对多个卷积算法进行基准测试并选择最快的。</span>
<span class="token comment">#耗费一些预处理时间，选择最好的卷积算法，大大减少之后的训练时间，网络结构不能变，输入输出不能变等</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic
<span class="token comment">#如果该bool为真，则导致cuDNN只使用确定性卷积算法。参见torch.is_deterministic()和torch.set_deterministic()。</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>enabled
<span class="token comment">#一个控制是否启用cuDNN的bool值,默认为True，启用cudnn</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="torch-distributed"><a href="#torch-distributed" class="headerlink" title="torch.distributed"></a>torch.distributed</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/178402798">DDP系列第一篇：入门教程</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>



<h4 id="DDP与DP模式的不同"><a href="#DDP与DP模式的不同" class="headerlink" title="DDP与DP模式的不同"></a>DDP与DP模式的不同</h4><p>DP模式是很早就出现的、单机多卡的、参数服务器架构的多卡训练模式，在PyTorch，即是:</p>
<p><code>model=torch.nn.DataParaller(model)</code></p>
<p>在DP模式中，<strong>总共只有一个进程</strong>(受到GIL很强限制)。<strong>master节点相当于参数服务器</strong>，<strong>其会向其他卡广播其参数</strong>；在<strong>梯度反向传播</strong>后，<strong>各卡将梯度集中到master节点，master节点对搜集来的参数进行平均后更新参数</strong>，<strong>再将参数统一发送到其他卡上</strong>。这种参数更新方式，会导致<strong>master节点的计算任务、通讯量很重</strong>，从而导致网络阻塞，<strong>降低训练速度</strong>。</p>
<p>但是<strong>DP也有优点</strong>,<strong>优点就是代码实现简单。要速度还是要方便，看官可以自行选用哟</strong>。</p>
<h4 id="DDP"><a href="#DDP" class="headerlink" title="DDP"></a>DDP</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/450912044">教程1</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012605037/article/details/115294898?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-115294898-blog-119606518.pc_relevant_multi_platform_whitelistv2_exp3w&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-115294898-blog-119606518.pc_relevant_multi_platform_whitelistv2_exp3w&amp;utm_relevant_index=1">基本概念</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/178402798">教程2</a></p>
<p>ddp运行代码:</p>
<p><code>python3 -m torch.distributed.launch --nproc_per_node=8 DDP.py</code></p>
<p>其中<code>python3 -m</code>的意思是指run library module as a script（将模块当作脚本运行）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 当文件作为脚本直接运行时，这段代码会产生副作用，输出字符串“模块直接运行”；</span>
<span class="token comment"># 当文件作为模块被导入时，不会产生副作用，不输出字符串“模块直接运行”；</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'模块直接运行'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment"># 当我们知道一个模块的名字，但不知道它的路径时，我们可以通过 -m 参数，在 shell 中将该模块当作脚本运行，例如：</span>
python <span class="token operator">-</span>m module_name
<span class="token comment"># 如果我们知道模块的完整路径（此处假设为"/path/to/module.py"），上述命令的效果，以下面的命令等同</span>
python <span class="token operator">/</span>path<span class="token operator">/</span>to<span class="token operator">/</span>module<span class="token punctuation">.</span>py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>rank：用于表示进程的编号/序号（在一些结构图中rank指的是软节点，rank可以看成一个计算单位），每一个进程对应了一个rank的进程，整个分布式由许多rank完成</p>
<p>node：物理节点，可以是一台机器也可以是一个容器，节点内部可以有多个GPU。</p>
<p>rank与local_rank： rank是指在整个分布式任务中进程的序号；local_rank是指在一个node上进程的相对序号，local_rank在node之间相互独立。</p>
<p>nnodes、node_rank与nproc_per_node： nnodes是指物理节点数量，node_rank是物理节点的序号；nproc_per_node是指每个物理节点上面进程的数量。word size ： 全局（一个分布式任务）中，rank的数量。</p>
<blockquote>
<p>上一个运算题： 每个node包含16个GPU，且nproc_per_node=8，nnodes=3，机器的node_rank=5，请问word_size是多少？<br>答案：word_size = 3*8 = 24 </p>
</blockquote>
<p>比如分布式中有三台机器，每台机器起4个进程，每个进程占用1个GPU，如下图所示：</p>
<img src="./medias/loading.gif" data-original="/pytorch-xue-xi/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20220725162527263.png" alt="image-20220725162527263" style="zoom: 80%;">

<p>Group：进程组，一个分布式任务对应了一个进程组。只有用户需要创立多个进程组时才会用到group来管理，默认情况下只有一个group。</p>
<h5 id="Groups"><a href="#Groups" class="headerlink" title="Groups"></a>Groups</h5><p>By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. new_group() function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a group argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pg1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>new_group<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
batch_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>BATCH_SIZE <span class="token operator">/</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
feature_extractor <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>
    feature_extractor<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>local_rank<span class="token punctuation">,</span>
    find_unused_parameters<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> process_group<span class="token operator">=</span>pg1
<span class="token punctuation">)</span>
pg2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>new_group<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
classifier <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>
    classifier<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>local_rank<span class="token punctuation">,</span>
    find_unused_parameters<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> process_group<span class="token operator">=</span>pg2
<span class="token punctuation">)</span>
<span class="token comment"># 开启求导的异常侦测</span>
torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>set_detect_anomaly<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment"># 保持两个进程同步</span>
torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>barrier<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h5 id="Collective-functions"><a href="#Collective-functions" class="headerlink" title="Collective functions"></a>Collective functions</h5><p>``torch.distributed.barrier(group=None, async_op=False, device_ids=None)`</p>
<p>同步所有进程。如果async_op为False，或者在wait()上调用async工作句柄，则该集合将阻塞进程，直到整个组进入此函数。</p>
<h5 id="Launch-utility"><a href="#Launch-utility" class="headerlink" title="Launch utility"></a>Launch utility</h5><p><code>torch.distributed.launch</code>是一个在每个训练节点上生成多个分布式训练过程的模块。</p>
<p>该实用程序可用于单节点分布式训练，其中每个节点将生成一个或多个进程。该实用程序可以用于CPU训练或GPU训练。如果该实用程序用于GPU培训，则每个分布式进程将在单个GPU上运行。这可以实现明显提升单节点训练性能。它还可以用于多节点分布式训练，通过在每个节点上生成多个进程，也可以很好地提高多节点分布式训练的性能。这对于具有多个直接gpu支持的Infiniband接口的系统尤其有利，因为所有这些接口都可以用于聚合的通信带宽。</p>
<p>在单节点分布式训练或多节点分布式训练的两种情况下，该实用程序将启动每个节点给定数量的进程(<code>--nproc_per_node</code>)。如果用于GPU培训，这个数字需要小于或等于当前系统上的GPU数量(<code>nproc_per_node</code>)，并且每个进程将运行在从GPU 0到GPU (<code>nproc_per_node - 1</code>)的单个GPU上。</p>
<p><strong>How to use this module:</strong></p>
<p>1.Single-Node multi-process distributed training</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span>NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT<span class="token punctuation">.</span>py <span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span>arg1 <span class="token operator">-</span><span class="token operator">-</span>arg2 <span class="token operator">-</span><span class="token operator">-</span>arg3 <span class="token keyword">and</span> <span class="token builtin">all</span> other arguments of your training script<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>2.Multi-Node multi-process distributed training: (e.g. two nodes)</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">Node <span class="token number">1</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>IP<span class="token punctuation">:</span> <span class="token number">192.168</span><span class="token number">.1</span><span class="token number">.1</span><span class="token punctuation">,</span> <span class="token keyword">and</span> has a free port<span class="token punctuation">:</span> <span class="token number">1234</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span>NUM_GPUS_YOU_HAVE
           <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">"192.168.1.1"</span>
           <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token number">1234</span> YOUR_TRAINING_SCRIPT<span class="token punctuation">.</span>py <span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span>arg1 <span class="token operator">-</span><span class="token operator">-</span>arg2 <span class="token operator">-</span><span class="token operator">-</span>arg3 <span class="token keyword">and</span> <span class="token builtin">all</span> other arguments of your training script<span class="token punctuation">)</span>
Node <span class="token number">2</span><span class="token punctuation">:</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span>NUM_GPUS_YOU_HAVE
           <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">"192.168.1.1"</span>
           <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token number">1234</span> YOUR_TRAINING_SCRIPT<span class="token punctuation">.</span>py <span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span>arg1 <span class="token operator">-</span><span class="token operator">-</span>arg2 <span class="token operator">-</span><span class="token operator">-</span>arg3 <span class="token keyword">and</span> <span class="token builtin">all</span> other arguments of your training script<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>3.To look up what optional arguments this module offers:</p>
<p><code>python -m torch.distributed.launch --help</code></p>
<p><strong>Important Notices:</strong></p>
<p>1.GPU训练目前使用$NCLL$后端达到最佳性能</p>
<p>2.在你的训练程序中，你必须解析命令行参数:–local_rank=LOCAL_PROCESS_RANK，<strong>它将由这个模块提供</strong>。如果你的training programs使用GPU，你应该确保你的代码只运行在LOCAL_PROCESS_RANK的GPU设备上。这可以通过以下方式实现:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> argparse
parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--local_rank"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">)</span>
args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># Set your device to local rank using either</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span>  <span class="token comment"># before your code runs</span>
<span class="token keyword">or</span><span class="token punctuation">:</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device<span class="token punctuation">(</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token comment"># your code to run</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>3.In your training program, you are supposed to call the following function at the beginning to start the distributed backend. It is strongly recommended that <code>init_method=env://</code>. Other init methods (e.g. <code>tcp://</code>) may work, but <code>env://</code> is the one that is officially supported by this module.</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'YOUR BACKEND'</span><span class="token punctuation">,</span>init_method<span class="token operator">=</span><span class="token string">'env://'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>4.加载数据集</p>
<p>多卡训练加载数据:</p>
<p>Dataset的设计上与单gpu一致，但是DataLoader上不一样。</p>
<p>首先解释下原因：多gpu训练是，我们希望同一时刻在每个gpu上的数据是不一样的，这样相当于batch size扩大了N倍，因此起到了加速训练的作用。在DataLoader时，如何做到每个gpu上的数据是不一样的，且gpu1上训练过的数据如何确保接下来不被别的gpu再次训练。这时候就得需要DistributedSampler。</p>
<p>Dataloader设置方式如下，注意shuffle与sampler是冲突的，并行训练需要设置sampler，此时务必要把shuffle设为False。但是这里shuffle=False并不意味着数据就不会乱序了，而是乱序的方式交给sampler来控制，实质上数据仍是乱序的。</p>
<pre class="line-numbers language-python3" data-language="python3"><code class="language-python3">train_sampler = torch.utils.data.distributed.DistributedSampler(My_Dataset)
dataloader = torch.utils.data.DataLoader(ds,
                                         batch_size=batch_size,
                                         shuffle=False,
                                         num_workers=16,
                                         pin_memory=True,
                                         drop_last=True,
                                         sampler=train_sampler)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>5.加载模型</p>
<p>多卡训练的模型设置：</p>
<p>最主要的是<code>find_unused_parameters</code>和<code>broadcast_buffers</code>参数；</p>
<p><code>find_unused_parameters</code>：如果模型的输出有不需要进行反传的(比如部分参数被冻结/或者网络前传是动态的)，设置此参数为True;如果你的代码运行</p>
<p>后卡住某个地方不动，基本上就是该参数的问题。</p>
<p><code>broadcast_buffers</code>：设置为True时，在模型执行forward之前，gpu0会把buffer中的参数值全部覆盖到别的gpu上。注意这和同步BN并不一样，同步BN应该使用SyncBatchNorm。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">My_model <span class="token operator">=</span> My_model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span>  <span class="token comment"># 将模型拷贝到每个gpu上.直接.cuda()也行，因为多进程时每个进程的device号是不一样的</span>
My_model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">.</span>convert_sync_batchnorm<span class="token punctuation">(</span>My_model<span class="token punctuation">)</span> <span class="token comment"># 设置多个gpu的BN同步</span>
My_model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>My_model<span class="token punctuation">,</span> 
                                                     device_ids<span class="token operator">=</span><span class="token punctuation">[</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> 
                                                     output_device<span class="token operator">=</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">,</span> 
                                                     find_unused_parameters<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
                                                     broadcast_buffers<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h3><p>网络结构图的基本构建模块<code>import torch.nn as nn</code></p>
<p>卷积层和线性层在<code>__init__</code>里面，而激活和池化在<code>forward</code>函数里面。</p>
<p><code>torch.nn</code>只支持小批量输入。整个<code>torch.nn</code>包都只支持小批量样本,而不支持单个样本。 例如,<code>nn.Conv2d</code>接受一个4维的张量,每一维分别是$Samples \times nChannels\times Height\times Width$(样本数x通道数x高x宽)。如果你有单个样本,<strong>只需使用 <code>input.unsqueeze(0) </code>来添加其它的维数</strong>.</p>
<h4 id="Parameter"><a href="#Parameter" class="headerlink" title="Parameter"></a>Parameter</h4><p><code>torch.nn.parameter.Parameter</code></p>
<p>A kind of Tensor that is to be considered as a module parameter.</p>
<p><strong>Parameters are <code>Tensor</code> subclasses, that have a very special property when used with <code>Module</code> s - when they’re assigned as Module attributes they are automatically added to the list of its parameters</strong>, and will appear e.g. in <code>parameters()</code> iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as <code>Parameter</code>, these temporaries would get registered too.</p>
<h4 id="Containers"><a href="#Containers" class="headerlink" title="Containers"></a>Containers</h4><p><strong>nn.Module</strong></p>
<p>所有神经网络模块的基类。你的模型也应该子类化这个类。</p>
<p>模块还可以包含其他模块，允许将它们嵌套在树结构中。</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wupiao/articles/13287061.html">Variables training 和 train()  eval()</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">net<span class="token punctuation">.</span>training <span class="token operator">=</span> <span class="token boolean">True</span> 
<span class="token comment"># 布尔值表示该模块是处于训练模式training mode还是评估模式evaluation mode。注意，对module的设置仅仅影响本层，子module不受影响</span>
net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 将本层及子层的training设定为True,使用BatchNormalizetion()和Dropout()</span>
net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 将本层及子层的training设定为False,不使用BatchNormalization()和Dropout()</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>add_module</code></p>
<p>在自定义网络的时候，由于自定义变量<strong>不是Module类型</strong>（例如，我们用List封装了几个网络），所以pytorch<strong>不会自动注册网络模块</strong>。<strong>add_module函数用来为网络添加模块</strong>的，所以我们可以使用这个函数手动添加自定义的网络模块。当然，这种情况，我们也可以使用ModuleList来封装自定义模块，pytorch就会自动注册了。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">)</span>
<span class="token comment"># self.add_module('layers',nn.Linear(28*28,28*28)) 跟上面的方式等价</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/marsggbo/p/12075244.html">buffers()和parameters()的区别</a></p>
<p><code>buffers()</code></p>
<p>指那些不需要参与反向传播的参数,反向传播不需要被optimizer更新</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">buffers<span class="token punctuation">(</span>recurse<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> → Iterator<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">for</span> buf <span class="token keyword">in</span> model<span class="token punctuation">.</span>buffers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>     <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>buf<span class="token punctuation">)</span><span class="token punctuation">,</span> buf<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span> 1L<span class="token punctuation">,</span> 5L<span class="token punctuation">,</span> 5L<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>parameters()</code></p>
<p>是<code>nn.parameter.Paramter</code>，也就是组成Module的参数。例如一个<code>nn.Linear</code>通常由<code>weight</code>和<code>bias</code>参数组成。它的特点是默认<code>requires_grad=True</code>,也就是说训练过程中需要反向传播的，反向传播需要被optimizer更新的。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">parameters<span class="token punctuation">(</span>recurse<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span> Iterator<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parameter<span class="token punctuation">.</span>Parameter<span class="token punctuation">]</span>
<span class="token comment">#recurse (bool)如果为True，则生成此模块和所有子模块的参数。否则，只生成此模块的直接成员参数。</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>     <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">,</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#输出的是一个w,一个b！别忘了b！</span>
<span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.Tensor'</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>20L<span class="token punctuation">,</span> 1L<span class="token punctuation">,</span> 5L<span class="token punctuation">,</span> 5L<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>named_parameters()</code></p>
<p>返回模块参数的迭代器，生成参数名称和参数本身</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">named_parameters<span class="token punctuation">(</span>prefix<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">''</span><span class="token punctuation">,</span><span class="token comment">#作为所有参数名称的前缀</span>
                 recurse<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>→ Iterator<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span>
				<span class="token comment">#如果为真，则生成该模块和所有子模块的参数。否则，只会产生作为该模块直接成员的参数。</span>
<span class="token keyword">for</span> name<span class="token punctuation">,</span>parameters <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#可同时返回名字和参数</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span><span class="token string">':'</span><span class="token punctuation">,</span>parameters<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment">#conv1.weight : torch.Size([6, 1, 3, 3])</span>
<span class="token comment">#conv1.bias : torch.Size([6])</span>
<span class="token comment">#fc1.weight : torch.Size([10, 1350])</span>
<span class="token comment">#fc1.bias : torch.Size([10])				</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>modules()</code></p>
<p>返回一个可以遍历网络所有模块的迭代器。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> l <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>l<span class="token punctuation">,</span> l<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">for</span> idx<span class="token punctuation">,</span> m <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>idx<span class="token punctuation">,</span> <span class="token string">'-&gt;'</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span>

<span class="token number">0</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Sequential<span class="token punctuation">(</span>
  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
<span class="token number">1</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>named_modules()</code></p>
<p>返回一个可以遍历网络所有模块的迭代器,产生模块的名字和模块本身。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> l <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>l<span class="token punctuation">,</span> l<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">for</span> idx<span class="token punctuation">,</span> m <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>idx<span class="token punctuation">,</span> <span class="token string">'-&gt;'</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span>

<span class="token number">0</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">,</span> Sequential<span class="token punctuation">(</span>
  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token number">1</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span><span class="token string">'0'</span><span class="token punctuation">,</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># PSPNet的一段实现</span>
<span class="token keyword">for</span> n<span class="token punctuation">,</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">if</span> <span class="token string">'conv2'</span> <span class="token keyword">in</span> n<span class="token punctuation">:</span>
		m<span class="token punctuation">.</span>dilation<span class="token punctuation">,</span> m<span class="token punctuation">.</span>padding<span class="token punctuation">,</span> m<span class="token punctuation">.</span>stride <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
	<span class="token keyword">elif</span> <span class="token string">'downsample.0'</span> <span class="token keyword">in</span> n<span class="token punctuation">:</span>
		m<span class="token punctuation">.</span>stride <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> n<span class="token punctuation">,</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>layer4<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">if</span> <span class="token string">'conv2'</span> <span class="token keyword">in</span> n<span class="token punctuation">:</span>
		m<span class="token punctuation">.</span>dilation<span class="token punctuation">,</span> m<span class="token punctuation">.</span>padding<span class="token punctuation">,</span> m<span class="token punctuation">.</span>stride <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
	<span class="token keyword">elif</span> <span class="token string">'downsample.0'</span> <span class="token keyword">in</span> n<span class="token punctuation">:</span>
		m<span class="token punctuation">.</span>stride <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># 注意要点有:</span>
<span class="token comment"># Sequential是没有name的,所有在sequential里的都按顺序从0开始编号</span>
<span class="token comment"># 遍历是按照深度优先遍历DFS,名字是不断叠加的，如0,0.conv1，0.conv2,之类的</span>
<span class="token comment"># PSP这段代码的意思就是block中的那个conv2加上空洞卷积，然后取消下采样</span>
params<span class="token operator">=</span>segmodel<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#获得模型的原始状态以及参数。</span>
    <span class="token keyword">for</span> k<span class="token punctuation">,</span>v <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span> <span class="token comment">#只打印key值，不打印具体参数。</span>
<span class="token triple-quoted-string string">"""
conv1.weight
bn1.weight
bn1.bias
bn1.running_mean
bn1.running_var
bn1.num_batches_tracked
conv2.weight
bn2.weight
bn2.bias
bn2.running_mean
bn2.running_var
bn2.num_batches_tracked
layer1.0.conv1.weight
layer1.0.bn1.weight
layer1.0.bn1.bias
layer1.0.bn1.running_mean
layer1.0.bn1.running_var
layer1.0.bn1.num_batches_tracked
layer1.0.conv2.weight
layer1.0.bn2.weight
layer1.0.bn2.bias
layer1.0.bn2.running_mean
layer1.0.bn2.running_var
layer1.0.bn2.num_batches_tracked
"""</span>
l <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>l<span class="token punctuation">,</span> l<span class="token punctuation">)</span>
params<span class="token operator">=</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> k<span class="token punctuation">,</span>v <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span>v<span class="token punctuation">)</span>
<span class="token number">0.</span>weight
<span class="token number">0.</span>bias
<span class="token number">1.</span>weight
<span class="token number">1.</span>bias
<span class="token comment"># 所有放在Sequential里面的都是按0,1,2,3...序号排列的</span>
<span class="token comment"># 中间少了几层就不对不上了就少了</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>state_dict()</code></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/98563721">state_dcit和load_state_dict源码详解</a></p>
<p>返回一个字典，其中包含模块的整个状态,存储了网络结构的名字和对应的参数。parameters和buffers(如运行平均值)都包括在内。键是对应的parameter和buffer名称。</p>
<p><code>torch.nn.Module</code>模块中的<code>state_dict</code>只包含<strong>卷积层和全连接层的参数</strong>，当网络中存在batchnorm时，例如vgg网络结构，torch.nn.Module模块中的state_dict也会存放<strong>batchnorm的running_mean。</strong></p>
<p><code>torch.optim</code>模块中的Optimizer优化器对象也存在一个state_dict对象，此处的state_dict字典对象包含两个字典对象，<strong>key 分别为state和param_groups</strong>，param_groups对应的value也是一个字典对象，由学习率，动量等参数组成。</p>
<p><strong>对于module</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">state_dict<span class="token punctuation">(</span>destination<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> keep_vars<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span><span class="token builtin">dict</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token string">'bias'</span><span class="token punctuation">,</span> <span class="token string">'weight'</span><span class="token punctuation">]</span>
<span class="token comment"># torch.nn.modules.module.py</span>

<span class="token keyword">class</span> <span class="token class-name">Module</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">state_dict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> destination<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> keep_vars<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> destination <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            destination <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
            destination<span class="token punctuation">.</span>_metadata <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
        destination<span class="token punctuation">.</span>_metadata<span class="token punctuation">[</span>prefix<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> local_metadata <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>version<span class="token operator">=</span>self<span class="token punctuation">.</span>_version<span class="token punctuation">)</span>
        <span class="token comment"># params</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>_parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> param <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                destination<span class="token punctuation">[</span>prefix <span class="token operator">+</span> name<span class="token punctuation">]</span> <span class="token operator">=</span> param <span class="token keyword">if</span> keep_vars <span class="token keyword">else</span> param<span class="token punctuation">.</span>data
        <span class="token comment"># buffers</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> buf <span class="token keyword">in</span> self<span class="token punctuation">.</span>_buffers<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> buf <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                destination<span class="token punctuation">[</span>prefix <span class="token operator">+</span> name<span class="token punctuation">]</span> <span class="token operator">=</span> buf <span class="token keyword">if</span> keep_vars <span class="token keyword">else</span> buf<span class="token punctuation">.</span>data
        <span class="token comment"># modules</span>
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> module <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span>destination<span class="token punctuation">,</span> prefix <span class="token operator">+</span> name <span class="token operator">+</span> <span class="token string">'.'</span><span class="token punctuation">,</span> keep_vars<span class="token operator">=</span>keep_vars<span class="token punctuation">)</span>
        <span class="token comment"># </span>
        <span class="token keyword">for</span> hook <span class="token keyword">in</span> self<span class="token punctuation">.</span>_state_dict_hooks<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            hook_result <span class="token operator">=</span> hook<span class="token punctuation">(</span>self<span class="token punctuation">,</span> destination<span class="token punctuation">,</span> prefix<span class="token punctuation">,</span> local_metadata<span class="token punctuation">)</span>
            <span class="token keyword">if</span> hook_result <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                destination <span class="token operator">=</span> hook_result
        <span class="token keyword">return</span> destination<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>通过<code>_modules</code>递归所有子模块,再通过<code>_parameters</code>和<code>_buffers</code>获得所有parameters和buffers,注意之前的parameters()等函数也是利用他们获取相应的值。而<code>_state_dict_hooks</code>就是在读取state_dict时希望执行的操作,一般为空，所以不做考虑。另外有一点需要注意的是，在读取<code>Module</code>时采用的递归的读取方式，并且名字间使用<code>.</code>做分割，以方便后面<code>load_state_dict</code>读取参数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>my_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 参数直接作为模型类成员变量</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'my_buffer'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 参数注册为 buffer</span>
        self<span class="token punctuation">.</span>my_param <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f3 <span class="token operator">=</span> self<span class="token punctuation">.</span>fc
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'my_param'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.3052</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token string">'my_buffer'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5583</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token string">'fc.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6322</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0255</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4747</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0530</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token string">'conv.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.3346</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.2962</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token string">'conv.bias'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5205</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token string">'fc2.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4949</span><span class="token punctuation">,</span>  <span class="token number">0.2815</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span> <span class="token number">0.3006</span><span class="token punctuation">,</span>  <span class="token number">0.0768</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token string">'f3.weight'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6322</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0255</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4747</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0530</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>对于optim</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">state_dict</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token triple-quoted-string string">r"""Returns the state of the optimizer as a :class:`dict`.

      It contains two entries:

      * state - a dict holding current optimization state. Its content
          differs between optimizer classes.
      * param_groups - a dict containing all parameter groups
      """</span>
      <span class="token comment"># Save order indices instead of Tensors</span>
      param_mappings <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
      start_index <span class="token operator">=</span> <span class="token number">0</span>

      <span class="token keyword">def</span> <span class="token function">pack_group</span><span class="token punctuation">(</span>group<span class="token punctuation">)</span><span class="token punctuation">:</span>
          <span class="token keyword">nonlocal</span> start_index
          packed <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> v <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> group<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> k <span class="token operator">!=</span> <span class="token string">'params'</span><span class="token punctuation">}</span>
          param_mappings<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token builtin">id</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">:</span> i <span class="token keyword">for</span> i<span class="token punctuation">,</span> p <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>group<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> start_index<span class="token punctuation">)</span>
                                 <span class="token keyword">if</span> <span class="token builtin">id</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span> <span class="token keyword">not</span> <span class="token keyword">in</span> param_mappings<span class="token punctuation">}</span><span class="token punctuation">)</span>
          packed<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>param_mappings<span class="token punctuation">[</span><span class="token builtin">id</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> group<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
          start_index <span class="token operator">+=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>packed<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
          <span class="token keyword">return</span> packed
      param_groups <span class="token operator">=</span> <span class="token punctuation">[</span>pack_group<span class="token punctuation">(</span>g<span class="token punctuation">)</span> <span class="token keyword">for</span> g <span class="token keyword">in</span> self<span class="token punctuation">.</span>param_groups<span class="token punctuation">]</span>
      <span class="token comment"># Remap state to use order indices as keys</span>
      packed_state <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">(</span>param_mappings<span class="token punctuation">[</span><span class="token builtin">id</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token keyword">else</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span> v
                      <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
      <span class="token keyword">return</span> <span class="token punctuation">{</span>
          <span class="token string">'state'</span><span class="token punctuation">:</span> packed_state<span class="token punctuation">,</span>
          <span class="token string">'param_groups'</span><span class="token punctuation">:</span> param_groups<span class="token punctuation">,</span>
      <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>load_state_dict()</code></p>
<p>将参数和缓冲区从state_dict复制到这个模块及其子模块中。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment">#传入一个state_dict</span>
				strict<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment">#state_dict就是你之前保存的模型参数序列，而_load_from_state_dict中的local_state表示你的代码中定义的模型的结构。</span>
<span class="token comment">#判断上面参数拷贝过程中是否有unexpected_keys或者missing_keys,如果有就报错，代码不能继续执行。当然，如果strict=False，则会忽略这些细节。</span>
<span class="token comment">#missing_keys is a list of str containing the missing keys</span>
<span class="token comment">#unexpected_keys is a list of str containing the unexpected keys</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>cuda</code>与<code>to</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#在这没有区别</span>
    <span class="token comment"># allocates a tensor on GPU 1</span>
    a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>cuda<span class="token punctuation">)</span>
 
    <span class="token comment"># transfers a tensor from CPU to GPU 1</span>
    b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># a.device and b.device are device(type='cuda', index=1)</span>
 
    <span class="token comment"># You can also use ``Tensor.to`` to transfer a tensor:</span>
    b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>cuda<span class="token punctuation">)</span>
    <span class="token comment"># b.device and b2.device are device(type='cuda', index=1)</span>

<span class="token comment"># .to(device)可以指定CPU或者GPU</span>
<span class="token comment"># 单GPU或者CPU</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span> 
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token comment"># model 是 model.to(device)</span>
<span class="token comment"># img 是 img = img.to(device)</span>
<span class="token comment">#如果是多GPU</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
  model <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model，device_ids<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token comment"># .cuda()只能指定GPU</span>
<span class="token comment">#指定某个GPU</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICE'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'1'</span>
model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#如果是多GPU</span>
os<span class="token punctuation">.</span>environment<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0,1,2,3'</span>
device_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span>
net  <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dataparallel<span class="token punctuation">(</span>net<span class="token punctuation">,</span> device_ids <span class="token operator">=</span>device_ids<span class="token punctuation">)</span>
net  <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dataparallel<span class="token punctuation">(</span>net<span class="token punctuation">)</span> <span class="token comment"># 默认使用所有的device_ids </span>
net <span class="token operator">=</span> net<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>apply(fn)</code></p>
<p>将<code>fn</code>递归应用于每个子模块(由<code>.children()</code>返回)以及自身self。典型用法包括初始化模型的参数(参见<code>torch.nn.init()</code>).</p>
<p><code>fn(Module -&gt; None)</code>:将被应用到每个子模块的函数。</p>
<p>Returns:<code>self</code>  Return type:<code>Module</code></p>
<p><code>register_buffer</code></p>
<p>向模块添加一个buffer.</p>
<p>这通常用于注册不应被视为模型参数的缓冲区。 例如,BatchNorm的running_mean不是参数,而是模块状态的一部分。 默认情况下，缓冲区是==持久性==的，并将与参数一起保存。 可以通过将persistent设置为False来更改此行为。</p>
<p> ==持久缓冲区和非持久缓冲区之间的唯一区别是，后者不会成为该模块的state_dict的一部分。==</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">register_buffer<span class="token punctuation">(</span>name<span class="token punctuation">,</span>
                tensor<span class="token punctuation">,</span>
                persistent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment">#是否为持久缓冲区</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p><code>children</code></p>
<p>返回immediate 子模块的迭代器。</p>
<p>Yields:Module - achildren module</p>
<p><code>register_forward_hook(hook)</code></p>
<p>在模块上注册一个前向钩子。</p>
<p>相当于插件。可以实现一些额外的功能，而又不用修改主体代码。把这些额外功能实现了挂在主代码上，所以叫钩子，很形象。</p>
<p>每当<code>forward()</code>计算<code>output</code>后，该钩子都会被调用。 该钩子应该具有以下签名:</p>
<p><code>hook(module, input, output) -&gt; None or modified output</code></p>
<p>输入仅包含提供给模块的positional arguments。 Kerword arguments不会传递给钩子,而只会传递给the forward。挂钩可以修改输出。 它可以就地修改输入,但不会对正向产生影响,因为这是在调用<code>forward()</code>之后调用的。</p>
<p>Returns:a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p>
<p>ReturnType:<code>torch.utils.hooks.RemovableHandle</code></p>
<p><strong>nn.ModuleList</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64990232">PyTorch 中的 ModuleList 和 Sequential: 区别和使用场景</a></p>
<p>可以把任意 nn.Module 的子类 (比如 nn.Conv2d, nn.Linear 之类的) 加到这个 list 里面，方法和 Python 自带的 list 一样，无非是 extend，append 等操作。但不同于一般的 list，加入到 nn.ModuleList 里面的 module 是会自动注册到整个网络上的，同时 module 的 parameters 也会自动添加到整个网络中。</p>
<p>例子1:使用 nn.ModuleList 来构建一个小型网络,包括2个全连接层</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">net1</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>net1<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">:</span>
            x <span class="token operator">=</span> m<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

net <span class="token operator">=</span> net1<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
<span class="token comment"># net1(</span>
<span class="token comment">#   (modules): ModuleList(</span>
<span class="token comment">#     (0): Linear(in_features=10, out_features=10, bias=True)</span>
<span class="token comment">#     (1): Linear(in_features=10, out_features=10, bias=True)</span>
<span class="token comment">#   )</span>
<span class="token comment"># )</span>

<span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># &lt;class 'torch.Tensor'&gt; torch.Size([10, 10])</span>
<span class="token comment"># &lt;class 'torch.Tensor'&gt; torch.Size([10])</span>
<span class="token comment"># &lt;class 'torch.Tensor'&gt; torch.Size([10, 10])</span>
<span class="token comment"># &lt;class 'torch.Tensor'&gt; torch.Size([10])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>网络包含两个全连接层，他们的权重 (weithgs) 和偏置 (bias) 都在这个网络之内。</p>
<p>例子2:使用Python自带的list</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">net2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>net2<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> <span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">:</span>
            x <span class="token operator">=</span> m<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

net <span class="token operator">=</span> net2<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
<span class="token comment"># net2()</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># []</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>使用 Python 的 list 添加的全连接层和它们的 parameters 并没有自动注册到我们的网络中。当然，我们还是可以使用 forward 来计算输出结果。但是如果用 net2 实例化的网络进行训练的时候，因为这些层的 parameters 不在整个网络之中，所以其网络参数也不会被更新，也就是无法训练。</strong></p>
<p>好,看到这里,我们大致明白了 nn.ModuleList 是干什么的了:它是一个<strong>储存不同 module,并自动将每个module的parameters添加到网络之中的容器</strong>。但是,我们需要注意到**,nn.ModuleList 并没有定义一个网络.它只是将不同的模块储存在一起,这些模块之间并没有什么先后顺序**可言,比如:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">net3</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>net3<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> 
        <span class="token keyword">return</span> x

net <span class="token operator">=</span> net3<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
<span class="token comment"># net3(</span>
<span class="token comment">#   (linears): ModuleList(</span>
<span class="token comment">#     (0): Linear(in_features=10, out_features=20, bias=True)</span>
<span class="token comment">#     (1): Linear(in_features=20, out_features=30, bias=True)</span>
<span class="token comment">#     (2): Linear(in_features=5, out_features=10, bias=True)</span>
<span class="token comment">#   )</span>
<span class="token comment"># )</span>
<span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment"># torch.Size([32, 30])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>根据 net3 的结果,我们可以看出来这个 <strong>ModuleList 里面的顺序并不能决定什么,网络的执行顺序是根据 forward 函数来决定的</strong>。如果你非要 ModuleList 和 forward 中的顺序不一样,<strong>PyTorch 表示它无所谓，但以后 review 你代码的人可能会意见比较大</strong>。</p>
<p>我们再考虑另外一种情况,既然这个 ModuleList 可以根据序号来调用，那么一个模块是否可以在 forward 函数中被调用多次呢？答案当然是可以的，但是，<strong>被调用多次的模块，是使用同一组 parameters 的，也就是它们的参数是共享的</strong>,无论之后怎么更新。</p>
<p><strong>nn.sequential</strong></p>
<p>顺序容器。模块将按照它们在构造函数中传递的顺序被添加到它。另外，也可以传入模块的有序dict。</p>
<p>不同于 nn.ModuleList,它<strong>已经实现了内部的 forward 函数，而且里面的模块必须是按照顺序进行排列的，所以我们必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Example of using Sequential</span>
model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
          nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

<span class="token comment"># Example of using Sequential with OrderedDict</span>
model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>
          <span class="token punctuation">(</span><span class="token string">'conv1'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          <span class="token punctuation">(</span><span class="token string">'relu1'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          <span class="token punctuation">(</span><span class="token string">'conv2'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          <span class="token punctuation">(</span><span class="token string">'relu2'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a>线性层</h4><p><code>nn.Linear</code>全连接层</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> out_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> bias<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment">#在全连接层之前通过view函数将其改为一维向量</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p><code>nn.Dropout</code></p>
<p>在训练过程中,使用伯努利分布样本，以概率p随机地将输入张量中的一些元素置零。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token comment">#元素置0的概inplace=False)#</span>
<span class="token comment"># 对所有元素中每个元素按照概率0.5置为0，对点执行</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>Furthermore, the outputs are scaled by a factor of $\frac{1}{1-p}$ during training. This means that during evaluation the module simply computes an identity function.</p>
<p><code>nn.Dropout2d</code></p>
<p>适用于有多个channel输出的,常用于图像处理。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token comment"># 对每个通道按照概率0.5置为0，对平面执行,直接将整个channel置为0.psp中dropout值设为0.1,在最后一个预测卷积之前使用的。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h4 id="Conv"><a href="#Conv" class="headerlink" title="Conv"></a>Conv</h4><p><code>nn.Conv2d()</code>卷积核是二维的</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> 
                out_channels<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
                kernel_size<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                stride<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
                padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> 
                dilation<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> 
                groups<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> 
                bias<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment">#是否要添加偏置参数作为可学习参数的一个，默认为True。</span>
                padding_mode<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">'zeros'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>教程：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/45a26d278473">https://www.jianshu.com/p/45a26d278473</a></p>
<p>接受$(N,C_{in},H,W)$,输出$(N,C_{out},H_{out},W_{out}$)</p>
<p>卷积核的规模就是kernel_size x input_channel x output_channel</p>
<p>$out(N_i,C_{out_j})=bias(C_{out_j})+∑<em>{k=0}^{C</em>{in-1}}weight(C_{out_j},k)⋆input(N_i,k)$</p>
<p>对于<strong>depthwise conv</strong>,groups这个参数是关键。当$groups=1$时,说明所有通道为一组;当$groups=in_channels$时,说明分了$in_channels$个组，即每个通道一组。然后分别对齐卷积，输出通道数为k。最后再将每组的输出串联，最后通道数为$in_channels*k$。</p>
<p>要实现depthwise conv,就讲groups设为in_channels,同时out_channels也设为与in_channels相同。</p>
<p><strong>Variables</strong></p>
<p>~Conv2d.weight(Tensor):维度为<code>(out_channels,in_channels/groups,kernel_size[0],kernel_size[1])</code>，权重值取样于$u(-\sqrt k,\sqrt k)$,$k=$</p>
<p>~Conv2d.bias(Tensor):维度为<code>(out_channels)</code>，值取样于$u(-\sqrt k,\sqrt k)$,$k=$</p>
<p><code>nn.ConvTranspose2d</code></p>
<p>在由几个输入平面组成的输入图像上应用一个二维转置卷积运算符。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
                         out_channels<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
                         kernel_size<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                         stride<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
                         padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
                         output_padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> T<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
                         groups<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
                         dilation<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
                         padding_mode<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">'zeros'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>Pytorch<strong>池化操作的步长默认与池化卷积核的大小一样</strong>，<strong>池化一般不考虑overlap</strong></p>
<p><code>nn.MaxPool2d()</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
				   stride<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token comment">#default value是kernel_size</span>
				   padding<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> 
				   dilation<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> 
				   return_indices<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token comment">#如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助</span>
				   ceil_mode<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment">#如果true，向上取整而不是floor向下取整</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>nn.AvgPool2d()</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span>
                   stride<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                   padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> 
                   ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment">#when True, will use ceil instead of floor to compute the output shape</span>
                   count_include_pad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                   divisor_override<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)</code></p>
<p>特殊性在于，输出张量的大小都是给定的output_size，可以利用这个实现全局平均池化，将output_size设为(1,1)。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># target output size of 5x7</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output <span class="token operator">=</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># target output size of 7x7 (square)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output <span class="token operator">=</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013382233/article/details/85948695">自适应池化详解1</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xiaosongshine/article/details/89453037">自适应池化详解2</a></p>
<h4 id="Padding-Layers"><a href="#Padding-Layers" class="headerlink" title="Padding Layers"></a>Padding Layers</h4><p><code>torch.nn.ReflectionPad2d(padding)</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReflectionPad2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h4><p><code>nn.BatchNorm2d()</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span><span class="token comment">#Channel数</span>
					eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span><span class="token comment">#为数值稳定性而加在分母上的值。</span>
					momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span><span class="token comment">#指数加权平均的参数</span>
					affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment">#是否有可学习参数</span>
					track_running_stats<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment">#这一层不用到测试</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>nn.SyncBatchNorm</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><code>nn.LayerNorm</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">,</span>
                   eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span>
                   elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># NLP和CV的应用是不同的</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># NLP Example</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> batch<span class="token punctuation">,</span> sentence_length<span class="token punctuation">,</span> embedding_dim <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">10</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> embedding <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> sentence_length<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># Activate module</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> layer_norm<span class="token punctuation">(</span>embedding<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># Image Example</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># Normalize over the last three dimensions (i.e. the channel and spatial dimensions)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># as shown in the image below</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span><span class="token punctuation">[</span>C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="非线性激活函数"><a href="#非线性激活函数" class="headerlink" title="非线性激活函数"></a>非线性激活函数</h4><p><code>nn.ReLU()</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment">#inplace-选择是否进行覆盖运算 x=x+1 还是 y=x+1 x=y 节省内存</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><code>nn.Softmax(dim=None)</code></p>
<p>对指定维度应用Softmax</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
m0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
m1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
output0 <span class="token operator">=</span> m0<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
output1 <span class="token operator">=</span> m1<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"input: "</span><span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"output0: "</span><span class="token punctuation">,</span> output0<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"output1: "</span><span class="token punctuation">,</span> output1<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
input:  tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])
dim=0,就是归一化的元素的维度1都相同,只遍历维度0
output0:  tensor([[0.0024, 0.0024, 0.0024],
        [0.0473, 0.0473, 0.0473],
        [0.9503, 0.9503, 0.9503]])
output1:  tensor([[0.0900, 0.2447, 0.6652],
        [0.0900, 0.2447, 0.6652],
        [0.0900, 0.2447, 0.6652]])'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>nn.Softmax2d()</code></p>
<p>输入为(N,C,H,W),输出为(N,C,H,W) 就是你想的那样</p>
<p><code>nn.LogSoftmax(dim=None)</code><br>$$<br>LogSoftmax(x_i)=log(\frac{exp(x_i)}{\sum_jexp(x_j)})<br>$$</p>
<h4 id="距离函数"><a href="#距离函数" class="headerlink" title="距离函数"></a>距离函数</h4><p><code>nn.CosineSimilarity</code><br>$$<br>similarity=\frac{x_1\cdot x_2}{max(||x_1||_2\cdot||x_2||_2,eps)}<br>$$</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CosineSimilarity<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment"># 计算余弦相似性的维度</span>
                          eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span><span class="token comment"># 避免除0的小数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>Input1:(*,D,*),D是要计算的维度</p>
<p>Input2:(*,D,*)</p>
<p>Output:(*,*)</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wanghui-garcia/p/10862733.html">常见损失函数总结,讲得好啊</a></p>
<p><code>nn.BCELoss</code></p>
<p>计算二元交叉熵</p>
<p>$l(x,y)=L={l_1,\cdots,l_N}^T,l_n=-w_n[y_n\cdot logx_n+(1-y_n)\cdot log(1-x_n)]$</p>
<p>我们的解决方案是BCELoss将其对数函数输出固定为大于或等于-100。这样，我们总是可以有一个有限的损失值和一个线性的反向方法。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span>weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#手动给的权重</span>
                 size_average<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#</span>
                 <span class="token builtin">reduce</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 reduction<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">'mean'</span><span class="token punctuation">)</span>
<span class="token comment">#指定要应用于输出的reduction操作:' none ' | 'mean' | ' sum '。none输出向量,其他输出标量</span>
<span class="token comment">#“none”:表示不进行任何reduction，“mean”:输出的和除以输出中的元素数，即求平均值，“sum”:输出求和。</span>
<span class="token comment">#注意:size_average和reduce正在被弃用，与此同时，指定这两个arg中的任何一个都将覆盖reduction参数。默认值:“mean”</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>nn.BCEWithLogitsLoss</code></p>
<p>这种损失结合了Sigmoid层和BCELoss在一个类里。这个版本通过将操作合并到一个层比使用一个简单的Sigmoid后面跟着一个BCELoss在数值上更稳定，我们利用<strong>log-sum-exp</strong>技巧的数值稳定性。</p>
<p>$l(x,y)=L={l_1,\cdots,l_N}^T,l_n=-w_n[y_n\cdot log\sigma(x_n)+(1-y_n)\cdot log(1-\sigma(x_n))]$</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span>weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                           size_average<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                           <span class="token builtin">reduce</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                           reduction<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">'mean'</span><span class="token punctuation">,</span>
                           pos_weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment">#正值例子的权重，必须是有着与分类数目相同的长度的向量.可以通过增加正值示例的权重来权衡召回率和准确性。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>nn.NLLLoss</code></p>
<p>负对数似然损失。用C类来训练分类问题是有用的。</p>
<p>==负对数似然损失函数，就是对似然函数取负再取log==</p>
<p>如果提供了可选参数weight，它应该是一个一维张量，为每一类赋权。当你有一个不平衡的训练集时，这是特别有用的。</p>
<p>$l(x,y)=L={l_1,\cdots,l_N}^T,l_n=-w_{y_nx_{n,y_n}},w_c=weight[c]\cdot1{c\ne ignore_index}$只选择第n个数据的实际yn类别作为loss</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                 <span class="token comment"># 一个手动标给每个类别的权重，如果给定，必须是一个C大小张量,否则默认所有的权重全是1</span>
                 size_average<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 ignore_index<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token comment">#指定一个被忽略的目标值，该目标值不影响输入梯度。</span>
                 <span class="token comment"># 当size_average为真时，对非忽略目标的损失进行平均。</span>
                 <span class="token builtin">reduce</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 <span class="token comment"># 默认情况下为True，根据size_average，通过每个小批的观察来对损失进行平均或求和。</span>
                 <span class="token comment"># 当reduce为False时，返回每个批处理元素的损失值，并忽略size_average</span>
                 reduction<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">'mean'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>==也就是说没有特殊处理的情况下，返回的是每个mini-bacth的平均cross-entropy loss，因为loss就是一个标量嘛，再之后还需要对所有的mini-batch的cross-entropy loss求平均。==</p>
<p>输入<code>(N,C)</code>, <code>C</code>代表类别的数量；或者在计算高维损失函数例子中输入大小为<code>(N,C,d1,d2,...,dK),k&gt;=1</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#低维示例:</span>
m <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># input is of size N x C = 3 x 5</span>
<span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token builtin">input</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8676</span><span class="token punctuation">,</span>  <span class="token number">1.5017</span><span class="token punctuation">,</span>  <span class="token number">0.2963</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.9431</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0929</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.3540</span><span class="token punctuation">,</span>  <span class="token number">1.0994</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1085</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4001</span><span class="token punctuation">,</span>  <span class="token number">0.0102</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">1.3653</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3828</span><span class="token punctuation">,</span>  <span class="token number">0.6257</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4996</span><span class="token punctuation">,</span>  <span class="token number">0.1928</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.8899</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5205</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.7259</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.9653</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.1152</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment">#1-&gt;0.5205</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.5082</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7628</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.9707</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.2623</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8520</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment">#0-&gt;1.5082</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6841</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4323</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4237</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4.5490</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8566</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment">#4-&gt;1.8566</span>
       grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>LogSoftmaxBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
<span class="token comment">#each element in target has to have 0 &lt;= value &lt; C</span>
target <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> loss<span class="token punctuation">(</span>m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>
output
tensor<span class="token punctuation">(</span><span class="token number">1.2951</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
<span class="token comment">#高维示例:就是逐像素返回呗,对每一个像素来说都是一个低维示例</span>
<span class="token comment"># 2D loss example (used, for example, with image inputs)</span>
N<span class="token punctuation">,</span> C <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">,</span><span class="token number">4</span>
loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># input is of size N x channel x height x width</span>
data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> C<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#输出为5*4*8*8</span>
m <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># each element in target has to have 0 &lt;= value &lt; C</span>
target <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>N<span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span><span class="token punctuation">.</span>random_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token comment">#target.size()=target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)</span>
output <span class="token operator">=</span> loss<span class="token punctuation">(</span>m<span class="token punctuation">(</span>conv<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>
output
tensor<span class="token punctuation">(</span><span class="token number">1.5501</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLoss2DBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>nn.CrossEntropyLoss</code></p>
<p>将<code>nn.LogSoftmax()</code>和<code>nn.NLLLoss()</code>方法结合到一个类中</p>
<p>当用C类训练分类问题时，它是有用的。如果提供了，可选的参数weight权重应该是一个一维张量，为每个类分配权重。当你有一个不平衡的训练集时，这是特别有用的。</p>
<p>损失函数:<br>$$<br>log(x,class)=-log(\frac{exp(x[class])}{\sum_jexp(x[j])})=-x[class]+log(\sum_jexp(x[j]))<br>$$<br>加上weight:<br>$$<br>log(x,class)=weight<a href="-x%5Bclass%5D+log(%5Csum_jexp(x%5Bj%5D))">class</a><br>$$<br>对于<code>ignore_index:</code></p>
<p>该ignore_index标签的样本损失不考虑</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token comment"># 假设两类{0:背景，1：前景}</span>
pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>
        <span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.7</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">)</span>  <span class="token comment"># shape=(N,C)=(3,2)，N为样本数，C为类数</span>
label <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># shape=(N)=(3)，3个样本的label分别为1，0，1</span>
out <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> label<span class="token punctuation">,</span> ignore_index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 忽略0类</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span>
<span class="token comment"># tensor(1.0421) </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>$$<br>loss=1/2{[-0.1+ln(e^{0.9}+e^{0.1})+[-0.3+ln(e^{0.7}+e^{0.3})]}=1.0421<br>$$</p>
<p>x选的只是class那个</p>
<p><strong>取平均是对每个batch维度取平均</strong></p>
<img src="./medias/loading.gif" data-original="/pytorch-xue-xi/image-20210402114047450.png" alt="shape" style="zoom:80%;">

<p>reduction=none的话，说明minibatch之间不做平均，就是个向量，否则就是个标量；</p>
<p><strong>其实整个思路都是明白的，就看如何返回了，上面取得了形式的一致性；</strong></p>
<p>$(N,d1,d2,…,dn)$<strong>是如何变成(1,1)的就是对后面维度求和，对N取平均。</strong></p>
<h4 id="视觉层"><a href="#视觉层" class="headerlink" title="视觉层"></a>视觉层</h4><p><code>nn.Upsample</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>size<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#指定目标输出的大小</span>
                  scale_factor<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>T<span class="token punctuation">,</span> Tuple<span class="token punctuation">[</span>T<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#输出为输入的倍数，和size只能指定一个</span>
                  mode<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">'nearest'</span><span class="token punctuation">,</span><span class="token comment">#上采样算法，包括最近邻、线性、双线性、双三次、三线性插值算法</span>
                  align_corners<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token comment">#如果为True，输入的角像素将与输出张量对齐，</span>
<span class="token comment">#因此将保存下来这些像素的值。仅当使用的算法为'linear', 'bilinear'or 'trilinear'时可以使用。默认设置为False</span>
<span class="token comment">#语义分割设置为true</span>
tor<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="数据并行层"><a href="#数据并行层" class="headerlink" title="数据并行层"></a>数据并行层</h4><p><code>nn.DataParallel</code></p>
<p>在module level实现数据并行.</p>
<p>该容器在batch维度上拆分,将输入分到指定的设备上,以此实现并行化(其他object每个设备都复制一次)。</p>
<p>前向传播过程中,module在每个设备上,每个副本处理一部分输入;</p>
<p>反向传播时,每个副本的梯度被累加到原始模块中。</p>
<p>batch size应该&gt;大于使用的GPU数量。</p>
<p>推荐使用<code>DistributedDataParallel</code></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86441879">pytorch多GPU并行训练</a></p>
<p>我一般在使用多GPU的时候, 会喜欢使用<code>os.environ['CUDA_VISIBLE_DEVICES']</code>来限制使用的GPU个数, 例如我要使用第0和第3编号的GPU, 那么只需要在程序中设置:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0,3'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>模型加载到多GPU:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>对于数据:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>如果我们不设定好要使用的<code>device_ids的</code>话, 程序会自动找到这个机器上面<strong>可以用的所有的显卡, 然后用于训练.</strong> </p>
<p>但是因为我们前面使用<code>os.environ['CUDA_VISIBLE_DEVICES']</code><strong>限定了这个程序可以使用的显卡, 所以这个地方程序如果自己获取的话, 获取到的其实就是我们上面设定的那几个显卡.</strong></p>
<p>我没有进行深入得到考究, 但是我感觉使用<code>os.environ['CUDA_VISIBLE_DEVICES']</code>对可以使用的显卡进行限定之后, 显卡的实际编号和程序看到的编号应该是不一样的, 例如上面我们设定的是<code>os.environ['CUDA_VISIBLE_DEVICES']="0,2"</code>, 但是程序看到的显卡编号应该被改成了<code>'0,1'</code>, 也就是说程序所使用的显卡编号实际上是经过了一次映射之后才会映射到真正的显卡编号上面的, 例如这里的程序看到的1对应实际的2</p>
<p><code>torch.nn.parallel.DistributedDataParallel</code></p>
<p>在module level 实现基于torch.distuributed包的分布式数据并行。这个容器通过在批处理维度上的分块，在指定的设备上分割输入，使给定模块的应用并行化。该模块被复制到每台机器和每个设备上，每个这样的副本处理一部分输入。在倒退过程中，每个节点的梯度被平均化。</p>
<p>batch_size应该大于本地使用的GPU数量。</p>
<p>创建这个类需要调用<code>torch.distributed.init_process_group()</code>使得<code>torch.distributed</code>已经被初始化了。</p>
<p>事实证明,在单节点多GPU的数据并行训练中,DistributedDataParallel明显比torch.nn.DataParallel快。</p>
<p>要在有N个GPU的主机上使用DistributedDataParallel，你应该催生N个进程，确保每个进程只在0到N-1的单个GPU上工作。这可以通过为每个进程设置CUDA_VISIBLE_DEVICES或通过调用:<code>torch.cuda.set_device(i)</code>,这里的i是从0到N-1。在每个进程中，你应该参考以下内容来构建这个模块：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>
    backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">,</span> world_size<span class="token operator">=</span>N<span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token string">'...'</span>
<span class="token punctuation">)</span>
model <span class="token operator">=</span> DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>i<span class="token punctuation">)</span>

torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>module<span class="token punctuation">,</span>
                                          device_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment"># </span>
                                          output_device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                                          dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                                          broadcast_buffers<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment">#设置为True时，在模型执行forward之前，gpu0会把buffer中的参数值全部覆盖</span>
<span class="token comment"># 到别的gpu上。注意这和同步BN并不一样，同步BN应该使用SyncBatchNorm。</span>
                                          process_group<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#用于分布式数据</span>
                                          bucket_cap_mb<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> 
                                          find_unused_parameters<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment"># 如果模型的输出有不需要进行反传的(比如部分参数被冻结/</span>
                                          <span class="token comment"># 或者网络前传是动态的)，设置此参数为True;如果你的代码运行后卡住某个地方不动，基本上就是该参数的问题。</span>
                                          check_reduction<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
                                          gradient_as_bucket_view<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
                                          static_graph<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="torch-nn-utils"><a href="#torch-nn-utils" class="headerlink" title="torch.nn.utils"></a>torch.nn.utils</h4><p><code>nn.utils.clip_grad_value_</code></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/99953668">深度炼丹之梯度裁剪</a></p>
<p>backward之后,step之前</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_value_<span class="token punctuation">(</span>parameters<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>Iterable<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                 <span class="token comment">#将梯度归一化的张量的可迭代或单个张量 net.parameters()</span>
                                clip_value<span class="token punctuation">:</span> <span class="token builtin">float</span><span class="token punctuation">)</span> → <span class="token boolean">None</span><span class="token comment">#梯度被裁剪到[-clip_value,clip_value]</span>
eg<span class="token punctuation">:</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_value_<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>nn.utils.clip_grad_norm</code></p>
<p>这个函数的主要目的是对$parameters$里的所有参数的梯度进行规范化。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span><span class="token comment"># 要规范的参数</span>
                               max_norm<span class="token punctuation">,</span><span class="token comment"># 所有参数的梯度的范数的上界</span>
                               norm_type<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token comment"># 范数的类型,"inf"为无穷范数</span>
                               error_if_nonfinite<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>设$parameters$里所有参数的梯度的范数为$total_norm$，<br>若$max_norm&gt;total_norm$, $parameters$里面的参数的梯度不做改变;<br>若$max_norm&lt;total_norm$, $parameters$里面的参数的梯度都要乘以一个系数$clip_coef$:<br>$$<br>clip_coef=max_norm/(total_norm+10^{-6})<br>$$</p>
<h4 id="自定义模块"><a href="#自定义模块" class="headerlink" title="自定义模块"></a>自定义模块</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DoubleConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment">#要继承</span>
    <span class="token triple-quoted-string string">"""(convolution =&gt; [BN] =&gt; ReLU) * 2"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> mid_channels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment">#父类__init__</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> mid_channels<span class="token punctuation">:</span>
            mid_channels <span class="token operator">=</span> out_channels
        self<span class="token punctuation">.</span>double_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> mid_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>mid_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mid_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment">#要定义一个forward函数</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>double_conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment">#返回自身</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="forward函数"><a href="#forward函数" class="headerlink" title="forward函数"></a>forward函数</h4><p>定义每次调用时执行的计算。应该被所有子类重写。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#主要是使用了__call__特殊方法,使得forward被自动调用。</span>
<span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> init_age<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'我年龄是:'</span><span class="token punctuation">,</span>init_age<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>age <span class="token operator">=</span> init_age
 
    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> added_age<span class="token punctuation">)</span><span class="token punctuation">:</span>
        res <span class="token operator">=</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>added_age<span class="token punctuation">)</span>
        <span class="token keyword">return</span> res
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'forward 函数被调用了'</span><span class="token punctuation">)</span>        
        <span class="token keyword">return</span> input_ <span class="token operator">+</span> self<span class="token punctuation">.</span>age

    
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'对象初始化。。。。'</span><span class="token punctuation">)</span>
a <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token comment">#初始化</span>
input_param <span class="token operator">=</span> a<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment">#__call__起作用</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"我现在的年龄是："</span><span class="token punctuation">,</span> input_param<span class="token punctuation">)</span>
<span class="token comment">#对象初始化。。。。</span>
<span class="token comment">#我年龄是: 10</span>
<span class="token comment">#forward 函数被调用了</span>
<span class="token comment">#我现在的年龄是： 12</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>关于 <code>__call__</code> 方法，不得不先提到一个概念，就是*可调用对象callable，我们平时自定义的函数、内置函数和类都属于可调用对象，但凡是可以把一对括号<code>()</code>应用到某个对象身上都可称之为可调用对象，判断对象是否为可调用对象可以用函数 <code>callable</code>。如果在类中实现了 <code>__call__</code> 方法，那么实例对象也将成为一个可调用对象。</p>
<p>利用这种特性，可以实现基于类的装饰器。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Counter</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> func<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>func <span class="token operator">=</span> func
        self<span class="token punctuation">.</span>count <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>count <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>func<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

<span class="token decorator annotation punctuation">@Counter</span>
<span class="token keyword">def</span> <span class="token function">foo</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">pass</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    foo<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>foo<span class="token punctuation">.</span>count<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012436149/article/details/70145598">pytorch结构介绍</a></p>
<p>forward函数使用的具体流程:</p>
<ol>
<li>调用module的<code>call</code>方法</li>
<li><code>module</code>的<code>call</code>里面调用<code>module</code>的<code>forward</code>方法</li>
<li><code>forward</code>里面如果碰到<code>Module</code>的子类，回到第1步，如果碰到的是<code>Function</code>的子类，继续往下</li>
<li>调用<code>Function</code>的<code>call</code>方法</li>
<li><code>Function</code>的<code>call</code>方法调用了Function的<code>forward</code>方法。</li>
<li><code>Function</code>的<code>forward</code>返回值</li>
<li><code>module</code>的<code>forward</code>返回值</li>
<li>在<code>module</code>的<code>call</code>进行<code>forward_hook</code>操作，然后返回值。</li>
</ol>
<h3 id="torch-hub"><a href="#torch-hub" class="headerlink" title="torch.hub"></a>torch.hub</h3><p>Pytorch Hub is a pre-trained model repository designed to facilitate(促进) research reproducibility.</p>
<h3 id="torch-nn-init"><a href="#torch-nn-init" class="headerlink" title="torch.nn.init"></a>torch.nn.init</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/luo3300612/article/details/97675312">Pytorch 默认参数初始化</a></p>
<p>pytorch中的各种参数层(Linear、Conv2d、BatchNorm等)在<code>__init__</code>方法中定义后，不需要手动初始化就可以直接使用，这是因为Pytorch对这些层都会进行默认初始化。但是有时候我们需要自定义参数的初始化，就需要用到torch.nn.init。具体的不同初始化，可以查看pytorch官方文档。</p>
<h4 id="初始化函数"><a href="#初始化函数" class="headerlink" title="初始化函数"></a>初始化函数</h4><p><strong>初始化方式</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
<span class="token comment">#从正态分布N(mean,std^2)中取值填充张量</span>
<span class="token comment">#eg:w=torch.empty(3,5)</span>
<span class="token comment">#nn.init.normal_(w)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong><code>kaiming_uniform_</code></strong></p>
<p>Also known as He initialization.</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">kaiming_uniform_</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> a<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_in'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'leaky_relu'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># a用于leaky_relu</span>
    <span class="token keyword">if</span> <span class="token number">0</span> <span class="token keyword">in</span> tensor<span class="token punctuation">.</span>shape<span class="token punctuation">:</span>
        warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span><span class="token string">"Initializing zero-element tensors is a no-op"</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> tensor
    fan <span class="token operator">=</span> _calculate_correct_fan<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mode<span class="token punctuation">)</span>
    gain <span class="token operator">=</span> calculate_gain<span class="token punctuation">(</span>nonlinearity<span class="token punctuation">,</span> a<span class="token punctuation">)</span>
    std <span class="token operator">=</span> gain <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan<span class="token punctuation">)</span>
    bound <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">)</span> <span class="token operator">*</span> std  <span class="token comment"># Calculate uniform bounds from standard deviation</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span>bound<span class="token punctuation">,</span> bound<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">_calculate_correct_fan</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mode<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 计算fan_in和fan_out并返回所要的</span>
    mode <span class="token operator">=</span> mode<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>
    valid_modes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'fan_in'</span><span class="token punctuation">,</span> <span class="token string">'fan_out'</span><span class="token punctuation">]</span>
    <span class="token keyword">if</span> mode <span class="token keyword">not</span> <span class="token keyword">in</span> valid_modes<span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Mode {} not supported, please use one of {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>mode<span class="token punctuation">,</span> valid_modes<span class="token punctuation">)</span><span class="token punctuation">)</span>

    fan_in<span class="token punctuation">,</span> fan_out <span class="token operator">=</span> _calculate_fan_in_and_fan_out<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
    <span class="token keyword">return</span> fan_in <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">'fan_in'</span> <span class="token keyword">else</span> fan_out

<span class="token keyword">def</span> <span class="token function">_calculate_fan_in_and_fan_out</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    dimensions <span class="token operator">=</span> tensor<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> dimensions <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions"</span><span class="token punctuation">)</span>
	<span class="token comment"># 这个tensor是w的tensor</span>
    num_input_fmaps <span class="token operator">=</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    num_output_fmaps <span class="token operator">=</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    
    receptive_field_size <span class="token operator">=</span> <span class="token number">1</span>
    <span class="token keyword">if</span> tensor<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">2</span><span class="token punctuation">:</span>
        <span class="token comment"># math.prod is not always available, accumulate the product manually</span>
        <span class="token comment"># we could use functools.reduce but that is not supported by TorchScript</span>
        <span class="token keyword">for</span> s <span class="token keyword">in</span> tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            receptive_field_size <span class="token operator">*=</span> s
    fan_in <span class="token operator">=</span> num_input_fmaps <span class="token operator">*</span> receptive_field_size
    fan_out <span class="token operator">=</span> num_output_fmaps <span class="token operator">*</span> receptive_field_size

    <span class="token keyword">return</span> fan_in<span class="token punctuation">,</span> fan_out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Xavier在tanh中表现的很好,但在Relu激活函数中表现的很差,所以何凯明提出了针对于relu的初始化方法。</p>
<p><code>kaiming_uniform_</code>从$u(-bound,bound)$中采样值填充张量,$bound=gain\cdot \sqrt{3/fan_{mode}}=\sqrt{\frac{6}{(1+a^2)fan_in}}$</p>
<p>$f_out=out_channels×kernel_size^2,f_in=in_channels×kernel_size^2$</p>
<p>$fan_in$可以保持前向传播的权重方差的数量级,$fan_out$可以保持反向传播的权重方差的数量级。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41917697/article/details/116033589">理论分析</a></p>
<p><strong><code>calculate_gain(nonlinearity, param=None)</code></strong></p>
<p>输入激活函数的名字,返回对应的$gain$值,增益值$gain$是一个比例,来调控输入数量级和输出数量级之间的关系。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">gain <span class="token operator">=</span> nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>calculate_gain<span class="token punctuation">(</span><span class="token string">'leaky_relu'</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">)</span>  <span class="token comment"># leaky_relu with </span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>$nonlinearity\to gain$:</p>
<p>$Linear/identity=1,Conv{1,2,3}D=1,Sigmoid=1,tanh=5/3,Relu=\sqrt{2}$</p>
<p>$Leaky Relu=\sqrt{2/(1+negative_slope^2)},SELU=3/4$</p>
<p><strong><code>kaiming_normal_</code></strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">kaiming_normal_</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> a<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_in'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'leaky_relu'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token number">0</span> <span class="token keyword">in</span> tensor<span class="token punctuation">.</span>shape<span class="token punctuation">:</span>
        warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span><span class="token string">"Initializing zero-element tensors is a no-op"</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> tensor
    fan <span class="token operator">=</span> _calculate_correct_fan<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mode<span class="token punctuation">)</span>
    gain <span class="token operator">=</span> calculate_gain<span class="token punctuation">(</span>nonlinearity<span class="token punctuation">,</span> a<span class="token punctuation">)</span>
    std <span class="token operator">=</span> gain <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan<span class="token punctuation">)</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>从$N~(0,std^2)$中采样值填充张量,$std=\frac{gain}{\sqrt{fan_mode}}$。</p>
<h4 id="参数层初始化"><a href="#参数层初始化" class="headerlink" title="参数层初始化"></a>参数层初始化</h4><p><strong>Linear</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> out_features<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> bias<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
                 device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
	factory_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'device'</span><span class="token punctuation">:</span> device<span class="token punctuation">,</span> <span class="token string">'dtype'</span><span class="token punctuation">:</span> dtype<span class="token punctuation">}</span>
	<span class="token builtin">super</span><span class="token punctuation">(</span>Linear<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
	self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features
	self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features
	self<span class="token punctuation">.</span>weight <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">**</span>factory_kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>
	<span class="token keyword">if</span> bias<span class="token punctuation">:</span>
		self<span class="token punctuation">.</span>bias <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> <span class="token operator">**</span>factory_kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>
	<span class="token keyword">else</span><span class="token punctuation">:</span>
		self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">'bias'</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
	self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
	init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
	<span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
		fan_in<span class="token punctuation">,</span> _ <span class="token operator">=</span> init<span class="token punctuation">.</span>_calculate_fan_in_and_fan_out<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
		bound <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan_in<span class="token punctuation">)</span> <span class="token keyword">if</span> fan_in <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token number">0</span>
		init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token operator">-</span>bound<span class="token punctuation">,</span> bound<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>$a=\sqrt{5}$,所以$bound=\sqrt{1/fan_in}$,所以$w,b\sim U(-bound,bound)$</p>
<p><strong>Conv</strong></p>
<p>比如一个输入channel为3,输出channel为64,kernel size=3的卷积层,其权值即为一个3×64×3×3的向量,它会这样进行初始化：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        fan_in<span class="token punctuation">,</span> _ <span class="token operator">=</span> init<span class="token punctuation">.</span>_calculate_fan_in_and_fan_out<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        bound <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan_in<span class="token punctuation">)</span>
        init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token operator">-</span>bound<span class="token punctuation">,</span> bound<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>BatchNorm</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>reset_running_stats<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">:</span>
        init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>$w\sim U(0,1),bias=0$</p>
<h4 id="网络初始化"><a href="#网络初始化" class="headerlink" title="网络初始化"></a>网络初始化</h4><p>在各种内置的网络模型中,初始化的方法也有不同。</p>
<p><strong>ResNet</strong></p>
<p>resnet在定义各层之后，pytorch官方代码的<code>__init__</code>方法会对不同的层进行手动的初始化.</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 卷积层使用kaiming_normal_</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_out'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token punctuation">(</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># BatchNorm、GroupNorm使用常数1和0</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>VGG</strong></p>
<p>VGG的pytorch官方初始化如下:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_initialize_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_out'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> m<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">)</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="设置随机数种子"><a href="#设置随机数种子" class="headerlink" title="设置随机数种子"></a>设置随机数种子</h4><p><code>torch.manual_seed(seed) → torch._C.Generator</code></p>
<p>在神经网络中，参数默认是进行随机初始化的。如果不设置的话每次训练时的初始化都是随机的，导致结果不确定。如果设置初始化，则每次初始化都是固定的。pytorch默认使用何恺明的初始化</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> args<span class="token punctuation">.</span>seed <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>    　　
    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span> <span class="token comment"># 如果使用random的随机的话，就需要设置</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span><span class="token comment"># 如果使用np.random的随机的话，</span>
    torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span>  <span class="token comment">#为CPU设置种子用于生成随机数,以使得结果是确定的</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span> <span class="token comment">#为当前GPU设置随机种子</span>
    cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment">#使用确定性卷积</span>
    <span class="token comment">#顾名思义，将这个 flag 置为True的话，每次返回的卷积算法将是确定的，即默认算法。</span>
    <span class="token comment">#如果配合上设置 Torch 的随机种子为固定值的话，应该可以保证每次运行网络的时候相同输入的输出是固定的</span>
    <span class="token comment">#如果使用多GPU,需要</span>
    <span class="token comment">#torch.cuda.manual_seed_all() 为所有GPU设置种子</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="torch-nn-functioal"><a href="#torch-nn-functioal" class="headerlink" title="torch.nn.functioal"></a>torch.nn.functioal</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/66782101">nn与nn.functional的区别</a></p>
<p><code>interpolate</code></p>
<p>下/上采样输入到给定的大小或给定的 scale_factor。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>
                                size<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                                scale_factor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                                mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">,</span>
                                align_corners<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment"># </span>
                                recompute_scale_factor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>upsample</code></p>
<p>将输入上采样到给定的大小或给定的scale_factor</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>upsample<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span><span class="token comment">#输入张量</span>
                             size<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#输出大小</span>
                             scale_factor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#变换因子</span>
                             mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">,</span><span class="token comment">#模式</span>
                             align_corners<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>avg_pool2d</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span><span class="token comment">#操作的Tensor</span>
                               kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> count_include_pad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> divisor_override<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span> → Tensor<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><code>max_pool2d</code></p>
<p><code>torch.nn.functional.adaptive_max_pool2d(*args, **kwargs)</code></p>
<p>在由几个输入平面组成的输入信号上应用2D自适应最大池化。</p>
<p><code>cross_entropy</code></p>
<p>See <code>CrossEntropyLoss</code></p>
<p><code>grid_sample</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">grid_sample<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> grid<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> padding_mode<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># input:input参数是输入特征图tensor，也就是特征图，可以是四维或者五维张量</span>
<span class="token comment"># 以四维形式为例(N,C,Hin,Win)，N可以理解为Batch_size,C可以理解为通道数，Hin和Win也就是特征图高和宽。</span>
<span class="token comment"># grid:包含输出特征图特征图的格网大小以及每个格网对应到输入特征图的采样点位，对应四维input，其张量形式为(N,Hout,Wout,2)，其中最后一维大小必须为2</span>
<span class="token comment"># 如果输入为五维张量，那么最后一维大小必须为3。为什么最后一维必须为2或者3？因为grid的最后一个维度实际上代表一个坐标(x,y)或者(xy,z)，</span>
<span class="token comment"># 对应到输入特征图的二维或三维特特征图的坐标维度，xy取值范围一般为[-1,1]，该范围映射到输入特征图的全图。</span>
<span class="token comment"># mode:选择采样方法，有三种内插算法可选，分别是'bilinear'双线性差值、'nearest'最邻近插值、'bicubic' 双三次插值.</span>
<span class="token comment"># padding_mode:为填充模式，即当(x,y)取值超过输入特征图采样范围，返回一个特定值，有'zeros' 、 'border' 、 'reflection'三种可选，一般用zero。</span>
<span class="token comment"># align_corners:为bool类型，指设定特征图坐标与特征值对应方式，设定为TRUE时，特征值位于像素中心。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="./medias/loading.gif" data-original="/pytorch-xue-xi/Users\17852\AppData\Roaming\Typora\typora-user-images\image-20220909153717935.png" alt="image-20220909153717935" style="zoom:67%;">

<p>经典的应用:pytorch光流函数warp</p>
<p><strong>关键点:图片的坐标加上光流即为在输出点的坐标</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">warp</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> flo<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        warp an image/tensor (im2) back to im1, according to the optical flow
        x: [B, C, H, W] (im2)
        flo: [B, 2, H, W] flow
        """</span>
        B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># mesh grid </span>
        xx <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>H<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        yy <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>W<span class="token punctuation">)</span>
        xx <span class="token operator">=</span> xx<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>H<span class="token punctuation">,</span>W<span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>B<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        yy <span class="token operator">=</span> yy<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>H<span class="token punctuation">,</span>W<span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>B<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        grid <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>xx<span class="token punctuation">,</span>yy<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
       

        x <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        grid <span class="token operator">=</span> grid<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        vgrid <span class="token operator">=</span> Variable<span class="token punctuation">(</span>grid<span class="token punctuation">)</span> <span class="token operator">+</span> flo <span class="token comment"># B,2,H,W</span>
        <span class="token comment">#图二的每个像素坐标加上它的光流即为该像素点对应在图一的坐标</span>

 <span class="token comment"># scale grid to [-1,1] </span>
 <span class="token comment">##2019 code</span>
        vgrid<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2.0</span><span class="token operator">*</span>vgrid<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token builtin">max</span><span class="token punctuation">(</span>W<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1.0</span> 
        <span class="token comment">#取出光流v这个维度，原来范围是0~W-1，再除以W-1，范围是0~1，再乘以2，范围是0~2，再-1，范围是-1~1</span>
        vgrid<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2.0</span><span class="token operator">*</span>vgrid<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token builtin">max</span><span class="token punctuation">(</span>H<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1.0</span> <span class="token comment">#取出光流u这个维度，同上</span>

        vgrid <span class="token operator">=</span> vgrid<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#from B,2,H,W -&gt; B,H,W,2，为什么要这么变呢？是因为要配合grid_sample这个函数的使用</span>
        output <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>grid_sample<span class="token punctuation">(</span>x<span class="token punctuation">,</span> vgrid<span class="token punctuation">,</span>align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        mask <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>grid_sample<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> vgrid<span class="token punctuation">,</span>align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

 <span class="token comment">##2019 author</span>
        mask<span class="token punctuation">[</span>mask<span class="token operator">&lt;</span><span class="token number">0.9999</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        mask<span class="token punctuation">[</span>mask<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>

 <span class="token comment">##2019 code</span>
 <span class="token comment"># mask = torch.floor(torch.clamp(mask, 0 ,1))</span>

 <span class="token keyword">return</span> output<span class="token operator">*</span>mask<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="Vision-functions"><a href="#Vision-functions" class="headerlink" title="Vision functions"></a>Vision functions</h4><p><code>pad</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>pad<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span>
                        pad<span class="token punctuation">,</span>
                        mode<span class="token operator">=</span><span class="token string">'constant'</span><span class="token punctuation">,</span>
                        value<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>从最后一个维度开始pad,例如只pad最后一个维度,pad的形式是(padding_left, padding_right);pad最后两个维度,pad的形式是</p>
<p>(padding_left, padding_right, padding_top, padding_bottom);(padding_left,padding_right,padding_top,padding_bottom,</p>
<p>padding_front,padding_back)。</p>
<p>假设原始是(2,3,4)</p>
<p>padding_left,padding_right是pad最后一个维度,即4这个维度</p>
<p>padding_top,padding_bottom是pad倒数第二个维度,即3这个维度</p>
<p>padding_front,padding_back是pad倒数第三个维度,即2这个维度</p>
<h3 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94019888">optim教程</a></p>
<p>是一个实现各种优化算法的包。已经支持了最常用的方法，接口也足够通用，因此将来还可以轻松地集成更复杂的方法。<br>$$<br>\begin{split}\begin{aligned} w_1 &amp;\leftarrow \left(1- \eta\lambda \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\ w_2 &amp;\leftarrow \left(1- \eta\lambda \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}\end{split}<br>$$<br><strong>整个包就是解决这一步的,就是参数优化</strong></p>
<p>To use <code>torch.optim</code> you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.</p>
<p>创建optim对象时，要给它一个包含模型参数的的可迭代对象(所有的都应该是 Variable)，然后指定learning rate,weight decay等参数.</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span>var1<span class="token punctuation">,</span> var2<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>注意：由于要把模型参数传给 optim ，所以如果要使用GPU时，要在把模型参数传给 optim之前写 model().cuda()，因为调用 .cuda() 前后不是一个参数对象，在此optimize期间，要保证 optimized parameters 在同一位置，不要 .cpu()或 .cuda() 乱用，注意下顺序 <strong>！！！有待验证！！！</strong></p>
<p>也支持为每个参数单独设置选项。若想这么做，不要直接传入Variable的iterable，而是传入dict的iterable。这种方法在对每层分别指定learning rate时很有用:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span>
                <span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>base<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                <span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>classifier<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">}</span>
            <span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>上面这样写，表示 $model.base$ 的 $lr$ 是 1e-2，$model.classifier$ 的 lr 是 $1e-3$，$momentum=0.9$ 同时用于这两个参数</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Optimizer</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> defaults<span class="token punctuation">)</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_log_api_usage_once<span class="token punctuation">(</span><span class="token string">"python.optimizer"</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>defaults <span class="token operator">=</span> defaults

        self<span class="token punctuation">.</span>_hook_for_profile<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">"params argument given to the optimizer should be "</span>
                            <span class="token string">"an iterable of Tensors or dicts, but got "</span> <span class="token operator">+</span>
                            torch<span class="token punctuation">.</span>typename<span class="token punctuation">(</span>params<span class="token punctuation">)</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>state <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">dict</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>param_groups <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 在源码中有一个参数param_groups来存储params</span>
        <span class="token comment"># 以resnet18为例,params是generator,list完之后维度为(64,),param_groups[0]的维度为torch.Size([64, 3, 7, 7])</span>
        param_groups <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>params<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>param_groups<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"optimizer got an empty parameter list"</span><span class="token punctuation">)</span>
        <span class="token comment"># 成为一个字典</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>param_groups<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            param_groups <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> param_groups<span class="token punctuation">}</span><span class="token punctuation">]</span>
		
        <span class="token keyword">for</span> param_group <span class="token keyword">in</span> param_groups<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>add_param_group<span class="token punctuation">(</span>param_group<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="torch-optim-Optimizer"><a href="#torch-optim-Optimizer" class="headerlink" title="torch.optim.Optimizer"></a>torch.optim.Optimizer</h4><p>所有优化器的基类。</p>
<p><code>zero_grad(set_to_none: bool = False)</code></p>
<p><a target="_blank" rel="noopener" href="https://www.jb51.net/article/189433.htm">zero_grad教程</a></p>
<p>设置被优化的张量的梯度为0,显然，我们进行下一次batch梯度计算的时候，前一个batch的梯度计算结果，没有保留的必要了。所以在下一次梯度更新的时候，先使用optimizer.zero_grad把梯度信息设置为0。</p>
<p>唯一一个参数意思是不设为0而设置为None</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">zero_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> set_to_none<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">r"""Sets the gradients of all optimized :class:`torch.Tensor` s to zero.

        Args:
            set_to_none (bool): instead of setting to zero, set the grads to None.
                This will in general have lower memory footprint, and can modestly improve performance.
                However, it changes certain behaviors. For example:
                1. When the user tries to access a gradient and perform manual ops on it,
                a None attribute or a Tensor full of 0s will behave differently.
                2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\ s
                are guaranteed to be None for params that did not receive a gradient.
                3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None
                (in one case it does the step with a gradient of 0 and in the other it skips
                the step altogether).
        """</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"_zero_grad_profile_name"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_hook_for_profile<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>profiler<span class="token punctuation">.</span>record_function<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_zero_grad_profile_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">for</span> group <span class="token keyword">in</span> self<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>
                <span class="token keyword">for</span> p <span class="token keyword">in</span> group<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
                    <span class="token comment"># p.grad可以直接查看梯度，只有不是None的时候才设为0,是None的话保持None就行</span>
                    <span class="token comment"># a = [None, None] a[0]=1 =&gt;a = [1, None]</span>
                    <span class="token keyword">if</span> p<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                        <span class="token keyword">if</span> set_to_none<span class="token punctuation">:</span>
                            p<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>
                        <span class="token keyword">else</span><span class="token punctuation">:</span>
                            <span class="token keyword">if</span> p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>grad_fn <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                                p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span>
                            <span class="token keyword">else</span><span class="token punctuation">:</span>
                                p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                            p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>optimizer执行的两种方式:<code>optimizer.step()</code>和``optimizer.step(closure)`，</p>
<p>所有的optim 都实现了前一种方法，第一种方法会更新所有参数，这是大多数 optim 都支持的方法，只要损失反向传播后就可以调用此函数:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> <span class="token builtin">input</span><span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
    <span class="token comment"># 求出每一阶段的损失</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 更新参数</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>关于第二种：optimizer.step(closure) 一些优化算法，例如 Conjugate Gradient 和 LBFGS 需要重复多次计算，因此你需要传入一个 closure 去允许它们重新计算你的模型。这个closure 应当清空梯度，计算损失，然后返回</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> <span class="token builtin">input</span><span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">closure</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> loss
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span>closure<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>torch.optim.RMSprop</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>RMSprop<span class="token punctuation">(</span>params<span class="token punctuation">,</span><span class="token comment">#用于优化的参数iterable或定义参数组的dicts</span>
                    lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
                    alpha<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">,</span>
                    eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span>
                    weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment">#权重衰减</span>
                    momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment">#动量</span>
                    centered<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>torch.optim.SGD</code></p>
<p>实现随机梯度下降(可选动量)。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span><span class="token comment"># 待优化参数的可迭代对象或者是定义了参数组的dict</span>
                lr<span class="token operator">=</span><span class="token operator">&lt;</span>required parameter<span class="token operator">&gt;</span><span class="token punctuation">,</span><span class="token comment">#学习率</span>
                momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment">#动量</span>
                dampening<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment">#</span>
                weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token comment">#权重衰减</span>
                nesterov<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment">#</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> loss_fn<span class="token punctuation">(</span>model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 随机梯度下降,就比较依赖于数据的随机程度,如果不对数据进行打乱处理,可能异常值集中在数据某一块,会对算法收敛拟合造成干扰。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>torch.optim.Adam</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>params<span class="token punctuation">,</span>
                 lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span>
                 betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment">#</span>
                 eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span>
                 weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                 amsgrad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="How-to-adjust-learning-rate"><a href="#How-to-adjust-learning-rate" class="headerlink" title="How to adjust learning rate?"></a>How to adjust learning rate?</h3><p><code>torch.optim.lr_scheduler</code> provides several methods to adjust the learning rate based on the number of epochs. <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau"><code>torch.optim.lr_scheduler.ReduceLROnPlateau</code></a> allows dynamic learning rate reducing based on some validation measurements.</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Learning rate scheduling should be applied after optimizer’s update:</span>
model <span class="token operator">=</span> <span class="token punctuation">[</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
optimizer <span class="token operator">=</span> SGD<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
scheduler <span class="token operator">=</span> ExponentialLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> <span class="token builtin">input</span><span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># Most learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). </span>
<span class="token comment"># The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it.</span>
model <span class="token operator">=</span> <span class="token punctuation">[</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
optimizer <span class="token operator">=</span> SGD<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
scheduler1 <span class="token operator">=</span> ExponentialLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
scheduler2 <span class="token operator">=</span> MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> <span class="token builtin">input</span><span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    scheduler1<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    scheduler2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># In many places in the documentation, we will use the following template to refer to schedulers algorithms.</span>
scheduler <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    validate<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>lr_scheduler.LambdaLR</code></p>
<p>将每个参数组的学习率设置为初始 lr 乘以给定函数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Assuming optimizer has two groups.</span>
lambda1 <span class="token operator">=</span> <span class="token keyword">lambda</span> epoch<span class="token punctuation">:</span> epoch <span class="token operator">//</span> <span class="token number">30</span>
lambda2 <span class="token operator">=</span> <span class="token keyword">lambda</span> epoch<span class="token punctuation">:</span> <span class="token number">0.95</span> <span class="token operator">**</span> epoch
scheduler <span class="token operator">=</span> LambdaLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> lr_lambda<span class="token operator">=</span><span class="token punctuation">[</span>lambda1<span class="token punctuation">,</span> lambda2<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    validate<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="torch-utils-tensorboard"><a href="#torch-utils-tensorboard" class="headerlink" title="torch.utils.tensorboard"></a>torch.utils.tensorboard</h3><p>一旦你安装了TensorBoard，这些工具可以让你将PyTorch模型和指标记录到一个目录中，以便在TensorBoard UI中可视化。标量、图像、直方图、图形和嵌入可视化都支持PyTorch模型和张量以及Caffe2 网络和blobs。</p>
<p>安装<code>pip install tensorboardX</code> 还需要安装tensorflow <code>pip install tensorflow</code></p>
<p>除此之外还需要设置隧道映射,多换几个端口试试</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/bigbennyguo/article/details/87956434">tensorboardX介绍</a></p>
<h4 id="SummaryWriter"><a href="#SummaryWriter" class="headerlink" title="SummaryWriter"></a>SummaryWriter</h4><p><code>CLASS torch.utils.tensorboard.writer.SummaryWriter</code></p>
<p>直接将条目写入log_dir中的事件文件中，以供TensorBoard使用。</p>
<p>SummaryWriter类提供了一个高级API，用于在<strong>给定目录中创建事件文件并向其添加摘要和事件</strong>。该类异步更新文件内容。这<strong>允许训练程序调用方法直接从训练循环向文件添加数据，而不会减慢训练速度。</strong></p>
<p><code>__init__</code>创建一个SummaryWriter，将事件和摘要写入事件文件中。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter
__init__<span class="token punctuation">(</span>log_dir<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#保存目录位置，默认值是'run/CURRENT_DATETIME_HOSTNAME',所以每次运行后都会更改，日期时间肯定会变</span>
         comment<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span><span class="token comment">#注解添加到默认log_dir的后缀，若log_dir被指定，则此参数不起作用。</span>
         purge_step<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#</span>
         max_queue<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token comment">#在一个'add'调用强制刷新磁盘之前，挂起事件和汇总的队列的大小。默认为10项。</span>
         flush_secs<span class="token operator">=</span><span class="token number">120</span><span class="token punctuation">,</span><span class="token comment">#将挂起事件和摘要刷新到磁盘的频率(以秒为单位)。默认每120s一次</span>
         filename_suffix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span><span class="token comment">#添加到log_dir目录中所有事件文件名的后缀。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<p>SummaryWriter类是您记录TensorBoard使用和可视化数据的主要入口。例如:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torchvision
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms

<span class="token comment"># Writer will output to ./runs/ directory by default</span>
writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token punctuation">)</span>

transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
trainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">'mnist_train'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>
trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet50<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token comment"># Have ResNet model take in grayscale rather than RGB</span>
model<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
images<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span><span class="token punctuation">)</span>

grid <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
writer<span class="token punctuation">.</span>add_image<span class="token punctuation">(</span><span class="token string">'images'</span><span class="token punctuation">,</span> grid<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
writer<span class="token punctuation">.</span>add_graph<span class="token punctuation">(</span>model<span class="token punctuation">,</span> images<span class="token punctuation">)</span>
writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#这可以通过TensorBoard进行可视化，TensorBoard安装和运行:</span>
pip install tensorboard
tensorboard <span class="token operator">-</span><span class="token operator">-</span>logdir<span class="token operator">=</span><span class="token comment">#包含记录文件的文件夹路径</span>
<span class="token comment">#记录文件名类似如下:events.out.tfevents.1616293286.vision806-desktop</span>
<span class="token comment"># 默认的终端号是6006，要是一台服务器多人使用，很麻烦，可以指定端口，输入</span>
tensorboard <span class="token operator">-</span><span class="token operator">-</span>logdir<span class="token operator">=</span>logs <span class="token operator">-</span><span class="token operator">-</span>port<span class="token operator">=</span><span class="token number">6007</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>一个实验可以记录很多信息。为了避免UI的混乱和更好的结果聚类，我们可以通过分级命名来对图进行分组。例如， Loss/train 和Loss/test 被分组在一起，Accuracy/train和Accuarcy/test被分组在一起。</p>
<p><code>add_scalar</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">add_scalar<span class="token punctuation">(</span>tag<span class="token punctuation">:</span>string<span class="token punctuation">,</span><span class="token comment"># 数据标识,train/loss,分成了两级</span>
           scalar_value<span class="token punctuation">,</span><span class="token comment"># 数字常量值,一定要是float类型</span>
           <span class="token comment"># 如果是 Pytorch scalar tensor,则需要调用.item()方法获取其数值。</span>
           global_step<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token comment"># 训练的step,作为横坐标</span>
           walltime<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token comment"># 记录发生的时间，默认为time.time()</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>添加标量,一般使用其<strong>记录训练过程的 loss、accuracy、learning rate 等数值的变化，直观地监控训练过程。</strong></p>
<p>如果tag相同标量会被放在一个图里，如<code>writer.add_scalar(‘y=2x’, i * 2, i)</code>和<code>writer.add_scalar('y=2x, i * i, i)</code></p>
<p><code>add_image</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">add_image<span class="token punctuation">(</span>self<span class="token punctuation">,</span> tag<span class="token punctuation">,</span><span class="token comment">#就是保存图片的名称</span>
          img_tensor<span class="token punctuation">,</span><span class="token comment">#图形:torch.Tensor numpy.array or string</span>
          global_step<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment"># 训练的step 作为横坐标</span>
          walltime<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment"># 记录发生的时间，默认为time.time()</span>
          dataformats<span class="token operator">=</span>‘CHW’<span class="token punctuation">)</span><span class="token comment">#默认为CHW tensor是CHW numpy是HWC</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>也可以画特征图的变化</p>
<h3 id="visdom"><a href="#visdom" class="headerlink" title="visdom"></a>visdom</h3><p>必须提前开启:</p>
<p><code>python -m visdom server -p 10086</code></p>
<h3 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h3><p>PyTorch数据加载工具的核心是torch.utils.data.DataLoader类。它表示一个数据集上的Python可迭代对象。</p>
<p>pytorch输入数据PipeLine一般遵循一个“三步走”的策略，一般pytorch 的数据加载到模型的操作顺序是这样的：<br>① 创建一个 Dataset 对象。必须实现<code>__len__()</code>、<code>__getitem__()</code>这两个方法，这里面会用到transform对数据集进行扩充。<br>② 创建一个 DataLoader 对象。它是对DataSet对象进行迭代的，一般不需要事先里面的其他方法了。<br>③ 循环遍历这个 DataLoader 对象。将img, label加载到模型中进行训练</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">dataset <span class="token operator">=</span> MyDataset<span class="token punctuation">(</span><span class="token punctuation">)</span>           <span class="token comment"># 第一步：构造Dataset对象</span>
dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token comment"># 第二步：通过DataLoader来构造迭代对象</span>
 
num_epoches <span class="token operator">=</span> <span class="token number">100</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epoches<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># 第三步：逐步迭代数据</span>
    <span class="token keyword">for</span> img<span class="token punctuation">,</span> label <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
        <span class="token comment"># 训练代码</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Map-style-datasets"><a href="#Map-style-datasets" class="headerlink" title="Map-style datasets"></a><strong>Map-style datasets</strong></h4><p>map风格的数据集是一个实现<code>__getitem__()</code>和<code>__len__()</code>协议的数据集，并表示从(可能非整数)索引/键到数据样本的映射。</p>
<p>例如，当使用dataset[idx]访问这样的数据集时，可以从磁盘上的文件夹中读取idx-th映像及其对应的标签。</p>
<p>查看Dataset了解更多细节。</p>
<h4 id="Iterable-style-datasets"><a href="#Iterable-style-datasets" class="headerlink" title="Iterable-style datasets"></a>Iterable-style datasets</h4><p>iterable风格的数据集是IterableDataset的一个子类的实例，它实现了<code>__iter__()</code>协议，并表示数据样本上的一个iterable。这种类型的数据集特别适合于这样的情况:随机读取代价很高，甚至不太可能，而且批大小取决于所获取的数据。</p>
<p>例如，当调用iter(dataset)时，这样的数据集可以返回从数据库、远程服务器甚至实时生成的日志读取的数据流。</p>
<p>查看IterableDataset了解更多细节。</p>
<h4 id="Data-Loading-Order-and-Sampler"><a href="#Data-Loading-Order-and-Sampler" class="headerlink" title="Data Loading Order and Sampler"></a>Data Loading Order and Sampler</h4><h4 id="Memory-Pinning"><a href="#Memory-Pinning" class="headerlink" title="Memory Pinning"></a>Memory Pinning</h4><p>pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。</p>
<p>主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。</p>
<p>而显卡中的显存全部是锁页内存！</p>
<p>当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。</p>
<h4 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a>torch.utils.data.DataLoader</h4><p>数据加载程序。<strong>组合数据集dataset和采样器sampler</strong>,并提供给定数据集上的迭代。</p>
<p>在训练模型时使用到此函数，用来把训练数据分成多个小组，此函数每次抛出一组数据。直至把所有的数据都抛出。就是做一个数据的初始化。</p>
<p>DataLoader支持map-style和iterable-style的数据集，具有单进程或多进程加载、自定义加载顺序和可选的自动批处理(整理)和内存固定.</p>
<p><strong>DataLoader本质是一个可迭代对象，所以:</strong></p>
<ol>
<li>可以使用<code>for inputs, labels in dataloaders</code>进行可迭代对象的访问</li>
<li>先使用iter对dataloader进行第一步包装，使用<code>iter(dataloader)</code>返回的是一个迭代器，然后就可以可以使用next访问了。Dataloader的<code>__iter__()</code>根据num_workers的数量返回单线程或多线程的迭代器</li>
<li>我们一般不需要再自己去实现DataLoader的方法了，只需要在构造函数中指定相应的参数即可，比如常见的batch_size，shuffle等等参数。所以使用DataLoader十分简洁方便。</li>
<li>DataLoader实际上一个较为高层的封装类，它的功能都是通过更底层的<code>_DataLoader</code>来完成的，但是<code>_DataLoader</code>类较为低层，这里就不再展开叙述了。DataLoaderIter就是<code>_DataLoaderIter</code>的一个框架, 用来传给<code>_DataLoaderIter</code> 一堆参数, 并把自己装进DataLoaderIter 里。</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>Dataset<span class="token punctuation">[</span>T_co<span class="token punctuation">]</span><span class="token punctuation">,</span>
                            batch_size<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
                            shuffle<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
                    <span class="token comment">#表示每一个epoch之后是否对样本进行随机打乱,所有的先打乱,再取batch.具体解析见下文</span>
                            sampler<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>Sampler<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                            <span class="token comment">#自定义从数据集中抽取样本的策略，如果指定这个参数，那么shuffle必须为False。</span>
                            batch_sampler<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>Sampler<span class="token punctuation">[</span>Sequence<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                            num_workers<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token comment">#要使用多少子进程装载数据。0表示数据将在主进程中加载。</span>
                            collate_fn<span class="token punctuation">:</span> Callable<span class="token punctuation">[</span>List<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">,</span> Any<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                   <span class="token comment"># 合并一个list的samples以形成mini-batch的Tensors</span>
                   <span class="token comment"># collate_fn这个函数的输入就是一个list，list的长度是一个batch size，list中的每个元素都是__getitem__得到的结果。</span>
                            pin_memory<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment">#是否锁页内存</span>
                            drop_last<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
<span class="token comment">#如果数据集大小不能被批大小整除，则设置为True可删除最后一个不完整的批处理。</span>
<span class="token comment">#如果为False，并且dataset的大小不能被batch-size整除，那么最后一批将变小。(默认值:False)</span>
                            timeout<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
           <span class="token comment">#timeout (numeric, optional): 如果是正数，表明等待从worker进程中收集一个batch等待的时间，若超出设定的时间还没有收集到，那就不收集这个内容了。这个numeric应总是大于等于0。默认为0</span>
                            worker_init_fn<span class="token punctuation">:</span> Callable<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                            multiprocessing_context<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                            generator<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span>
                            prefetch_factor<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>
                            persistent_workers<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">"""
    批训练，把数据变成一小批一小批数据进行训练。
    DataLoader就是用来包装所使用的数据，每次抛出一批数据
"""</span>
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data

BATCH_SIZE <span class="token operator">=</span> <span class="token number">5</span>

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token comment"># 把数据放在数据库中</span>
torch_dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>
    <span class="token comment"># 从数据库中每次抽出batch size个样本</span>
    dataset<span class="token operator">=</span>torch_dataset<span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span>
    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">show_batch</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># training</span>


            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"steop:{}, batch_x:{}, batch_y:{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>step<span class="token punctuation">,</span> batch_x<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    show_batch<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#loader在这里就是迭代器,在for里面一旦取用自动更新</span>
<span class="token comment">#for data in testloader: 这里的data就是一个batch的数据,是一个tuple,既有train_X,也有train_Y.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27825451/article/details/96130126">pytorch数据预处理三剑客之Dataset，DataLoader，Transform</a></p>
<p>上面这篇讲的非常好</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/hesse-summer/p/11343870.html">num_worker的作用</a></p>
<p><strong>为什么机器学习需要打乱数据?</strong></p>
<p>防止数据按一定规律排列,这样神经网络学习时会把这种规律当做一种特征学习,从而过拟合。这样做使得数据更接近于真实分布。</p>
<p>比如随机梯度下降,就比较依赖于数据的随机程度,如果不对数据进行打乱处理,可能异常值集中在数据某一块,会对算法收敛拟合造成干扰。</p>
<h4 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h4><p>表示一个Dataset的抽象类。</p>
<p><strong>所有表示从键到数据样本映射的数据集都应该子类化它</strong>。所有子类都应该覆盖<code>__getitem__()</code>，从而支持获取一个给定键的数据样本。子类也可以选择性地覆盖<code>__len__()</code>，许多Sampler实现和DataLoader默认选项都希望它返回数据集的大小。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#dataset的抽象父类定义如下</span>
<span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> T_co<span class="token punctuation">:</span>
        <span class="token keyword">raise</span> NotImplementedError
<span class="token keyword">def</span> <span class="token function">__add__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">:</span> <span class="token string">'Dataset[T_co]'</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token string">'ConcatDataset[T_co]'</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> ConcatDataset<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">,</span> other<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>__init__(self)</code>:主要是数据的获取，比如从某个文件中获取</p>
<p><code>__len__(self)</code>:整个数据集的长度</p>
<p><code>__getitem__(self,index)</code>:这个是最重要的，一般情况下它会包含以下几个业务需要处理</p>
<ol>
<li>比如如果我们需要在读取数据的同时对图像进行增强的话，当然，图像增强的方法可以使用Pytorch内置的图像增强方式，也可以使用自定义或者其他的图像增强库,这个很灵活。</li>
<li>在Pytorch中得到的图像必须是tensor，也就是说我们必须要将数据格式转化成pytorch的tensor格式才行。</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># coding: utf-8</span>
 
<span class="token keyword">import</span> os
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image
<span class="token keyword">import</span> cv2
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
 
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToTensor
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms
 
<span class="token keyword">import</span> random
 
<span class="token keyword">class</span> <span class="token class-name">LaneDataSet</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataset<span class="token punctuation">,</span> transform<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''
        param：
            detaset: 实际上就是tusimple数据集的三个文本文件train.txt、val.txt、test.txt三者的文件路径
            transform: 决定是否进行变换,它其实是一个函数或者是几个函数的组合
        构造三个列表，存储每一张图片的文件路径          
        '''</span>
        self<span class="token punctuation">.</span>_gt_img_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>_gt_label_binary_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>_gt_label_instance_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform
 
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token builtin">file</span><span class="token punctuation">:</span>  <span class="token comment"># 打开其实是那个 training下面的那个train.txt 文件</span>
            <span class="token keyword">for</span> _info <span class="token keyword">in</span> <span class="token builtin">file</span><span class="token punctuation">:</span>
                info_tmp <span class="token operator">=</span> _info<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
                self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info_tmp<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info_tmp<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info_tmp<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
 
        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">)</span>
 
        self<span class="token punctuation">.</span>_shuffle<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">_shuffle</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 将gt_image、binary_image、instance_image三者所对应的图片路径组合起来，再进行随机打乱</span>
    c <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">)</span><span class="token punctuation">)</span>
    random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>c<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_gt_label_instance_list <span class="token operator">=</span> <span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span>c<span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">)</span> \
               <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">)</span>
 
    <span class="token comment"># 读取图片</span>
    img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_img_list<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>IMREAD_COLOR<span class="token punctuation">)</span> <span class="token comment">#真实图片 (720,1280,3)</span>
 
    label_instance_img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_instance_list<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>IMREAD_UNCHANGED<span class="token punctuation">)</span> <span class="token comment"># instance图片 （720,1280）</span>
 
    label_binary_img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_gt_label_binary_list<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>IMREAD_GRAYSCALE<span class="token punctuation">)</span> <span class="token comment">#binary图片 （720,1280)</span>
 
    <span class="token comment"># optional transformations,裁剪成（256,512）</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">:</span>
        img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        label_binary_img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>label_binary_img<span class="token punctuation">)</span>
        label_instance_img <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>label_instance_img<span class="token punctuation">)</span>
 
    img <span class="token operator">=</span> img<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#（3,720,1280） 这里都没有问题</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>img<span class="token punctuation">,</span> label_binary_img<span class="token punctuation">,</span> label_instance_img<span class="token punctuation">)</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="torch-utils-data-TensorDataset-tensors"><a href="#torch-utils-data-TensorDataset-tensors" class="headerlink" title="torch.utils.data.TensorDataset(*tensors)"></a>torch.utils.data.TensorDataset(*tensors)</h4><p>TensorDataset 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能。该类通过每一个 tensor 的第一个维度进行索引。因此，该类中的 tensor 第一维度必须相等。</p>
<p>举个例子,六张图片,六个label,维度分别是(6,H,W,C)和(6,)</p>
<h4 id="torch-utils-data-Concat-datasets"><a href="#torch-utils-data-Concat-datasets" class="headerlink" title="torch.utils.data.Concat(datasets)"></a>torch.utils.data.Concat(datasets)</h4><p>连接多个数据集产生一个新的数据集,该类用于组装不同的现有数据集。</p>
<p>datasets:(<em>sequence</em>) – List of datasets to be concatenated</p>
<h4 id="torch-utils-data-Subset"><a href="#torch-utils-data-Subset" class="headerlink" title="torch.utils.data.Subset"></a>torch.utils.data.Subset</h4><p><code>torch.utils.data.random_split</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>random_split<span class="token punctuation">(</span>dataset<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>Dataset<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment">#要被划分的数据集</span>
                              lengths<span class="token punctuation">:</span> Sequence<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token comment">#要生成的切片长度</span>
                              generator<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>Generator<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">&lt;</span>torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>Generator <span class="token builtin">object</span><span class="token operator">&gt;</span><span class="token punctuation">)</span> → List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>Subset<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment">#用于随机排列的生成器。</span>
<span class="token comment">#eg:random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))</span>
<span class="token comment">#eg:train, val = random_split(dataset, [n_train, n_val])</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="torch-utils-data-Sampler"><a href="#torch-utils-data-Sampler" class="headerlink" title="torch.utils.data.Sampler"></a>torch.utils.data.Sampler</h4><p>所有采样器的基类。</p>
<p>每个采样器子类都必须提供一个<code>__iter__()</code>方法，提供一种遍历数据集元素索引的方法，以及一个<code>__len__()</code>方法，该方法返回返回的迭代器的长度。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#pytorch采样器</span>
<span class="token keyword">class</span> <span class="token class-name">Sampler</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
<span class="token keyword">class</span> <span class="token class-name">SequentialSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">class</span> <span class="token class-name">RandomSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">class</span> <span class="token class-name">SubsetRandomSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">class</span> <span class="token class-name">WeightedRandomSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">class</span> <span class="token class-name">BatchSampler</span><span class="token punctuation">(</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token comment">#Dataloader中的采样器</span>
<span class="token keyword">if</span> sampler <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>  <span class="token comment"># give default samplers</span>
	<span class="token keyword">if</span> self<span class="token punctuation">.</span>_dataset_kind <span class="token operator">==</span> _DatasetKind<span class="token punctuation">.</span>Iterable<span class="token punctuation">:</span>
		<span class="token comment"># See NOTE [ Custom Samplers and IterableDataset ]</span>
		sampler <span class="token operator">=</span> _InfiniteConstantSampler<span class="token punctuation">(</span><span class="token punctuation">)</span>
	<span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># map-style</span>
		<span class="token keyword">if</span> shuffle<span class="token punctuation">:</span>
			sampler <span class="token operator">=</span> RandomSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
		<span class="token keyword">else</span><span class="token punctuation">:</span>
			sampler <span class="token operator">=</span> SequentialSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/houjun/p/10406640.html">torchvision简介</a></p>
<p>该库是Pytorch项目的一部分。安装pytorch时，torchvision独立于torch。torchvision包由流行的数据集（torchvision.datasets）、模型架构(torchvision.models)和用于计算机视觉的常见图像转换组成t(torchvision.transforms)。</p>
<h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torchvision
mnist <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">"path/to/mnist/"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transforms<span class="token punctuation">,</span> target_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span><span class="token comment">#存放training.pt和test.pt的root directory</span>
                           train<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span><span class="token comment">#从training.pt创建数据集,否则从test.pt</span>
                           transform<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>Callable<span class="token punctuation">,</span> NoneType<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span><span class="token comment">#转换操作</span>
                           target_transform<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>Callable<span class="token punctuation">,</span> NoneType<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                           download<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span> → <span class="token boolean">None</span><span class="token comment">#下载到本地并存放到root directory</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torchvision
vgg16 <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment">#pretrained=True加载别人预训练好的模型,否则就是权重随机初始化的模型</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38410428/article/details/94719553">transforms的二十二个方法</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27825451/article/details/97145592?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control">第三篇-pytorch数据预处理三剑客</a></p>
<p>变换是常见的图像变换。它们可以使用<code>Compose</code>链接在一起。此外，还有<code>torchvision.transforms.functional</code>模块。<strong>Functional transforms可以对转换进行细粒度控制</strong>。如果您必须构建更复杂的转换管道（例如，在分割任务的情况下），这将非常有用。</p>
<p>所有的转换都接受<strong>PIL Image，Tensor Image或batch of tensor Image</strong>作为输入。Tensor Image是一个具有(C, H, W)形状的张量，其中C是通道，H和W是图像的高度和宽度。Batch of Tensor Images是(B, C, H, W) 形状的张量，其中B是一个Batch中图像的个数。对Batch of Tensor Image应用确定或随机变换，就能对这批图像的所有图像进行相同的变换。</p>
<p>注意事项：</p>
<p>（1）transfroms中的数据增强操作针对的是pillow的Image图像格式，而我们很多时候在使用opencv读进去的又是ndarray格式，所以需要第一步先将ndarray转化成Image格式，即<code>transforms.ToPILImage()</code>.</p>
<p>（2）但是我们后需要的数据又是需要ndarray格式或者是tensor格式，故而有需要将Image转换回来，即<code>transforms.ToTensor()</code>。</p>
<p>自从v0.8.0以来，所有的<strong>随机转换都使用torch默认的随机生成器来采样随机参数</strong>。这是一个破坏后项兼容的更改，后向兼容是指向低版本兼容，用户应该设置随机状态如下:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Previous versions</span>
<span class="token comment"># import random</span>
<span class="token comment"># random.seed(12)</span>

<span class="token comment"># Now</span>
<span class="token keyword">import</span> torch
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">17</span><span class="token punctuation">)</span>
<span class="token comment">#请记住，pytorch随机数生成器和Python随机数生成器的相同种子不会产生相同的结果。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>torchvision.transforms.Compose(transforms)</code><br>将几个变换组合在一起</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Compose</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
 
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transforms<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>transforms <span class="token operator">=</span> transforms
 
    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#self.transform(img)实际上是一个函数调用形式,果然实现了__call__()s</span>
        <span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>transforms<span class="token punctuation">:</span><span class="token comment">#从这可以看出,传入的是一个容器,列表就可以</span>
            img <span class="token operator">=</span> t<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        <span class="token keyword">return</span> img <span class="token comment">#返回的直接就是img,注意,处理的是单张img,返回的也是单张img</span>
 
    <span class="token keyword">def</span> <span class="token function">__repr__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        format_string <span class="token operator">=</span> self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__ <span class="token operator">+</span> <span class="token string">'('</span>
        <span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>transforms<span class="token punctuation">:</span>
            format_string <span class="token operator">+=</span> <span class="token string">'\n'</span>
            format_string <span class="token operator">+=</span> <span class="token string">'    {0}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span>
        format_string <span class="token operator">+=</span> <span class="token string">'\n)'</span>
        <span class="token keyword">return</span> format_string
    
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>     transforms<span class="token punctuation">.</span>CenterCrop<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>     transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="裁剪-Crop"><a href="#裁剪-Crop" class="headerlink" title="裁剪(Crop)"></a>裁剪(Crop)</h4><h4 id="翻转和旋转-Flip-and-Rotation"><a href="#翻转和旋转-Flip-and-Rotation" class="headerlink" title="翻转和旋转(Flip and Rotation)"></a>翻转和旋转(Flip and Rotation)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>



<h4 id="图像变换-resize"><a href="#图像变换-resize" class="headerlink" title="图像变换(resize)"></a>图像变换(resize)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 1.转换通道顺序:HWC-&gt;CHW</span>
<span class="token comment"># 2.将PIL Image或者 ndarray 转换为tensor float</span>
<span class="token comment"># 3.归一化至[0-1] 注意事项：归一化至[0-1]是直接除以255，若自己的ndarray数据尺度有变化，则需要自行修改。</span>
torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token punctuation">,</span> std<span class="token punctuation">)</span>
<span class="token comment">#用平均值和标准偏差归一化张量图像。给定mean：(M1,…,Mn)和std：(S1,…,Sn)对于n通道，此变换将标准化输入的每个通道，torch.*Tensor即 input[channel] = (input[channel] - mean[channel]) / std[channel]</span>
<span class="token comment">#mean(sequence)-每个通道的均值序列。</span>
<span class="token comment">#std(sequence)-每个通道的标准偏差序列。</span>
torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToPILImage<span class="token punctuation">(</span>mode<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape H x W x C to a PIL Image while preserving the value range.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>对transforms操作，使数据增强更灵活</p>
<p>Normalize参数解惑<a target="_blank" rel="noopener" href="https://blog.csdn.net/xys430381_1/article/details/85724668">https://blog.csdn.net/xys430381_1/article/details/85724668</a></p>
<h4 id="functional"><a href="#functional" class="headerlink" title="functional"></a>functional</h4><p><code>torchvision.transforms.functional.adjust_gamma(img: torch.Tensor, gamma: float, gain: float = 1) → torch.Tensor</code></p>
<p>对图片进行gamma校正<br>$$<br>I_{out}=255\cdot gain \cdot (\frac{I_{in}}{255})^y<br>$$<br>img:PIL Image或Tensor </p>
<h3 id="torchvision-utils"><a href="#torchvision-utils" class="headerlink" title="torchvision.utils"></a>torchvision.utils</h3><p><code>torchvision.utils.make_grid()</code></p>
<p>制作图像网格</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>tensor<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              <span class="token comment">#4D mini-batch Tensor of shape (B x C x H x W) or a list of images all of the same size.</span>
                            nrow<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">,</span>
              <span class="token comment">#网格中每一行中显示的图像数,最终尺寸为(B/nrow, nrow)</span>
                            padding<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>
              <span class="token comment">#子图像与子图像之间的pad有多宽。</span>
                            normalize<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
              <span class="token comment">#If True, 归一化图像到(0, 1)区间</span>
                            value_range<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span> NoneType<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
              <span class="token comment"># 用来normalize</span>
                            scale_each<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
                            pad_value<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
                            <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> → torch<span class="token punctuation">.</span>Tensor
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>torchvision.utils.save_image()</code></p>
<h2 id="Docs"><a href="#Docs" class="headerlink" title="Docs"></a>Docs</h2><h3 id="Features-for-large-scale-deployments"><a href="#Features-for-large-scale-deployments" class="headerlink" title="Features for large-scale deployments"></a>Features for large-scale deployments</h3><h4 id="API-usage-logging"><a href="#API-usage-logging" class="headerlink" title="API usage logging"></a>API usage logging</h4><p>When running in a broader ecosystem, for example in managed job scheduler, it’s often useful to track which binaries invoke particular PyTorch APIs. There exists simple instrumentation injected at several important API points that triggers a given callback. Because usually PyTorch is invoked in <em>one-off 一次性的</em> python scripts, the callback fires only once for a given process for each of the APIs.</p>
<p>Note for developers: new API trigger points can be added in code with <code>C10_LOG_API_USAGE_ONCE("my_api")</code> in C++ or <code>torch._C._log_api_usage_once("my.api")</code> in Python.</p>
<p><code>eg:torch._C._log_api_usage_once("python.optimizer")</code></p>
<p><strong>回调函数:被中间函数回调的函数</strong></p>
<blockquote>
<p>作者：no.body<br>链接：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/19801131/answer/27459821">https://www.zhihu.com/question/19801131/answer/27459821</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p><strong>什么是回调函数？</strong></p>
<p>我们绕点远路来回答这个问题。</p>
<p>编程分为两类：系统编程（system programming）和应用编程（application programming）。所谓系统编程，简单来说，就是编写<strong>库</strong>；而应用编程就是利用写好的各种库来编写具某种功用的程序，也就是<strong>应用</strong>。系统程序员会给自己写的库留下一些接口，即API（application programming interface，应用编程接口），以供应用程序员使用。所以在抽象层的图示里，库位于应用的底下。</p>
<p>当程序跑起来时，一般情况下，应用程序（application program）会时常通过API调用库里所预先备好的函数。但是有些库函数（library function）却要求应用先传给它一个函数，好在合适的时候调用，以完成目标任务。这个被传入的、后又被调用的函数就称为<strong>回调函数</strong>（callback function）。</p>
<p>打个比方，有一家旅馆提供叫醒服务，但是要求旅客自己决定叫醒的方法。可以是打客房电话，也可以是派服务员去敲门，睡得死怕耽误事的，还可以要求往自己头上浇盆水。这里，“叫醒”这个行为是旅馆提供的，相当于库函数，但是叫醒的方式是由旅客决定并告诉旅馆的，也就是回调函数。而旅客告诉旅馆怎么叫醒自己的动作，也就是把回调函数传入库函数的动作，称为<strong>登记回调函数</strong>（to <strong>register a callback function</strong>）。如下图所示（图片来源：维基百科）：</p>
<p><img src="./medias/loading.gif" data-original="/pytorch-xue-xi/new1\jianguoyun_posts\pytorch学习\image-20210909144400210.png" alt="回调函数"></p>
<p>可以看到，回调函数通常和应用处于同一抽象层（因为传入什么样的回调函数是在应用级别决定的）。而回调就成了一个高层调用底层，底层再<strong>回</strong>过头来<strong>调</strong>用高层的过程。（我认为）这应该是回调最早的应用之处，也是其得名如此的原因。</p>
<p><strong>回调机制的优势</strong></p>
<p>从上面的例子可以看出，回调机制提供了非常大的灵活性。请注意，从现在开始，我们把图中的库函数改称为<strong>中间函数</strong>了，这是因为回调并不仅仅用在应用和库之间。任何时候，只要想获得类似于上面情况的灵活性，都可以利用回调。</p>
<p>这种灵活性是怎么实现的呢？乍看起来，回调似乎只是函数间的调用，但仔细一琢磨，可以发现两者之间的一个关键的不同：在回调中，我们利用某种方式，把回调函数像参数一样传入中间函数。可以这么理解，在传入一个回调函数之前，中间函数是不完整的。换句话说，程序可以在运行时，通过登记不同的回调函数，来决定、改变中间函数的行为。这就比简单的函数调用要灵活太多了。请看下面这段Python写成的回调的简单示例：</p>
<p><code>even.py</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#回调函数1</span>
<span class="token comment">#生成一个2k形式的偶数</span>
<span class="token keyword">def</span> <span class="token function">double</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x <span class="token operator">*</span> <span class="token number">2</span>
    
<span class="token comment">#回调函数2</span>
<span class="token comment">#生成一个4k形式的偶数</span>
<span class="token keyword">def</span> <span class="token function">quadruple</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x <span class="token operator">*</span> <span class="token number">4</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>callback_demo.py</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> even <span class="token keyword">import</span> <span class="token operator">*</span>

<span class="token comment">#中间函数</span>
<span class="token comment">#接受一个生成偶数的函数作为参数</span>
<span class="token comment">#返回一个奇数</span>
<span class="token keyword">def</span> <span class="token function">getOddNumber</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> getEvenNumber<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">+</span> getEvenNumber<span class="token punctuation">(</span>k<span class="token punctuation">)</span>
    
<span class="token comment">#起始函数，这里是程序的主函数</span>
<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    
    k <span class="token operator">=</span> <span class="token number">1</span>
    <span class="token comment">#当需要生成一个2k+1形式的奇数时</span>
    i <span class="token operator">=</span> getOddNumber<span class="token punctuation">(</span>k<span class="token punctuation">,</span> double<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>
    <span class="token comment">#当需要一个4k+1形式的奇数时</span>
    i <span class="token operator">=</span> getOddNumber<span class="token punctuation">(</span>k<span class="token punctuation">,</span> quadruple<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>
    <span class="token comment">#当需要一个8k+1形式的奇数时</span>
    i <span class="token operator">=</span> getOddNumber<span class="token punctuation">(</span>k<span class="token punctuation">,</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>
    
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    main<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 最终输出: 3 5 9</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>上面的代码里，给<code>getOddNumber</code>传入不同的回调函数，它的表现也不同，这就是回调机制的优势所在。值得一提的是，上面的第三个回调函数是一个匿名函数。</p>
<p><strong>易被忽略的第三方</strong></p>
<p>通过上面的论述可知，中间函数和回调函数是回调的两个必要部分，不过人们往往忽略了回调里的第三位要角，就是中间函数的调用者。绝大多数情况下，这个调用者可以和程序的主函数等同起来，但为了表示区别，我这里把它称为<strong>起始函数</strong>（如上面的代码中注释所示）。</p>
<p>之所以特意强调这个第三方，是因为我在网上读相关文章时得到一种印象，很多人把它简单地理解为两个个体之间的来回调用。譬如，很多中文网页在解释“回调”（callback）时，都会提到这么一句话：“If you call me, I will call you back.”我没有查到这句英文的出处。我个人揣测，很多人把起始函数和回调函数看作为一体，大概有两个原因：第一，可能是“回调”这一名字的误导；第二，给中间函数传入什么样的回调函数，是在起始函数里决定的。实际上，回调并不是“你我”两方的互动，而是ABC的三方联动。有了这个清楚的概念，在自己的代码里实现回调时才不容易混淆出错。</p>
<p>另外，回调实际上有两种：阻塞式回调和延迟式回调。两者的区别在于：<strong>阻塞式回调里，回调函数的调用一定发生在起始函数返回之前；而延迟式回调里，回调函数的调用有可能是在起始函数返回之后</strong>。这里不打算对这两个概率做更深入的讨论，之所以把它们提出来，也是为了说明强调起始函数的重要性。网上的很多文章，提到这两个概念时，只是笼统地说阻塞式回调发生在主调函数返回之前，却没有明确这个主调函数到底是起始函数还是中间函数，不免让人糊涂，所以这里特意说明一下。另外还请注意，本文中所举的示例均为阻塞式回调。延迟式回调通常牵扯到多线程，我自己还没有完全搞明白，所以这里就不多说了。</p>
</blockquote>
<h3 id="REPRODUCIBILITY"><a href="#REPRODUCIBILITY" class="headerlink" title="REPRODUCIBILITY"></a>REPRODUCIBILITY</h3><p>终归无法一模一样!</p>
<p>torch.manual_seed() :<a target="_blank" rel="noopener" href="https://blog.csdn.net/X_singing/article/details/104447052?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">https://blog.csdn.net/X_singing/article/details/104447052?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p>
<p>为了保证能够复现，初始随机梯度是固定的</p>
<p><strong>随机数种子</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">cudnn<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">True</span>
cudnn<span class="token punctuation">.</span>benchmark <span class="token operator">=</span> <span class="token boolean">True</span>
cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">False</span>
<span class="token keyword">if</span> args<span class="token punctuation">.</span>seed <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
	random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span>
	np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span>
	torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span>
	torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>Data Loader</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">seed_worker</span><span class="token punctuation">(</span>worker_id<span class="token punctuation">)</span><span class="token punctuation">:</span>
    worker_seed <span class="token operator">=</span> torch<span class="token punctuation">.</span>initial_seed<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">2</span><span class="token operator">**</span><span class="token number">32</span>
    numpy<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>worker_seed<span class="token punctuation">)</span>
    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>worker_seed<span class="token punctuation">)</span>

DataLoader<span class="token punctuation">(</span>
    train_dataset<span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span>
    num_workers<span class="token operator">=</span>num_workers<span class="token punctuation">,</span>
    worker_init_fn<span class="token operator">=</span>seed_worker
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="相关库"><a href="#相关库" class="headerlink" title="相关库"></a>相关库</h2><h3 id="torchstat-amp-thop"><a href="#torchstat-amp-thop" class="headerlink" title="torchstat&amp;thop"></a>torchstat&amp;thop</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35407318/article/details/109359006">相关教程</a></p>
<p>FLOPs&amp;MACs</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/137719986">FLOPs详解</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/364543528">FLOPs与MACs的一些区分</a></p>
<p>FLOPS：注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。</p>
<p>FLOPs：==注意s小写，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。==</p>
<p>我们知道，通常我们去<strong>评价一个模型时，首先看的应该是它的精确度</strong>，当你精确度不行的时候，你和别人说我的模型预测的多么多么的快，部署的时候占的内存多么多么的小，都是白搭。但当你模型达到一定的精确度之后，就需要更<strong>进一步的评价指标来评价你模型</strong>：1）<strong>前向传播时所需的计算力</strong>，它反应了对硬件如GPU性能要求的高低；2）<strong>参数个数</strong>，它反应所占内存大小。为什么要加上这两个指标呢？因为这事关你模型算法的落地。比如你要在手机和汽车上部署深度学习模型，对模型大小和计算力就有严格要求。模型参数想必大家都知道是什么怎么算了，而前向传播时所需的计算力可能还会带有一点点疑问。所以这里总计一下前向传播时所需的计算力。它正是由<strong>FLOPs</strong>体现，那么<strong>FLOPs</strong>该怎么计算呢？</p>
<p>==时间复杂度是看计算的量级,FLOPs应该是具体把计算次数算出来,应该是这个意思==</p>
<p>MACs(Multiply-Accumulate Operations):乘加累计操作数,1个MACs包含一个乘法操作与一个加法操作,大约包含2FLOPs.</p>
<p>而人们常常将<strong>MACs</strong>与<strong>FLOPs</strong>混淆使用，甚至一些代码与论文中，都没对两个概念进行区分.</p>
<h4 id="torchstat基本介绍"><a href="#torchstat基本介绍" class="headerlink" title="torchstat基本介绍"></a>torchstat基本介绍</h4><p>[委实拉胯]</p>
<p>这是基于PyTorch的轻量级神经网络分析库。 它旨在使您能够快速，轻松地构建网络并进行调试。 注意：此存储库当前正在开发中。 因此，某些API可能会更改。</p>
<p>该工具可以显示</p>
<ul>
<li>网络参数总数  Total number of network parameters</li>
<li>FLOPs  Theoretical amount of floating point arithmetics (FLOPs)</li>
<li>理论乘积MAdd   Theoretical amount of multiply-adds (MAdd)</li>
<li>内存使用情况</li>
</ul>
<p>应用示例:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchstat <span class="token keyword">import</span> stat
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models

model <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span><span class="token punctuation">)</span>
stat<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>Support Layers</strong></p>
<img src="./medias/loading.gif" data-original="/pytorch-xue-xi/image-20210409152152829.png" alt="Support Layers" style="zoom:50%;">

<h4 id="thop基本介绍"><a href="#thop基本介绍" class="headerlink" title="thop基本介绍"></a>thop基本介绍</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> thop <span class="token keyword">import</span> profile
<span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>
flops<span class="token punctuation">,</span> params <span class="token operator">=</span> profile<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token operator">=</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>flops<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>params<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>卷积核FLOPs计算</strong></p>
<h3 id="torchsummary"><a href="#torchsummary" class="headerlink" title="torchsummary"></a>torchsummary</h3><h3 id="wandb"><a href="#wandb" class="headerlink" title="wandb"></a>wandb</h3><p><a target="_blank" rel="noopener" href="https://docs.wandb.ai/">官方文档</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42686816/article/details/123953150">CSDN教程</a></p>
<h4 id="基础结构"><a href="#基础结构" class="headerlink" title="基础结构"></a>基础结构</h4><blockquote>
<p>代码大致可分为以下步骤：<br>①导包<br>②初始化一个项目<br>③设置参数<br>④设定好模型和数据集<br>⑤追踪模型参数并记录<br>⑥储存模型</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># import the library</span>
<span class="token keyword">import</span> wandb
<span class="token comment"># start a new experiment</span>
wandb<span class="token punctuation">.</span>init<span class="token punctuation">(</span>project<span class="token operator">=</span><span class="token string">"new-sota-model"</span><span class="token punctuation">)</span>
<span class="token comment"># capture a dictionary of hyperparameters with config</span>
wandb<span class="token punctuation">.</span>config <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"learning_rate"</span><span class="token punctuation">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span> <span class="token string">"epochs"</span><span class="token punctuation">:</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token string">"batch_size"</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">}</span>
<span class="token comment"># set up model and data</span>
model<span class="token punctuation">,</span> dataloader <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> get_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># optional: track gradients</span>
wandb<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
<span class="token keyword">for</span> batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
  metrics <span class="token operator">=</span> model<span class="token punctuation">.</span>training_step<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token comment"># log metrics inside your training loop to visualize model performance</span>
  wandb<span class="token punctuation">.</span>log<span class="token punctuation">(</span>metrics<span class="token punctuation">)</span>
<span class="token comment"># optional: save model at the end</span>
model<span class="token punctuation">.</span>to_onnx<span class="token punctuation">(</span><span class="token punctuation">)</span>
wandb<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"model.onnx"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>The most commonly used functions/objects are:</p>
<ul>
<li>wandb.init — initialize a new run at the top of your training script</li>
<li>wandb.config — track hyperparameters and metadata</li>
<li>wandb.log — log metrics and media over time within your training loop</li>
</ul>
<h4 id="函数释义"><a href="#函数释义" class="headerlink" title="函数释义"></a>函数释义</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Hooks into the torch model to collect gradients and the topology. 记录梯度</span>
wandb<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>models<span class="token punctuation">,</span> criterion<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> log<span class="token operator">=</span><span class="token string">"gradients"</span><span class="token punctuation">,</span> log_freq<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> idx<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>log_graph<span class="token operator">=</span><span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>



<h3 id="albumentations"><a href="#albumentations" class="headerlink" title="albumentations"></a>albumentations</h3><p><a target="_blank" rel="noopener" href="https://albumentations.ai/docs/">官方文档</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014264373/article/details/114144303">最快最好用的数据增强库</a></p>
<p>There are two types of image augmentations: pixel-level augmentations and spatial-level augmentations.</p>
<p>Pixel-level augmentations change the values of pixels of the original image, but they don’t change the output mask. Image transformations such as changing brightness or contrast of adjusting values of the RGB-palette of the image are pixel-level augmentations.</p>
<p>On the contrary, spatial-level augmentations change both the image and the mask. When you apply image transformations such as mirroring or rotation or cropping a part of the input image, you also need to apply the same transformation to the output label to preserve its correctness.</p>
<h4 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h4><p>If the image has one associated mask, you need to call <code>transform</code> with two arguments: <code>image</code> and <code>mask</code>. In <code>image</code> you should pass the input image, in <code>mask</code> you should pass the output mask. <code>transform</code> will return a dictionary with two keys: <code>image</code> will contain the augmented image, and <code>mask</code> will contain the augmented mask.</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># </span>
transformed <span class="token operator">=</span> transform<span class="token punctuation">(</span>image<span class="token operator">=</span>image<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>
transformed_image <span class="token operator">=</span> transformed<span class="token punctuation">[</span><span class="token string">'image'</span><span class="token punctuation">]</span>
transformed_mask <span class="token operator">=</span> transformed<span class="token punctuation">[</span><span class="token string">'mask'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="Core"><a href="#Core" class="headerlink" title="Core"></a>Core</h4><h4 id="Augmentations"><a href="#Augmentations" class="headerlink" title="Augmentations"></a>Augmentations</h4><p>只有物体的位置(平移变换)和朝向(旋转变换)发生改变，而形状不变，得到的变换称为刚性变换。非刚性变换就是比这更复杂的变换，如伸缩，仿射，透射，多项式等一些比较复杂的变换。</p>
<p>albumentations 中主要提供了三种非刚体变换方法：ElasticTransform、GridDistortion 和 OpticalDistortion。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">albumentations<span class="token punctuation">.</span>augmentations<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>GridDistortion <span class="token punctuation">(</span>num_steps<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> distort_limit<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> interpolation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> border_mode<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> mask_value<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> always_apply<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h4 id="ImgAug-Helpers"><a href="#ImgAug-Helpers" class="headerlink" title="ImgAug Helpers"></a>ImgAug Helpers</h4><h4 id="Pytorch-Helpers"><a href="#Pytorch-Helpers" class="headerlink" title="Pytorch Helpers"></a>Pytorch Helpers</h4><h3 id="segmentation-models-pytorch"><a href="#segmentation-models-pytorch" class="headerlink" title="segmentation_models_pytorch"></a>segmentation_models_pytorch</h3><p><a target="_blank" rel="noopener" href="https://smp.readthedocs.io/en/latest/">文档</a></p>
<h3 id="cupy"><a href="#cupy" class="headerlink" title="cupy"></a>cupy</h3><p><a target="_blank" rel="noopener" href="https://docs.cupy.dev/en/stable/">官方文档</a></p>
<p>CuPy 项目的目标是为 Python 用户提供 GPU 加速能力,无需深入了解底层 GPU 技术。 CuPy 团队专注于提供：</p>
<ul>
<li><strong>完整的 NumPy 和 SciPy API 覆盖成为完全的替代品</strong>，以及高级 CUDA 功能以最大限度地提高性能。</li>
<li>成熟且优质的库，作为所有需要加速的项目的基础包，从实验室环境到大规模集群。</li>
</ul>
<h3 id="NNI"><a href="#NNI" class="headerlink" title="NNI"></a>NNI</h3><p><a target="_blank" rel="noopener" href="https://nni.readthedocs.io/zh/stable/index.html">官方文档</a></p>
<h3 id="yacs"><a href="#yacs" class="headerlink" title="yacs"></a>yacs</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/gefeng1209/article/details/90668882">文档</a></p>
<h3 id="einops"><a href="#einops" class="headerlink" title="einops"></a>einops</h3><p><a target="_blank" rel="noopener" href="https://einops.rocks/">文档</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># concatenate images along height (vertical axis), 960 = 32 * 30</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> rearrange<span class="token punctuation">(</span>images<span class="token punctuation">,</span> <span class="token string">'b h w c -&gt; (b h) w c'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape <span class="token comment"># 括号表示为b和h相乘</span>
<span class="token punctuation">(</span><span class="token number">960</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>



<h2 id="奇迹淫巧"><a href="#奇迹淫巧" class="headerlink" title="奇迹淫巧"></a>奇迹淫巧</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77952356">pytorch常见的坑汇总</a></p>
<h4 id="GPU利用率不高-gpu显存占用浪费"><a href="#GPU利用率不高-gpu显存占用浪费" class="headerlink" title="GPU利用率不高+gpu显存占用浪费"></a>GPU利用率不高+gpu显存占用浪费</h4><p> 1 主函数前面加(这个会牺牲一点点显存提高模型精度):</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">cudnn<span class="token punctuation">.</span>benchmark <span class="token operator">=</span> <span class="token boolean">True</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">False</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73711222">设置torch.backends.cudnn.benchmark</a></p>
<p>通过预先搜索最适合的卷积算法的实现，因为卷积算法有很多种不同的实现，最简单的实现方式就是使用多层循环嵌套，对于每张输入图像，对于每个要输出的通道，对于每个输入的通道，选取一个区域，同指定卷积核进行卷积操作，然后逐行滑动，直到整张图像都处理完毕，这个方法一般被称为 direct 法，这个方法虽然简单，但是看到这么多循环，我们就知道效率在一般情况下不会很高了。除此之外，实现卷积层的算法还有基于 GEMM (General Matrix Multiply) 的，基于 FFT 的，基于 Winograd 算法的等等，而且每个算法还有自己的一些变体。在一个开源的 <a href="https://link.zhihu.com/?target=https://www.scss.tcd.ie/~andersan/projects/live/triNNity.html">C++ 库 triNNity</a> 中，就实现了接近 80 种的卷积前向传播算法！</p>
<p>要求有：</p>
<ol>
<li>网络结构固定</li>
<li>输入尺寸固定</li>
</ol>
<p>2 训练时,epoch前面加:(定期清空模型，效果感觉不明显）</p>
<p><code>torch.cuda.empty_cache()</code></p>
<p>3 无用变量前面加:(同上，效果某些操作上还挺明显的）</p>
<p><code>del xxx(变量名)</code></p>
<p>4 dataloader的长度<code>__len__</code>设置:(dataloader会间歇式出现卡顿,设置成这样会避免不少）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>images<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>5 dataloader的预加载设置:(会在模型训练的时候加载数据，提高一点点gpu利用率）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>
        train_dataset<span class="token punctuation">,</span>
        pin_memory<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>6 loss的item()</p>
<p><img src="./medias/loading.gif" data-original="/pytorch-xue-xi/image-20211009185157414.png" alt="image-20211009185157414"></p>
<p>没有item()的话会导致显存占用变高</p>
<p>7 混合精度计算</p>
<h4 id="加速Pytorch训练"><a href="#加速Pytorch训练" class="headerlink" title="加速Pytorch训练"></a>加速Pytorch训练</h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/97190313">加速pytorch训练</a></p>
<h5 id="prefetch-generator"><a href="#prefetch-generator" class="headerlink" title="prefetch_generator"></a>prefetch_generator</h5><p>使用 prefetch_generator库在后台加载下一batch的数据，原本Pytorch默认的DataLoader会创建一些worker线程来预读取新的数据，但是除非这些线程的数据全部都被清空，这些线程才会读下一批数据。使用prefetch_generator，我们可以保证线程不会等待，每个线程都总有至少一个数据在加载。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#使用</span>
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> prefetch_generator <span class="token keyword">import</span> BackgroundGenerator

<span class="token keyword">class</span> <span class="token class-name">DataLoaderX</span><span class="token punctuation">(</span>DataLoader<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> BackgroundGenerator<span class="token punctuation">(</span><span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__iter__<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#然后用DataLoaderX替换原本的DataLoader</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h3><p>获取参数量</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">print_network</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token punctuation">:</span>
    num_params <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        num_params <span class="token operator">+=</span> param<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Total number of parameters: %d'</span> <span class="token operator">%</span> num_params<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Dch</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://chenghaoDong666.github.io/pytorch-xue-xi/">http://chenghaoDong666.github.io/pytorch-xue-xi/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Dch</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/pytorch/">
                                    <span class="chip bg-color">pytorch</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="./medias/loading.gif" data-original="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="./medias/loading.gif" data-original="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'TWPqcJz808PYWJLo2bO5Ni0A-gzGzoHsz',
        appKey: 'wBrlMIP11gP74GA1P1Rrkpbj',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: 'just go go'
    });
</script>

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/python-xue-xi/">
                    <div class="card-image">
                        
                        
                        <img src="./medias/loading.gif" data-original="/medias/featureimages/5.jpg" class="responsive-img" alt="python学习">
                        
                        <span class="card-title">python学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            python学习大杂烩
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-11-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/python/" class="post-category">
                                    python
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/python/">
                        <span class="chip bg-color">python</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/np-pd-plt/">
                    <div class="card-image">
                        
                        
                        <img src="./medias/loading.gif" data-original="/medias/featureimages/7.jpg" class="responsive-img" alt="numpy、pandas、matplotlib学习">
                        
                        <span class="card-title">numpy、pandas、matplotlib学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            np、pd、plt、cv2等库学习
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-11-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/python/" class="post-category">
                                    python
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/numpy/">
                        <span class="chip bg-color">numpy</span>
                    </a>
                    
                    <a href="/tags/pandas/">
                        <span class="chip bg-color">pandas</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        //if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
        newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        //}

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 月源<br />'
            + '文章作者: Dch<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="779322177"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="/about" target="_blank">Dch</a>
			<br>
            <span id="sitetime"></span>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">174.7k</span>&nbsp;字
            
            
            
            
            
            
				<span id="busuanzi_container_site_pv" style='display:none'> 
					<i class="fa fa-heart-o"></i> 
					本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span> 
				</span> 
			
			 
				<span id="busuanzi_container_site_uv" style='display:none'> 
					人次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人. 
				</span> 
			
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/chenghaoDong666" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1785246872@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1785246872" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1785246872" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>
<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */
        var t1 = Date.UTC(2020, 10, 13, 00, 00, 00); //北京时间2020-10-13 00:00:00
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "本站已勉强运行 " +diffYears+" 年 "+diffDays + " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
    }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
    siteTime();
</script>
<script> 
	$(document).ready(function () { 
		var int = setInterval(fixCount, 50); // 50ms周期检测函数 
		var pvcountOffset = 80000; // 初始化首次数据 
		var uvcountOffset = 20000; 
		function fixCount() { 
			if (document.getElementById("busuanzi_container_site_pv").style.display != "none") { 
				$("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + pvcountOffset); 
				clearInterval(int); 
			} 
			if ($("#busuanzi_container_site_pv").css("display") != "none") {
				$("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + uvcountOffset); // 加上初始数据 
				clearInterval(int); // 停止检测 
			} 
		} 
	}); 
</script>



    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    
    <script>
        (function (i, s, o, g, r, a, m) {
            i["DaoVoiceObject"] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o), m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            a.charset = "utf-8";
            m.parentNode.insertBefore(a, m)
        })(window, document, "script", ('https:' == document.location.protocol ? 'https:' : 'http:') +
            "//widget.daovoice.io/widget/6984b559.js", "daovoice")
        daovoice('init', {
            app_id: "6ef42b68"
        });
        daovoice('update');
    </script>
    

    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=i;var e=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight+240||document.documentElement.clientHeight+240)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}i(),n.addEventListener("scroll",function(){var t,e;t=i,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
